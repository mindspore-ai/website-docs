<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations.nn_ops &mdash; MindSpore 0.1.0-alpha documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.dtype.html">mindspore.dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.ops.composite.html">mindspore.ops.composite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.ops.operations.html">mindspore.ops.operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.dataset.transforms.c_transforms.html">mindspore.dataset.transforms.c_transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.dataset.transforms.vision.c_transforms.html">mindspore.dataset.transforms.vision.c_transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.dataset.transforms.py_transforms.html">mindspore.dataset.transforms.py_transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.dataset.transforms.vision.py_transforms.html">mindspore.dataset.transforms.vision.py_transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindspore/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindinsight/mindinsight.lineagemgr.html">mindinsight.lineagemgr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindarmour/mindarmour.evaluations.html">mindarmour.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindarmour/mindarmour.detectors.html">mindarmour.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindarmour/mindarmour.attacks.html">mindarmour.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/mindarmour/mindarmour.defenses.html">mindarmour.defenses</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.operations.nn_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.operations.nn_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for nn.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">ParamValidator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span><span class="p">,</span> <span class="n">check_bool</span><span class="p">,</span> <span class="n">check_int_positive</span>
<span class="kn">from</span> <span class="nn">...common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">..primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">prim_attr_register</span>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Flatten">[docs]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)` to be flattened.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, X)`, where :math:`X` is</span>
<span class="sd">        the product of the remaining dimension.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; flatten = Flatten()</span>
<span class="sd">        &gt;&gt;&gt; output = flatten(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; assert output.shape() == (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;input_x rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">prod</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prod</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Softmax">[docs]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax operation.</span>

<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice along the given aixs :math:`x` then for each element :math:`x_i`</span>
<span class="sd">    the Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple]): The axis to do the Softmax operation. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input of Softmax.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;item of axis&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_shape_length</span><span class="p">(</span><span class="s2">&quot;axis shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">axis_v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>


<div class="viewcode-block" id="LogSoftmax"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.LogSoftmax">[docs]</a><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log Softmax activation function.</span>

<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice along the given aixs :math:`x` then for each element :math:`x_i`</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The axis to do the Log softmax operation. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input of Log Softmax.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU(Rectified Linear Unit) of input tensor element-wise.</span>

<span class="sd">    It returns :math:`\max(x,\  0)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; relu = ReLU()</span>
<span class="sd">        &gt;&gt;&gt; result = relu(input_x)</span>
<span class="sd">        [[0, 4.0, 0.0], [2.0, 0.0, 9.0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init ReLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.ReLU6">[docs]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU(Rectified Linear Unit) upper bounded by 6 of input tensor element-wise.</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; relu6 = ReLU6()</span>
<span class="sd">        &gt;&gt;&gt; result = relu6(input_x)</span>
<span class="sd">        &gt;&gt;&gt; assert result.asnumpy() == Tensor(np.array([[0, 4.0, 0.0], [2.0, 0.0, 6.0]], np.float32)).asnumpy()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init ReLU6&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sigmoid activation function.</span>

<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + exp(-x_i)},</span>

<span class="sd">    where :math:`x_i` is the element of the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Sigmoid.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tanh activation function.</span>

<span class="sd">    Computes hyperbolic tangent of input element-wise. The Tanh function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Tanh.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="FusedBatchNorm"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.FusedBatchNorm">[docs]</a><span class="k">class</span> <span class="nc">FusedBatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FusedBatchNorm is a BatchNorm that moving mean and moving variance will be computed instead of being loaded.</span>

<span class="sd">    Batch Normalization is widely used in convolutional networks. This operation applies</span>
<span class="sd">    Batch Normalization over input to avoid internal covariate shift as described in the</span>
<span class="sd">    paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    feature using a mini-batch of data and the learned parameters which can be described</span>
<span class="sd">    in the following formula.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (int): Mode of batch normalization, value is 0 or 1. Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value should be [0, 1]. Default: 0.9.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensor, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the `input_x`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;running_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_inv_variance&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">IN</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">)</span></div>


<div class="viewcode-block" id="BatchNorm"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.BatchNorm">[docs]</a><span class="k">class</span> <span class="nc">BatchNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over input to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters which can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_training (bool): If `is_training` is True, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `is_training` is False, they&#39;re loaded from checkpoint during inference. Default: False.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensor, the normalized inputs and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the input_x. The shape is :math:`(N, C)`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_1** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_2** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_3** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;is_training&#39;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_1&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_2&#39;</span><span class="p">,</span>
                                         <span class="s1">&#39;reserve_space_3&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;BatchNorm scale shape length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;BatchNorm scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;BatchNorm bias shape&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;BatchNorm scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;BatchNorm input_x shape[1]&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;BatchNorm mean shape length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;BatchNorm mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;BatchNorm variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;BatchNorm mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;BatchNorm scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;BatchNorm scale type&quot;</span><span class="p">:</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;BatchNorm bias type&quot;</span><span class="p">:</span> <span class="n">bias</span><span class="p">}</span>
        <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;BatchNorm mean type&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;BatchNorm variance type&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">])</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Conv2D">[docs]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Applies a 2D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size and :math:`C_{in}` is channel number. For each batch of shape</span>
<span class="sd">    :math:`(C_{in}, H_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is cross correlation operator, :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{ks_h}, \text{ks_w})`, where :math:`\text{ks_h}` and</span>
<span class="sd">    :math:`\text{ks_w}` are height and width of the convolution kernel. The full kernel has shape</span>
<span class="sd">    :math:`(C_{out}, C_{in} // \text{group}, \text{ks_h}, \text{ks_w})`, where group is the group number</span>
<span class="sd">    to split the input in the channel dimension.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -</span>
<span class="sd">    (\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -</span>
<span class="sd">    (\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` respectively.</span>


<span class="sd">    The first introduction can be found in paper `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_. More detailed introduction can be found here:</span>
<span class="sd">    http://cs231n.github.io/convolutional-networks/.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimension of the output.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 2D convolution.</span>
<span class="sd">        mode (int): 0 Math convolutiuon, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 1.</span>
<span class="sd">        pad_mode (str): &quot;valid&quot;, &quot;same&quot;, &quot;pad&quot; the mode to fill padding. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (int): The pad value to fill. Default: 0.</span>
<span class="sd">        stride (int): The stride to apply conv filter. Default: 1.</span>
<span class="sd">        dilation (int): Specify the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Split input into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 2D convolution.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init Conv2D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;length of kernel_size&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="s1">&#39;type of pad&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="s1">&#39;not bool&#39;</span><span class="p">,</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="s1">&#39;type of pad&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="s1">&#39;int&#39;</span><span class="p">,</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_pad_value_by_mode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">kernel_size</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">Conv2D</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                             <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be a int or tuple and equal to or greater than 1.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span>
                                               <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                               <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                               <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">kernel_size</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">Conv2D</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                             <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be a int or tuple and equal to or greater than 1.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;weight_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;w_shape[0]&#39;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]))</span>

        <span class="n">kernel_size_h</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">))</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x_dtype&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w_dtype&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="DepthwiseConv2dNative"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.DepthwiseConv2dNative">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseConv2dNative</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the depth-wise convolution value for the input.</span>

<span class="sd">    Applies depthwise conv2d for the input, which will generate more channels with channel_multiplier.</span>
<span class="sd">    Given an input tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})` where :math:`N` is the batch size and a</span>
<span class="sd">    filter tensor with kernel size :math:`(ks_{h}, ks_{w})`, containing :math:`C_{in} * \text{channel_multiplier}`</span>
<span class="sd">    convolutional filters of depth 1; it applies different filters to each input channel (channel_multiplier channels</span>
<span class="sd">    for each with default value 1), then concatenates the results together. The output has</span>
<span class="sd">    :math:`\text{in_channels} * \text{channel_multiplier}` channels.</span>

<span class="sd">    Args:</span>
<span class="sd">        channel_multiplier (int): The multipiler for the original output conv.</span>
<span class="sd">        kernel_size (int or tuple): The size of the conv kernel.</span>
<span class="sd">        mode (int): 0 Math convolution, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 3.</span>
<span class="sd">        pad_mode (str): &quot;valid&quot;, &quot;same&quot;, &quot;pad&quot; the mode to fill padding. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (int): The pad value to fill. Default: 0.</span>
<span class="sd">        stride (int): The stride to apply conv filter. Default: 1.</span>
<span class="sd">        dilation (int): Specifies the dilation rate to use for dilated convolution. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in} * \text{channel_multiplier}, H_{out}, W_{out})`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">channel_multiplier</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init DepthwiseConv2dNative&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_pad_value_by_mode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span>
                                               <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                               <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                               <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attr kernel_size of DepthwiseConv2dNative Op not passed &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">, should be a int or tuple and equal to or greater than 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;same&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="s2">&quot;pad&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attr pad_mode of DepthwiseConv2dNative Op not passed&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2"> not in valid, same, pad.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;channel_multiplier&quot;</span><span class="p">,</span> <span class="n">channel_multiplier</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_on_integer</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">pad</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;weight_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]))</span>

        <span class="n">kernel_size_h</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attr pad_mode of DepthwiseConv2dNative Op not passed&quot;</span>
                             <span class="s2">&quot;</span><span class="si">{pad_mode}</span><span class="s2"> not in valid, same, pad.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x_dtype&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w_dtype&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="MaxPoolWithArgmax"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.MaxPoolWithArgmax">[docs]</a><span class="k">class</span> <span class="nc">MaxPoolWithArgmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max pooling on the input Tensor and return both max values and indices.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_mode (str): &quot;valid&quot;, &quot;same&quot;, &quot;pad&quot; the mode to fill padding. Default: &quot;valid&quot;.</span>
<span class="sd">        window (Union[int, tuple[int]]): The size of window, which is the kernel size, two `int` for width</span>
<span class="sd">            and height. Default: 1.</span>
<span class="sd">        pad (Union[int, tuple[int]]): If `pad_mode` is `pad`, the pad value to fill, two `int` for width</span>
<span class="sd">            and height. Default: 0.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride of the window, that should be a tuple of two `int` for</span>
<span class="sd">           width and height. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensor, the maxpool result and where max values from.</span>

<span class="sd">        - **output** (Tensor) -  Maxpooling result, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        - **mask** (Tensor) -  Max values&#39; index represented by the mask.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">window</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">ceil_mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">beta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;window&#39;</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">window</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">window</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">window</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">MaxPoolWithArgmax</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                             <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;, should be a int or tuple and greater than 0.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">window</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">window</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span>
                                          <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">window</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                          <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">window</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                          <span class="n">window</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">window</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">window</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">MaxPoolWithArgmax</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                             <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;, should be a int or tuple and greater than 0.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span> <span class="o">=</span> <span class="n">window</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;pad&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;data_mode&#39;</span><span class="p">,</span> <span class="n">data_mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span>
        <span class="n">h_input</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">w_input</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">h_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_input</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">w_input</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">shape_value</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape_value</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The kernel size is not valid please check it if is larger than data&#39;s shape size.&quot;</span><span class="p">)</span>
        <span class="n">k_size_vec</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">argmax_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">dim</span> <span class="o">=</span> <span class="n">k_size_vec</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">k_size_vec</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                    <span class="n">argmax_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                    <span class="n">dim</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">out_shape</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">out_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">argmax_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">argmax_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">argmax_shape</span> <span class="o">=</span> <span class="n">out_shape</span>
        <span class="k">return</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">argmax_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">out_dtype</span> <span class="o">=</span> <span class="n">x_dtype</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;x_type&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">argmax_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">return</span> <span class="n">out_dtype</span><span class="p">,</span> <span class="n">argmax_dtype</span></div>


<span class="k">class</span> <span class="nc">_Pool</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max/avg pooling operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of the window to take a max over, that should be a tuple</span>
<span class="sd">           of two `int` for width and height. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride of the window, that should be a tuple of two `int` for</span>
<span class="sd">           width and height. Default: 1.</span>
<span class="sd">        padding (str): The optional values for pad mode &quot;SAME&quot;, &quot;VALID&quot;. Default: &quot;VALID&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">strides</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span>
                    <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Each value of attr </span><span class="se">\&#39;</span><span class="s1">ksize</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">MaxPool</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span> <span class="o">+</span>
                                 <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be a int or a tuple of length 2 and greater than 0.&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride_h</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_h</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                    <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Each value of attr </span><span class="se">\&#39;</span><span class="s1">strides</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">MaxPool</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span> <span class="o">+</span>
                                 <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be a int or a tuple of length 2 and greater than 0.&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The padding should be str and must be SAME or VALID,&#39;</span>
                             <span class="s1">&#39; but got </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">h_input</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">w_input</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">h_input</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">w_input</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pool_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">h_input</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">w_input</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride_w</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The padding should be str and must be SAME or VALID,&#39;</span>
                             <span class="s1">&#39; but got </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">))</span>

        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">shape_value</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape_value</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The kernel size is not valid please check it if is larger than data&#39;s shape size.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<div class="viewcode-block" id="MaxPool"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.MaxPool">[docs]</a><span class="k">class</span> <span class="nc">MaxPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling operation.</span>

<span class="sd">    Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of the window to take a max over, that should be a tuple</span>
<span class="sd">           of two `int` for width and height. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride of the window, that should be a tuple of two `int` for</span>
<span class="sd">           width and height. Default: 1.</span>
<span class="sd">        padding (str): The optional values for pad mode &quot;SAME&quot;, &quot;VALID&quot;. Default: &quot;VALID&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="AvgPool"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.AvgPool">[docs]</a><span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, AvgPool2d outputs</span>
<span class="sd">    regional average in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of the window to take a average over, that should be a tuple</span>
<span class="sd">           of two `int` for width and height. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride of the window, that should be a tuple of two `int` for</span>
<span class="sd">           width and height. Default: 1.</span>
<span class="sd">        padding (str): The optional values for pad mode &quot;SAME&quot;, &quot;VALID&quot;. Default: &quot;VALID&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;GPU&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;OTHER&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DBackpropInput"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Conv2DBackpropInput">[docs]</a><span class="k">class</span> <span class="nc">Conv2DBackpropInput</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of convolution with respect to the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimensionality of the output space.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution window.</span>
<span class="sd">        pad_mode (str): &quot;valid&quot;, &quot;same&quot;, &quot;pad&quot; the mode to fill padding. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (int): The pad value to fill. Default: 0.</span>
<span class="sd">        mode (int): 0 Math convolutiuon, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 1.</span>
<span class="sd">        stride (int): The stride to apply conv filter. Default: 1.</span>
<span class="sd">        dilation (int): Specifies the dilation rate to use for dilated convolution. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the gradients of convolution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init Conv2DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;input_sizes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">kernel_size</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">Conv2DBackpropInput</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                                 <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be a int or tuple and equal to or greater than 1.&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span>
                                                 <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                                 <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">or</span>
                                                 <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Attr </span><span class="se">\&#39;</span><span class="s1">kernel_size</span><span class="se">\&#39;</span><span class="s1"> of </span><span class="se">\&#39;</span><span class="s1">Conv2DBackpropInput</span><span class="se">\&#39;</span><span class="s1"> Op passed &#39;</span>
                             <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;, should be a int or tuple and equal to or greater than 1.&#39;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="s1">&#39;type of pad&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="s1">&#39;not bool&#39;</span><span class="p">,</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="s1">&#39;type of pad&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="s1">&#39;int&#39;</span><span class="p">,</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_pad_value_by_mode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_list</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_lsit</span> <span class="o">=</span> <span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pad_list</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doutput</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_size</span><span class="p">):</span>
        <span class="n">x_size_v</span> <span class="o">=</span> <span class="n">x_size</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;x_size&#39;</span><span class="p">,</span> <span class="n">x_size_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_size_v</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;x_size[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s1">&#39;w_dtype&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_two_types_same</span><span class="p">(</span><span class="s1">&#39;doutput_dtype&#39;</span><span class="p">,</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s1">&#39;w_dtype&#39;</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">])</span>

        <span class="c1"># infer shape</span>
        <span class="n">dout_shape</span> <span class="o">=</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">kernel_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># default pad mode is valid</span>
        <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="n">kernel_h</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">+</span> <span class="n">kernel_w</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">:</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="n">pad_list</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_size_v</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="BiasAdd"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.BiasAdd">[docs]</a><span class="k">class</span> <span class="nc">BiasAdd</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sum of input and bias tensor.</span>

<span class="sd">    Adds the 1-D bias tensor to the input tensor, and boardcasts the shape on all axis</span>
<span class="sd">    except for the channel axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input value, with shape :math:`(N, C)` or :math:`(N, C, H, W)`.</span>
<span class="sd">        - **bias** (Tensor) - Bias value, with shape :math:`(C)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as `input_x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input_x and bias shapes do not match&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;(require: rank of input_x must be at least 2, rank of bias must be 1, &quot;</span>
                             <span class="s2">&quot;input_x.dim[1] must equal bias.dim[0]),&quot;</span>
                             <span class="s2">&quot; but got input_x shape </span><span class="si">{}</span><span class="s2">, bias shape </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">b_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_x type&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s2">&quot;bias type&quot;</span><span class="p">:</span> <span class="n">b_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="TopK"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.TopK">[docs]</a><span class="k">class</span> <span class="nc">TopK</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values and indices of the `k` largest entries along the last dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        sorted (bool): If true, the resulting elements will</span>
<span class="sd">            be sorted by the values in descending order. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to be computed.</span>
<span class="sd">        - **k** (int) - Number of top elements to be computed along the last dimension, constant input is needed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensor, the values and the indices.</span>

<span class="sd">        - **values** (Tensor) - The `k` largest elements along each last dimensional slice.</span>
<span class="sd">        - **indices** (Tensor) - The indices of values within the last dimension of input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; topk = TopK(sorted=True)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; values, indices = topk(x)</span>
<span class="sd">        &gt;&gt;&gt; assert values == Tensor(np.array([5, 4, 3]))</span>
<span class="sd">        &gt;&gt;&gt; assert indices == Tensor(np.array([4, 3, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;sorted&quot;</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">k_v</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">x_shape</span><span class="p">[</span><span class="n">ndim</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_v</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;TopK input_dtype&quot;</span><span class="p">,</span>
                                 <span class="n">input_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k_v</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The k must int.&#39;</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">),</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.SoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the softmax cross-entropy value between logits and labels which shoule be one-hot encoding.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = -\sum_j{Y_{ij} * ln(p_{ij})}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N, C)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensor, the loss shape is `(N,)`, and the dlogits with the same shape as `logits`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;SoftmaxCrossEntropyWithLogits logits_shape&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span>
                                    <span class="s2">&quot;SoftmaxCrossEntropyWithLogits labels_shape&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">)</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">dlogits_shape</span> <span class="o">=</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss_shape</span><span class="p">,</span> <span class="n">dlogits_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;SoftmaxCrossEntropyWithLogits logits_type&quot;</span><span class="p">:</span> <span class="n">logits_type</span><span class="p">,</span>
                <span class="s2">&quot;SoftmaxCrossEntropyWithLogits labels_type&quot;</span><span class="p">:</span> <span class="n">labels_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">logits_type</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseSoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.SparseSoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr -ln(1 - p_{ij}), &amp; j \neq y_i \end{cases}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss = \sum_{ij} loss_{ij}</span>

<span class="sd">    Args:</span>
<span class="sd">        is_grad (bool): If it&#39;s true, this operation returns the computed gradient. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if `is_grad` is False, the output tensor is the value of loss which is a scalar tensor;</span>
<span class="sd">        if `is_grad` is True, the output tensor is the gradient of input with the same shape as `logits`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span> <span class="o">=</span> <span class="n">is_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;sens&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;SparseSoftmaxCrossEntropyWithLogits logits_shape&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                    <span class="s2">&quot;SparseSoftmaxCrossEntropyWithLogits labels_shape&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="n">loss_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;SparseSoftmaxCrossEntropyWithLogits logits_type&quot;</span><span class="p">,</span>
                                 <span class="n">logits_type</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;SparseSoftmaxCrossEntropyWithLogits labels_type&quot;</span><span class="p">,</span>
                                 <span class="n">labels_type</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">logits_type</span></div>


<div class="viewcode-block" id="ApplyMomentum"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.ApplyMomentum">[docs]</a><span class="k">class</span> <span class="nc">ApplyMomentum</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Momentum algorithm.</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Enable a lock to protect the update of variable and accumlation tensors. Default: False.</span>
<span class="sd">        use_nesterov (bool): Enable Nesterov momentum. Default: False.</span>
<span class="sd">        gradient_scale (float): The scale of the gradient. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Tensor) - Weights to be updated.</span>
<span class="sd">        - **accumulation** (Tensor) - Accumulated gradient value by moment weight.</span>
<span class="sd">        - **learning_rate** (float) - Learning rate.</span>
<span class="sd">        - **gradient** (Tensor) - Gradients.</span>
<span class="sd">        - **momentum** (float) - Momentum.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; net = ResNet50()</span>
<span class="sd">        &gt;&gt;&gt; loss = SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; opt = ApplyMomentum(Tensor(np.array([0.001])), Tensor(np.array([0.9])),</span>
<span class="sd">                                filter(lambda x: x.requires_grad, net.get_parameters()))</span>
<span class="sd">        &gt;&gt;&gt; model = Model(net, loss, opt)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">a_shape</span><span class="p">,</span> <span class="n">l_shape</span><span class="p">,</span> <span class="n">g_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;variable shape </span><span class="si">{</span><span class="n">v_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;accumulation shape </span><span class="si">{</span><span class="n">a_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">a_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;learning rate shape </span><span class="si">{</span><span class="n">l_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">l_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;gradient shape </span><span class="si">{</span><span class="n">g_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">g_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;momentum shape </span><span class="si">{</span><span class="n">m_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">m_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">a_dtype</span><span class="p">,</span> <span class="n">l_dtype</span><span class="p">,</span> <span class="n">g_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;v_dtype&quot;</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;a_dtype&quot;</span><span class="p">,</span> <span class="n">a_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">v_type</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;v_dtype&quot;</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;a_dtype&quot;</span><span class="p">,</span> <span class="n">a_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;l_dtype&quot;</span><span class="p">,</span> <span class="n">l_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;g_dtype&quot;</span><span class="p">,</span> <span class="n">g_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;m_dtype&quot;</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">v_type</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.SmoothL1Loss">[docs]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes smooth L1 loss, a robust L1 loss.</span>

<span class="sd">    SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the</span>
<span class="sd">    `Fast R-CNN &lt;https://arxiv.org/abs/1504.08083&gt;`_ by Ross Girshick.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input prediction as `X`, input target as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{SmoothL1Loss} = \begin{cases}0.5x^{2}, &amp;if \left |x \right |\leq \text{sigma} \cr</span>
<span class="sd">            \left |x \right|-0.5, &amp;\text{otherwise}\end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        sigma (float): A parameter used to control the point where the function will change from</span>
<span class="sd">            quadratic to linear. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **prediction** (Tensor) - Predict data.</span>
<span class="sd">        - **target** (Tensor) - Ground truth data, with the same type and shape as `prediction`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `prediction`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;prediction shape&#39;</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="s1">&#39;target shape&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prediction&quot;</span><span class="p">:</span> <span class="n">prediction</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">target</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">prediction</span></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.SGD">[docs]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes stochastic gradient descent (optionally with momentum).</span>

<span class="sd">    Nesterov momentum is based on the formula from On the importance of</span>
<span class="sd">    initialization and momentum in deep learning.</span>

<span class="sd">    Note:</span>
<span class="sd">        For details, please refer to `nn.SGD` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        dampening (float): The dampening for momentum. Default: 0.0.</span>
<span class="sd">        weight_decay (float): Weight decay (L2 penalty). Default: 0.0.</span>
<span class="sd">        nesterov (bool): Enable Nesterov momentum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **parameters** (Tensor) - Parameters to be updated.</span>
<span class="sd">        - **gradient** (Tensor) - Gradients.</span>
<span class="sd">        - **learning_rate** (Tensor) - Learning rate. e.g. Tensor(0.1, mindspore.float32).</span>
<span class="sd">        - **accum** (Tensor) - Accum(velocity) to be updated.</span>
<span class="sd">        - **momentum** (Tensor) - Momentum. e.g. Tensor(0.1, mindspore.float32).</span>
<span class="sd">        - **stat** (Tensor) - States to be updated with the same shape as gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;stat&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span>
                    <span class="n">accum_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;parameters shape </span><span class="si">{</span><span class="n">parameters_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;gradient shape </span><span class="si">{</span><span class="n">gradient_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradient_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;learning rate shape </span><span class="si">{</span><span class="n">learning_rate_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;accumulation shape </span><span class="si">{</span><span class="n">accum_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">accum_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;momentum shape </span><span class="si">{</span><span class="n">momentum_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">momentum_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;stat shape </span><span class="si">{</span><span class="n">stat_shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">stat_shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="s2">&quot;stat shape&quot;</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">parameters_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span>
                    <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;parameters_dtype&quot;</span><span class="p">,</span> <span class="n">parameters_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;gradient_dtype&quot;</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;learning_rate_dtype&quot;</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;accum_dtype&quot;</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;momentum_dtype&quot;</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;stat_dtype&quot;</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">parameters_dtype</span></div>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.LayerNorm">[docs]</a><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean]}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_norm_axis (int): The begin axis of the `input_x` to apply LayerNorm,</span>
<span class="sd">            the value should be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        begin_params_axis (int): The begin axis of the parameter input (`gamma`, `beta`) to</span>
<span class="sd">            apply LayerNorm, the value should be in [-1, rank(input)). Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          The input of LayerNorm.</span>
<span class="sd">        - **gamma** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `gamma` as the scale on norm.</span>
<span class="sd">        - **beta** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `beta` as the scale on norm.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.</span>
<span class="sd">          The shape is :math:`(N, C)`.</span>
<span class="sd">        - **updated_gamma** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_beta** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;begin_norm_axis&#39;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;begin_params_axis&#39;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span></div>


<div class="viewcode-block" id="L2Normalize"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.L2Normalize">[docs]</a><span class="k">class</span> <span class="nc">L2Normalize</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L2 normalization Operator.</span>

<span class="sd">    This operator will normalizes the input using the given axis. The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac{x}{\sqrt{\text{max}(\text{sum} (\text{input_x}^2), \epsilon)}},</span>

<span class="sd">    where :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The begin axis for the input to apply L2 normalize. Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-4.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the normalization.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s1">&#39;axis value&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="DropoutGenMask"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.DropoutGenMask">[docs]</a><span class="k">class</span> <span class="nc">DropoutGenMask</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates the mask value for the input shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        Seed0 (int): Seed0 value for random generating. Default: 0.</span>
<span class="sd">        Seed1 (int): Seed1 value for random generating. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (tuple[int]) - The shape of target mask.</span>
<span class="sd">        - **keep_prob** (Tensor) - The keep rate, between 0 and 1, e.g. keep_prob = 0.9,</span>
<span class="sd">          means dropping out 10% of input units.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value of generated mask for input shape.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout_gen_mask = DropoutGenMask()</span>
<span class="sd">        &gt;&gt;&gt; shape = (20, 16, 50)</span>
<span class="sd">        &gt;&gt;&gt; keep_prob = Tensor(0.5, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = dropout_gen_mask(shape, keep_prob)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;keep_prob&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span></div>


<div class="viewcode-block" id="DropoutDoMask"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.DropoutDoMask">[docs]</a><span class="k">class</span> <span class="nc">DropoutDoMask</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies dropout mask on the input tensor.</span>

<span class="sd">    Take the mask output of DropoutGenMask as input, and apply dropout on the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">        - **mask** (Tensor) - The mask to be applied on `input_x`, which is the output of `DropoutGenMask`. And the</span>
<span class="sd">          shape of `input_x` must be same as the value of `DropoutGenMask`&#39;s input `shape`. If input wrong `mask`,</span>
<span class="sd">          the output of `DropoutDoMask` are unpredictable.</span>
<span class="sd">        - **keep_prob** (Tensor) - The keep rate, between 0 and 1, e.g. keep_prob = 0.9,</span>
<span class="sd">          means dropping out 10% of input units. The value of `keep_prob` is same as the input `keep_prob` of</span>
<span class="sd">          `DropoutGenMask`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied dropout on.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([20, 16, 50]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (20, 16, 50)</span>
<span class="sd">        &gt;&gt;&gt; keep_prob = Tensor(0.5, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; dropout_gen_mask = DropoutGenMask()</span>
<span class="sd">        &gt;&gt;&gt; dropout_do_mask = DropoutDoMask()</span>
<span class="sd">        &gt;&gt;&gt; mask = dropout_gen_mask(shape, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; output = dropout_do_mask(x, mask, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; assert output.shape() == (20, 16, 50)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
        <span class="n">input_x_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">mask_shape</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">keep_prob_shape</span> <span class="o">=</span> <span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;keep_prob&#39;s dim&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_prob_shape</span><span class="p">),</span> <span class="s1">&#39;0(scalar)&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">size_x</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DropoutDoMask mask shape should be 1-dimension.&quot;</span><span class="p">)</span>
        <span class="n">size_y</span> <span class="o">=</span> <span class="n">mask_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span>
        <span class="k">if</span> <span class="n">size_x</span> <span class="o">&gt;</span> <span class="n">size_y</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DropoutDoMask y mask do not math input input_x shape:&quot;</span>
                             <span class="s2">&quot;</span><span class="si">{input_x_shape}</span><span class="s2">, mask shape: </span><span class="si">{mask_shape}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x type&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_mask type&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">])</span>

        <span class="n">keep_prob_v</span> <span class="o">=</span> <span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">keep_prob_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_const_input</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="n">keep_prob_v</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="n">keep_prob_v</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">input_x_shape</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="ResizeBilinear"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.ResizeBilinear">[docs]</a><span class="k">class</span> <span class="nc">ResizeBilinear</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the image to certain size using bilinear interpolation.</span>

<span class="sd">    The resizing only affects the lower two dimensions which represent the height and width. The input images</span>
<span class="sd">    can be represented by different data types, but the data types of output images are always float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (tuple[int]): A tuple of 2 int elements `(new_height, new_width)`, the new size for the images.</span>
<span class="sd">        align_corners (bool): If it&#39;s true, rescale input by `(new_height - 1) / (height - 1)`,</span>
<span class="sd">                       which exactly aligns the 4 corners of images and resized images. If it&#39;s false,</span>
<span class="sd">                       rescale by `new_height / height`. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Image to be resized. Tensor of shape `(N_i, ..., N_n, height, width)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, resized image. Tensor of shape `(N_i, ..., N_n, new_height, new_width)` in `float32`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; resize_bilinear = P.ResizeBilinear((5, 5))</span>
<span class="sd">        &gt;&gt;&gt; result = resize_bilinear(tensor)</span>
<span class="sd">        &gt;&gt;&gt; assert result.shape() == (5, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="OneHot"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.OneHot">[docs]</a><span class="k">class</span> <span class="nc">OneHot</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    Makes a new tensor, whose locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Position to insert the value. e.g. If `indices` shape is [n, c], and `axis` is `-1` the output shape</span>
<span class="sd">            will be [n, c, depth], If `axis` is `0` the output shape will be [depth, n, c]. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">        - **depth** (int) - A scalar defining the depth of the one hot dimension.</span>
<span class="sd">        - **on_value** (Tensor) - A value to fill in output when `indices[j] = i`.</span>
<span class="sd">        - **off_value** (Tensor) - A value to fill in output when `indices[j] != i`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, one_hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; onehot = OneHot()</span>
<span class="sd">        &gt;&gt;&gt; result = onehot(indices, depth, on_value, off_value)</span>
<span class="sd">        [[1, 0, 0], [0, 1, 0], [0, 0, 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;depth&#39;</span><span class="p">,</span> <span class="s1">&#39;on_value&#39;</span><span class="p">,</span> <span class="s1">&#39;off_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">):</span>
        <span class="c1"># check type</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;depth&quot;</span><span class="p">,</span> <span class="n">depth</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;on_value&quot;</span><span class="p">,</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;off_value&quot;</span><span class="p">,</span> <span class="n">off_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;on_value dtype&quot;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s2">&quot;off_value dtype&quot;</span><span class="p">:</span> <span class="n">off_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

        <span class="c1"># check shape</span>
        <span class="n">indices_shp</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shp</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">)</span>
        <span class="n">depth_val</span> <span class="o">=</span> <span class="n">depth</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;depth&quot;</span><span class="p">,</span> <span class="n">depth_val</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="c1"># create new dimension at end if self.axis is -1</span>
        <span class="n">indices_shp</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">depth_val</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">indices_shp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">depth_val</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">indices_shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="Gelu"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Gelu">[docs]</a><span class="k">class</span> <span class="nc">Gelu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = 0.5 * x * (1 + erf(x / \sqrt{2})),</span>

<span class="sd">    where :math:`erf` is the &quot;Gauss error function&quot; .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the Gelu.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gelu = Gelu()</span>
<span class="sd">        &gt;&gt;&gt; result = gelu(tensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init GeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="GetNext"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.GetNext">[docs]</a><span class="k">class</span> <span class="nc">GetNext</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next element in the dataset queue.</span>

<span class="sd">    Note:</span>
<span class="sd">        GetNext op needs to be associated with network and also depends on the init_dataset interface,</span>
<span class="sd">        it can&#39;t be used directly as a single op.</span>
<span class="sd">        For details, please refer to `nn.cell_wrapper.DataWrapper` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        types (list[:class:`mindspore.dtype`]): The type of the outputs.</span>
<span class="sd">        shapes (list[tuple[int]]): The dimensionality of the outputs.</span>
<span class="sd">        output_num (int): The output number, length of `types` and `shapes`.</span>
<span class="sd">        shared_name (str): The queue name of `init_dataset` interface.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        No inputs.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the output of Dataset. The shape is described in `shapes`</span>
<span class="sd">        and the type is described is `types`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; get_next = GetNext([mindspore.float32, mindspore.int32], [[32, 1, 28, 28], [10]], &#39;shared_name&#39;)</span>
<span class="sd">        &gt;&gt;&gt; feature, label = get_next()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;types&quot;</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;shapes&quot;</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;types length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">),</span> <span class="s2">&quot;shapes length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shapes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">types</span><span class="p">)</span></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.PReLU">[docs]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of an channel of the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Float tensor, representing the output of the preview layer.</span>
<span class="sd">        - **weight** (Tensor) -  Float Tensor, w &gt; 0, there is only two shapes are legitimate,</span>
<span class="sd">          1 or the number of channels at input.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as `input_x`.</span>

<span class="sd">    Detailed information, please refer to `nn.PReLU`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">):</span>
        <span class="n">input_x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x_shape</span><span class="p">)</span>
        <span class="n">weight_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_dim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;weight_dim must be 1, while weight_dim is </span><span class="si">{</span><span class="n">weight_dim</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_x_dim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;when input_x_dim is 1, weight_shape[0] must be 1, &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;while weight_shape[0] is </span><span class="si">{</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_x_dim</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">input_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;channel of input_x and weight must be matched,&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39; while channel of input_x is </span><span class="si">{</span><span class="n">input_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">,&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39; weight_shape[0] is </span><span class="si">{</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x_dtype&quot;</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;weight_dtype&quot;</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;input_x_dtype&quot;</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;weight_dtype&quot;</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">input_x_dtype</span></div>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.LSTM">[docs]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the long short term memory(LSTM) on the input.</span>

<span class="sd">    Detailed information, please refer to `nn.LSTM`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">check_int_positive</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">check_int_positive</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">check_int_positive</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">check_bool</span><span class="p">(</span><span class="n">has_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">check_bool</span><span class="p">(</span><span class="n">bidirectional</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="c1"># (batch, seq, feature)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>

        <span class="c1"># h and c should be same shape</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>

        <span class="c1"># (num_layers * num_directions, batch, hidden_size)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h[2]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>

        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">)</span>

        <span class="c1"># set arbitrary shape for reserved space</span>
        <span class="n">reserved_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">y_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">reserved_shape</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;h_dtype&quot;</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;c_dtype&quot;</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;w_dtype&quot;</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;datatype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">h_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">(),))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;datatype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">c_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">(),))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;datatype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">w_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">(),))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.SigmoidCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SigmoidCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the given logits to compute sigmoid cross entropy.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = -[Y_{ij} * ln(p_{ij}) + (1 - Y_{ij})ln(1 - p_{ij})]</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as input `logits`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Init SigmoidCrossEntropyWithLogits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;y_shape&quot;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y_dtype&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Pad"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Pad">[docs]</a><span class="k">class</span> <span class="nc">Pad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads input tensor according to the paddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        paddings (tuple): The shape of parameter `paddings` is (N, 2). N is the rank of input data. All elements of</span>
<span class="sd">            paddings are int type. For `D` th dimension of input, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">            extended ahead of the `D` th dimension of the input tensor, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">            be extended behind of the `D` th dimension of the input tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_op = Pad(((1, 2), (2, 1)))</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = pad_op(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; assert output_tensor == Tensor(np.array([[ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. , -0.1,  0.3,  3.6,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. ,  0.4,  0.5, -3.2,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ]]), mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Init Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Paddings must be tuple type.&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">paddings</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The shape of paddings must be (n, 2).&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paddings</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;paddings.shape&#39;</span><span class="p">,</span> <span class="n">paddings</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All elements of paddings must be &gt;= 0.&#39;</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">paddings</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="n">y_shape</span> <span class="o">+=</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),)</span>
        <span class="k">return</span> <span class="n">y_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="ROIAlign"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.ROIAlign">[docs]</a><span class="k">class</span> <span class="nc">ROIAlign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Region of Interest (RoI) Align operator.</span>

<span class="sd">    The operator computes the value of each sampling point by bilinear interpolation from the nearby grid points on the</span>
<span class="sd">    feature map. No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling</span>
<span class="sd">    points. The details of (RoI) Align operator are described in `Mask R-CNN &lt;https://arxiv.org/abs/1703.06870&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooled_height (int): The output features&#39; height.</span>
<span class="sd">        pooled_width (int): The output features&#39; width.</span>
<span class="sd">        spatial_scale (float): A scaling factor that maps the raw image coordinates to the input</span>
<span class="sd">            feature map coordinates. Suppose the height of a RoI is `ori_h` in the raw image and `fea_h` in the</span>
<span class="sd">            input feature map, the `spatial_scale` should be `fea_h / ori_h`.</span>
<span class="sd">        sample_num (int): Number of sampling points. Default: 2.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape should be `(N, C, H, W)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is `(rois_n, 5)`. `rois_n` represents the number of RoI. The size of</span>
<span class="sd">          the second dimension should be `5` and the `5` colunms are</span>
<span class="sd">          `(image_index, top_left_x, top_left_y, bottom_right_x, bottom_right_y)`. `image_index` represents the</span>
<span class="sd">          index of image. `top_left_x` and `top_left_y` represent the `x, y` coordinates of the top left corner</span>
<span class="sd">          of corresponding RoI, respectively. `bottom_right_x` and `bottom_right_y` represent the `x, y`</span>
<span class="sd">          coordinates of the bottom right corner of corresponding RoI, respectively.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is `(rois_n, C, pooled_height, pooled_width)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[[[1., 2.], [3., 4.]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor(np.array([[0, 0.2, 0.3, 0.2, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; roi_align = P.ROIAlign(1, 1, 0.5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = roi_align(input_tensor, rois)</span>
<span class="sd">        &gt;&gt;&gt; assert output_tensor == Tensor(np.array([[[[2.15]]]]), mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">sample_num</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init ROIAlign&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;pooled_height&quot;</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;pooled_width&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;sample_num&quot;</span><span class="p">,</span> <span class="n">sample_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span> <span class="o">=</span> <span class="n">pooled_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span> <span class="o">=</span> <span class="n">pooled_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_num</span> <span class="o">=</span> <span class="n">sample_num</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">,</span> <span class="n">rois_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">rois_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_type</span><span class="p">,</span> <span class="n">rois_type</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">inputs_type</span></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by Adaptive Moment Estimation (Adam) algorithm.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect updating variable tensors.</span>
<span class="sd">            If True, updating of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If False, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If True, updates the gradients using NAG.</span>
<span class="sd">            If False, updates the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated.</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula.</span>
<span class="sd">        - **beta1_power** (float) - :math:`beta_1^t` in the updating formula.</span>
<span class="sd">        - **beta2_power** (float) - :math:`beta_2^t` in the updating formula.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimates.</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimates.</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">        - **gradient** (Tensor) - Gradients.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `var`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m_dtype&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v_dtype&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power_dtype&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power_dtype&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr_dtype&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1_dtype&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_dtype&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon_dtype&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="BinaryCrossEntropy"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.BinaryCrossEntropy">[docs]</a><span class="k">class</span> <span class="nc">BinaryCrossEntropy</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Binary Cross Entropy between the target and the output.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input as :math:`x`, input label as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">        Let,</span>

<span class="sd">        .. math::</span>
<span class="sd">            L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">            l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">        Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            \ell(x, y) = \begin{cases}</span>
<span class="sd">            L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to apply to the output.</span>
<span class="sd">            Its value should be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor.</span>
<span class="sd">        - **input_y** (Tensor) - The label Tensor which has same shape as `input_x`.</span>
<span class="sd">        - **weight** (Tensor, optional) - A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">          And it should have same shape as `input_x`. Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and same shape as `input_x`.</span>
<span class="sd">        Otherwise it is a scalar.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;x_shape&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="s1">&#39;weight_shape&#39;</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span><span class="p">,</span> <span class="n">weight_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x_type&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;y_type&#39;</span><span class="p">:</span> <span class="n">y_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">weight_type</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_two_types_same</span><span class="p">(</span><span class="s1">&#39;x_type&#39;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;weight_type&#39;</span><span class="p">,</span> <span class="n">weight_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="SparseApplyAdagrad"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.SparseApplyAdagrad">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update relevant entries according to the adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * (1 / sqrt(accum))</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        use_locking (bool): If True, updating of the var and accum tensors will be protected. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Variable to be updated. The type must be float32.</span>
<span class="sd">        - **accum** (Tensor) - Accum to be updated. The shape must be the same as `var`&#39;s shape,</span>
<span class="sd">          the type must be float32.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The shape must be the same as `var`&#39;s shape</span>
<span class="sd">          except first dimension, the type must be float32.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in first dimension, the type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `var`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;len of var shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">),</span> <span class="s1">&#39;len of grad shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;len of indices shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;the first dimension of grad&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="s1">&#39;the shape of indices&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;var_type&quot;</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;accum_type&quot;</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;grad_type&quot;</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;indices_type&quot;</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var_type&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum_type&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;grad_type&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s1">&#39;indices_type&#39;</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">var_type</span></div>


<div class="viewcode-block" id="LARSUpdate"><a class="viewcode-back" href="../../../../api/python/mindspore/mindspore.ops.operations.html#mindspore.ops.operations.LARSUpdate">[docs]</a><span class="k">class</span> <span class="nc">LARSUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Conduct lars (layer-wise adaptive rate scaling) update on the square sum of gradient.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): Term added to the denominator to improve numerical stability. Default: 1e-05.</span>
<span class="sd">        hyperpara (float): Trust coefficient for calculating the local learning rate. Default: 0.001.</span>
<span class="sd">        use_clip (bool): Whether to use clip operation for calculating the local learning rate. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - The weight to be updated.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of weight, which has the same shape and dtype with weight.</span>
<span class="sd">        - **norm_weight** (Tensor) - A scalar tensor, representing the square sum of weight.</span>
<span class="sd">        - **norm_gradient** (Tensor) - A scalar tensor, representing the square sum of gradient.</span>
<span class="sd">        - **weight_decay** (Union[Number, Tensor]) - Weight decay. It should be a scalar tensor or number.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. It should be a scalar tensor or number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, representing the new gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">hyperpara</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">use_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;hyperpara&quot;</span><span class="p">,</span> <span class="n">hyperpara</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type</span><span class="p">(</span><span class="s2">&quot;use_clip&quot;</span><span class="p">,</span> <span class="n">use_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">norm_weight_shape</span><span class="p">,</span> <span class="n">norm_gradient_shape</span><span class="p">,</span> <span class="n">weight_decay_shape</span><span class="p">,</span>
                    <span class="n">learning_rate_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;Weight shape&quot;</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_param_equal</span><span class="p">(</span><span class="s2">&quot;Norm weight shape&quot;</span><span class="p">,</span> <span class="n">norm_weight_shape</span><span class="p">,</span> <span class="s2">&quot;norm gradient shape&quot;</span><span class="p">,</span> <span class="n">norm_gradient_shape</span><span class="p">)</span>
        <span class="n">shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_shape_length</span><span class="p">(</span><span class="s2">&quot;Weight decay&#39;s shape&quot;</span><span class="p">,</span> <span class="n">shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;Weight decay&#39;s shape&quot;</span><span class="p">,</span> <span class="n">weight_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="n">shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_shape_length</span><span class="p">(</span><span class="s2">&quot;Learning rate&#39;s shape&quot;</span><span class="p">,</span> <span class="n">shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;Learning rate&#39;s shape&quot;</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">norm_weight_dtype</span><span class="p">,</span> <span class="n">norm_gradient_dtype</span><span class="p">,</span>
                    <span class="n">weight_decay_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Weight dtype&quot;</span><span class="p">:</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="s2">&quot;gradient dtype&quot;</span><span class="p">:</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="s2">&quot;norm weight dtype&quot;</span><span class="p">:</span> <span class="n">norm_weight_dtype</span><span class="p">,</span>
                <span class="s2">&quot;norm gradient dtype&quot;</span><span class="p">:</span> <span class="n">norm_gradient_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_args_tensor</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;weight_decay_dtype&quot;</span><span class="p">,</span> <span class="n">weight_decay_dtype</span><span class="p">,</span>
                                 <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_typename</span><span class="p">(</span><span class="s2">&quot;learning_rate_dtype&quot;</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span>
                                 <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">weight_dtype</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>