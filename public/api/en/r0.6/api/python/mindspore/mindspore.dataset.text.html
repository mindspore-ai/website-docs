

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.dataset.text &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="mindspore.dataset.transforms" href="mindspore.dataset.transforms.html" />
    <link rel="prev" title="mindspore.dataset.config" href="mindspore.dataset.config.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dtype.html">mindspore.dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.hub.html">mindspore.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.dynamic_lr.html">mindspore.nn.dynamic_lr</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.learning_rate_schedule.html">mindspore.nn.learning_rate_schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.composite.html">mindspore.ops.composite</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.operations.html">mindspore.ops.operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.dataset.text</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-mindspore.dataset.text.transforms">mindspore.dataset.text.transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-mindspore.dataset.text.utils">mindspore.dataset.text.utils</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.vision.html">mindspore.dataset.transforms.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.profiler.html">mindspore.profiler</a></li>
</ul>
<p class="caption"><span class="caption-text">MindInsight Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindinsight/mindinsight.lineagemgr.html">mindinsight.lineagemgr</a></li>
</ul>
<p class="caption"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.evaluations.html">mindarmour.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.detectors.html">mindarmour.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.attacks.html">mindarmour.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.defenses.html">mindarmour.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.fuzzing.html">mindarmour.fuzzing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.diff_privacy.html">mindarmour.diff_privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>mindspore.dataset.text</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/api/python/mindspore/mindspore.dataset.text.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-dataset-text">
<h1>mindspore.dataset.text<a class="headerlink" href="#mindspore-dataset-text" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-mindspore.dataset.text.transforms">
<span id="mindspore-dataset-text-transforms"></span><h2>mindspore.dataset.text.transforms<a class="headerlink" href="#module-mindspore.dataset.text.transforms" title="Permalink to this headline">¶</a></h2>
<p>The module text.transforms is inheritted from _c_dataengine
which is implemented basing on icu4c and cppjieba in C++.
It’s a high performance module to process nlp text.
Users can use Vocab to build their own dictionary,
use appropriate tokenizers to split sentences into different tokens,
and use Lookup to find the index of tokens in Vocab.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Constructor’s arguments for every class in this module must be saved into the
class attributes (self.xxx) to support save() and load().</p>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset_file</span> <span class="o">=</span> <span class="s2">&quot;path/to/text_file_path&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># sentences as line data saved in a file</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">TextFileDataset</span><span class="p">(</span><span class="n">dataset_file</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># tokenize sentence to unicode characters</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># load vocabulary form list</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">([</span><span class="s1">&#39;深&#39;</span><span class="p">,</span> <span class="s1">&#39;圳&#39;</span><span class="p">,</span> <span class="s1">&#39;欢&#39;</span><span class="p">,</span> <span class="s1">&#39;迎&#39;</span><span class="p">,</span> <span class="s1">&#39;您&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># lookup is an operation for mapping tokens to ids</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lookup</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Lookup</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="p">[</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">lookup</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># if text line in dataset_file is:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 深圳欢迎您</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># then the output will be:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># {&#39;text&#39;: array([0, 1, 2, 3, 4], dtype=int32)}</span>
</pre></div>
</div>
<dl class="class">
<dt id="mindspore.dataset.text.transforms.BasicTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">BasicTokenizer</code><span class="sig-paren">(</span><em class="sig-param">lower_case=False</em>, <em class="sig-param">keep_whitespace=False</em>, <em class="sig-param">normalization_form=&lt;NormalizeForm.NONE: 0&gt;</em>, <em class="sig-param">preserve_unused_token=True</em>, <em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#BasicTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.BasicTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a scalar tensor of UTF-8 string by specific rules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lower_case</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, apply CaseFold, NormalizeUTF8(NFD mode), RegexReplace operation
on input text to make the text to lower case and strip accents characters; If False, only apply
NormalizeUTF8(‘normalization_form’ mode) operation on input text(default=False).</p></li>
<li><p><strong>keep_whitespace</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, the whitespace will be kept in out tokens(default=False).</p></li>
<li><p><strong>normalization_form</strong> (<em>NormalizeForm</em><em>, </em><em>optional</em>) – Used to specify a specific normlaize mode,
only effective when ‘lower_case’ is False. See NormalizeUTF8 for details(default=’NONE’).</p></li>
<li><p><strong>preserve_unused_token</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, do not split special tokens like
‘[CLS]’, ‘[SEP]’, ‘[UNK]’, ‘[PAD]’, ‘[MASK]’(default=True).</p></li>
<li><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BasicTokenizer</span><span class="p">(</span><span class="n">lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">keep_whitespace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">normalization_form</span><span class="o">=</span><span class="n">NormalizeForm</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">preserve_unused_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">with_offsets</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BasicTokenizer</span><span class="p">(</span><span class="n">lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">keep_whitespace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">normalization_form</span><span class="o">=</span><span class="n">NormalizeForm</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">preserve_unused_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">with_offsets</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.BertTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">BertTokenizer</code><span class="sig-paren">(</span><em class="sig-param">vocab</em>, <em class="sig-param">suffix_indicator='##'</em>, <em class="sig-param">max_bytes_per_token=100</em>, <em class="sig-param">unknown_token='[UNK]'</em>, <em class="sig-param">lower_case=False</em>, <em class="sig-param">keep_whitespace=False</em>, <em class="sig-param">normalization_form=&lt;NormalizeForm.NONE: 0&gt;</em>, <em class="sig-param">preserve_unused_token=True</em>, <em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#BertTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.BertTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenizer used for Bert text process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab</strong> (<a class="reference internal" href="#mindspore.dataset.text.utils.Vocab" title="mindspore.dataset.text.utils.Vocab"><em>Vocab</em></a>) – a Vocab object.</p></li>
<li><p><strong>suffix_indicator</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – Used to show that the subword is the last part of a word(default=’##’).</p></li>
<li><p><strong>max_bytes_per_token</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Tokens exceeding this length will not be further split(default=100).</p></li>
<li><p><strong>unknown_token</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – When we can not found the token: if ‘unknown_token’ is empty string,
return the token directly, else return ‘unknown_token’(default=’[UNK]’).</p></li>
<li><p><strong>lower_case</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, apply CaseFold, NormalizeUTF8(NFD mode), RegexReplace operation
on input text to make the text to lower case and strip accents characters; If False, only apply
NormalizeUTF8(‘normalization_form’ mode) operation on input text(default=False).</p></li>
<li><p><strong>keep_whitespace</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, the whitespace will be kept in out tokens(default=False).</p></li>
<li><p><strong>normalization_form</strong> (<em>NormalizeForm</em><em>, </em><em>optional</em>) – Used to specify a specific normlaize mode,
only effective when ‘lower_case’ is False. See NormalizeUTF8 for details(default=’NONE’).</p></li>
<li><p><strong>preserve_unused_token</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If True, do not split special tokens like
‘[CLS]’, ‘[SEP]’, ‘[UNK]’, ‘[PAD]’, ‘[MASK]’(default=True).</p></li>
<li><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">suffix_indicator</span><span class="o">=</span><span class="s1">&#39;##&#39;</span><span class="p">,</span> <span class="n">max_bytes_per_token</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                 <span class="n">unknown_token</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_whitespace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                 <span class="n">normalization_form</span><span class="o">=</span><span class="n">NormalizeForm</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span> <span class="n">preserve_unused_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                 <span class="n">with_offsets</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">suffix_indicator</span><span class="o">=</span><span class="s1">&#39;##&#39;</span><span class="p">,</span> <span class="n">max_bytes_per_token</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                 <span class="n">unknown_token</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keep_whitespace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                 <span class="n">normalization_form</span><span class="o">=</span><span class="n">NormalizeForm</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span> <span class="n">preserve_unused_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                 <span class="n">with_offsets</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.CaseFold">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">CaseFold</code><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#CaseFold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.CaseFold" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply case fold operation on utf-8 string tensor.</p>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.JiebaTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">JiebaTokenizer</code><span class="sig-paren">(</span><em class="sig-param">hmm_path</em>, <em class="sig-param">mp_path</em>, <em class="sig-param">mode=&lt;JiebaMode.MIX: 0&gt;</em>, <em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#JiebaTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.JiebaTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize Chinese string into words based on dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hmm_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – the dictionary file is used by  HMMSegment algorithm,
the dictionary can be obtained on the official website of cppjieba.</p></li>
<li><p><strong>mp_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – the dictionary file is used by MPSegment algorithm,
the dictionary can be obtained on the official website of cppjieba.</p></li>
<li><p><strong>mode</strong> (<em>JiebaMode</em><em>, </em><em>optional</em>) – <p>Valid values can be any of [JiebaMode.MP, JiebaMode.HMM,
JiebaMode.MIX](default=JiebaMode.MIX).</p>
<ul>
<li><p>JiebaMode.MP, tokenize with MPSegment algorithm.</p></li>
<li><p>JiebaMode.HMM, tokenize with Hiddel Markov Model Segment algorithm.</p></li>
<li><p>JiebaMode.MIX, tokenize with a mix of MPSegment and HMMSegment algorithm.</p></li>
</ul>
</p></li>
<li><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">JiebaTokenizer</span><span class="p">(</span><span class="n">HMM_FILE</span><span class="p">,</span> <span class="n">MP_FILE</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">JiebaMode</span><span class="o">.</span><span class="n">MP</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str], [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">JiebaTokenizer</span><span class="p">(</span><span class="n">HMM_FILE</span><span class="p">,</span> <span class="n">MP_FILE</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">JiebaMode</span><span class="o">.</span><span class="n">MP</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.dataset.text.transforms.JiebaTokenizer.add_dict">
<code class="sig-name descname">add_dict</code><span class="sig-paren">(</span><em class="sig-param">user_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#JiebaTokenizer.add_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.JiebaTokenizer.add_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Add user defined word to JiebaTokenizer’s dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>user_dict</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em>) – <p>Dictionary to be added, file path or Python dictionary,
Python Dict format: {word1:freq1, word2:freq2,…}.
Jieba dictionary format : word(required), freq(optional), such as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">word1</span> <span class="n">freq1</span>
<span class="n">word2</span>
<span class="n">word3</span> <span class="n">freq3</span>
</pre></div>
</div>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.dataset.text.transforms.JiebaTokenizer.add_word">
<code class="sig-name descname">add_word</code><span class="sig-paren">(</span><em class="sig-param">word</em>, <em class="sig-param">freq=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#JiebaTokenizer.add_word"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.JiebaTokenizer.add_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Add user defined word to JiebaTokenizer’s dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>word</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The word to be added to the JiebaTokenizer instance.
The added word will not be written into the built-in dictionary on disk.</p></li>
<li><p><strong>freq</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – The frequency of the word to be added, The higher the frequency,
the better change the word will be tokenized(default=None, use default frequency).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.Lookup">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">Lookup</code><span class="sig-paren">(</span><em class="sig-param">vocab</em>, <em class="sig-param">unknown_token=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#Lookup"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.Lookup" title="Permalink to this definition">¶</a></dt>
<dd><p>Lookup operator that looks up a word to an id.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab</strong> (<a class="reference internal" href="#mindspore.dataset.text.utils.Vocab" title="mindspore.dataset.text.utils.Vocab"><em>Vocab</em></a>) – a Vocab object.</p></li>
<li><p><strong>unknown_token</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – word to use for lookup if the word being looked up is out of Vocabulary (oov).
If unknown_token is oov, runtime error will be thrown (default=None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.Ngram">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">Ngram</code><span class="sig-paren">(</span><em class="sig-param">n</em>, <em class="sig-param">left_pad=(''</em>, <em class="sig-param">0)</em>, <em class="sig-param">right_pad=(''</em>, <em class="sig-param">0)</em>, <em class="sig-param">separator=' '</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#Ngram"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.Ngram" title="Permalink to this definition">¶</a></dt>
<dd><p>TensorOp to generate n-gram from a 1-D string Tensor.</p>
<p>Refer to <a class="reference external" href="https://en.wikipedia.org/wiki/N-gram#Examples">https://en.wikipedia.org/wiki/N-gram#Examples</a> for an overview of what n-gram is and how it works.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – n in n-gram, n &gt;= 1. n is a list of positive integers, for e.g. n=[4,3], The result
would be a 4-gram followed by a 3-gram in the same tensor. If number of words is not enough to make up for
a n-gram, an empty string would be returned. For e.g. 3 grams on [“mindspore”,”best”] would result in an
empty string be produced.</p></li>
<li><p><strong>left_pad</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – (“pad_token”, pad_width). Padding performed on left side of the sequence. pad_width
will be capped at n-1. left_pad=(“_”,2) would pad left side of the sequence with “__” (default=None).</p></li>
<li><p><strong>right_pad</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – (“pad_token”, pad_width). Padding performed on right side of the sequence.
pad_width will be capped at n-1. right_pad=(“-“:2) would pad right side of the sequence with “–”
(default=None).</p></li>
<li><p><strong>separator</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – symbol used to join strings together. for e.g. if 2-gram the [“mindspore”, “amazing”]
with separator=”-” the result would be [“mindspore-amazing”] (default=None, which means whitespace is
used).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.NormalizeUTF8">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">NormalizeUTF8</code><span class="sig-paren">(</span><em class="sig-param">normalize_form=&lt;NormalizeForm.NFKC: 2&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#NormalizeUTF8"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.NormalizeUTF8" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply normalize operation on utf-8 string tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>normalize_form</strong> (<em>NormalizeForm</em><em>, </em><em>optional</em>) – <p>Valid values can be any of [NormalizeForm.NONE,
NormalizeForm.NFC, NormalizeForm.NFKC, NormalizeForm.NFD,
NormalizeForm.NFKD](default=NormalizeForm.NFKC).
And you can see <a class="reference external" href="http://unicode.org/reports/tr15/">http://unicode.org/reports/tr15/</a> for details.</p>
<ul class="simple">
<li><p>NormalizeForm.NONE, do nothing for input string tensor.</p></li>
<li><p>NormalizeForm.NFC, normalize with Normalization Form C.</p></li>
<li><p>NormalizeForm.NFKC, normalize with Normalization Form KC.</p></li>
<li><p>NormalizeForm.NFD, normalize with Normalization Form D.</p></li>
<li><p>NormalizeForm.NFKD, normalize with Normalization Form KD.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.PythonTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">PythonTokenizer</code><span class="sig-paren">(</span><em class="sig-param">tokenizer</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#PythonTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.PythonTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Callable class to be used for user-defined string tokenizer.
:param tokenizer: Python function that takes a <cite>str</cite> and returns a list of <cite>str</cite> as tokens.
:type tokenizer: Callable</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_tokenizer</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">PythonTokenizer</span><span class="p">(</span><span class="n">my_tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.RegexReplace">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">RegexReplace</code><span class="sig-paren">(</span><em class="sig-param">pattern</em>, <em class="sig-param">replace</em>, <em class="sig-param">replace_all=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#RegexReplace"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.RegexReplace" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace utf-8 string tensor with ‘replace’ according to regular expression ‘pattern’.</p>
<p>See <a class="reference external" href="http://userguide.icu-project.org/strings/regexp">http://userguide.icu-project.org/strings/regexp</a> for support regex pattern.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pattern</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – the regex expression patterns.</p></li>
<li><p><strong>replace</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – the string to replace matched element.</p></li>
<li><p><strong>replace_all</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If False, only replace first matched element;
if True, replace all matched elements(default=True).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.RegexTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">RegexTokenizer</code><span class="sig-paren">(</span><em class="sig-param">delim_pattern</em>, <em class="sig-param">keep_delim_pattern=''</em>, <em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#RegexTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.RegexTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a scalar tensor of UTF-8 string by regex expression pattern.</p>
<p>See <a class="reference external" href="http://userguide.icu-project.org/strings/regexp">http://userguide.icu-project.org/strings/regexp</a> for support regex pattern.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>delim_pattern</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The pattern of regex delimiters.
The original string will be split by matched elements.</p></li>
<li><p><strong>keep_delim_pattern</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – The string matched by ‘delim_pattern’ can be kept as a token
if it can be matched by ‘keep_delim_pattern’. And the default value is empty str(‘’),
in this situation, delimiters will not kept as a output token(default=’’).</p></li>
<li><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">RegexTokenizer</span><span class="p">(</span><span class="n">delim_pattern</span><span class="p">,</span> <span class="n">keep_delim_pattern</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">RegexTokenizer</span><span class="p">(</span><span class="n">delim_pattern</span><span class="p">,</span> <span class="n">keep_delim_pattern</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.SentencePieceTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">SentencePieceTokenizer</code><span class="sig-paren">(</span><em class="sig-param">mode</em>, <em class="sig-param">out_type</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#SentencePieceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.SentencePieceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize scalar token or 1-D tokens to tokens by sentencepiece.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="#mindspore.dataset.text.utils.SentencePieceVocab" title="mindspore.dataset.text.utils.SentencePieceVocab"><em>SentencePieceVocab</em></a><em>]</em>) – If the input parameter is a file, then it is of type string,
if the input parameter is a SentencePieceVocab object, then it is of type SentencePieceVocab.</p></li>
<li><p><strong>out_type</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – The type of output.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.SlidingWindow">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">SlidingWindow</code><span class="sig-paren">(</span><em class="sig-param">width</em>, <em class="sig-param">axis=0</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#SlidingWindow"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.SlidingWindow" title="Permalink to this definition">¶</a></dt>
<dd><p>TensorOp to construct a tensor from data (only 1-D for now), where each element in the dimension axis
is a slice of data starting at the corresponding position, with a specified width.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>width</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The width of the window. Must be an integer and greater than zero.</p></li>
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – The axis along which sliding window is computed (default=0).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Data before</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |    col1     |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +-------------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># | [1,2,3,4,5] |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +-------------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">SlidingWindow</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Data after</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |     col1    |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +-------------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |  [[1,2,3],  |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |   [2,3,4],  |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |   [3,4,5]]  |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +--------------+</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.ToNumber">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">ToNumber</code><span class="sig-paren">(</span><em class="sig-param">data_type</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#ToNumber"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.ToNumber" title="Permalink to this definition">¶</a></dt>
<dd><p>Tensor operation to convert every element of a string tensor to a number.</p>
<p>Strings are casted according to the rules specified in the following links:
<a class="reference external" href="https://en.cppreference.com/w/cpp/string/basic_string/stof">https://en.cppreference.com/w/cpp/string/basic_string/stof</a>,
<a class="reference external" href="https://en.cppreference.com/w/cpp/string/basic_string/stoul">https://en.cppreference.com/w/cpp/string/basic_string/stoul</a>,
except that any strings which represent negative numbers cannot be casted to an
unsigned integer type.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data_type</strong> (<a class="reference internal" href="mindspore.dtype.html#mindspore.dtype" title="mindspore.dtype"><em>mindspore.dtype</em></a>) – mindspore.dtype to be casted to. Must be
a numeric type.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If strings are invalid to cast, or are out of range after being casted.</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.TruncateSequencePair">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">TruncateSequencePair</code><span class="sig-paren">(</span><em class="sig-param">max_length</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#TruncateSequencePair"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.TruncateSequencePair" title="Permalink to this definition">¶</a></dt>
<dd><p>Truncate a pair of rank-1 tensors such that the total length is less than max_length.</p>
<p>This operation takes two input tensors and returns two output Tenors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Maximum length required.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Data before</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |  col1   |  col2   |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +---------+---------|</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># | [1,2,3] | [4,5]   |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +---------+---------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">TruncateSequencePair</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Data after</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># |  col1   |  col2   |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +---------+---------+</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># | [1,2]   | [4,5]   |</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># +---------+---------+</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.UnicodeCharTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">UnicodeCharTokenizer</code><span class="sig-paren">(</span><em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#UnicodeCharTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.UnicodeCharTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a scalar tensor of UTF-8 string to Unicode characters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str], [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.UnicodeScriptTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">UnicodeScriptTokenizer</code><span class="sig-paren">(</span><em class="sig-param">keep_whitespace=False</em>, <em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#UnicodeScriptTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.UnicodeScriptTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a scalar tensor of UTF-8 string on Unicode script boundaries.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keep_whitespace</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not emit whitespace tokens (default=False).</p></li>
<li><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeScriptTokenizerOp</span><span class="p">(</span><span class="n">keep_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeScriptTokenizerOp</span><span class="p">(</span><span class="n">keep_whitespace</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.WhitespaceTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">WhitespaceTokenizer</code><span class="sig-paren">(</span><em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#WhitespaceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.WhitespaceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize a scalar tensor of UTF-8 string on ICU defined whitespaces(such as: ‘ ‘, ‘\t’, ‘\r’, ‘\n’).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.transforms.WordpieceTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.transforms.</code><code class="sig-name descname">WordpieceTokenizer</code><span class="sig-paren">(</span><em class="sig-param">vocab</em>, <em class="sig-param">suffix_indicator='##'</em>, <em class="sig-param">max_bytes_per_token=100</em>, <em class="sig-param">unknown_token='[UNK]'</em>, <em class="sig-param">with_offsets=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/transforms.html#WordpieceTokenizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.transforms.WordpieceTokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Tokenize scalar token or 1-D tokens to 1-D subword tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab</strong> (<a class="reference internal" href="#mindspore.dataset.text.utils.Vocab" title="mindspore.dataset.text.utils.Vocab"><em>Vocab</em></a>) – a Vocab object.</p></li>
<li><p><strong>suffix_indicator</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – Used to show that the subword is the last part of a word(default=’##’).</p></li>
<li><p><strong>max_bytes_per_token</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Tokens exceeding this length will not be further split(default=100).</p></li>
<li><p><strong>unknown_token</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – When we can not found the token: if ‘unknown_token’ is empty string,
return the token directly, else return ‘unknown_token’(default=’[UNK]’).</p></li>
<li><p><strong>with_offsets</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – If or not output offsets of tokens (default=False).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, default output one column {[&quot;text&quot;, dtype=str]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WordpieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;UNK&#39;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                      <span class="n">max_bytes_per_token</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If with_offsets=False, then output three columns {[&quot;token&quot;, dtype=str], [&quot;offsets_start&quot;, dtype=uint32],</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                                                   [&quot;offsets_limit&quot;, dtype=uint32]}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WordpieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span> <span class="n">unknown_token</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;UNK&#39;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                      <span class="n">max_bytes_per_token</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">with_offsets</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">output_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">columns_order</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;token&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_start&quot;</span><span class="p">,</span> <span class="s2">&quot;offsets_limit&quot;</span><span class="p">],</span> <span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-mindspore.dataset.text.utils">
<span id="mindspore-dataset-text-utils"></span><h2>mindspore.dataset.text.utils<a class="headerlink" href="#module-mindspore.dataset.text.utils" title="Permalink to this headline">¶</a></h2>
<p>The module text.utils provides some general methods for nlp text processing.
For example, you can use Vocab to build a dictionary,
use to_bytes and to_str to encode and decode strings into a specified format.</p>
<dl class="class">
<dt id="mindspore.dataset.text.utils.Vocab">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.utils.</code><code class="sig-name descname">Vocab</code><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#Vocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.Vocab" title="Permalink to this definition">¶</a></dt>
<dd><p>Vocab object that is used to lookup a word.</p>
<p>It contains a map that maps each word(str) to an id (int).</p>
<dl class="method">
<dt id="mindspore.dataset.text.utils.Vocab.from_dataset">
<em class="property">classmethod </em><code class="sig-name descname">from_dataset</code><span class="sig-paren">(</span><em class="sig-param">dataset</em>, <em class="sig-param">columns=None</em>, <em class="sig-param">freq_range=None</em>, <em class="sig-param">top_k=None</em>, <em class="sig-param">special_tokens=None</em>, <em class="sig-param">special_first=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#Vocab.from_dataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.Vocab.from_dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a vocab from a dataset.</p>
<p>This would collect all unique words in a dataset and return a vocab within
the frequency range specified by user in freq_range. User would be warned if no words fall into the frequency.
Words in vocab are ordered from highest frequency to lowest frequency. Words with the same frequency would be
ordered lexicographically.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>Dataset</em>) – dataset to build vocab from.</p></li>
<li><p><strong>columns</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>]</em><em>, </em><em>optional</em>) – column names to get words from. It can be a list of column names.
(default=None, where all columns will be used. If any column isn’t string type, will return error).</p></li>
<li><p><strong>freq_range</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><em>optional</em>) – A tuple of integers (min_frequency, max_frequency). Words within the frequency
range would be kept. 0 &lt;= min_frequency &lt;= max_frequency &lt;= total_words. min_frequency=0 is the same as
min_frequency=1. max_frequency &gt; total_words is the same as max_frequency = total_words.
min_frequency/max_frequency can be None, which corresponds to 0/total_words separately
(default=None, all words are included).</p></li>
<li><p><strong>top_k</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – top_k &gt; 0. Number of words to be built into vocab. top_k most frequent words are
taken. top_k is taken after freq_range. If not enough top_k, all words will be taken (default=None,
all words are included).</p></li>
<li><p><strong>special_tokens</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – a list of strings, each one is a special token. for example
special_tokens=[“&lt;pad&gt;”,”&lt;unk&gt;”] (default=None, no special tokens will be added).</p></li>
<li><p><strong>special_first</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – whether special_tokens will be prepended/appended to vocab. If special_tokens
is specified and special_first is set to True, special_tokens will be prepended (default=True).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Vocab, Vocab object built from dataset.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.dataset.text.utils.Vocab.from_dict">
<em class="property">classmethod </em><code class="sig-name descname">from_dict</code><span class="sig-paren">(</span><em class="sig-param">word_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#Vocab.from_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.Vocab.from_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a vocab object from a dict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>word_dict</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – dict contains word, id pairs where word should be str and id int. id is recommended to
start from 0 and be continuous. ValueError will be raised if id is negative.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.dataset.text.utils.Vocab.from_file">
<em class="property">classmethod </em><code class="sig-name descname">from_file</code><span class="sig-paren">(</span><em class="sig-param">file_path</em>, <em class="sig-param">delimiter=''</em>, <em class="sig-param">vocab_size=None</em>, <em class="sig-param">special_tokens=None</em>, <em class="sig-param">special_first=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#Vocab.from_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.Vocab.from_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a vocab object from a list of word.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – path to the file which contains the vocab list.</p></li>
<li><p><strong>delimiter</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><em>optional</em>) – a delimiter to break up each line in file, the first element is taken to be
the word (default=””).</p></li>
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – number of words to read from file_path (default=None, all words are taken).</p></li>
<li><p><strong>special_tokens</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – a list of strings, each one is a special token. for example
special_tokens=[“&lt;pad&gt;”,”&lt;unk&gt;”] (default=None, no special tokens will be added).</p></li>
<li><p><strong>special_first</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – whether special_tokens will be prepended/appended to vocab,
If special_tokens is specified and special_first is set to True,
special_tokens will be prepended (default=True).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.dataset.text.utils.Vocab.from_list">
<em class="property">classmethod </em><code class="sig-name descname">from_list</code><span class="sig-paren">(</span><em class="sig-param">word_list</em>, <em class="sig-param">special_tokens=None</em>, <em class="sig-param">special_first=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#Vocab.from_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.Vocab.from_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a vocab object from a list of word.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>word_list</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – a list of string where each element is a word of type string.</p></li>
<li><p><strong>special_tokens</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – a list of strings, each one is a special token. for example
special_tokens=[“&lt;pad&gt;”,”&lt;unk&gt;”] (default=None, no special tokens will be added).</p></li>
<li><p><strong>special_first</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – whether special_tokens will be prepended/appended to vocab, If special_tokens
is specified and special_first is set to True, special_tokens will be prepended (default=True).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.dataset.text.utils.SentencePieceVocab">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.dataset.text.utils.</code><code class="sig-name descname">SentencePieceVocab</code><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#SentencePieceVocab"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.SentencePieceVocab" title="Permalink to this definition">¶</a></dt>
<dd><p>SentencePiece obiect that is used to segmentate words</p>
<dl class="method">
<dt id="mindspore.dataset.text.utils.SentencePieceVocab.from_dataset">
<em class="property">classmethod </em><code class="sig-name descname">from_dataset</code><span class="sig-paren">(</span><em class="sig-param">dataset</em>, <em class="sig-param">col_names</em>, <em class="sig-param">vocab_size</em>, <em class="sig-param">character_coverage</em>, <em class="sig-param">model_type</em>, <em class="sig-param">params</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#SentencePieceVocab.from_dataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.SentencePieceVocab.from_dataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a sentencepiece from a dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to build sentencepiece.</p></li>
<li><p><strong>col_names</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The list of the col name.</p></li>
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Vocabulary size, the type of uint32_t.</p></li>
<li><p><strong>character_coverage</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Amount of characters covered by the model, good defaults are: 0.9995 for
languages. with rich character set like Japanse or Chinese and 1.0 for other languages with small
character set.</p></li>
<li><p><strong>model_type</strong> (<em>SentencePieceModel</em>) – Choose from unigram (default), bpe, char, or word. The input sentence
must be pretokenized when using word type.</p></li>
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – A dictionary with no incoming parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>SentencePiece, SentencePiece object from dataset.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.dataset.text.utils.SentencePieceVocab.from_file">
<em class="property">classmethod </em><code class="sig-name descname">from_file</code><span class="sig-paren">(</span><em class="sig-param">file_path</em>, <em class="sig-param">vocab_size</em>, <em class="sig-param">character_coverage</em>, <em class="sig-param">model_type</em>, <em class="sig-param">params</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#SentencePieceVocab.from_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.SentencePieceVocab.from_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a SentencePiece object from a list of word.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – Path to the file which contains the sentencepiece list.</p></li>
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Vocabulary size, the type of uint32_t.</p></li>
<li><p><strong>character_coverage</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Amount of characters covered by the model, good defaults are: 0.9995 for
languages. with rich character set like Japanse or Chinese and 1.0 for other languages with small
character set.</p></li>
<li><p><strong>model_type</strong> (<em>SentencePieceModel</em>) – Choose from unigram (default), bpe, char, or word. The input sentence
must be pretokenized when using word type.</p></li>
<li><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – A dictionary with no incoming parameters.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.dataset.text.utils.SentencePieceVocab.save_model">
<em class="property">classmethod </em><code class="sig-name descname">save_model</code><span class="sig-paren">(</span><em class="sig-param">vocab</em>, <em class="sig-param">path</em>, <em class="sig-param">filename</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#SentencePieceVocab.save_model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.SentencePieceVocab.save_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Save model to filepath</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab</strong> (<a class="reference internal" href="#mindspore.dataset.text.utils.SentencePieceVocab" title="mindspore.dataset.text.utils.SentencePieceVocab"><em>SentencePieceVocab</em></a>) – A sentencepiece object.</p></li>
<li><p><strong>path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Path to store model.</p></li>
<li><p><strong>filename</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The name of the file.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="mindspore.dataset.text.utils.to_str">
<code class="sig-prename descclassname">mindspore.dataset.text.utils.</code><code class="sig-name descname">to_str</code><span class="sig-paren">(</span><em class="sig-param">array</em>, <em class="sig-param">encoding='utf8'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#to_str"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.to_str" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert numpy array of <cite>bytes</cite> to array of <cite>str</cite> by decoding each element based on charset <cite>encoding</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>array</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><em>numpy.ndarray</em></a>) – Array of type <cite>bytes</cite> representing strings.</p></li>
<li><p><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Indicating the charset for decoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>numpy.ndarray, numpy array of <cite>str</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mindspore.dataset.text.utils.to_bytes">
<code class="sig-prename descclassname">mindspore.dataset.text.utils.</code><code class="sig-name descname">to_bytes</code><span class="sig-paren">(</span><em class="sig-param">array</em>, <em class="sig-param">encoding='utf8'</em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/dataset/text/utils.html#to_bytes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.dataset.text.utils.to_bytes" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert numpy array of <cite>str</cite> to array of <cite>bytes</cite> by encoding each element based on charset <cite>encoding</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>array</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><em>numpy.ndarray</em></a>) – Array of type <cite>str</cite> representing strings.</p></li>
<li><p><strong>encoding</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Indicating the charset for encoding.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>numpy.ndarray, numpy array of <cite>bytes</cite>.</p>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="mindspore.dataset.transforms.html" class="btn btn-neutral float-right" title="mindspore.dataset.transforms" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="mindspore.dataset.config.html" class="btn btn-neutral float-left" title="mindspore.dataset.config" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, MindSpore

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>