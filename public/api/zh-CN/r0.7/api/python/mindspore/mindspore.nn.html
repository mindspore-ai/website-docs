<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script><script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/js/theme.js"></script><script src="../../../_static/underscore.js"></script><script src="../../../_static/doctools.js"></script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="mindspore.nn.dynamic_lr" href="mindspore.nn.dynamic_lr.html" />
    <link rel="prev" title="mindspore.hub" href="mindspore.hub.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dtype.html">mindspore.dtype</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.hub.html">mindspore.hub</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.dynamic_lr.html">mindspore.nn.dynamic_lr</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.learning_rate_schedule.html">mindspore.nn.learning_rate_schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.composite.html">mindspore.ops.composite</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.operations.html">mindspore.ops.operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.vision.html">mindspore.dataset.transforms.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.profiler.html">mindspore.profiler</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindInsight Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindinsight/mindinsight.lineagemgr.html">mindinsight.lineagemgr</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.evaluations.html">mindarmour.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.detectors.html">mindarmour.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.attacks.html">mindarmour.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.defenses.html">mindarmour.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.fuzzing.html">mindarmour.fuzzing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.diff_privacy.html">mindarmour.diff_privacy</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.nn</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/api/python/mindspore/mindspore.nn.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-mindspore.nn">
<span id="mindspore-nn"></span><h1>mindspore.nn<a class="headerlink" href="#module-mindspore.nn" title="Permalink to this headline"></a></h1>
<p>Neural Networks Cells.</p>
<p>Pre-defined building blocks or computing units to construct Neural Networks.</p>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Accuracy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eval_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/accuracy.html#Accuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Accuracy" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the accuracy for classification and multilabel data.</p>
<p>The accuracy class creates two local variables, the correct number and the total number that are used to compute the
frequency with which predictions matches labels. This frequency is ultimately returned as the accuracy: an
idempotent operation that simply divides the correct number by the total number.</p>
<div class="math notranslate nohighlight">
\[\text{accuracy} =\frac{\text{true_positive} + \text{true_negative}}
{\text{true_positive} + \text{true_negative} + \text{false_positive} + \text{false_negative}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Metric to calculate the accuracy over a dataset, for
classification (single-label), and multilabel (multilabel classification).
Default: ‘classification’.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">(</span><span class="s1">&#39;classification&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Accuracy.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/accuracy.html#Accuracy.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Accuracy.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clears the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Accuracy.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/accuracy.html#Accuracy.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Accuracy.eval" title="Permalink to this definition"></a></dt>
<dd><p>Computes the accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float, the computed result.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If the sample size is 0.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Accuracy.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/accuracy.html#Accuracy.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Accuracy.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result <span class="math notranslate nohighlight">\(y_{pred}\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input <cite>y_pred</cite> and <cite>y</cite>. <cite>y_pred</cite> and <cite>y</cite> are a <cite>Tensor</cite>, a list or an array.
For the ‘classification’ evaluation type, <cite>y_pred</cite> is in most cases (not strictly) a list
of floating numbers in range <span class="math notranslate nohighlight">\([0, 1]\)</span>
and the shape is <span class="math notranslate nohighlight">\((N, C)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of cases and <span class="math notranslate nohighlight">\(C\)</span>
is the number of categories. Shape of <cite>y</cite> can be <span class="math notranslate nohighlight">\((N, C)\)</span> with values 0 and 1 if one-hot
encoding is used or the shape is <span class="math notranslate nohighlight">\((N,)\)</span> with integer values if index of category is used.
For ‘multilabel’ evaluation type, <cite>y_pred</cite> and <cite>y</cite> can only be one-hot encoding with
values 0 or 1. Indices with 1 indicate the positive category. The shape of <cite>y_pred</cite> and <cite>y</cite>
are both <span class="math notranslate nohighlight">\((N, C)\)</span>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the number of the inputs is not 2.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ActQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ActQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#ActQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ActQuant" title="Permalink to this definition"></a></dt>
<dd><p>Quantization aware training activation function.</p>
<p>Add the fake quant op to the end of activation op, by which the output of activation op will be truncated.
Please check <cite>FakeQuantWithMinMax</cite> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Activation cell class.</p></li>
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global steps. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of ReLU6Quant.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">act_quant</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ActQuant</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">act_quant</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Adam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_locking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_nesterov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/adam.html#Adam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Adam" title="Permalink to this definition"></a></dt>
<dd><p>Updates gradients by the Adaptive Moment Estimation (Adam) algorithm.</p>
<p>The Adam algorithm is proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m = \beta_1 * m + (1 - \beta_1) * g \\
    v = \beta_2 * v + (1 - \beta_2) * g * g \\
    l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\
    w = w - l * \frac{m}{\sqrt{v} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector <cite>moment1</cite>, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector <cite>moment2</cite>,
<span class="math notranslate nohighlight">\(g\)</span> represents <cite>gradients</cite>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent
<cite>beta1</cite> and <cite>beta2</cite>, <span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(beta_1^t\)</span> and <span class="math notranslate nohighlight">\(beta_2^t\)</span> represent
<cite>beta1_power</cite> and <cite>beta2_power</cite>, <span class="math notranslate nohighlight">\(\alpha\)</span> represents <cite>learning_rate</cite>, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>params</cite>,
<span class="math notranslate nohighlight">\(\epsilon\)</span> represents <cite>eps</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters is supported.</p>
<p>The sparse strategy is applied while the SparseGatherV2 operator is used for forward network.
The sparse feature is under continuous development. The sparse
behavior is currently performed on the CPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” is in the keys, the value of the corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” is in the keys, the value of the corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” is in the keys, the value should be the order of parameters and
the order will be followed in the optimizer. There are no other keys in the <cite>dict</cite> and the parameters
which in the ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use the dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 1e-3.</p></li>
<li><p><strong>beta1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 1st moment estimations. Should be in range (0.0, 1.0).
Default: 0.9.</p></li>
<li><p><strong>beta2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 2nd moment estimations. Should be in range (0.0, 1.0).
Default: 0.999.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Should be greater than 0. Default:
1e-8.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect updating variable tensors.
If true, updates of the var, m, and v tensors will be protected by a lock.
If false, the result is unpredictable. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.
If true, update the gradients using NAG.
If false, update the gradients without using NAG. Default: False.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). It should be equal to or greater than 0. Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. Should be greater than 0. Default: 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[bool], the value is True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use default learning rate of 0.1 and weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use learning rate of 0.01 and defaule weight decay of 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.AdamWeightDecay">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">AdamWeightDecay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/adam.html#AdamWeightDecay"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.AdamWeightDecay" title="Permalink to this definition"></a></dt>
<dd><p>Implements the Adam algorithm to fix the weight decay.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” is in the keys, the value of the corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” is in the keys, the value of the corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” is in the keys, the value should be the order of parameters and
the order will be followed in the optimizer. There are no other keys in the <cite>dict</cite> and the parameters
which in the ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use the dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 1e-3.</p></li>
<li><p><strong>beta1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 1st moment estimations. Default: 0.9.
Should be in range (0.0, 1.0).</p></li>
<li><p><strong>beta2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 2nd moment estimations. Default: 0.999.
Should be in range (0.0, 1.0).</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Default: 1e-6.
Should be greater than 0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). It should be equal to or greater than 0. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[bool], all elements are True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdamWeightDecay</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use default learning rate of 0.1 and weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use learning rate of 0.01 and default weight decay of 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.AvgPool1d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">AvgPool1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/pooling.html#AvgPool1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.AvgPool1d" title="Permalink to this definition"></a></dt>
<dd><p>Average pooling for temporal data.</p>
<p>Applies a 1D average pooling over an input Tensor which can be regarded as a composition of 1D input planes.</p>
<p>Typically the input is of shape <span class="math notranslate nohighlight">\((N_{in}, C_{in}, L_{in})\)</span>, AvgPool1d outputs
regional average in the <span class="math notranslate nohighlight">\((L_{in})\)</span>-dimension. Given kernel size
<span class="math notranslate nohighlight">\(ks = l_{ker}\)</span> and stride <span class="math notranslate nohighlight">\(s = s_0\)</span>, the operation is as follows.</p>
<div class="math notranslate nohighlight">
\[\text{output}(N_i, C_j, l) = \frac{1}{l_{ker}} \sum_{n=0}^{l_{ker}-1}
\text{input}(N_i, C_j, s_0 \times l + n)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>pad_mode for training only supports “same” and “valid”.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The size of kernel window used to take the average value, Default: 1.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The distance of kernel moving, an int number that represents
the width of movement is strides, Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The optional value for pad mode, is “same” or “valid”, not case sensitive.
Default: “valid”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible.
Otherwise, the last extra padding will be done from the bottom and the right side.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output
will be returned without padding. Extra pixels will be discarded.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, L_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, L_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 3, 1)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.AvgPool2d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">AvgPool2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/pooling.html#AvgPool2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.AvgPool2d" title="Permalink to this definition"></a></dt>
<dd><p>Average pooling for temporal data.</p>
<p>Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</p>
<p>Typically the input is of shape <span class="math notranslate nohighlight">\((N_{in}, C_{in}, H_{in}, W_{in})\)</span>, AvgPool2d outputs
regional average in the <span class="math notranslate nohighlight">\((H_{in}, W_{in})\)</span>-dimension. Given kernel size
<span class="math notranslate nohighlight">\(ks = (h_{ker}, w_{ker})\)</span> and stride <span class="math notranslate nohighlight">\(s = (s_0, s_1)\)</span>, the operation is as follows.</p>
<div class="math notranslate nohighlight">
\[\text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}
\text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>pad_mode for training only supports “same” and “valid”.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of kernel used to take the average value.
The data type of kernel_size should be int and the value represents the height and width,
or a tuple of two int numbers that represent height and width respectively.
Default: 1.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The optional value for pad mode, is “same” or “valid”, not case sensitive.
Default: “valid”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible.
Otherwise, the last extra padding will be done from the bottom and the right side.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output
will be returned without padding. Extra pixels will be discarded.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="go">[[[[5. 5. 9. 9.]</span>
<span class="go">    [8. 4. 3. 0.]</span>
<span class="go">    [2. 7. 1. 2.]</span>
<span class="go">    [1. 8. 3. 3.]]</span>
<span class="go">   [[6. 8. 2. 4.]</span>
<span class="go">    [3. 0. 2. 1.]</span>
<span class="go">    [0. 8. 9. 7.]</span>
<span class="go">    [2. 1. 4. 9.]]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 2, 2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[[[[4.888889  4.4444447]</span>
<span class="go">   [4.111111  3.4444444]]</span>
<span class="go">  [[4.2222223 4.5555553]</span>
<span class="go">   [3.2222223 4.5555553]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.BatchNorm1d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">BatchNorm1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_mean_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_var_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_batch_statistics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#BatchNorm1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.BatchNorm1d" title="Permalink to this definition"></a></dt>
<dd><p>Batch normalization layer over a 2D input.</p>
<p>Batch Normalization is widely used in convolutional networks. This layer
applies Batch Normalization over a 2D input (a mini-batch of 1D inputs) to
reduce internal covariate shift as described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift</a>. It
rescales and recenters the feature using a mini-batch of data and
the learned parameters which can be described in the following formula.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The implementation of BatchNorm is different in graph mode and pynative mode, therefore the mode is not
recommended to be changed after net was initilized.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – <cite>C</cite> from an expected input of size (N, C).</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A value added to the denominator for numerical stability. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating hyperparameter of the momentum for the
running_mean and running_var computation. Default: 0.9.</p></li>
<li><p><strong>affine</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – A bool value. When set to True, gamma and beta can be learned. Default: True.</p></li>
<li><p><strong>gamma_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the gamma weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>beta_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the beta weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>moving_mean_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the moving mean.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>moving_var_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the moving variance.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>use_batch_statistics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, use the mean value and variance value of current batch data. If false,
use the mean value and variance value of specified value. If None, the training process will use the mean
and variance of current batch data and track the running mean and variance, the evaluation process will use
the running mean and variance. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the normalized, scaled, offset tensor, of shape <span class="math notranslate nohighlight">\((N, C_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.BatchNorm2d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">BatchNorm2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_mean_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_var_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_batch_statistics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#BatchNorm2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.BatchNorm2d" title="Permalink to this definition"></a></dt>
<dd><p>Batch normalization layer over a 4D input.</p>
<p>Batch Normalization is widely used in convolutional networks. This layer
applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with
additional channel dimension) to avoid internal covariate shift as described
in the paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift</a>. It
rescales and recenters the feature using a mini-batch of data and
the learned parameters which can be described in the following formula.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The implementation of BatchNorm is different in graph mode and pynative mode, therefore that mode can not be
changed after net was initilized.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – <cite>C</cite> from an expected input of size (N, C, H, W).</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A value added to the denominator for numerical stability. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating hyperparameter of the momentum for the
running_mean and running_var computation. Default: 0.9.</p></li>
<li><p><strong>affine</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – A bool value. When set to True, gamma and beta can be learned. Default: True.</p></li>
<li><p><strong>gamma_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the gamma weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>beta_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the beta weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>moving_mean_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the moving mean.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>moving_var_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the moving variance.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>use_batch_statistics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, use the mean value and variance value of current batch data. If false,
use the mean value and variance value of specified value. If None, the training process will use the mean
and variance of current batch data and track the running mean and variance, the evaluation process will use
the running mean and variance. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the normalized, scaled, offset tensor, of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Cell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Cell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">auto_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flags</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell" title="Permalink to this definition"></a></dt>
<dd><p>Base class for all neural networks.</p>
<p>A ‘Cell’ could be a single neural network cell, such as conv2d, relu, batch_norm, etc. or a composition of
cells to constructing a network.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, the autograd algorithm will automatically generate the implementation of the gradient function,
but if bprop method is implemented, the gradient function
will be replaced by the bprop. The bprop implementation will receive a Tensor <cite>dout</cite> containing the gradient
of the loss w.r.t. the output, and a Tensor <cite>out</cite> containing the forward result. The bprop needs to compute the
gradient of the loss w.r.t. the inputs, gradient of the loss w.r.t. Parameter variables are not supported
currently.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>auto_prefix</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Recursively generate namespaces. Default: True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyCell</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">super</span><span class="p">(</span><span class="n">MyCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="mindspore.nn.Cell.bprop_debug">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">bprop_debug</span></span><a class="headerlink" href="#mindspore.nn.Cell.bprop_debug" title="Permalink to this definition"></a></dt>
<dd><p>Get whether cell custom bprop debug is enabled.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.cast_param">
<span class="sig-name descname"><span class="pre">cast_param</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.cast_param"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.cast_param" title="Permalink to this definition"></a></dt>
<dd><p>Cast parameter according to auto mix precison level in pynative mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param</strong> (<a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a>) – The parameter to cast.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.cells">
<span class="sig-name descname"><span class="pre">cells</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.cells"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.cells" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over immediate cells.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.cells_and_names">
<span class="sig-name descname"><span class="pre">cells_and_names</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cells</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.cells_and_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.cells_and_names" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over all cells in the network.</p>
<p>Includes the cell’s name and itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cells</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Cells to iterate over. Default: None.</p></li>
<li><p><strong>name_prefix</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Namespace. Default: ‘’.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">names</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">n</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.compile" title="Permalink to this definition"></a></dt>
<dd><p>Compiles cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – Input parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.compile_and_run">
<span class="sig-name descname"><span class="pre">compile_and_run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.compile_and_run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.compile_and_run" title="Permalink to this definition"></a></dt>
<dd><p>Compiles and runs cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – Input parameters.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Object, the result of executing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.construct">
<span class="sig-name descname"><span class="pre">construct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.construct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.construct" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation to be performed.</p>
<p>This method should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The inputs of the top cell only allow Tensor.
Other types (tuple, list, int etc.) are forbidden.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tensor, returns the computed result.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.exec_checkpoint_graph">
<span class="sig-name descname"><span class="pre">exec_checkpoint_graph</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.exec_checkpoint_graph"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.exec_checkpoint_graph" title="Permalink to this definition"></a></dt>
<dd><p>Executes saving checkpoint graph operation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.extend_repr">
<span class="sig-name descname"><span class="pre">extend_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.extend_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.extend_repr" title="Permalink to this definition"></a></dt>
<dd><p>Sets the extended representation of the Cell.</p>
<p>To print customized extended information, re-implement this method in your own cells.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.generate_scope">
<span class="sig-name descname"><span class="pre">generate_scope</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.generate_scope"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.generate_scope" title="Permalink to this definition"></a></dt>
<dd><p>Generate the scope for each cell object in the network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.get_func_graph_proto">
<span class="sig-name descname"><span class="pre">get_func_graph_proto</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.get_func_graph_proto"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.get_func_graph_proto" title="Permalink to this definition"></a></dt>
<dd><p>Return graph binary proto.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.get_parameters">
<span class="sig-name descname"><span class="pre">get_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.get_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.get_parameters" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over cell parameters.</p>
<p>Yields parameters of this cell. If <cite>expand</cite> is True, yield parameters of this cell and all subcells.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>expand</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, yields parameters of this cell and all subcells. Otherwise, only yield parameters
that are direct members of this cell. Default: True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.get_scope">
<span class="sig-name descname"><span class="pre">get_scope</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.get_scope"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.get_scope" title="Permalink to this definition"></a></dt>
<dd><p>Returns the scope of a cell object in one network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.init_parameters_data">
<span class="sig-name descname"><span class="pre">init_parameters_data</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">auto_parallel_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.init_parameters_data"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.init_parameters_data" title="Permalink to this definition"></a></dt>
<dd><p>Initialize all parameters and replace the original saved parameters in cell.</p>
<p class="rubric">Notes</p>
<p>trainable_params() and other similar interfaces may return different parameter instance after
<cite>init_parameters_data</cite>, do not save these result.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>auto_parallel_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If running in auto_parallel_mode.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict[Parameter, Parameter], returns a dict of original parameter and replaced parameter.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.insert_child_to_cell">
<span class="sig-name descname"><span class="pre">insert_child_to_cell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">child_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">child</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.insert_child_to_cell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.insert_child_to_cell" title="Permalink to this definition"></a></dt>
<dd><p>Adds a child cell to the current cell.</p>
<p>Inserts a subcell with a given name to the current cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>child_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the child cell.</p></li>
<li><p><strong>child</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The child cell to be inserted.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#KeyError" title="(in Python v3.8)"><strong>KeyError</strong></a> – Child Cell’s name is incorrect or duplicated with the other child name.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – Child Cell’s type is incorrect.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.insert_param_to_cell">
<span class="sig-name descname"><span class="pre">insert_param_to_cell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.insert_param_to_cell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.insert_param_to_cell" title="Permalink to this definition"></a></dt>
<dd><p>Adds a parameter to the current cell.</p>
<p>Inserts a parameter with given name to the cell. Please refer to the usage in
source code of <cite>mindspore.nn.Cell.__setattr__</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the parameter.</p></li>
<li><p><strong>param</strong> (<a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a>) – Parameter to be inserted to the cell.</p></li>
<li><p><strong>check_name</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Determines whether the name input is compatible. Default: True.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#KeyError" title="(in Python v3.8)"><strong>KeyError</strong></a> – If the name of parameter is null or contains dot.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#AttributeError" title="(in Python v3.8)"><strong>AttributeError</strong></a> – If user did not call init() first.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If the type of parameter is not Parameter.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.load_parameter_slice">
<span class="sig-name descname"><span class="pre">load_parameter_slice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.load_parameter_slice"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.load_parameter_slice" title="Permalink to this definition"></a></dt>
<dd><p>Replace parameters with sliced tensors by parallel strategies.</p>
<p>Please refer to the usage in source code of <cite>mindspore.common._Executor.compile</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>params</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – The parameters dictionary used for initializing the data graph.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.name_cells">
<span class="sig-name descname"><span class="pre">name_cells</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.name_cells"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.name_cells" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over all cells in the network.</p>
<p>Include name of the cell and cell itself.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.nn.Cell.param_prefix">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_prefix</span></span><a class="headerlink" href="#mindspore.nn.Cell.param_prefix" title="Permalink to this definition"></a></dt>
<dd><p>Param prefix is the prefix of current cell’s direct child parameter.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.parameters_and_names">
<span class="sig-name descname"><span class="pre">parameters_and_names</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expand</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.parameters_and_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.parameters_and_names" title="Permalink to this definition"></a></dt>
<dd><p>Returns an iterator over cell parameters.</p>
<p>Includes the parameter’s name  and itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name_prefix</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Namespace. Default: ‘’.</p></li>
<li><p><strong>expand</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, yields parameters of this cell and all subcells. Otherwise, only yield parameters
that are direct members of this cell. Default: True.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">names</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">n</span><span class="o">.</span><span class="n">parameters_and_names</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.parameters_dict">
<span class="sig-name descname"><span class="pre">parameters_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.parameters_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.parameters_dict" title="Permalink to this definition"></a></dt>
<dd><p>Gets parameters dictionary.</p>
<p>Gets the parameters dictionary of this cell.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether contains the parameters of subcells. Default: True.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>OrderedDict, return parameters dictionary.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.register_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.register_backward_hook" title="Permalink to this definition"></a></dt>
<dd><p>Set the cell backward hook function. Note that this function is only supported in Pynative Mode.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>fn should be defined as the following code. <cite>cell_name</cite> is the name of registered cell.
<cite>grad_input</cite> is gradient passed to the cell. <cite>grad_output</cite> is the gradient computed and passed to the
next cell or primitve, which may be modified and returned.
&gt;&gt;&gt; hook_fn(cell_name, grad_input, grad_output) -&gt; Tensor or None</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<em>function</em>) – Specifies the hook function with grad as input.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.set_auto_parallel">
<span class="sig-name descname"><span class="pre">set_auto_parallel</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.set_auto_parallel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.set_auto_parallel" title="Permalink to this definition"></a></dt>
<dd><p>Set the cell to auto parallel mode.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a cell needs to use the auto parallel or semi auto parallel mode for training, evaluation or prediction,
this interface needs to be called by the cell.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.set_broadcast_flag">
<span class="sig-name descname"><span class="pre">set_broadcast_flag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.set_broadcast_flag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.set_broadcast_flag" title="Permalink to this definition"></a></dt>
<dd><p>Set the cell to data_parallel mode.</p>
<p>The cell can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the model is data_parallel. Default: True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.set_grad">
<span class="sig-name descname"><span class="pre">set_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.set_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.set_grad" title="Permalink to this definition"></a></dt>
<dd><p>Sets the cell flag for gradient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies if the net need to grad, if it is
True, cell will construct backward network in pynative mode. Default: True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.set_parallel_input_with_inputs">
<span class="sig-name descname"><span class="pre">set_parallel_input_with_inputs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.set_parallel_input_with_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.set_parallel_input_with_inputs" title="Permalink to this definition"></a></dt>
<dd><p>Slice inputs tensors by parallel strategies, and set the sliced inputs to <cite>_parallel_input_run</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – inputs of construct method.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.set_param_ps">
<span class="sig-name descname"><span class="pre">set_param_ps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_in_server</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.set_param_ps"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.set_param_ps" title="Permalink to this definition"></a></dt>
<dd><p>Set whether the trainable parameter is updated by parameter server.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It only works when a running task is in the parameter server mode.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether sets the trainable parameters of subcells. Default: True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.set_train">
<span class="sig-name descname"><span class="pre">set_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.set_train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.set_train" title="Permalink to this definition"></a></dt>
<dd><p>Sets the cell to training mode.</p>
<p>The cell itself and all children cells will be set to training mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the model is training. Default: True.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.to_float">
<span class="sig-name descname"><span class="pre">to_float</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.to_float"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.to_float" title="Permalink to this definition"></a></dt>
<dd><p>Add cast on all inputs of cell and child cells to run with certain float type.</p>
<p>If <cite>dst_type is mindspore.dtype.float16</cite>, all the inputs of Cell including input, Parameter, Tensor
as const will be cast to float16. Please refer to the usage in source code of
<cite>mindspore.train.amp.build_train_network</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Multiple calls will overwrite.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dst_type</strong> (<a class="reference internal" href="mindspore.dtype.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – Transfer Cell to Run with dst_type.
dst_type can be <cite>mindspore.dtype.float16</cite> or <cite>mindspore.dtype.float32</cite>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If dst_type is not float32 nor float16.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.trainable_params">
<span class="sig-name descname"><span class="pre">trainable_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.trainable_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.trainable_params" title="Permalink to this definition"></a></dt>
<dd><p>Returns all trainable parameters.</p>
<p>Returns a list of all trainable parmeters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether contains the trainable parameters of subcells. Default: True.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List, the list of trainable parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.untrainable_params">
<span class="sig-name descname"><span class="pre">untrainable_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.untrainable_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.untrainable_params" title="Permalink to this definition"></a></dt>
<dd><p>Returns all untrainable parameters.</p>
<p>Returns a list of all untrainable parmeters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether contains the untrainable parameters of subcells. Default: True.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List, the list of untrainable parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.update_cell_prefix">
<span class="sig-name descname"><span class="pre">update_cell_prefix</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.update_cell_prefix"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.update_cell_prefix" title="Permalink to this definition"></a></dt>
<dd><p>Update the all child cells’ self.param_prefix.</p>
<p>After being invoked, it can get all the cell’s children’s name prefix by ‘_param_prefix’.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.update_cell_type">
<span class="sig-name descname"><span class="pre">update_cell_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cell_type</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.update_cell_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.update_cell_type" title="Permalink to this definition"></a></dt>
<dd><p>The current cell type is updated when a quantization aware training network is encountered.</p>
<p>After being invoked, it can set the cell type to ‘cell_type’.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Cell.update_parameters_name">
<span class="sig-name descname"><span class="pre">update_parameters_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#Cell.update_parameters_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Cell.update_parameters_name" title="Permalink to this definition"></a></dt>
<dd><p>Updates the names of parameters with given prefix string.</p>
<p>Adds the given prefix to the names of parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The prefix string.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether contains the parameters of subcells. Default: True.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.CellList">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">CellList</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/container.html#CellList"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.CellList" title="Permalink to this definition"></a></dt>
<dd><p>Holds Cells in a list.</p>
<p>CellList can be used like a regular Python list, support
‘__getitem__’, ‘__setitem__’, ‘__delitem__’, ‘__len__’, ‘__iter__’ and ‘__iadd__’,
but cells it contains are properly registered, and will be visible by all Cell methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>optional</em>) – List of subclass of Cell.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cell_ls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">([</span><span class="n">bn</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cell_ls</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">conv</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cell_ls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># not same as nn.SequentialCell, `cell_ls(x)` is not correct</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cell_ls</span>
<span class="go">CellList&lt; (0): Conv2d&lt;input_channels=100, ..., bias_init=None&gt;</span>
<span class="go">          (1): BatchNorm2d&lt;num_features=20, ..., moving_variance=Parameter (name=variance)&gt;</span>
<span class="go">          (2): ReLU&lt;&gt; &gt;</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.CellList.append">
<span class="sig-name descname"><span class="pre">append</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cell</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/container.html#CellList.append"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.CellList.append" title="Permalink to this definition"></a></dt>
<dd><p>Appends a given cell to the end of the list.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.CellList.extend">
<span class="sig-name descname"><span class="pre">extend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cells</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/container.html#CellList.extend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.CellList.extend" title="Permalink to this definition"></a></dt>
<dd><p>Appends cells from a Python iterable to the end of the list.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If the cells are not a list of subcells.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.CellList.insert">
<span class="sig-name descname"><span class="pre">insert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/container.html#CellList.insert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.CellList.insert" title="Permalink to this definition"></a></dt>
<dd><p>Inserts a given cell before a given index in the list.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.CentralCrop">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">CentralCrop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">central_fraction</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/image.html#CentralCrop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.CentralCrop" title="Permalink to this definition"></a></dt>
<dd><p>Crop the centeral region of the images with the central_fraction.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>central_fraction</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Fraction of size to crop. It must be float and in range (0.0, 1.0].</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>image</strong> (Tensor) - A 3-D tensor of shape [C, H, W], or a 4-D tensor of shape [N, C, H, W].</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, 3-D or 4-D float tensor, according to the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CentralCrop</span><span class="p">(</span><span class="n">central_fraction</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ClipByNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ClipByNorm</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#ClipByNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ClipByNorm" title="Permalink to this definition"></a></dt>
<dd><p>Clips tensor values to a maximum <span class="math notranslate nohighlight">\(L_2\)</span>-norm.</p>
<p>The output of this layer remains the same if the <span class="math notranslate nohighlight">\(L_2\)</span>-norm of the input tensor
is not greater than the argument clip_norm. Otherwise the tensor will be normalized as:</p>
<div class="math notranslate nohighlight">
\[\text{output}(X) = \frac{\text{clip_norm} * X}{L_2(X)},\]</div>
<p>where <span class="math notranslate nohighlight">\(L_2(X)\)</span> is the <span class="math notranslate nohighlight">\(L_2\)</span>-norm of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape N-D. The type should be float32 or float16.</p></li>
<li><p><strong>clip_norm</strong> (Tensor) - A scalar Tensor of shape <span class="math notranslate nohighlight">\(()\)</span> or <span class="math notranslate nohighlight">\((1)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, clipped tensor with the same shape as the input, whose type is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ClipByNorm</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clip_norm</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">100</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">clip_norm</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.ClipByNorm.construct">
<span class="sig-name descname"><span class="pre">construct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_norm</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#ClipByNorm.construct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ClipByNorm.construct" title="Permalink to this definition"></a></dt>
<dd><p>add ms_function decorator for pynative mode</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv1d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/conv.html#Conv1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv1d" title="Permalink to this definition"></a></dt>
<dd><p>1D convolution layer.</p>
<p>Applies a 1D convolution over an input tensor which is typically of shape <span class="math notranslate nohighlight">\((N, C_{in}, W_{in})\)</span>,
where <span class="math notranslate nohighlight">\(N\)</span> is batch size and <span class="math notranslate nohighlight">\(C_{in}\)</span> is channel number. For each batch of shape
<span class="math notranslate nohighlight">\((C_{in}, W_{in})\)</span>, the formula is defined as:</p>
<div class="math notranslate nohighlight">
\[out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,\]</div>
<p>where <span class="math notranslate nohighlight">\(ccor\)</span> is the cross correlation operator, <span class="math notranslate nohighlight">\(C_{in}\)</span> is the input channel number, <span class="math notranslate nohighlight">\(j\)</span> ranges
from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(C_{out} - 1\)</span>, <span class="math notranslate nohighlight">\(W_{ij}\)</span> corresponds to the <span class="math notranslate nohighlight">\(i\)</span>-th channel of the <span class="math notranslate nohighlight">\(j\)</span>-th
filter and <span class="math notranslate nohighlight">\(out_{j}\)</span> corresponds to the <span class="math notranslate nohighlight">\(j\)</span>-th channel of the output. <span class="math notranslate nohighlight">\(W_{ij}\)</span> is a slice
of kernel and it has shape <span class="math notranslate nohighlight">\((\text{ks_w})\)</span>, where <span class="math notranslate nohighlight">\(\text{ks_w}\)</span> is the width of the convolution kernel.
The full kernel has shape <span class="math notranslate nohighlight">\((C_{out}, C_{in} // \text{group}, \text{ks_w})\)</span>, where group is the group number
to split the input in the channel dimension.</p>
<p>If the ‘pad_mode’ is set to be “valid”, the output width will be
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -
(\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> respectively.</p>
<p>The first introduction of convolution layer can be found in paper <a class="reference external" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">Gradient Based Learning Applied to Document
Recognition</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The data type is int. Specifies the
width of the 1D convolution window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The distance of kernel moving, an int number that represents
the width of movement. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Specifies padding mode. The optional values are
“same”, “valid”, “pad”. Default: “same”.</p>
<ul>
<li><p>same: Adopts the way of completion. The output width will be the same as the input.
The total number of padding will be calculated in the horizontal
direction and evenly distributed to left and right if possible. Otherwise, the
last extra padding will be done from the bottom and the right side. If this mode is set, <cite>padding</cite>
must be 0.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest width of the output will be returned
without padding. Extra pixels will be discarded. If this mode is set, <cite>padding</cite>
must be 0.</p></li>
<li><p>pad: Implicit paddings on both sides of the input. The number of <cite>padding</cite> will be padded to the input
Tensor borders. <cite>padding</cite> should be greater than or equal to 0.</p></li>
</ul>
</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The data type is int. Specifies the dilation rate
to use for dilated convolution. If set to be <span class="math notranslate nohighlight">\(k &gt; 1\)</span>, there will
be <span class="math notranslate nohighlight">\(k - 1\)</span> pixels skipped for each sampling location. Its value should
be greater or equal to 1 and bounded by the height and width of the
input. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_ channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – An initializer for the convolution kernel.
It can be a Tensor, a string, an Initializer or a number. When a string is specified,
values from ‘TruncatedNormal’, ‘Normal’, ‘Uniform’, ‘HeUniform’ and ‘XavierUniform’ distributions as well
as constant ‘One’ and ‘Zero’ distributions are possible. Alias ‘xavier_uniform’, ‘he_uniform’, ‘ones’
and ‘zeros’ are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of
Initializer for more details. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Possible
Initializer and string are the same as ‘weight_init’. Refer to the values of
Initializer for more details. Default: ‘zeros’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">240</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">640</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 240, 640)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv1dTranspose">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv1dTranspose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/conv.html#Conv1dTranspose"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv1dTranspose" title="Permalink to this definition"></a></dt>
<dd><p>1D transposed convolution layer.</p>
<p>Compute a 1D transposed convolution, which is also known as a deconvolution
(although it is not an actual deconvolution).</p>
<p>Input is typically of shape <span class="math notranslate nohighlight">\((N, C, W)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is batch size and <span class="math notranslate nohighlight">\(C\)</span> is channel number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the input space.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the output space.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – int, which specifies the width of the 1D convolution window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The distance of kernel moving, an int number that represents
the width of movement. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Select the mode of the pad. The optional values are
“pad”, “same”, “valid”. Default: “same”.</p>
<ul>
<li><p>pad: Implicit paddings on both sides of the input.</p></li>
<li><p>same: Adopted the way of completion.</p></li>
<li><p>valid: Adopted the way of discarding.</p></li>
</ul>
</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The data type is int. Specifies the dilation rate
to use for dilated convolution. If set to be <span class="math notranslate nohighlight">\(k &gt; 1\)</span>, there will
be <span class="math notranslate nohighlight">\(k - 1\)</span> pixels skipped for each sampling location. Its value should
be greater or equal to 1 and bounded by the width of the
input. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. This is not support for Davinci devices when group &gt; 1. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
It can be a Tensor, a string, an Initializer or a numbers.Number. When a string is specified,
values from ‘TruncatedNormal’, ‘Normal’, ‘Uniform’, ‘HeUniform’ and ‘XavierUniform’ distributions as well
as constant ‘One’ and ‘Zero’ distributions are possible. Alias ‘xavier_uniform’, ‘he_uniform’, ‘ones’
and ‘zeros’ are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of
Initializer for more details. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Possible
Initializer and string are the same as ‘weight_init’. Refer to the values of
Initializer for more details. Default: ‘zeros’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1dTranspose</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv2d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/conv.html#Conv2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv2d" title="Permalink to this definition"></a></dt>
<dd><p>2D convolution layer.</p>
<p>Applies a 2D convolution over an input tensor which is typically of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>,
where <span class="math notranslate nohighlight">\(N\)</span> is batch size and <span class="math notranslate nohighlight">\(C_{in}\)</span> is channel number. For each batch of shape
<span class="math notranslate nohighlight">\((C_{in}, H_{in}, W_{in})\)</span>, the formula is defined as:</p>
<div class="math notranslate nohighlight">
\[out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,\]</div>
<p>where <span class="math notranslate nohighlight">\(ccor\)</span> is the cross correlation operator, <span class="math notranslate nohighlight">\(C_{in}\)</span> is the input channel number, <span class="math notranslate nohighlight">\(j\)</span> ranges
from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(C_{out} - 1\)</span>, <span class="math notranslate nohighlight">\(W_{ij}\)</span> corresponds to the <span class="math notranslate nohighlight">\(i\)</span>-th channel of the <span class="math notranslate nohighlight">\(j\)</span>-th
filter and <span class="math notranslate nohighlight">\(out_{j}\)</span> corresponds to the <span class="math notranslate nohighlight">\(j\)</span>-th channel of the output. <span class="math notranslate nohighlight">\(W_{ij}\)</span> is a slice
of kernel and it has shape <span class="math notranslate nohighlight">\((\text{ks_h}, \text{ks_w})\)</span>, where <span class="math notranslate nohighlight">\(\text{ks_h}\)</span> and
<span class="math notranslate nohighlight">\(\text{ks_w}\)</span> are the height and width of the convolution kernel. The full kernel has shape
<span class="math notranslate nohighlight">\((C_{out}, C_{in} // \text{group}, \text{ks_h}, \text{ks_w})\)</span>, where group is the group number
to split the input in the channel dimension.</p>
<p>If the ‘pad_mode’ is set to be “valid”, the output height and width will be
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -
(\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> and
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -
(\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> respectively.</p>
<p>The first introduction can be found in paper <a class="reference external" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">Gradient Based Learning Applied to Document Recognition</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The data type is int or a tuple of 2 integers. Specifies the height
and width of the 2D convolution window. Single int means the value is for both the height and the width of
the kernel. A tuple of 2 ints means the first value is for the height and the other is for the
width of the kernel.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Specifies padding mode. The optional values are
“same”, “valid”, “pad”. Default: “same”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible. Otherwise, the
last extra padding will be done from the bottom and the right side. If this mode is set, <cite>padding</cite>
must be 0.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output will be returned
without padding. Extra pixels will be discarded. If this mode is set, <cite>padding</cite>
must be 0.</p></li>
<li><p>pad: Implicit paddings on both sides of the input. The number of <cite>padding</cite> will be padded to the input
Tensor borders. <cite>padding</cite> should be greater than or equal to 0.</p></li>
</ul>
</p></li>
<li><p><strong>padding</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – Implicit paddings on both sides of the input. If <cite>padding</cite> is one integer,
the padding of top, bottom, left and right is the same, equal to padding. If <cite>padding</cite> is a tuple
with four integers, the padding of top, bottom, left and right will be equal to padding[0],
padding[1], padding[2], and padding[3] accordingly. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The data type is int or a tuple of 2 integers. Specifies the dilation rate
to use for dilated convolution. If set to be <span class="math notranslate nohighlight">\(k &gt; 1\)</span>, there will
be <span class="math notranslate nohighlight">\(k - 1\)</span> pixels skipped for each sampling location. Its value should
be greater or equal to 1 and bounded by the height and width of the
input. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_ channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
It can be a Tensor, a string, an Initializer or a number. When a string is specified,
values from ‘TruncatedNormal’, ‘Normal’, ‘Uniform’, ‘HeUniform’ and ‘XavierUniform’ distributions as well
as constant ‘One’ and ‘Zero’ distributions are possible. Alias ‘xavier_uniform’, ‘he_uniform’, ‘ones’
and ‘zeros’ are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of
Initializer for more details. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Possible
Initializer and string are the same as ‘weight_init’. Refer to the values of
Initializer for more details. Default: ‘zeros’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">240</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">640</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 240, 1024, 640)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv2dBnAct">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv2dBnAct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">after_fake</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#Conv2dBnAct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv2dBnAct" title="Permalink to this definition"></a></dt>
<dd><p>A combination of convolution, Batchnorm, activation layer.</p>
<p>This part is a more detailed overview of Conv2d op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – The data type is int or a tuple of 2 integers. Specifies the height
and width of the 2D convolution window. Single int means the value is for both height and width of
the kernel. A tuple of 2 ints means the first value is for the height and the other is for the
width of the kernel.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies stride for all spatial dimensions with the same value. The value of stride should be
greater than or equal to 1 and lower than any one of the height and width of the input. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies padding mode. The optional values are “same”, “valid”, “pad”. Default: “same”.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifying the dilation rate to use for dilated convolution. If set to be <span class="math notranslate nohighlight">\(k &gt; 1\)</span>,
there will be <span class="math notranslate nohighlight">\(k - 1\)</span> pixels skipped for each sampling location. Its value should be greater than
or equal to 1 and lower than any one of the height and width of the input. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_ channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
It can be a Tensor, a string, an Initializer or a number. When a string is specified,
values from ‘TruncatedNormal’, ‘Normal’, ‘Uniform’, ‘HeUniform’ and ‘XavierUniform’ distributions as well
as constant ‘One’ and ‘Zero’ distributions are possible. Alias ‘xavier_uniform’, ‘he_uniform’, ‘ones’
and ‘zeros’ are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of
Initializer for more details. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Possible
Initializer and string are the same as ‘weight_init’. Refer to the values of
Initializer for more details. Default: ‘zeros’.</p></li>
<li><p><strong>has_bn</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies to used batchnorm or not. Default: False.</p></li>
<li><p><strong>activation</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Specifies activation type. The optional values are as following:
‘softmax’, ‘logsoftmax’, ‘relu’, ‘relu6’, ‘tanh’, ‘gelu’, ‘sigmoid’,
‘prelu’, ‘leakyrelu’, ‘hswish’, ‘hsigmoid’. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Conv2dBnAct</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">240</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">has_bn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">640</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 240, 1024, 640)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv2dBnFoldQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv2dBnFoldQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.997</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fake</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_bn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100000</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#Conv2dBnFoldQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv2dBnFoldQuant" title="Permalink to this definition"></a></dt>
<dd><p>2D convolution with BatchNormal op folded construct.</p>
<p>This part is a more detailed overview of Conv2d op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – Specifies the height and width of the 2D convolution window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies stride for all spatial dimensions with the same value.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies padding mode. The optional values are “same”, “valid”, “pad”. Default: “same”.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Parameters for BatchNormal. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Parameters for BatchNormal op. Default: 0.997.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the
convolution kernel. Default: ‘normal’.</p></li>
<li><p><strong>beta_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the
beta vector. Default: ‘zeros’.</p></li>
<li><p><strong>gamma_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the
gamma vector. Default: ‘ones’.</p></li>
<li><p><strong>mean_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the
mean vector. Default: ‘zeros’.</p></li>
<li><p><strong>var_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the
variance vector. Default: ‘ones’.</p></li>
<li><p><strong>fake</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether Conv2dBnFoldQuant Cell adds FakeQuantWithMinMax op. Default: True.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – FakeQuantWithMinMax Parameters. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The Quantization delay parameters according to the global step. Default: 0.</p></li>
<li><p><strong>freeze_bn</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The quantization freeze BatchNormal op is according to the global step. Default: 100000.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d_bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2dBnFoldQuant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">conv2d_bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv2dBnWithoutFoldQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv2dBnWithoutFoldQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.997</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#Conv2dBnWithoutFoldQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv2dBnWithoutFoldQuant" title="Permalink to this definition"></a></dt>
<dd><p>2D convolution + batchnorm without fold with fake quant construct.</p>
<p>This part is a more detailed overview of Conv2d op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – Specifies the height and width of the 2D convolution window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies stride for all spatial dimensions with the same value. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies padding mode. The optional values are “same”, “valid”, “pad”. Default: “same”.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifying the dilation rate to use for dilated convolution. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_ channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Parameters for BatchNormal. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Parameters for BatchNormal op. Default: 0.997.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Default: ‘zeros’.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – FakeQuantWithMinMax Parameters. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d_quant</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2dBnWithoutFoldQuant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">conv2d_quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv2dQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv2dQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#Conv2dQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv2dQuant" title="Permalink to this definition"></a></dt>
<dd><p>2D convolution with fake quant op layer.</p>
<p>This part is a more detailed overview of Conv2d op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – Specifies the height and width of the 2D convolution window.</p></li>
<li><p><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies stride for all spatial dimensions with the same value. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies padding mode. The optional values are “same”, “valid”, “pad”. Default: “same”.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifying the dilation rate to use for dilated convolution. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_ channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Default: ‘zeros’.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – FakeQuantWithMinMax Parameters. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d_quant</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2dQuant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">conv2d_quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Conv2dTranspose">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Conv2dTranspose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/conv.html#Conv2dTranspose"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Conv2dTranspose" title="Permalink to this definition"></a></dt>
<dd><p>2D transposed convolution layer.</p>
<p>Compute a 2D transposed convolution, which is also known as a deconvolution
(although it is not an actual deconvolution).</p>
<p>Input is typically of shape <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is batch size and <span class="math notranslate nohighlight">\(C\)</span> is channel number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the input space.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the output space.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – int or a tuple of 2 integers, which specifies the  height
and width of the 2D convolution window. Single int means the value is for both the height and the width of
the kernel. A tuple of 2 ints means the first value is for the height and the other is for the
width of the kernel.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Select the mode of the pad. The optional values are
“pad”, “same”, “valid”. Default: “same”.</p>
<ul>
<li><p>pad: Implicit paddings on both sides of the input.</p></li>
<li><p>same: Adopted the way of completion.</p></li>
<li><p>valid: Adopted the way of discarding.</p></li>
</ul>
</p></li>
<li><p><strong>padding</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – Implicit paddings on both sides of the input. If <cite>padding</cite> is one integer,
the padding of top, bottom, left and right is the same, equal to padding. If <cite>padding</cite> is a tuple
with four integers, the padding of top, bottom, left and right will be equal to padding[0],
padding[1], padding[2], and padding[3] accordingly. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The data type is int or a tuple of 2 integers. Specifies the dilation rate
to use for dilated convolution. If set to be <span class="math notranslate nohighlight">\(k &gt; 1\)</span>, there will
be <span class="math notranslate nohighlight">\(k - 1\)</span> pixels skipped for each sampling location. Its value should
be greater than or equal to 1 and bounded by the height and width of the
input. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. This does not support for Davinci devices when group &gt; 1. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
It can be a Tensor, a string, an Initializer or a number. When a string is specified,
values from ‘TruncatedNormal’, ‘Normal’, ‘Uniform’, ‘HeUniform’ and ‘XavierUniform’ distributions as well
as constant ‘One’ and ‘Zero’ distributions are possible. Alias ‘xavier_uniform’, ‘he_uniform’, ‘ones’
and ‘zeros’ are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of
Initializer for more details. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Possible
Initializer and string are the same as ‘weight_init’. Refer to the values of
Initializer for more details. Default: ‘zeros’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2dTranspose</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.CosineEmbeddingLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">CosineEmbeddingLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/loss/loss.html#CosineEmbeddingLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.CosineEmbeddingLoss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the similarity between two tensors using cosine distance.</p>
<p>Given two tensors <cite>x1</cite>, <cite>x2</cite>, and a Tensor label <cite>y</cite> with values 1 or -1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}loss(x_1, x_2, y) = \begin{cases}
1-cos(x_1, x_2), &amp; \text{if } y = 1\\
max(0, cos(x_1, x_2)-margin), &amp; \text{if } y = -1\\
\end{cases}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>margin</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Should be in [-1.0, 1.0]. Default 0.0.</p></li>
<li><p><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies which reduction to be applied to the output. It should be one of
“none”, “mean”, and “sum”, meaning no reduction, reduce mean and sum on output, respectively. Default “mean”.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x1</strong> (Tensor) - Input tensor.</p></li>
<li><p><strong>input_x2</strong> (Tensor) - Its shape and data type should be the same as <cite>input_x1</cite>’s shape and data type.</p></li>
<li><p><strong>y</strong> (Tensor) - Contains value 1 or -1. Suppose the shape of <cite>input_x1</cite> is
<span class="math notranslate nohighlight">\((x_1, x_2, x_3,..., x_R)\)</span>, then the shape of <cite>target</cite> should be <span class="math notranslate nohighlight">\((x_1, x_3, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - If <cite>reduction</cite> is “none”, its shape is the same as <cite>y</cite>’s shape, otherwise a scalar value
will be returned.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cosine_embedding_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CosineEmbeddingLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">[0.0003426671]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DataWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DataWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_types</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_shapes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#DataWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DataWrapper" title="Permalink to this definition"></a></dt>
<dd><p>Network training package class for dataset.</p>
<p>DataWrapper wraps the input network with a dataset which automatically fetches data with ‘GetNext’
function from the dataset channel ‘queue_name’ and does forward computation in the construct function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network for dataset.</p></li>
<li><p><strong>dataset_types</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The type of dataset. The list contains the types of the inputs.</p></li>
<li><p><strong>dataset_shapes</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The shapes of dataset. The list contains multiple sublists that describe
the shape of the inputs.</p></li>
<li><p><strong>queue_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The identification of dataset channel which specifies the dataset channel to supply
data for the network.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Outputs:</dt><dd><p>Tensor, network output whose shape depends on the network.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># call create_dataset function to create a regular dataset, refer to mindspore.dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset_helper</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">DatasetHelper</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">DataWrapper</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="o">*</span><span class="p">(</span><span class="n">dataset_helper</span><span class="o">.</span><span class="n">types_shapes</span><span class="p">()),</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">queue_name</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Dense">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Dense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#Dense"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Dense" title="Permalink to this definition"></a></dt>
<dd><p>The fully connected layer.</p>
<p>Applies dense-connected layer for the input. This layer implements the operation as:</p>
<div class="math notranslate nohighlight">
\[\text{outputs} = \text{activation}(\text{inputs} * \text{kernel} + \text{bias}),\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{activation}\)</span> is the activation function passed as the activation
argument (if passed in), <span class="math notranslate nohighlight">\(\text{activation}\)</span> is a weight matrix with the same
data type as the inputs created by the layer, and <span class="math notranslate nohighlight">\(\text{bias}\)</span> is a bias vector
with the same data type as the inputs created by the layer (only if has_bias is True).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the input space.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the output space.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – The trainable weight_init parameter. The dtype
is same as input x. The values of str refer to the function <cite>initializer</cite>. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – The trainable bias_init parameter. The dtype is
same as input x. The values of str refer to the function <cite>initializer</cite>. Default: ‘zeros’.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: True.</p></li>
<li><p><strong>activation</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – activate function applied to the output of the fully connected layer, eg. ‘ReLU’.
Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If weight_init or bias_init shape is incorrect.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, in\_channels)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, out\_channels)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">[[ 2.5246444   2.2738023   0.5711005  -3.9399147 ]</span>
<span class="go"> [ 1.0739875   4.0155234   0.94188046 -5.459526  ]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DenseBnAct">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DenseBnAct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">after_fake</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#DenseBnAct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DenseBnAct" title="Permalink to this definition"></a></dt>
<dd><p>A combination of Dense, Batchnorm, and the activation layer.</p>
<p>This part is a more detailed overview of Dense op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the input space.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels in the output space.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – The trainable weight_init parameter. The dtype
is same as input x. The values of str refer to the function <cite>initializer</cite>. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – The trainable bias_init parameter. The dtype is
same as input x. The values of str refer to the function <cite>initializer</cite>. Default: ‘zeros’.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: True.</p></li>
<li><p><strong>activation</strong> (<em>string</em>) – The regularization function applied to the output of the layer, eg. ‘ReLU’. Default: None.</p></li>
<li><p><strong>has_bn</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies to use batchnorm or not. Default: False.</p></li>
<li><p><strong>activation</strong> – Specifies activation type. The optional values are as following:
‘Softmax’, ‘LogSoftmax’, ‘ReLU’, ‘ReLU6’, ‘Tanh’, ‘GELU’, ‘Sigmoid’,
‘PReLU’, ‘LeakyReLU’, ‘h-Swish’, and ‘h-Sigmoid’. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, in\_channels)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, out\_channels)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DenseBnAct</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DenseQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DenseQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#DenseQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DenseQuant" title="Permalink to this definition"></a></dt>
<dd><p>The fully connected layer with fake quant op.</p>
<p>This part is a more detailed overview of Dense op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of the input space.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of the output space.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – The trainable weight_init parameter. The dtype
is same as input x. The values of str refer to the function <cite>initializer</cite>. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – The trainable bias_init parameter. The dtype is
same as input x. The values of str refer to the function <cite>initializer</cite>. Default: ‘zeros’.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: True.</p></li>
<li><p><strong>activation</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The regularization function applied to the output of the layer, eg. ‘relu’. Default: None.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – FakeQuantWithMinMax Parameters. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dense_quant</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DenseQuant</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">dense_quant</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.DenseQuant.construct">
<span class="sig-name descname"><span class="pre">construct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#DenseQuant.construct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DenseQuant.construct" title="Permalink to this definition"></a></dt>
<dd><p>Use operators to construct the Dense layer.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.DenseQuant.extend_repr">
<span class="sig-name descname"><span class="pre">extend_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#DenseQuant.extend_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DenseQuant.extend_repr" title="Permalink to this definition"></a></dt>
<dd><p>A pretty print for Dense layer.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DepthwiseConv2d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DepthwiseConv2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/conv.html#DepthwiseConv2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DepthwiseConv2d" title="Permalink to this definition"></a></dt>
<dd><p>2D depthwise convolution layer.</p>
<p>Applies a 2D depthwise convolution over an input tensor which is typically of shape:
math:<cite>(N, C_{in}, H_{in}, W_{in})</cite>, where <span class="math notranslate nohighlight">\(N\)</span> is batch size and <span class="math notranslate nohighlight">\(C_{in}\)</span> is channel number.
For each batch of shape:math:<cite>(C_{in}, H_{in}, W_{in})</cite>, the formula is defined as:</p>
<div class="math notranslate nohighlight">
\[out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,\]</div>
<p>where <span class="math notranslate nohighlight">\(ccor\)</span> is the cross correlation operator, <span class="math notranslate nohighlight">\(C_{in}\)</span> is the input channel number, <span class="math notranslate nohighlight">\(j\)</span> ranges
from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(C_{out} - 1\)</span>, <span class="math notranslate nohighlight">\(W_{ij}\)</span> corresponds to the <span class="math notranslate nohighlight">\(i\)</span>-th channel of the <span class="math notranslate nohighlight">\(j\)</span>-th
filter and <span class="math notranslate nohighlight">\(out_{j}\)</span> corresponds to the <span class="math notranslate nohighlight">\(j\)</span>-th channel of the output. <span class="math notranslate nohighlight">\(W_{ij}\)</span> is a slice
of kernel and it has shape <span class="math notranslate nohighlight">\((\text{ks_h}, \text{ks_w})\)</span>, where <span class="math notranslate nohighlight">\(\text{ks_h}\)</span> and
<span class="math notranslate nohighlight">\(\text{ks_w}\)</span> are the height and width of the convolution kernel. The full kernel has shape
<span class="math notranslate nohighlight">\((C_{out}, C_{in} // \text{group}, \text{ks_h}, \text{ks_w})\)</span>, where group is the group number
to split the input in the channel dimension.</p>
<p>If the ‘pad_mode’ is set to be “valid”, the output height and width will be
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -
(\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> and
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -
(\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> respectively.</p>
<p>The first introduction can be found in paper <a class="reference external" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">Gradient Based Learning Applied to Document Recognition</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of input channel <span class="math notranslate nohighlight">\(C_{in}\)</span>.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output channel <span class="math notranslate nohighlight">\(C_{out}\)</span>.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The data type is int or a tuple of 2 integers. Specifies the height
and width of the 2D convolution window. Single int means the value is for both the height and the width of
the kernel. A tuple of 2 ints means the first value is for the height and the other is for the
width of the kernel.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Specifies padding mode. The optional values are
“same”, “valid”, “pad”. Default: “same”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible. Otherwise, the
last extra padding will be done from the bottom and the right side. If this mode is set, <cite>padding</cite>
must be 0.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output will be returned
without padding. Extra pixels will be discarded. If this mode is set, <cite>padding</cite>
must be 0.</p></li>
<li><p>pad: Implicit paddings on both sides of the input. The number of <cite>padding</cite> will be padded to the input
Tensor borders. <cite>padding</cite> should be greater than or equal to 0.</p></li>
</ul>
</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Implicit paddings on both sides of the input. Default: 0.</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The data type is int or a tuple of 2 integers. Specifies the dilation rate
to use for dilated convolution. If set to be <span class="math notranslate nohighlight">\(k &gt; 1\)</span>, there will
be <span class="math notranslate nohighlight">\(k - 1\)</span> pixels skipped for each sampling location. Its value should
be greater than or equal to 1 and bounded by the height and width of the
input. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Split filter into groups, <cite>in_ channels</cite> and <cite>out_channels</cite> should be
divisible by the number of groups. Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the layer uses a bias vector. Default: False.</p></li>
<li><p><strong>weight_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the convolution kernel.
It can be a Tensor, a string, an Initializer or a number. When a string is specified,
values from ‘TruncatedNormal’, ‘Normal’, ‘Uniform’, ‘HeUniform’ and ‘XavierUniform’ distributions as well
as constant ‘One’ and ‘Zero’ distributions are possible. Alias ‘xavier_uniform’, ‘he_uniform’, ‘ones’
and ‘zeros’ are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of
Initializer for more details. Default: ‘normal’.</p></li>
<li><p><strong>bias_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the bias vector. Possible
Initializer and string are the same as ‘weight_init’. Refer to the values of
Initializer for more details. Default: ‘zeros’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DepthwiseConv2d</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">240</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">640</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 240, 1024, 640)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DistributedGradReducer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DistributedGradReducer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/grad_reducer.html#DistributedGradReducer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DistributedGradReducer" title="Permalink to this definition"></a></dt>
<dd><p>A distributed optimizer.</p>
<p>Constructs a gradient reducer Cell, which applies communication and average operations on
single-process gradient values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – the parameters to be updated.</p></li>
<li><p><strong>mean</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – When mean is true, the mean coefficient (degree) would apply on gradients. Default: False.</p></li>
<li><p><strong>degree</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The mean coefficient. Usually it equals to device number. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If degree is not a int or less than 0.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_group_size</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ParallelMode</span><span class="p">,</span> <span class="n">ParameterTuple</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;DEVICE_ID&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                    <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">device_id</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">reset_auto_parallel_context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">TrainingWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">TrainingWrapper</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">add_flags</span><span class="p">(</span><span class="n">defer_inline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="n">sens</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                           <span class="n">ParallelMode</span><span class="o">.</span><span class="n">HYBRID_PARALLEL</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">mean</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;mirror_mean&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="k">if</span> <span class="n">mean</span><span class="o">.</span><span class="n">get_device_num_is_set</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">degree</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;device_num&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="n">degree</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DistributedGradReducer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">sens</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="c1"># apply grad reducer on grads</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_cell</span> <span class="o">=</span> <span class="n">TrainingWrapper</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="n">train_cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.DistributedGradReducer.construct">
<span class="sig-name descname"><span class="pre">construct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/grad_reducer.html#DistributedGradReducer.construct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DistributedGradReducer.construct" title="Permalink to this definition"></a></dt>
<dd><p>Under certain circumstances, the data precision of grads could be mixed with float16 and float32. Thus, the
result of AllReduce is unreliable. To solve the problem, grads should be cast to float32 before AllReduce,
and cast back after the operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grads</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em><em>]</em>) – The gradient tensor or tuple before operation.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>new_grads (Union[Tensor, tuple[Tensor]]), the gradient tensor or tuple after operation.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Dropout">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">keep_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mindspore.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#Dropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Dropout" title="Permalink to this definition"></a></dt>
<dd><p>Dropout layer for the input.</p>
<p>Randomly set some elements of the input tensor to zero with probability <span class="math notranslate nohighlight">\(1 - keep\_prob\)</span> during training
using samples from a Bernoulli distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Each channel will be zeroed out independently on every construct call.</p>
<p>The outputs are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{keep\_prob}\)</span> during training so
that the output layer remains at a similar scale. During inference, this
layer returns the same tensor as the input.</p>
<p>This technique is proposed in paper <a class="reference external" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> and proved to be effective to reduce
over-fitting and prevents neurons from co-adaptation. See more details in <a class="reference external" href="https://arxiv.org/pdf/1207.0580.pdf">Improving neural networks by
preventing co-adaptation of feature detectors</a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keep_prob</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The keep rate, greater than 0 and less equal than 1. E.g. rate=0.9,
dropping out 10% of input units. Default: 0.5.</p></li>
<li><p><strong>seed0</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The first random seed. Default: 0.</p></li>
<li><p><strong>seed1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The second random seed. Default: 0.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="mindspore.dtype.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – Data type of input. Default: mindspore.float32.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>keep_prob</cite> is not in range (0, 1).</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - An N-D Tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, output tensor with the same shape as the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DynamicLossScaleUpdateCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DynamicLossScaleUpdateCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_scale_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_factor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_window</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/loss_scale.html#DynamicLossScaleUpdateCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DynamicLossScaleUpdateCell" title="Permalink to this definition"></a></dt>
<dd><p>Dynamic Loss scale update cell.</p>
<p>For loss scaling training, the initial loss scaling value will be set to be <cite>loss_scale_value</cite>.
In each training step, the loss scaling value  will be updated by loss scaling value/<cite>scale_factor</cite>
when there is an overflow. And it will be increased by loss scaling value * <cite>scale_factor</cite> if there is no
overflow for a continuous <cite>scale_window</cite> steps. This cell is used for Graph mode training in which all
logic will be executed on device side(Another training mode is normal(non-sink) mode in which some logic will be
executed on host).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_scale_value</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Init loss scale.</p></li>
<li><p><strong>scale_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Coefficient of increase and decrease.</p></li>
<li><p><strong>scale_window</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Maximum continuous training steps that do not have overflow.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
<li><p><strong>label</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a scalar Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manager</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DynamicLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_window</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_update_cell</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ELU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ELU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#ELU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ELU" title="Permalink to this definition"></a></dt>
<dd><p>Exponential Linear Uint activation function.</p>
<p>Applies the exponential linear unit function element-wise.
The activation function is defined as:</p>
<div class="math notranslate nohighlight">
\[E_{i} =
\begin{cases}
x, &amp;\text{if } x \geq 0; \cr
\text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}
\end{cases}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The coefficient of negative factor whose type is float. Default: 1.0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of ELU.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">elu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">elu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Embedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Embedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_one_hot</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_table</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mindspore.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/embedding.html#Embedding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Embedding" title="Permalink to this definition"></a></dt>
<dd><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p>
<p>This module is often used to store word embeddings and retrieve them using
indices. The input to the module is a list of indices, and the output is
the corresponding word embeddings.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ‘use_one_hot’ is set to True, the type of the input should be mindspore.int32.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Size of the dictionary of embeddings.</p></li>
<li><p><strong>embedding_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The size of each embedding vector.</p></li>
<li><p><strong>use_one_hot</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether to apply one_hot encoding form. Default: False.</p></li>
<li><p><strong>embedding_table</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the embedding_table.
Refer to class <cite>initializer</cite> for the values of string when a string
is specified. Default: ‘normal’.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="mindspore.dtype.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – Data type of input. Default: mindspore.float32.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((\text{batch_size}, \text{input_length})\)</span>. The elements of
the Tensor should be integer and not larger than vocab_size. Otherwise the corresponding embedding vector will
be zero.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((\text{batch_size}, \text{input_length}, \text{embedding_size})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">20000</span><span class="p">,</span> <span class="mi">768</span><span class="p">,</span>  <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">128</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Maps the input word IDs to word embedding.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(8, 128, 768)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.EmbeddingLookUpSplitMode">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">EmbeddingLookUpSplitMode</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/embedding.html#EmbeddingLookUpSplitMode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.EmbeddingLookUpSplitMode" title="Permalink to this definition"></a></dt>
<dd><p>EmbeddingLookUp slice options in auto parallel and semi auto parallel mode.</p>
<p>There are five kinds of slice options, “BATCH_SLICE”, “FIELD_SLICE”,
“TABLE_ROW_SLICE” and “TABLE_COLUMN_SLICE”. Default: “BATCH_SLICE”.</p>
<blockquote>
<div><ul class="simple">
<li><p>BATCH_SLICE: Slicing batch dimensions of indices.</p></li>
<li><p>FIELD_SLICE: Slicing field dimensions of indices.</p></li>
<li><p>TABLE_ROW_SLICE: Slicing row of table.</p></li>
<li><p>TABLE_COLUMN_SLICE: Slicing column of table.</p></li>
</ul>
</div></blockquote>
<p>MODE_LIST: The list for all supported parallel modes.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.EmbeddingLookup">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">EmbeddingLookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CPU'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">slice_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'batch_slice'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">manual_shapes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/embedding.html#EmbeddingLookup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.EmbeddingLookup" title="Permalink to this definition"></a></dt>
<dd><p>Returns a slice of input tensor based on the specified indices.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ‘target’ is set to ‘CPU’, this module will use
P.EmbeddingLookup().add_prim_attr(‘primitive_target’, ‘CPU’) which
specified ‘offset = 0’ to lookup table.
When ‘target’ is set to ‘DEVICE’, this module will use P.GatherV2() which
specified ‘axis = 0’ to lookup table.
In field slice mode, the manual_shapes should be given. It is a tuple ,where
the element is vocab[i], vocab[i] is the row numbers for i-th
part.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Size of the dictionary of embeddings.</p></li>
<li><p><strong>embedding_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The size of each embedding vector.</p></li>
<li><p><strong>param_init</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The initialize way of embedding table. Default: ‘normal’.</p></li>
<li><p><strong>target</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specify the target where the op is executed. The value should in
[‘DEVICE’, ‘CPU’]. Default: ‘CPU’.</p></li>
<li><p><strong>slice_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The slicing way in semi auto parallel/auto parallel. The value should get through
nn.EmbeddingLookUpSplitMode. Default: nn.EmbeddingLookUpSplitMode.BATCH_SLICE.</p></li>
<li><p><strong>manual_shapes</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The accompaniment array in field slice mode.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_indices</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.
Specifies the indices of elements of the original Tensor. Values can be out of range of embedding_table,
and the exceeding part will be filled with 0 in the output. Input_indices should only be a 2d tensor in
this interface.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((z_1, z_2, ..., z_N)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)(</span><span class="n">input_indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.F1">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">F1</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/fbeta.html#F1"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.F1" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the F1 score. F1 is a special case of Fbeta when beta is 1.
Refer to class <cite>Fbeta</cite> for more details.</p>
<div class="math notranslate nohighlight">
\[F_\beta=\frac{2\cdot true\_positive}{2\cdot true\_positive + false\_negative + false\_positive}\]</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">F1</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.FTRL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">FTRL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_accum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_locking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/ftrl.html#FTRL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.FTRL" title="Permalink to this definition"></a></dt>
<dd><p>Implement the FTRL algorithm with ApplyFtrl Operator.</p>
<p>FTRL is an online convex optimization algorithm that adaptively chooses its regularization function
based on the loss functions. Refer to paper <a class="reference external" href="https://arxiv.org/abs/1002.4908">Adaptive Bound Optimization for Online Convex Optimization</a>. Refer to paper <a class="reference external" href="https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf">Ad Click Prediction: a View from the Trenches</a> for engineering document.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on all of the parameters.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
<p>The sparse strategy is applied while the SparseGatherV2 operator being used for forward network.
The sparse feature is under continuous development. The sparse behavior is currently performed on the CPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Using different learning rate by separating parameters is currently not supported.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>initial_accum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The starting value for accumulators, must be zero or positive values. Default: 0.1.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The learning rate value, should be zero or positive, dynamic learning rate is currently
not supported. Default: 0.001.</p></li>
<li><p><strong>lr_power</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Learning rate power controls how the learning rate decreases during training, must be less
than or equal to zero. Use fixed learning rate if lr_power is zero. Default: -0.5.</p></li>
<li><p><strong>l1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l1 regularization strength, must be greater than or equal to zero. Default: 0.0.</p></li>
<li><p><strong>l2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l2 regularization strength, must be greater than or equal to zero. Default: 0.0.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, use locks for updating operation. Default: False.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Value for the loss scale. It should be equal to or greater than 1.0. Default: 1.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay value to multiply weight, must be zero or positive value. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>grads</strong> (tuple[Tensor]) - The gradients of <cite>params</cite> in the optimizer, the shape is the same as the <cite>params</cite>
in optimizer.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[Parameter], the updated parameters, the shape is the same as <cite>params</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FTRL</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FTRL</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use default weight decay of 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.FakeQuantWithMinMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">FakeQuantWithMinMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">min_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ema</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#FakeQuantWithMinMax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.FakeQuantWithMinMax" title="Permalink to this definition"></a></dt>
<dd><p>Quantization aware op. This OP provides the fake quantization observer function on data with min and max.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min_init</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dimension of channel or 1(layer). Default: -6.</p></li>
<li><p><strong>max_init</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dimension of channel or 1(layer). Default: 6.</p></li>
<li><p><strong>ema</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The exponential Moving Average algorithm updates min and max. Default: False.</p></li>
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>channel_axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization by channel axis. Default: 1.</p></li>
<li><p><strong>num_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – declarate the min and max channel size, Default: 1.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of FakeQuantWithMinMax.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fake_quant</span> <span class="o">=</span> <span class="n">FakeQuantWithMinMax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">fake_quant</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Fbeta">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Fbeta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">beta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/fbeta.html#Fbeta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Fbeta" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the fbeta score.</p>
<p>Fbeta score is a weighted mean of precison and recall.</p>
<div class="math notranslate nohighlight">
\[F_\beta=\frac{(1+\beta^2) \cdot true\_positive}
        {(1+\beta^2) \cdot true\_positive +\beta^2 \cdot false\_negative + false\_positive}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The weight of precision.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Fbeta</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Fbeta.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/fbeta.html#Fbeta.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Fbeta.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clears the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Fbeta.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/fbeta.html#Fbeta.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Fbeta.eval" title="Permalink to this definition"></a></dt>
<dd><p>Computes the fbeta.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>average</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to calculate the average fbeta. Default value is False.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Float, computed result.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Fbeta.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/fbeta.html#Fbeta.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Fbeta.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result <cite>y_pred</cite> and <cite>y</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input <cite>y_pred</cite> and <cite>y</cite>. <cite>y_pred</cite> and <cite>y</cite> are Tensor, list or numpy.ndarray.
<cite>y_pred</cite> is in most cases (not strictly) a list of floating numbers in range <span class="math notranslate nohighlight">\([0, 1]\)</span>
and the shape is <span class="math notranslate nohighlight">\((N, C)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of cases and <span class="math notranslate nohighlight">\(C\)</span>
is the number of categories. y contains values of integers. The shape is <span class="math notranslate nohighlight">\((N, C)\)</span>
if one-hot encoding is used. Shape can also be <span class="math notranslate nohighlight">\((N,)\)</span> if category index is used.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.FixedLossScaleUpdateCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">FixedLossScaleUpdateCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_scale_value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/loss_scale.html#FixedLossScaleUpdateCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.FixedLossScaleUpdateCell" title="Permalink to this definition"></a></dt>
<dd><p>Static scale update cell, the loss scaling value will not be updated.</p>
<p>For usage, refer to <cite>DynamicLossScaleUpdateCell</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>loss_scale_value</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Init loss scale.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manager</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">FixedLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_update_cell</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Flatten" title="Permalink to this definition"></a></dt>
<dd><p>Flatten layer for the input.</p>
<p>Flattens a tensor without changing dimension of batch size on the 0-th axis.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span> to be flattened.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of the output tensor is <span class="math notranslate nohighlight">\((N, X)\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> is
the product of the remaining dimensions.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2, 2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="go">[[1.2 1.2 2.1 2.1]</span>
<span class="go"> [2.2 2.2 3.2 3.2]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.GELU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">GELU</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#GELU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.GELU" title="Permalink to this definition"></a></dt>
<dd><p>Gaussian error linear unit activation function.</p>
<p>Applies GELU function to each element of the input. The input is a Tensor with any valid shape.</p>
<p>GELU is defined as:
<span class="math notranslate nohighlight">\(GELU(x_i) = x_i*P(X &lt; x_i)\)</span>, where <span class="math notranslate nohighlight">\(P\)</span> is the cumulative distribution function
of standard Gaussian distribution and <span class="math notranslate nohighlight">\(x_i\)</span> is the element of the input.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of GELU.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gelu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[-1.5880802e-01  3.9999299e+00 -3.1077917e-21]</span>
<span class="go"> [ 1.9545976e+00 -2.2918017e-07  9.0000000e+00]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.GetNextSingleOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">GetNextSingleOp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset_types</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_shapes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#GetNextSingleOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.GetNextSingleOp" title="Permalink to this definition"></a></dt>
<dd><p>Cell to run for getting the next operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_types</strong> (list[<a class="reference internal" href="mindspore.dtype.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>]) – The types of dataset.</p></li>
<li><p><strong>dataset_shapes</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The shapes of dataset.</p></li>
<li><p><strong>queue_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Queue name to fetch the data.</p></li>
</ul>
</dd>
</dl>
<p>For detailed information, refer to <cite>ops.operations.GetNext</cite>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.GlobalBatchNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">GlobalBatchNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_mean_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_var_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_batch_statistics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_num_each_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#GlobalBatchNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.GlobalBatchNorm" title="Permalink to this definition"></a></dt>
<dd><p>Global normalization layer over a N-dimension input.</p>
<p>Global Normalization is cross device synchronized batch normalization. The implementation of Batch Normalization
only normalizes the data within each device. Global normalization will normalize the input within the group.
It has been described in the paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift</a>. It rescales and recenters the
feature using a mini-batch of data and the learned parameters which can be described in the following formula.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Currently, GlobalBatchNorm only supports 2D and 4D inputs.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – <cite>C</cite> from an expected input of size (N, C, H, W).</p></li>
<li><p><strong>device_num_each_group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of devices in each group. Default: 1.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A value added to the denominator for numerical stability. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating hyperparameter of the momentum for the
running_mean and running_var computation. Default: 0.9.</p></li>
<li><p><strong>gamma_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the gamma weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>beta_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the beta weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>moving_mean_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the moving mean.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>moving_var_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the moving variance.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>use_batch_statistics</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, use the mean value and variance value of current batch data. If false,
use the mean value and variance value of specified value. If None, training process will use the mean and
variance of current batch data and track the running mean and variance, eval process will use the running
mean and variance. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the normalized, scaled, offset tensor, of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">global_bn_op</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GlobalBatchNorm</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">device_num_each_group</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">global_bn_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.GraphKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">GraphKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">auto_prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pips</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/cell.html#GraphKernel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.GraphKernel" title="Permalink to this definition"></a></dt>
<dd><p>Base class for GraphKernel.</p>
<p>A <cite>GraphKernel</cite> a composite of basic primitives and can be compiled into a fused kernel automatically when
enable_graph_kernel in context is set to True.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Relu</span><span class="p">(</span><span class="n">GraphKernel</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">super</span><span class="p">(</span><span class="n">Relu</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="bp">self</span><span class="o">.</span><span class="n">max</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.GroupNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">GroupNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_groups</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#GroupNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.GroupNorm" title="Permalink to this definition"></a></dt>
<dd><p>Group Normalization over a mini-batch of inputs.</p>
<p>Group normalization is widely used in recurrent neural networks. It applies
normalization on a mini-batch of inputs for each single training case as described
in the paper <a class="reference external" href="https://arxiv.org/pdf/1803.08494.pdf">Group Normalization</a>. Group normalization
divides the channels into groups and computes within each group the mean and variance for normalization,
and it performs very stable over a wide range of batch size. It can be described using the following formula.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_groups</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of groups to be divided along the channel dimension.</p></li>
<li><p><strong>num_channels</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of channels per group.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A value added to the denominator for numerical stability. Default: 1e-5.</p></li>
<li><p><strong>affine</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – A bool value, this layer will have learnable affine parameters when set to true. Default: True.</p></li>
<li><p><strong>gamma_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the gamma weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>beta_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the beta weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input feature with shape [N, C, H, W].</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the normalized and scaled offset tensor, has the same shape and data type as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">goup_norm_op</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">goup_norm_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.GroupNorm.extend_repr">
<span class="sig-name descname"><span class="pre">extend_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#GroupNorm.extend_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.GroupNorm.extend_repr" title="Permalink to this definition"></a></dt>
<dd><p>Display instance object as string.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.HSigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">HSigmoid</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#HSigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.HSigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Hard sigmoid activation function.</p>
<p>Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</p>
<p>Hard sigmoid is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th slice in the given dimension of the input Tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of HSigmoid.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hsigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hsigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.HSigmoidQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">HSigmoidQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#HSigmoidQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.HSigmoidQuant" title="Permalink to this definition"></a></dt>
<dd><p>HSigmoidQuant activation function. Add Fake Quant OP before and after HSigmoid OP.</p>
<p>This part is a more detailed overview of HSigmoid op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Activation cell class.</p></li>
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of HSigmoidQuant.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HSigmoidQuant</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">HSigmoid</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.HSwish">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">HSwish</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#HSwish"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.HSwish" title="Permalink to this definition"></a></dt>
<dd><p>Hard swish activation function.</p>
<p>Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</p>
<p>Hard swish is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th slice in the given dimension of the input Tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of HSwish.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hswish</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HSwish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hswish</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.HSwishQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">HSwishQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#HSwishQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.HSwishQuant" title="Permalink to this definition"></a></dt>
<dd><p>HSwishQuant activation function. Add Fake Quant OP after HSwish OP.</p>
<p>This part is a more detailed overview of HSwish op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Activation cell class.</p></li>
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of HSwishQuant.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">HSwishQuant</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">HSwish</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ImageGradients">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ImageGradients</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/image.html#ImageGradients"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ImageGradients" title="Permalink to this definition"></a></dt>
<dd><p>Returns two tensors, the first is along the height dimension and the second is along the width dimension.</p>
<p>Assume an image shape is <span class="math notranslate nohighlight">\(h*w\)</span>. The gradients along the height and the width are <span class="math notranslate nohighlight">\(dy\)</span> and <span class="math notranslate nohighlight">\(dx\)</span>,
respectively.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}dy[i] = \begin{cases} image[i+1, :]-image[i, :], &amp;if\ 0&lt;=i&lt;h-1 \cr
0, &amp;if\ i==h-1\end{cases}\\dx[i] = \begin{cases} image[:, i+1]-image[:, i], &amp;if\ 0&lt;=i&lt;w-1 \cr
0, &amp;if\ i==w-1\end{cases}\end{aligned}\end{align} \]</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>images</strong> (Tensor) - The input image data, with format ‘NCHW’.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>dy</strong> (Tensor) - vertical image gradients, the same type and shape as input.</p></li>
<li><p><strong>dx</strong> (Tensor) - horizontal image gradients, the same type and shape as input.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ImageGradients</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]]]]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="go">[[[[2,2]</span>
<span class="go">   [0,0]]]]</span>
<span class="go">[[[[1,0]</span>
<span class="go">   [1,0]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.L1Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">L1Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/loss/loss.html#L1Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.L1Loss" title="Permalink to this definition"></a></dt>
<dd><p>L1Loss creates a criterion to measure the mean absolute error (MAE) between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> by element,
where <span class="math notranslate nohighlight">\(x\)</span> is the input Tensor and <span class="math notranslate nohighlight">\(y\)</span> is the target Tensor.</p>
<p>For simplicity, let <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> be 1-dimensional Tensor with length <span class="math notranslate nohighlight">\(N\)</span>,
the unreduced loss (i.e. with argument reduction set to ‘none’) of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is given as:</p>
<div class="math notranslate nohighlight">
\[L(x, y) = \{l_1,\dots,l_N\}, \quad \text{with } l_n = \left| x_n - y_n \right|\]</div>
<p>When argument reduction is ‘mean’, the mean value of <span class="math notranslate nohighlight">\(L(x, y)\)</span> will be returned.
When argument reduction is ‘sum’, the sum of <span class="math notranslate nohighlight">\(L(x, y)\)</span> will be returned. <span class="math notranslate nohighlight">\(N\)</span> is the batch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Type of reduction to be applied to loss. The optional values are “mean”, “sum”, and “none”.
Default: “mean”.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>target_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, loss float tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LARS">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LARS</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon=1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coefficient=0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_clip=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lars_filter=&lt;function</span> <span class="pre">LARS.&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/lars.html#LARS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LARS" title="Permalink to this definition"></a></dt>
<dd><p>Implements the LARS algorithm with LARSUpdate Operator.</p>
<p>LARS is an optimization algorithm employing a large batch optimization technique. Refer to paper <a class="reference external" href="https://arxiv.org/abs/1708.03888">LARGE BATCH
TRAINING OF CONVOLUTIONAL NETWORKS</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<a class="reference internal" href="#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><em>Optimizer</em></a>) – MindSpore optimizer for which to wrap and modify gradients.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Default: 1e-05.</p></li>
<li><p><strong>coefficient</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Trust coefficient for calculating the local learning rate. Default: 0.001.</p></li>
<li><p><strong>use_clip</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use clip operation for calculating the local learning rate. Default: False.</p></li>
<li><p><strong>lars_filter</strong> (<em>Function</em>) – A function to determine whether apply the LARS algorithm. Default:
lambda x: ‘LayerNorm’ not in x.name and ‘bias’ not in x.name.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite> in the optimizer, the shape is the
as same as the <cite>params</cite> in the optimizer.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Union[Tensor[bool], tuple[Parameter]], it depends on the output of <cite>optimizer</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">opt_lars</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LARS</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">coefficient</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt_lars</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LGamma">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LGamma</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/math.html#LGamma"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LGamma" title="Permalink to this definition"></a></dt>
<dd><p>Calculate LGamma using Lanczos’ approximation refering to “A Precision Approximationof the Gamma Function”.
The algorithm is:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}lgamma(z + 1) = \frac{(\log(2) + \log(pi))}{2} + (z + 1/2) * log(t(z)) - t(z) + A(z)\\t(z) = z + kLanczosGamma + 1/2\\A(z) = kBaseLanczosCoeff + \sum_{k=1}^n \frac{kLanczosCoefficients[i]}{z + k}\end{aligned}\end{align} \]</div>
<p>However, if the input is less than 0.5 use Euler’s reflection formula:</p>
<div class="math notranslate nohighlight">
\[lgamma(x) = \log(pi) - lgamma(1-x) - \log(abs(sin(pi * x)))\]</div>
<p>And please note that</p>
<div class="math notranslate nohighlight">
\[lgamma(+/-inf) = +inf\]</div>
<p>Thus, the behaviour of LGamma follows:
when x &gt; 0.5, return log(Gamma(x))
when x &lt; 0.5 and is not an interger, return the real part of Log(Gamma(x)) where Log is the complex logarithm
when x is an integer less or equal to 0, return +inf
when x = +/- inf, return +inf</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor. Only float16, float32 are supported.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the ‘input_x’.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LGamma</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/lstm.html#LSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LSTM" title="Permalink to this definition"></a></dt>
<dd><p>LSTM (Long Short-Term Memory) layer.</p>
<p>Applies a LSTM to the input.</p>
<p>There are two pipelines connecting two consecutive cells in a LSTM model; one is cell state pipeline
and the other is hidden state pipeline. Denote two consecutive time nodes as <span class="math notranslate nohighlight">\(t-1\)</span> and <span class="math notranslate nohighlight">\(t\)</span>.
Given an input <span class="math notranslate nohighlight">\(x_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, an hidden state <span class="math notranslate nohighlight">\(h_{t-1}\)</span> and an cell
state <span class="math notranslate nohighlight">\(c_{t-1}\)</span> of the layer at time <span class="math notranslate nohighlight">\({t-1}\)</span>, the cell state and hidden state at
time <span class="math notranslate nohighlight">\(t\)</span> is computed using an gating mechanism. Input gate <span class="math notranslate nohighlight">\(i_t\)</span> is designed to protect the cell
from perturbation by irrelevant inputs. Forget gate <span class="math notranslate nohighlight">\(f_t\)</span> affords protection of the cell by forgetting
some information in the past, which is stored in <span class="math notranslate nohighlight">\(h_{t-1}\)</span>. Output gate <span class="math notranslate nohighlight">\(o_t\)</span> protects other
units from perturbation by currently irrelevant memory contents. Candidate cell state <span class="math notranslate nohighlight">\(\tilde{c}_t\)</span> is
calculated with the current input, on which the input gate will be applied. Finally, current cell state
<span class="math notranslate nohighlight">\(c_{t}\)</span> and hidden state <span class="math notranslate nohighlight">\(h_{t}\)</span> are computed with the calculated gates and cell states. The complete
formulation is as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    i_t = \sigma(W_{ix} x_t + b_{ix} + W_{ih} h_{(t-1)} + b_{ih}) \\
    f_t = \sigma(W_{fx} x_t + b_{fx} + W_{fh} h_{(t-1)} + b_{fh}) \\
    \tilde{c}_t = \tanh(W_{cx} x_t + b_{cx} + W_{ch} h_{(t-1)} + b_{ch}) \\
    o_t = \sigma(W_{ox} x_t + b_{ox} + W_{oh} h_{(t-1)} + b_{oh}) \\
    c_t = f_t * c_{(t-1)} + i_t * \tilde{c}_t \\
    h_t = o_t * \tanh(c_t) \\
\end{array}\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function, and <span class="math notranslate nohighlight">\(*\)</span> is the Hadamard product. <span class="math notranslate nohighlight">\(W, b\)</span>
are learnable weights between the output and the input in the formula. For instance,
<span class="math notranslate nohighlight">\(W_{ix}, b_{ix}\)</span> are the weight and bias used to transform from input <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(i\)</span>.
Details can be found in paper <a class="reference external" href="https://www.bioinf.jku.at/publications/older/2604.pdf">LONG SHORT-TERM MEMORY</a> and
<a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43905.pdf">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of features of input.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of features of hidden layer.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of layers of stacked LSTM . Default: 1.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the cell has bias <cite>b_ih</cite> and <cite>b_hh</cite>. Default: True.</p></li>
<li><p><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the first dimension of input is batch_size. Default: False.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – If not 0, append <cite>Dropout</cite> layer on the outputs of each
LSTM layer except the last layer. Default 0. The range of dropout is [0.0, 1.0].</p></li>
<li><p><strong>bidirectional</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether it is a bidirectional LSTM. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape (seq_len, batch_size, <cite>input_size</cite>).</p></li>
<li><p><strong>hx</strong> (tuple) - A tuple of two Tensors (h_0, c_0) both of data type mindspore.float32 or
mindspore.float16 and shape (num_directions * <cite>num_layers</cite>, batch_size, <cite>hidden_size</cite>).
Data type of <cite>hx</cite> should be the same as <cite>input</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple constains (<cite>output</cite>, (<cite>h_n</cite>, <cite>c_n</cite>)).</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - Tensor of shape (seq_len, batch_size, num_directions * <cite>hidden_size</cite>).</p></li>
<li><p><strong>hx_n</strong> (tuple) - A tuple of two Tensor (h_n, c_n) both of shape
(num_directions * <cite>num_layers</cite>, batch_size, <cite>hidden_size</cite>).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">LstmNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">LstmNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">has_bias</span><span class="o">=</span><span class="n">has_bias</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LstmNet</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">)</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_first</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/lstm.html#LSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LSTMCell" title="Permalink to this definition"></a></dt>
<dd><p>LSTM (Long Short-Term Memory) layer.</p>
<p>Applies a LSTM layer to the input.</p>
<p>There are two pipelines connecting two consecutive cells in a LSTM model; one is cell state pipeline
and the other is hidden state pipeline. Denote two consecutive time nodes as <span class="math notranslate nohighlight">\(t-1\)</span> and <span class="math notranslate nohighlight">\(t\)</span>.
Given an input <span class="math notranslate nohighlight">\(x_t\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, an hidden state <span class="math notranslate nohighlight">\(h_{t-1}\)</span> and an cell
state <span class="math notranslate nohighlight">\(c_{t-1}\)</span> of the layer at time <span class="math notranslate nohighlight">\({t-1}\)</span>, the cell state and hidden state at
time <span class="math notranslate nohighlight">\(t\)</span> is computed using an gating mechanism. Input gate <span class="math notranslate nohighlight">\(i_t\)</span> is designed to protect the cell
from perturbation by irrelevant inputs. Forget gate <span class="math notranslate nohighlight">\(f_t\)</span> affords protection of the cell by forgetting
some information in the past, which is stored in <span class="math notranslate nohighlight">\(h_{t-1}\)</span>. Output gate <span class="math notranslate nohighlight">\(o_t\)</span> protects other
units from perturbation by currently irrelevant memory contents. Candidate cell state <span class="math notranslate nohighlight">\(\tilde{c}_t\)</span> is
calculated with the current input, on which the input gate will be applied. Finally, current cell state
<span class="math notranslate nohighlight">\(c_{t}\)</span> and hidden state <span class="math notranslate nohighlight">\(h_{t}\)</span> are computed with the calculated gates and cell states. The complete
formulation is as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    i_t = \sigma(W_{ix} x_t + b_{ix} + W_{ih} h_{(t-1)} + b_{ih}) \\
    f_t = \sigma(W_{fx} x_t + b_{fx} + W_{fh} h_{(t-1)} + b_{fh}) \\
    \tilde{c}_t = \tanh(W_{cx} x_t + b_{cx} + W_{ch} h_{(t-1)} + b_{ch}) \\
    o_t = \sigma(W_{ox} x_t + b_{ox} + W_{oh} h_{(t-1)} + b_{oh}) \\
    c_t = f_t * c_{(t-1)} + i_t * \tilde{c}_t \\
    h_t = o_t * \tanh(c_t) \\
\end{array}\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function, and <span class="math notranslate nohighlight">\(*\)</span> is the Hadamard product. <span class="math notranslate nohighlight">\(W, b\)</span>
are learnable weights between the output and the input in the formula. For instance,
<span class="math notranslate nohighlight">\(W_{ix}, b_{ix}\)</span> are the weight and bias used to transform from input <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(i\)</span>.
Details can be found in paper <a class="reference external" href="https://www.bioinf.jku.at/publications/older/2604.pdf">LONG SHORT-TERM MEMORY</a> and
<a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43905.pdf">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of features of input.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of features of hidden layer.</p></li>
<li><p><strong>layer_index</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – index of current layer of stacked LSTM . Default: 0.</p></li>
<li><p><strong>has_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the cell has bias <cite>b_ih</cite> and <cite>b_hh</cite>. Default: True.</p></li>
<li><p><strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether the first dimension of input is batch_size. Default: False.</p></li>
<li><p><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – If not 0, append <cite>Dropout</cite> layer on the outputs of each
LSTM layer except the last layer. Default 0. The range of dropout is [0.0, 1.0].</p></li>
<li><p><strong>bidirectional</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether this is a bidirectional LSTM. If set True,
number of directions will be 2 otherwise number of directions is 1. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape (seq_len, batch_size, <cite>input_size</cite>).</p></li>
<li><p><strong>h</strong> - data type mindspore.float32 or
mindspore.float16 and shape (num_directions * <cite>num_layers</cite>, batch_size, <cite>hidden_size</cite>).</p></li>
<li><p><strong>c</strong> - data type mindspore.float32 or
mindspore.float16 and shape (num_directions * <cite>num_layers</cite>, batch_size, <cite>hidden_size</cite>).
Data type of <cite>h’ and ‘c’ should be the same of `input</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p><cite>output</cite>, <cite>h_n</cite>, <cite>c_n</cite>, ‘reserve’, ‘state’.</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - Tensor of shape (seq_len, batch_size, num_directions * <cite>hidden_size</cite>).</p></li>
<li><p><strong>h</strong> - A Tensor with shape (num_directions * <cite>num_layers</cite>, batch_size, <cite>hidden_size</cite>).</p></li>
<li><p><strong>c</strong> - A Tensor with shape (num_directions * <cite>num_layers</cite>, batch_size, <cite>hidden_size</cite>).</p></li>
<li><p><strong>reserve</strong> - reserved</p></li>
<li><p><strong>state</strong> - reserved</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">LstmNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">LstmNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTMCell</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">layer_index</span><span class="o">=</span><span class="n">layer_index</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">has_bias</span><span class="o">=</span><span class="n">has_bias</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="p">(</span><span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LstmNet</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h0</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c0</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span><span class="p">,</span> <span class="n">cn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">c0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Lamb">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Lamb</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/lamb.html#Lamb"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Lamb" title="Permalink to this definition"></a></dt>
<dd><p>Lamb Dynamic Learning Rate.</p>
<p>LAMB is an optimization algorithm employing a layerwise adaptive large batch
optimization technique. Refer to the paper <a class="reference external" href="https://arxiv.org/abs/1904.00962">LARGE BATCH OPTIMIZATION FOR DEEP LEARNING: TRAINING BERT IN 76
MINUTES</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.</p></li>
<li><p><strong>beta1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 1st moment estimations. Default: 0.9.
Should be in range (0.0, 1.0).</p></li>
<li><p><strong>beta2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 2nd moment estimations. Default: 0.999.
Should be in range (0.0, 1.0).</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Default: 1e-6.
Should be greater than 0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). Default: 0.0. Should be equal to or greater than 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[bool], all elements are True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly_decay_lr</span> <span class="o">=</span> <span class="n">learning_rate_schedule</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">poly_decay_lr</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use default learning rate of 0.1 and weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use dynamic learning rate of poly decay learning rate and default</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># weight decay of 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">begin_norm_axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">begin_params_axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LayerNorm" title="Permalink to this definition"></a></dt>
<dd><p>Applies Layer Normalization over a mini-batch of inputs.</p>
<p>Layer normalization is widely used in recurrent neural networks. It applies
normalization on a mini-batch of inputs for each single training case as described
in the paper <a class="reference external" href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a>. Unlike batch
normalization, layer normalization performs exactly the same computation at training and
testing time. It can be described using the following formula. It is applied across all channels
and pixel but only one batch size.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalized_shape</strong> (<em>Union</em><em>(</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – The normalization is performed over axis
<cite>begin_norm_axis … R - 1</cite>.</p></li>
<li><p><strong>begin_norm_axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – It first normalization dimension: normalization will be performed along dimensions
<cite>begin_norm_axis: rank(inputs)</cite>, the value should be in [-1, rank(input)). Default: -1.</p></li>
<li><p><strong>begin_params_axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The first parameter(beta, gamma)dimension: scale and centering parameters
will have dimensions <cite>begin_params_axis: rank(inputs)</cite> and will be broadcast with
the normalized inputs accordingly, the value should be in [-1, rank(input)). Default: -1.</p></li>
<li><p><strong>gamma_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the gamma weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘ones’.</p></li>
<li><p><strong>beta_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the beta weight.
The values of str refer to the function <cite>initializer</cite> including ‘zeros’, ‘ones’, ‘xavier_uniform’,
‘he_uniform’, etc. Default: ‘zeros’.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A value added to the denominator for numerical stability. Default: 1e-7.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of ‘input_x’ is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>,
and <cite>input_shape[begin_norm_axis:]</cite> is equal to <cite>normalized_shape</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the normalized and scaled offset tensor, has the same shape and data type as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span>  <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.LayerNorm.extend_repr">
<span class="sig-name descname"><span class="pre">extend_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/normalization.html#LayerNorm.extend_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LayerNorm.extend_repr" title="Permalink to this definition"></a></dt>
<dd><p>Display instance object as string.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LazyAdam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LazyAdam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_locking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_nesterov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/lazyadam.html#LazyAdam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LazyAdam" title="Permalink to this definition"></a></dt>
<dd><p>Updates gradients by Adaptive Moment Estimation (Adam) algorithm.</p>
<p>The Adam algorithm is proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m = \beta_1 * m + (1 - \beta_1) * g \\
    v = \beta_2 * v + (1 - \beta_2) * g * g \\
    l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\
    w = w - l * \frac{m}{\sqrt{v} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector <cite>moment1</cite>, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector <cite>moment2</cite>,
<span class="math notranslate nohighlight">\(g\)</span> represents <cite>gradients</cite>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent
<cite>beta1</cite> and <cite>beta2</cite>, <span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(beta_1^t\)</span> and <span class="math notranslate nohighlight">\(beta_2^t\)</span> represent
<cite>beta1_power</cite> and <cite>beta2_power</cite>, <span class="math notranslate nohighlight">\(\alpha\)</span> represents <cite>learning_rate</cite>, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>params</cite>,
<span class="math notranslate nohighlight">\(\epsilon\)</span> represents <cite>eps</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
<p>The sparse strategy is applied while the SparseGatherV2 operator being used for forward network.
The sparse behavior, to be notice, is not equivalent to the
original Adam algorithm, as only the current indices parames will be updated. The sparse feature is under
continuous development. The sparse behavior is currently performed on the CPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr” and “weight_decay” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 1e-3.</p></li>
<li><p><strong>beta1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 1st moment estimations. Should be in range (0.0, 1.0).
Default: 0.9.</p></li>
<li><p><strong>beta2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The exponential decay rate for the 2nd moment estimations. Should be in range (0.0, 1.0).
Default: 0.999.</p></li>
<li><p><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Should be greater than 0. Default:
1e-8.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect updating variable tensors.
If true, updates of the var, m, and v tensors will be protected by a lock.
If false, the result is unpredictable. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.
If true, update the gradients using NAG.
If true, update the gradients without using NAG. Default: False.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. Should be equal to or greater than 1. Default:
1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[bool], the value is True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LazyAdam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LazyAdam</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use default learning rate of 0.1 and weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use learning rate of 0.01 and default weight decay of 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LeakyReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LeakyReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#LeakyReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LeakyReLU" title="Permalink to this definition"></a></dt>
<dd><p>Leaky ReLU activation function.</p>
<p>LeakyReLU is similar to ReLU, but LeakyReLU has a slope that makes it not equal to 0 at x &lt; 0.
The activation function is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{leaky_relu}(x) = \begin{cases}x, &amp;\text{if } x \geq 0; \cr
\text{alpha} * x, &amp;\text{otherwise.}\end{cases}\]</div>
<p>See <a class="reference external" href="https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf">https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Slope of the activation function at x &lt; 0. Default: 0.2.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input of LeakyReLU.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type and shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">leaky_relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[-0.2  4.  -1.6]</span>
<span class="go"> [ 2   -1.   9.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LeakyReLUQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LeakyReLUQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#LeakyReLUQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LeakyReLUQuant" title="Permalink to this definition"></a></dt>
<dd><p>LeakyReLUQuant activation function. Add Fake Quant OP after HSwish OP.</p>
<p>This part is a more detailed overview of HSwish op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>activation</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Activation cell class.</p></li>
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of LeakyReLUQuant.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLUQuant</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LinSpace">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LinSpace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/math.html#LinSpace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LinSpace" title="Permalink to this definition"></a></dt>
<dd><p>Generates values in an interval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – The start of interval. With shape of 0-D.</p></li>
<li><p><strong>stop</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – The end of interval. With shape of 0-D.</p></li>
<li><p><strong>num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – ticks number in the interval, the ticks include start and stop value. With shape of 0-D.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Outputs:</dt><dd><p>Tensor, With type same as <cite>start</cite>. The shape is 1-D with length of <cite>num</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linspace</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LinSpace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">linspace</span><span class="p">()</span>
<span class="go">[1, 3.25, 5.5, 7.75, 10]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LogSigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LogSigmoid</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#LogSigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LogSigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Logsigmoid activation function.</p>
<p>Applies logsigmoid activation element-wise. The input is a Tensor with any valid shape.</p>
<p>Logsigmoid is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{logsigmoid}(x_{i}) = log(\frac{1}{1 + \exp(-x_i)}),\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the element of the input.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of LogSigmoid.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-3.1326166e-01, -1.2692806e-01, -4.8587345e-02]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.LogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#LogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.LogSoftmax" title="Permalink to this definition"></a></dt>
<dd><p>LogSoftmax activation function.</p>
<p>Applies the LogSoftmax function to n-dimensional input tensor.</p>
<p>The input is transformed by the Softmax function and then by the log function to lie in range[-inf,0).</p>
<p>Logsoftmax is defined as:
<span class="math notranslate nohighlight">\(\text{logsoftmax}(x_i) = \log \left(\frac{\exp(x_i)}{\sum_{j=0}^{n-1} \exp(x_j)}\right)\)</span>,
where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th slice in the given dimension of the input Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The axis to apply LogSoftmax operation, -1 means the last dimension. Default: -1.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of LogSoftmax.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, which has the same type and shape as the input as <cite>x</cite> with values in the range[-inf,0).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_softmax</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[-5.00672150e+00 -6.72150636e-03 -1.20067215e+01]</span>
<span class="go"> [-7.00091219e+00 -1.40009127e+01 -9.12250078e-04]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/loss.html#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Loss" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the average of the loss. If method ‘update’ is called every <span class="math notranslate nohighlight">\(n\)</span> iterations, the result of
evaluation will be:</p>
<div class="math notranslate nohighlight">
\[loss = \frac{\sum_{k=1}^{n}loss_k}{n}\]</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Loss.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/loss.html#Loss.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Loss.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clears the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Loss.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/loss.html#Loss.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Loss.eval" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the average of the loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float, the average of the loss.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If the total number is 0.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Loss.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/loss.html#Loss.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Loss.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Inputs contain only one element, the element is loss. The dimension of
loss should be 0 or 1.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the length of inputs is not 1.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the dimensions of loss is not 1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MAE">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MAE</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MAE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MAE" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the mean absolute error.</p>
<p>Creates a criterion that measures the mean absolute error (MAE)
between each element in the input: <span class="math notranslate nohighlight">\(x\)</span> and the target: <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\text{MAE} = \frac{\sum_{i=1}^n \|y_i - x_i\|}{n}\]</div>
<p>Here <span class="math notranslate nohighlight">\(y_i\)</span> is the prediction and <span class="math notranslate nohighlight">\(x_i\)</span> is the true value.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The method <cite>update</cite> must be called with the form <cite>update(y_pred, y)</cite>.</p>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">error</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MAE</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">error</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">error</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">error</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.MAE.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MAE.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MAE.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clears the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.MAE.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MAE.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MAE.eval" title="Permalink to this definition"></a></dt>
<dd><p>Computes the mean absolute error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float, the computed result.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If the number of the total samples is 0.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.MAE.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MAE.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MAE.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result <span class="math notranslate nohighlight">\(y_{pred}\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input <cite>y_pred</cite> and <cite>y</cite> for calculating mean absolute error where the shape of
<cite>y_pred</cite> and <cite>y</cite> are both N-D and the shape are the same.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the number of the input is not 2.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MSE">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MSE</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MSE"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MSE" title="Permalink to this definition"></a></dt>
<dd><p>Measures the mean squared error.</p>
<p>Creates a criterion that measures the mean squared error (squared L2
norm) between each element in the input: <span class="math notranslate nohighlight">\(x\)</span> and the target: <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\text{MSE}(x,\ y) = \frac{\sum_{i=1}^n(y_i - x_i)^2}{n},\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is batch size.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">error</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSE</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">error</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">error</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">error</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.MSE.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MSE.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MSE.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clear the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.MSE.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MSE.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MSE.eval" title="Permalink to this definition"></a></dt>
<dd><p>Compute the mean squared error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float, the computed result.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If the number of samples is 0.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.MSE.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/error.html#MSE.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MSE.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result <span class="math notranslate nohighlight">\(y_{pred}\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input <cite>y_pred</cite> and <cite>y</cite> for calculating mean square error where the shape of
<cite>y_pred</cite> and <cite>y</cite> are both N-D and the shape are the same.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the number of input is not 2.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MSELoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/loss/loss.html#MSELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MSELoss" title="Permalink to this definition"></a></dt>
<dd><p>MSELoss creates a criterion to measure the mean squared error (squared L2-norm) between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>
by element, where <span class="math notranslate nohighlight">\(x\)</span> is the input and <span class="math notranslate nohighlight">\(y\)</span> is the target.</p>
<p>For simplicity, let <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> be 1-dimensional Tensor with length <span class="math notranslate nohighlight">\(N\)</span>,
the unreduced loss (i.e. with argument reduction set to ‘none’) of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is given as:</p>
<div class="math notranslate nohighlight">
\[L(x, y) = \{l_1,\dots,l_N\}, \quad \text{with} \quad l_n = (x_n - y_n)^2.\]</div>
<p>When argument reduction is ‘mean’, the mean value of <span class="math notranslate nohighlight">\(L(x, y)\)</span> will be returned.
When argument reduction is ‘sum’, the sum of <span class="math notranslate nohighlight">\(L(x, y)\)</span> will be returned. <span class="math notranslate nohighlight">\(N\)</span> is the batch size.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Type of reduction to be applied to loss. The optional values are “mean”, “sum”, and “none”.
Default: “mean”.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>target_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, weighted loss float tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MSSSIM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MSSSIM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_factors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.0448,</span> <span class="pre">0.2856,</span> <span class="pre">0.3001,</span> <span class="pre">0.2363,</span> <span class="pre">0.1333)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">11</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.03</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/image.html#MSSSIM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MSSSIM" title="Permalink to this definition"></a></dt>
<dd><p>Returns MS-SSIM index between img1 and img2.</p>
<p>Its implementation is based on Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. <a class="reference external" href="https://ieeexplore.ieee.org/document/1292216">Multiscale structural similarity
for image quality assessment</a>.
Signals, Systems and Computers, 2004.</p>
<div class="math notranslate nohighlight">
\[\begin{split}l(x,y)&amp;=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}, C_1=(K_1L)^2.\\
c(x,y)&amp;=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}, C_2=(K_2L)^2.\\
s(x,y)&amp;=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}, C_3=C_2/2.\\
MSSSIM(x,y)&amp;=l^alpha_M*{\prod_{1\leq j\leq M} (c^beta_j*s^gamma_j)}.\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_val</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – The dynamic range of the pixel values (255 for 8-bit grayscale images).
Default: 1.0.</p></li>
<li><p><strong>power_factors</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em>) – Iterable of weights for each scal e.
Default: (0.0448, 0.2856, 0.3001, 0.2363, 0.1333). Default values obtained by Wang et al.</p></li>
<li><p><strong>filter_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The size of the Gaussian filter. Default: 11.</p></li>
<li><p><strong>filter_sigma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The standard deviation of Gaussian kernel. Default: 1.5.</p></li>
<li><p><strong>k1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The constant used to generate c1 in the luminance comparison function. Default: 0.01.</p></li>
<li><p><strong>k2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The constant used to generate c2 in the contrast comparison function. Default: 0.03.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>img1</strong> (Tensor) - The first image batch with format ‘NCHW’. It should be the same shape and dtype as img2.</p></li>
<li><p><strong>img2</strong> (Tensor) - The second image batch with format ‘NCHW’. It should be the same shape and dtype as img1.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as img1. It is a 1-D tensor with shape N, where N is the batch num of img1.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSSSIM</span><span class="p">(</span><span class="n">power_factors</span><span class="o">=</span><span class="p">(</span><span class="mf">0.033</span><span class="p">,</span> <span class="mf">0.033</span><span class="p">,</span> <span class="mf">0.033</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">msssim</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MatrixDiag">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MatrixDiag</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#MatrixDiag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MatrixDiag" title="Permalink to this definition"></a></dt>
<dd><p>Returns a batched diagonal tensor with a given batched diagonal values.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The diagonal values. It can be one of the following data types:
float32, float16, int32, int8, and uint8.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type as input <cite>x</cite>. The shape should be x.shape + (x.shape[-1], ).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matrix_diag</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MatrixDiag</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">matrix_diag</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[[1.   0.]</span>
<span class="go"> [0.  -1.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MatrixDiagPart">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MatrixDiagPart</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#MatrixDiagPart"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MatrixDiagPart" title="Permalink to this definition"></a></dt>
<dd><p>Returns the batched diagonal part of a batched tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The batched tensor. It can be one of the following data types:
float32, float16, int32, int8, and uint8.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type as input <cite>x</cite>. The shape should be x.shape[:-2] + [min(x.shape[-2:])].</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matrix_diag_part</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MatrixDiagPart</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">matrix_diag_part</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[[-1., 1.], [-1., 1.], [-1., 1.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MatrixSetDiag">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MatrixSetDiag</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#MatrixSetDiag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MatrixSetDiag" title="Permalink to this definition"></a></dt>
<dd><p>Modify the batched diagonal part of a batched tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The batched tensor. It can be one of the following data types:
float32, float16, int32, int8, and uint8.</p></li>
<li><p><strong>diagonal</strong> (Tensor) - The diagonal values.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type and shape as input <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diagonal</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matrix_set_diag</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MatrixSetDiag</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">matrix_set_diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">)</span>
<span class="go">[[[-1, 0], [0, 2]], [[-1, 0], [0, 1]], [[-1, 0], [0, 1]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MaxPool2d">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MaxPool2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/pooling.html#MaxPool2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MaxPool2d" title="Permalink to this definition"></a></dt>
<dd><p>Max pooling operation for temporal data.</p>
<p>Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</p>
<p>Typically the input is of shape <span class="math notranslate nohighlight">\((N_{in}, C_{in}, H_{in}, W_{in})\)</span>, MaxPool2d outputs
regional maximum in the <span class="math notranslate nohighlight">\((H_{in}, W_{in})\)</span>-dimension. Given kernel size
<span class="math notranslate nohighlight">\(ks = (h_{ker}, w_{ker})\)</span> and stride <span class="math notranslate nohighlight">\(s = (s_0, s_1)\)</span>, the operation is as follows.</p>
<div class="math notranslate nohighlight">
\[\text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}
\text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>pad_mode for training only supports “same” and “valid”.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of kernel used to take the max value,
is an int number that represents height and width are both kernel_size,
or a tuple of two int numbers that represent height and width respectively.
Default: 1.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The optional value for pad mode, is “same” or “valid”, not case sensitive.
Default: “valid”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible.
Otherwise, the last extra padding will be done from the bottom and the right side.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output
will be returned without padding. Extra pixels will be discarded.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="go">[[[[1. 5. 5. 1.]</span>
<span class="go">   [0. 3. 4. 8.]</span>
<span class="go">   [4. 2. 7. 6.]</span>
<span class="go">   [4. 9. 0. 1.]]</span>
<span class="go">  [[3. 6. 2. 6.]</span>
<span class="go">   [4. 4. 7. 8.]</span>
<span class="go">   [0. 0. 4. 0.]</span>
<span class="go">   [1. 8. 7. 0.]]]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 2, 2, 2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[[[[7. 8.]</span>
<span class="go">   [9. 9.]]</span>
<span class="go">  [[7. 8.]</span>
<span class="go">   [8. 8.]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Metric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Metric</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/metric.html#Metric"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Metric" title="Permalink to this definition"></a></dt>
<dd><p>Base class of metric.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For examples of subclasses, please refer to the definition of class <cite>MAE</cite>, ‘Recall’ etc.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Metric.clear">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/metric.html#Metric.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Metric.clear" title="Permalink to this definition"></a></dt>
<dd><p>An interface describes the behavior of clearing the internal evaluation result.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All subclasses should override this interface.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Metric.eval">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/metric.html#Metric.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Metric.eval" title="Permalink to this definition"></a></dt>
<dd><p>An interface describes the behavior of computing the evaluation result.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All subclasses should override this interface.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Metric.update">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/metric.html#Metric.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Metric.update" title="Permalink to this definition"></a></dt>
<dd><p>An interface describes the behavior of updating the internal evaluation result.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All subclasses should override this interface.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – A variable-length input argument list.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Momentum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Momentum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_nesterov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/momentum.html#Momentum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Momentum" title="Permalink to this definition"></a></dt>
<dd><p>Implements the Momentum algorithm.</p>
<p>Refer to the paper on the importance of initialization and momentum in deep learning for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<div class="math notranslate nohighlight">
\[v_{t} = v_{t-1} \ast u + gradients\]</div>
<dl>
<dt>If use_nesterov is True:</dt><dd><div class="math notranslate nohighlight">
\[p_{t} =  p_{t-1} - (grad \ast lr + v_{t} \ast u \ast lr)\]</div>
</dd>
<dt>If use_nesterov is Flase:</dt><dd><div class="math notranslate nohighlight">
\[p_{t} = p_{t-1} - lr \ast v_{t}\]</div>
</dd>
</dl>
<p>Here: where grad, lr, p, v and u denote the gradients, learning_rate, params, moments, and momentum respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Hyperparameter of type float, means momentum for the moving average.
It should be at least 0.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). It should be equal to or greater than 0.0. Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. It should be greater than 0.0. Default: 1.0.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enable Nesterov momentum. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[bool], all elements are True.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the momentum is less than 0.0.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use a learning rate of default value 0.1 and a weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use a learning rate of 0.01 and a weight decay of default value 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.MulQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">MulQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#MulQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.MulQuant" title="Permalink to this definition"></a></dt>
<dd><p>Add Fake Quant OP after Mul OP.</p>
<p>This part is a more detailed overview of Mul op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of MulQuant.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Norm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#Norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Norm" title="Permalink to this definition"></a></dt>
<dd><p>Computes the norm of vectors, currently including Euclidean norm, i.e., <span class="math notranslate nohighlight">\(L_2\)</span>-norm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The axis over which to compute vector norms. Default: ().</p></li>
<li><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, the axis indicated in <cite>axis</cite> are kept with size 1. Otherwise,
the dimensions in <cite>axis</cite> are removed from the output shape. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor which is not empty.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, output tensor with dimensions in ‘axis’ reduced to 1 will be returned if ‘keep_dims’ is True;
otherwise a Tensor with dimensions in ‘axis’ removed is returned.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Norm</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.OneHot">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">OneHot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">off_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mindspore.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#OneHot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.OneHot" title="Permalink to this definition"></a></dt>
<dd><p>Returns a one-hot tensor.</p>
<p>The locations represented by indices in argument ‘indices’ take value on_value,
while all other locations take value off_value.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the input indices is rank <span class="math notranslate nohighlight">\(N\)</span>, the output will have rank <span class="math notranslate nohighlight">\(N+1\)</span>. The new
axis is created at dimension <cite>axis</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Features x depth if axis is -1, depth x features
if axis is 0. Default: -1.</p></li>
<li><p><strong>depth</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – A scalar defining the depth of the one hot dimension. Default: 1.</p></li>
<li><p><strong>on_value</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A scalar defining the value to fill in output[i][j]
when indices[j] = i. Default: 1.0.</p></li>
<li><p><strong>off_value</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A scalar defining the value to fill in output[i][j]
when indices[j] != i. Default: 0.0.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="mindspore.dtype.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – Data type of ‘on_value’ and ‘off_value’, not the
data type of indices. Default: mindspore.float32.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>indices</strong> (Tensor) - A tensor of indices of data type mindspore.int32 and arbitrary shape.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the one-hot tensor of data type ‘dtype’ with dimension at ‘axis’ expanded to ‘depth’ and filled with
on_value and off_value.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">OneHot</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="go">[[[0. 0.]</span>
<span class="go">  [1. 0.]</span>
<span class="go">  [0. 0.]</span>
<span class="go">  [0. 1.]]</span>
<span class="go"> [[1. 0.]</span>
<span class="go">  [0. 0.]</span>
<span class="go">  [0. 1.]</span>
<span class="go">  [0. 0.]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/optimizer.html#Optimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Base class for all optimizers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class defines the API to add Ops to train a model. Never use
this class directly, but instead instantiate one of its subclasses.</p>
<p>Different parameter groups can set different <cite>learning_rate</cite> and <cite>weight_decay</cite>.</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight_decay is positive. For most optimizer, when not separating parameters, the <cite>weight_decay</cite> in the API will
be applied on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning
rate. When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.</p></li>
<li><p><strong>parameters</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>parameters</cite> is a list of <cite>Parameter</cite> which will be
updated, the element in <cite>parameters</cite> should be class <cite>Parameter</cite>. When the <cite>parameters</cite> is a list of <cite>dict</cite>,
the “params”, “lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the weight decay. It should be equal to or greater than 0.
If the type of <cite>weight_decay</cite> input is int, it will be converted to float. Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. It should be greater than 0. If the
type of <cite>loss_scale</cite> input is int, it will be converted to float. Default: 1.0.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the learning_rate is a Tensor, but the dimension of tensor is greater than 1.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If the learning_rate is not any of the three types: float, Tensor, nor Iterable.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.broadcast_params">
<span class="sig-name descname"><span class="pre">broadcast_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optim_result</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.broadcast_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.broadcast_params" title="Permalink to this definition"></a></dt>
<dd><p>Apply Broadcast operations in the sequential order of parameter groups.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>bool, the status flag.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.decay_weight">
<span class="sig-name descname"><span class="pre">decay_weight</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.decay_weight"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.decay_weight" title="Permalink to this definition"></a></dt>
<dd><p>Weight decay.</p>
<p>An approach to reduce the overfitting of a deep learning neural network model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of <cite>self.parameters</cite>, and have the same shape as
<cite>self.parameters</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after weight decay.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.get_lr">
<span class="sig-name descname"><span class="pre">get_lr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_lr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_lr" title="Permalink to this definition"></a></dt>
<dd><p>Get the learning rate of current step.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>float, the learning rate of current step.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.get_lr_parameter">
<span class="sig-name descname"><span class="pre">get_lr_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_lr_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_lr_parameter" title="Permalink to this definition"></a></dt>
<dd><p>Get the learning rate of parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>]</em>) – The <cite>Parameter</cite> or list of <cite>Parameter</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter, single <cite>Parameter</cite> or <cite>list[Parameter]</cite> according to the input type.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Optimizer.scale_grad">
<span class="sig-name descname"><span class="pre">scale_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.scale_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Optimizer.scale_grad" title="Permalink to this definition"></a></dt>
<dd><p>Loss scale for mixed precision.</p>
<p>An approach of mixed precision training to improve the speed and energy efficiency of training deep neural
network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of <cite>self.parameters</cite>, and have the same shape as
<cite>self.parameters</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after loss scale.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.PReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">PReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">w</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.25</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#PReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.PReLU" title="Permalink to this definition"></a></dt>
<dd><p>PReLU activation function.</p>
<p>Applies the PReLU function element-wise.</p>
<p>PReLU is defined as: <span class="math notranslate nohighlight">\(prelu(x_i)= \max(0, x_i) + w * \min(0, x_i)\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span>
is an element of an channel of the input.</p>
<p>Here <span class="math notranslate nohighlight">\(w\)</span> is a learnable parameter with a default initial value 0.25.
Parameter <span class="math notranslate nohighlight">\(w\)</span> has dimensionality of the argument channel. If called without argument
channel, a single parameter <span class="math notranslate nohighlight">\(w\)</span> will be shared across all channels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of input. Default: 1.</p></li>
<li><p><strong>w</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The initial value of w. Default: 0.25.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of PReLU.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prelu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prelu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.PSNR">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">PSNR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/image.html#PSNR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.PSNR" title="Permalink to this definition"></a></dt>
<dd><p>Returns Peak Signal-to-Noise Ratio of two image batches.</p>
<p>It produces a PSNR value for each image in batch.
Assume inputs are <span class="math notranslate nohighlight">\(I\)</span> and <span class="math notranslate nohighlight">\(K\)</span>, both with shape <span class="math notranslate nohighlight">\(h*w\)</span>.
<span class="math notranslate nohighlight">\(MAX\)</span> represents the dynamic range of pixel values.</p>
<div class="math notranslate nohighlight">
\[\begin{split}MSE&amp;=\frac{1}{hw}\sum\limits_{i=0}^{h-1}\sum\limits_{j=0}^{w-1}[I(i,j)-K(i,j)]^2\\
PSNR&amp;=10*log_{10}(\frac{MAX^2}{MSE})\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_val</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – The dynamic range of the pixel values (255 for 8-bit grayscale images).
Default: 1.0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>img1</strong> (Tensor) - The first image batch with format ‘NCHW’. It should be the same shape and dtype as img2.</p></li>
<li><p><strong>img2</strong> (Tensor) - The second image batch with format ‘NCHW’. It should be the same shape and dtype as img1.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with dtype mindspore.float32. It is a 1-D tensor with shape N, where N is the batch num of img1.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PSNR</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">psnr</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Pad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">paddings</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'CONSTANT'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#Pad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Pad" title="Permalink to this definition"></a></dt>
<dd><p>Pads the input tensor according to the paddings and mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>paddings</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of parameter <cite>paddings</cite> is (N, 2). N is the rank of input data. All elements of
paddings are int type. For <cite>D</cite> th dimension of input, paddings[D, 0] indicates how many sizes to be
extended ahead of the <cite>D</cite> th dimension of the input tensor, and paddings[D, 1] indicates how many sizes to
be extended behind of the <cite>D</cite> th dimension of the input tensor.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies padding mode. The optional values are “CONSTANT”, “REFLECT”, “SYMMETRIC”.
Default: “CONSTANT”.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the tensor after padding.</p>
<ul class="simple">
<li><p>If <cite>mode</cite> is “CONSTANT”, it fills the edge with 0, regardless of the values of the <cite>input_x</cite>.
If the <cite>input_x</cite> is [[1,2,3],[4,5,6],[7,8,9]] and <cite>paddings</cite> is [[1,1],[2,2]], then the
Outputs is [[0,0,0,0,0,0,0],[0,0,1,2,3,0,0],[0,0,4,5,6,0,0],[0,0,7,8,9,0,0],[0,0,0,0,0,0,0]].</p></li>
<li><p>If <cite>mode</cite> is “REFLECT”, it uses a way of symmetrical copying throught the axis of symmetry to fill in.
If the <cite>input_x</cite> is [[1,2,3],[4,5,6],[7,8,9]] and <cite>paddings</cite> is [[1,1],[2,2]], then the
Outputs is [[6,5,4,5,6,5,4],[3,2,1,2,3,2,1],[6,5,4,5,6,5,4],[9,8,7,8,9,8,7],[6,5,4,5,6,5,4]].</p></li>
<li><p>If <cite>mode</cite> is “SYMMETRIC”, the filling method is similar to the “REFLECT”. It is also copied
according to the symmetry axis, except that it includes the symmetry axis. If the <cite>input_x</cite>
is [[1,2,3],[4,5,6],[7,8,9]] and <cite>paddings</cite> is [[1,1],[2,2]], then the Outputs is
[[2,1,1,2,3,3,2],[2,1,1,2,3,3,2],[5,4,4,5,6,6,5],[8,7,7,8,9,9,8],[8,7,7,8,9,9,8]].</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Pad</span><span class="p">(</span><span class="n">paddings</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;CONSTANT&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms_output</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ParameterUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ParameterUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#ParameterUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ParameterUpdate" title="Permalink to this definition"></a></dt>
<dd><p>Cell that updates parameters.</p>
<p>With this Cell, one can manually update <cite>param</cite> with the input <cite>Tensor</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param</strong> (<a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a>) – The parameter to be updated manually.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#KeyError" title="(in Python v3.8)"><strong>KeyError</strong></a> – If parameter with the specified name does not exist.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">parameters_dict</span><span class="p">()[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ParameterUpdate</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="s2">&quot;update_param&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Precision">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eval_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/precision.html#Precision"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Precision" title="Permalink to this definition"></a></dt>
<dd><p>Calculates precision for classification and multilabel data.</p>
<p>The precision function creates two local variables, <span class="math notranslate nohighlight">\(\text{true_positive}\)</span> and
<span class="math notranslate nohighlight">\(\text{false_positive}\)</span>, that are used to compute the precision. This value is
ultimately returned as the precision, an idempotent operation that simply divides
<span class="math notranslate nohighlight">\(\text{true_positive}\)</span> by the sum of <span class="math notranslate nohighlight">\(\text{true_positive}\)</span> and <span class="math notranslate nohighlight">\(\text{false_positive}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\text{precision} = \frac{\text{true_positive}}{\text{true_positive} + \text{false_positive}}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the multi-label cases, the elements of <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(y_{pred}\)</span> should be 0 or 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Metric to calculate accuracy over a dataset, for classification or
multilabel. Default: ‘classification’.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Precision</span><span class="p">(</span><span class="s1">&#39;classification&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Precision.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/precision.html#Precision.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Precision.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clears the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Precision.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/precision.html#Precision.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Precision.eval" title="Permalink to this definition"></a></dt>
<dd><p>Computes the precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>average</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specify whether calculate the average precision. Default value is False.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Float, the computed result.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Precision.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/precision.html#Precision.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Precision.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result with <cite>y_pred</cite> and <cite>y</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input <cite>y_pred</cite> and <cite>y</cite>. <cite>y_pred</cite> and <cite>y</cite> are Tensor, list or numpy.ndarray.
For ‘classification’ evaluation type, <cite>y_pred</cite> is in most cases (not strictly) a list
of floating numbers in range <span class="math notranslate nohighlight">\([0, 1]\)</span>
and the shape is <span class="math notranslate nohighlight">\((N, C)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of cases and <span class="math notranslate nohighlight">\(C\)</span>
is the number of categories. Shape of <cite>y</cite> can be <span class="math notranslate nohighlight">\((N, C)\)</span> with values 0 and 1 if one-hot
encoding is used or the shape is <span class="math notranslate nohighlight">\((N,)\)</span> with integer values if index of category is used.
For ‘multilabel’ evaluation type, <cite>y_pred</cite> and <cite>y</cite> can only be one-hot encoding with
values 0 or 1. Indices with 1 indicate positive category. The shape of <cite>y_pred</cite> and <cite>y</cite>
are both <span class="math notranslate nohighlight">\((N, C)\)</span>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the number of input is not 2.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ProximalAdagrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ProximalAdagrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_locking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/proximal_ada_grad.html#ProximalAdagrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ProximalAdagrad" title="Permalink to this definition"></a></dt>
<dd><p>Implement the ProximalAdagrad algorithm with ApplyProximalAdagrad Operator.</p>
<p>ProximalAdagrad is an online Learning and Stochastic Optimization.
Refer to paper <a class="reference external" href="http://papers.nips.cc//paper/3793-efficient-learning-using-forward-backward-splitting.pdf">Efficient Learning using Forward-Backward Splitting</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
<p>The sparse strategy is applied while the SparseGatherV2 operator being used for forward network.
The sparse feature is under continuous development. The sparse
behavior is currently performed on the CPU.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>accum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The starting value for accumulators, must be zero or positive values. Default: 0.1.</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 0.001.</p></li>
<li><p><strong>l1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l1 regularization strength, must be greater than or equal to zero. Default: 0.0.</p></li>
<li><p><strong>l2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l2 regularization strength, must be greater than or equal to zero. Default: 0.0.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, use locks for updating operation. Default: False.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Value for the loss scale. It should be greater than 0.0. Default: 1.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay value to multiply weight, must be zero or positive value. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>grads</strong> (tuple[Tensor]) - The gradients of <cite>params</cite> in the optimizer, the shape is the same as the <cite>params</cite>
in optimizer.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[bool], the value is True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ProximalAdagrad</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ProximalAdagrad</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use default learning rate of 0.1 and weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use learning rate of 0.01 and default weight decay of 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.RMSProp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">RMSProp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_locking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">centered</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/rmsprop.html#RMSProp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.RMSProp" title="Permalink to this definition"></a></dt>
<dd><p>Implements Root Mean Squared Propagation (RMSProp) algorithm.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
<p>Update <cite>params</cite> according to the RMSProp algorithm.</p>
<p>The equation is as follows:</p>
<div class="math notranslate nohighlight">
\[s_{t} = \rho s_{t-1} + (1 - \rho)(\nabla Q_{i}(w))^2\]</div>
<div class="math notranslate nohighlight">
\[m_{t} = \beta m_{t-1} + \frac{\eta} {\sqrt{s_{t} + \epsilon}} \nabla Q_{i}(w)\]</div>
<div class="math notranslate nohighlight">
\[w = w - m_{t}\]</div>
<p>The first equation calculates moving average of the squared gradient for
each weight. Then dividing the gradient by <span class="math notranslate nohighlight">\(\sqrt{ms_{t} + \epsilon}\)</span>.</p>
<p>if centered is True:</p>
<div class="math notranslate nohighlight">
\[g_{t} = \rho g_{t-1} + (1 - \rho)\nabla Q_{i}(w)\]</div>
<div class="math notranslate nohighlight">
\[s_{t} = \rho s_{t-1} + (1 - \rho)(\nabla Q_{i}(w))^2\]</div>
<div class="math notranslate nohighlight">
\[m_{t} = \beta m_{t-1} + \frac{\eta} {\sqrt{s_{t} - g_{t}^2 + \epsilon}} \nabla Q_{i}(w)\]</div>
<div class="math notranslate nohighlight">
\[w = w - m_{t}\]</div>
<p>where, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>params</cite>, which will be updated.
<span class="math notranslate nohighlight">\(g_{t}\)</span> is mean gradients, <span class="math notranslate nohighlight">\(g_{t-1}\)</span> is the last moment of <span class="math notranslate nohighlight">\(g_{t}\)</span>.
<span class="math notranslate nohighlight">\(s_{t}\)</span> is the mean square gradients, <span class="math notranslate nohighlight">\(s_{t-1}\)</span> is the last moment of <span class="math notranslate nohighlight">\(s_{t}\)</span>,
<span class="math notranslate nohighlight">\(m_{t}\)</span> is moment, the delta of <cite>w</cite>, <span class="math notranslate nohighlight">\(m_{t-1}\)</span> is the last moment of <span class="math notranslate nohighlight">\(m_{t}\)</span>.
<span class="math notranslate nohighlight">\(\rho\)</span> represents <cite>decay</cite>. <span class="math notranslate nohighlight">\(\beta\)</span> is the momentum term, represents <cite>momentum</cite>.
<span class="math notranslate nohighlight">\(\epsilon\)</span> is a smoothing term to avoid division by zero, represents <cite>epsilon</cite>.
<span class="math notranslate nohighlight">\(\eta\)</span> is learning rate, represents <cite>learning_rate</cite>. <span class="math notranslate nohighlight">\(\nabla Q_{i}(w)\)</span> is gradientse,
represents <cite>gradients</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 0.1.</p></li>
<li><p><strong>decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Decay rate. Should be equal to or greater than 0. Default: 0.9.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Hyperparameter of type float, means momentum for the moving average. Should be equal to or
greater than 0. Default: 0.0.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Should be greater than
0. Default: 1e-10.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enable a lock to protect the update of variable and accumlation tensors. Default: False.</p></li>
<li><p><strong>centered</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, gradients are normalized by the estimated variance of the gradient. Default: False.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. Should be greater than 0. Default: 1.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). Should be equal to or greater than 0. Default: 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[bool], the value is True.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSProp</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use a learning rate of default value 0.1 and a weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use a learning rate of 0.01 and a weight decay of default value 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Range">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Range</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">start</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/math.html#Range"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Range" title="Permalink to this definition"></a></dt>
<dd><p>Creates a sequence of numbers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>start</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – If <cite>limit</cite> is <cite>None</cite>, the value acts as limit in the range and first entry
defaults to <cite>0</cite>. Otherwise, it acts as first entry in the range.</p></li>
<li><p><strong>limit</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – Acts as upper limit of sequence. If <cite>None</cite>, defaults to the value of <cite>start</cite>
while set the first entry of the range to <cite>0</cite>. It can not be equal to <cite>start</cite>.</p></li>
<li><p><strong>delta</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – Increment of the range. It can not be equal to zero. Default: 1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Outputs:</dt><dd><p>Tensor, the dtype is int if the dtype of <cite>start</cite>, <cite>limit</cite> and <cite>delta</cite> all are int. Otherwise, dtype is float.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">()</span>
<span class="go">[1, 3, 5, 7]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ReLU" title="Permalink to this definition"></a></dt>
<dd><p>Rectified Linear Unit activation function.</p>
<p>Applies the rectified linear unit function element-wise. It returns
element-wise <span class="math notranslate nohighlight">\(\max(0, x)\)</span>, specially, the neurons with the negative output
will be suppressed and the active neurons will stay the same.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of ReLU.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.  2.  0.  2.  0.]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ReLU6">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ReLU6</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#ReLU6"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ReLU6" title="Permalink to this definition"></a></dt>
<dd><p>Compute ReLU6 activation function.</p>
<p>ReLU6 is similar to ReLU with a upper limit of 6, which if the inputs are greater than 6, the outputs
will be suppressed to 6.
It computes element-wise as <span class="math notranslate nohighlight">\(\min(\max(0, x), 6)\)</span>. The input is a Tensor of any valid shape.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of ReLU6.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, which has the same type as <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu6</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu6</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.  0.  0.  2.  1.]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Recall">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eval_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'classification'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/recall.html#Recall"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Recall" title="Permalink to this definition"></a></dt>
<dd><p>Calculate recall for classification and multilabel data.</p>
<p>The recall class creates two local variables, <span class="math notranslate nohighlight">\(\text{true_positive}\)</span> and <span class="math notranslate nohighlight">\(\text{false_negative}\)</span>,
that are used to compute the recall. This value is ultimately returned as the recall, an idempotent operation
that simply divides <span class="math notranslate nohighlight">\(\text{true_positive}\)</span> by the sum of <span class="math notranslate nohighlight">\(\text{true_positive}\)</span> and
<span class="math notranslate nohighlight">\(\text{false_negative}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\text{recall} = \frac{\text{true_positive}}{\text{true_positive} + \text{false_negative}}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the multi-label cases, the elements of <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(y_{pred}\)</span> should be 0 or 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>eval_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Metric to calculate the recall over a dataset, for classification or
multilabel. Default: ‘classification’.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Recall</span><span class="p">(</span><span class="s1">&#39;classification&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Recall.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/recall.html#Recall.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Recall.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clears the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Recall.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/recall.html#Recall.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Recall.eval" title="Permalink to this definition"></a></dt>
<dd><p>Computes the recall.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>average</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specify whether calculate the average recall. Default value is False.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Float, the computed result.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.Recall.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/recall.html#Recall.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Recall.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result with <cite>y_pred</cite> and <cite>y</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input <cite>y_pred</cite> and <cite>y</cite>. <cite>y_pred</cite> and <cite>y</cite> are a <cite>Tensor</cite>, a list or an array.
For ‘classification’ evaluation type, <cite>y_pred</cite> is in most cases (not strictly) a list
of floating numbers in range <span class="math notranslate nohighlight">\([0, 1]\)</span>
and the shape is <span class="math notranslate nohighlight">\((N, C)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of cases and <span class="math notranslate nohighlight">\(C\)</span>
is the number of categories. Shape of <cite>y</cite> can be <span class="math notranslate nohighlight">\((N, C)\)</span> with values 0 and 1 if one-hot
encoding is used or the shape is <span class="math notranslate nohighlight">\((N,)\)</span> with integer values if index of category is used.
For ‘multilabel’ evaluation type, <cite>y_pred</cite> and <cite>y</cite> can only be one-hot encoding with
values 0 or 1. Indices with 1 indicate positive category. The shape of <cite>y_pred</cite> and <cite>y</cite>
are both <span class="math notranslate nohighlight">\((N, C)\)</span>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the number of input is not 2.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.ReduceLogSumExp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">ReduceLogSumExp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_dims</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/math.html#ReduceLogSumExp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.ReduceLogSumExp" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by calculating exponential for all elements in the dimension,
then calculate logarithm of the sum.</p>
<p>The dtype of the tensor to be reduced is number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, keep these reduced dimensions and the length is 1.
If False, don’t keep these dimensions.
Default : False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the ‘input_x’.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is false,
the output is a 0-D tensor representing the sum of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is false,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is false,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReduceLogSumExp</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dampening</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nesterov</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/optim/sgd.html#SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SGD" title="Permalink to this definition"></a></dt>
<dd><p>Implements stochastic gradient descent (optionally with momentum).</p>
<p>Introduction to SGD can be found at <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">https://en.wikipedia.org/wiki/Stochastic_gradient_descent</a>.
Nesterov momentum is based on the formula from paper <a class="reference external" href="http://proceedings.mlr.press/v28/sutskever13.html">On the importance of initialization and
momentum in deep learning</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<div class="math notranslate nohighlight">
\[v_{t+1} = u \ast v_{t} + gradient \ast (1-dampening)\]</div>
<dl>
<dt>If nesterov is True:</dt><dd><div class="math notranslate nohighlight">
\[p_{t+1} = p_{t} - lr \ast (gradient + u \ast v_{t+1})\]</div>
</dd>
<dt>If nesterov is Flase:</dt><dd><div class="math notranslate nohighlight">
\[p_{t+1} = p_{t} - lr \ast v_{t+1}\]</div>
</dd>
</dl>
<p>To be noticed, for the first step, v_{t+1} = gradient</p>
<p>Here : where p, v and u denote the parameters, accum, and momentum respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> should be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value should be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value should be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ should be in one of group parameters.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate should be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 0.1.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value the momentum. should be at least 0.0. Default: 0.0.</p></li>
<li><p><strong>dampening</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value of dampening for momentum. should be at least 0.0. Default: 0.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). It should be equal to or greater than 0. Default: 0.0.</p></li>
<li><p><strong>nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enables the Nesterov momentum. If use nesterov, momentum must be positive,
and dampening must equal to 0.0. Default: False.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale, which should be larger
than 0.0. Default: 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[bool], the value is True.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the momentum, dampening or weight_decay value is less than 0.0.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use a learning rate of default value 0.1 and a weight decay of 0.01.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use a learning rate of 0.01 and a weight decay of default value 0.0.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SSIM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SSIM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">11</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.03</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/image.html#SSIM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SSIM" title="Permalink to this definition"></a></dt>
<dd><p>Returns SSIM index between img1 and img2.</p>
<p>Its implementation is based on Wang, Z., Bovik, A. C., Sheikh, H. R., &amp; Simoncelli, E. P. (2004). <a class="reference external" href="https://ieeexplore.ieee.org/document/1284395">Image quality
assessment: from error visibility to structural similarity</a>.
IEEE transactions on image processing.</p>
<div class="math notranslate nohighlight">
\[\begin{split}l(x,y)&amp;=\frac{2\mu_x\mu_y+C_1}{\mu_x^2+\mu_y^2+C_1}, C_1=(K_1L)^2.\\
c(x,y)&amp;=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}, C_2=(K_2L)^2.\\
s(x,y)&amp;=\frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y+C_3}, C_3=C_2/2.\\
SSIM(x,y)&amp;=l*c*s\\&amp;=\frac{(2\mu_x\mu_y+C_1)(2\sigma_{xy}+C_2}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2+\sigma_y^2+C_2)}.\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_val</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>]</em>) – The dynamic range of the pixel values (255 for 8-bit grayscale images).
Default: 1.0.</p></li>
<li><p><strong>filter_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The size of the Gaussian filter. Default: 11.</p></li>
<li><p><strong>filter_sigma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The standard deviation of Gaussian kernel. Default: 1.5.</p></li>
<li><p><strong>k1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The constant used to generate c1 in the luminance comparison function. Default: 0.01.</p></li>
<li><p><strong>k2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The constant used to generate c2 in the contrast comparison function. Default: 0.03.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>img1</strong> (Tensor) - The first image batch with format ‘NCHW’. It should be the same shape and dtype as img2.</p></li>
<li><p><strong>img2</strong> (Tensor) - The second image batch with format ‘NCHW’. It should be the same shape and dtype as img1.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as img1. It is a 1-D tensor with shape N, where N is the batch num of img1.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SSIM</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">16</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ssim</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SequentialCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SequentialCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/container.html#SequentialCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SequentialCell" title="Permalink to this definition"></a></dt>
<dd><p>Sequential cell container.</p>
<p>A list of Cells will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of cells can also be passed in.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>, </em><em>OrderedDict</em>) – List of subclass of Cell.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If the type of the argument is not list or OrderedDict.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor with shape according to the first Cell in the sequence.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the output Tensor with shape depending on the input and defined sequence of Cells.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">([</span><span class="n">conv</span><span class="p">,</span> <span class="n">bn</span><span class="p">,</span> <span class="n">relu</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[[[[0.02531557 0.        ]</span>
<span class="go">   [0.04933941 0.04880078]]</span>
<span class="go">  [[0.         0.        ]</span>
<span class="go">   [0.         0.        ]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Sigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Sigmoid activation function.</p>
<p>Applies sigmoid-type activation element-wise.</p>
<p>Sigmoid function is defined as:
<span class="math notranslate nohighlight">\(\text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span> is the element of the input.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of Tanh.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.2688  0.11914  0.5  0.881  0.7305]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SmoothL1Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SmoothL1Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sigma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/loss/loss.html#SmoothL1Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SmoothL1Loss" title="Permalink to this definition"></a></dt>
<dd><p>A loss class for learning region proposals.</p>
<p>SmoothL1Loss can be regarded as modified version of L1Loss or a combination of L1Loss and L2Loss.
L1Loss computes the element-wise absolute difference between two input Tensor while L2Loss computes the
squared difference between two input Tensor. L2Loss often leads to faster convergence but it is less
robust to outliers.</p>
<p>Given two input <span class="math notranslate nohighlight">\(x,\  y\)</span> of length <span class="math notranslate nohighlight">\(N\)</span>, the unreduced SmoothL1Loss can be described
as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L_{i} =
\begin{cases}
0.5 (x_i - y_i)^2, &amp; \text{if } |x_i - y_i| &lt; \text{sigma}; \\
|x_i - y_i| - 0.5, &amp; \text{otherwise. }
\end{cases}\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\text{sigma}\)</span> controls the point where the loss function changes from quadratic to linear.
Its default value is 1.0. <span class="math notranslate nohighlight">\(N\)</span> is the batch size. This function returns an
unreduced loss Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A parameter used to control the point where the function will change from
quadratic to linear. Default: 1.0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>target_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, loss float tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Softmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Softmax" title="Permalink to this definition"></a></dt>
<dd><p>Softmax activation function.</p>
<p>Applies the Softmax function to an n-dimensional input Tensor.</p>
<p>The input is a Tensor of logits transformed with exponential function and then
normalized to lie in range [0, 1] and sum up to 1.</p>
<p>Softmax is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(x_{i}) =  \frac{\exp(x_i)}{\sum_{j=0}^{n-1}\exp(x_j)},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th slice in the given dimension of the input Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The axis to apply Softmax operation, -1 means the last dimension. Default: -1.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of Softmax.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, which has the same type and shape as <cite>x</cite> with values in the range[0,1].</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.03168  0.01166  0.0861  0.636  0.2341]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SoftmaxCrossEntropyExpand">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SoftmaxCrossEntropyExpand</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sparse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/loss/loss.html#SoftmaxCrossEntropyExpand"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SoftmaxCrossEntropyExpand" title="Permalink to this definition"></a></dt>
<dd><p>Computes softmax cross entropy between logits and labels. Implemented by expanded formula.</p>
<p>This is a wrapper of several functions.</p>
<div class="math notranslate nohighlight">
\[\ell(x_i, t_i) = -log\left(\frac{\exp(x_{t_i})}{\sum_j \exp(x_j)}\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is a 1D score Tensor, <span class="math notranslate nohighlight">\(t_i\)</span> is the target class.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When argument sparse is set to True, the format of the label is the index
ranging from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(C - 1\)</span> instead of one-hot vectors.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether labels use sparse format or not. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>label</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a scalar tensor including the mean loss.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyExpand</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">512</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SoftmaxCrossEntropyWithLogits">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SoftmaxCrossEntropyWithLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">is_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'none'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">smooth_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/loss/loss.html#SoftmaxCrossEntropyWithLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SoftmaxCrossEntropyWithLogits" title="Permalink to this definition"></a></dt>
<dd><p>Computes softmax cross entropy between logits and labels.</p>
<p>Measures the distribution error between the probabilities of the input (computed with softmax function) and the
target where the classes are mutually exclusive (only one class is positive) using cross entropy loss.</p>
<p>Typical input into this function is unnormalized scores and target of each class.
Scores Tensor <span class="math notranslate nohighlight">\(x\)</span> is of shape <span class="math notranslate nohighlight">\((N, C)\)</span> and target Tensor <span class="math notranslate nohighlight">\(t\)</span> is a
Tensor of shape <span class="math notranslate nohighlight">\((N, C)\)</span> which contains one-hot labels of length <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>For each instance <span class="math notranslate nohighlight">\(N_i\)</span>, the loss is given as:</p>
<div class="math notranslate nohighlight">
\[\ell(x_i, t_i) = - \log\left(\frac{\exp(x_{t_i})}{\sum_j \exp(x_j)}\right)
=  -x_{t_i} + \log\left(\sum_j \exp(x_i)\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is a 1D score Tensor, <span class="math notranslate nohighlight">\(t_i\)</span> is a scalar.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While the target classes are mutually exclusive, i.e., only one class is positive in the target, the predicted
probabilities need not to be exclusive. It is only required that the predicted probability distribution
of entry is a valid one.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>is_grad</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether calculate grad only. Default: True.</p></li>
<li><p><strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Specifies whether labels use sparse format or not. Default: False.</p></li>
<li><p><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Type of reduction to be applied to loss. The optional values are “mean”, “sum”, and “none”.
If “none”, do not perform reduction. Default: “none”.</p></li>
<li><p><strong>smooth_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Label smoothing factor. It is a optional input which should be in range [0, 1].
Default: 0.</p></li>
<li><p><strong>num_classes</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of classes in the task. It is a optional input Default: 2.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - Tensor of shape (N, C).</p></li>
<li><p><strong>labels</strong> (Tensor) - Tensor of shape (N, ). If <cite>sparse</cite> is True, The type of
<cite>labels</cite> is mindspore.int32. If <cite>sparse</cite> is False, the type of <cite>labels</cite> is the same as the type of <cite>logits</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a tensor of the same shape as logits with the component-wise
logistic losses.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">labels_np</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.SparseToDense">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">SparseToDense</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/sparse/sparse.html#SparseToDense"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.SparseToDense" title="Permalink to this definition"></a></dt>
<dd><p>Convert a sparse tensor into dense.</p>
<p>Not yet supported by any backend at the moment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sparse_tensor</strong> (<a class="reference internal" href="mindspore.html#mindspore.SparseTensor" title="mindspore.SparseTensor"><em>SparseTensor</em></a>) – the sparse tensor to convert.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor, the tensor converted.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SparseToDenseCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dense_shape</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SparseToDenseCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">dense_shape</span> <span class="o">=</span> <span class="n">dense_shape</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_to_dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SparseToDense</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">sparse</span> <span class="o">=</span> <span class="n">SparseTensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_to_dense</span><span class="p">(</span><span class="n">sparse</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">SparseToDenseCell</span><span class="p">(</span><span class="n">dense_shape</span><span class="p">)(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Tanh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Tanh</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#Tanh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Tanh" title="Permalink to this definition"></a></dt>
<dd><p>Tanh activation function.</p>
<p>Applies the Tanh function element-wise, returns a new tensor with the hyperbolic tangent of the elements of input,
The input is a Tensor with any valid shape.</p>
<p>Tanh function is defined as:</p>
<div class="math notranslate nohighlight">
\[tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is an element of the input Tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of Tanh.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.7617  0.964  0.995  0.964 0.7617]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.TensorAddQuant">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">TensorAddQuant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ema_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/quant.html#TensorAddQuant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TensorAddQuant" title="Permalink to this definition"></a></dt>
<dd><p>Add Fake Quant OP after TensorAdd OP.</p>
<p>This part is a more detailed overview of TensorAdd op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ema_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Exponential Moving Average algorithm parameter. Default: 0.999.</p></li>
<li><p><strong>per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. Default: False.</p></li>
<li><p><strong>num_bits</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The bit number of quantization, supporting 4 and 8bits. Default: 8.</p></li>
<li><p><strong>symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm is symmetric or not. Default: False.</p></li>
<li><p><strong>narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – The quantization algorithm uses narrow range or not. Default: False.</p></li>
<li><p><strong>quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Quantization delay parameters according to the global step. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input of TensorAddQuant.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">add_quant</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TensorAddQuant</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">add_quant</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Top1CategoricalAccuracy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Top1CategoricalAccuracy</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/topk.html#Top1CategoricalAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Top1CategoricalAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the top-1 categorical accuracy. This class is a specialized class for TopKCategoricalAccuracy.
Refer to class ‘TopKCategoricalAccuracy’ for more details.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Top1CategoricalAccuracy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">topk</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Top5CategoricalAccuracy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Top5CategoricalAccuracy</span></span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/topk.html#Top5CategoricalAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Top5CategoricalAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the top-5 categorical accuracy. This class is a specialized class for TopKCategoricalAccuracy.
Refer to class ‘TopKCategoricalAccuracy’ for more details.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Top5CategoricalAccuracy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">topk</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.TopKCategoricalAccuracy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">TopKCategoricalAccuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/topk.html#TopKCategoricalAccuracy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TopKCategoricalAccuracy" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the top-k categorical accuracy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The method <cite>update</cite> must receive input of the form <span class="math notranslate nohighlight">\((y_{pred}, y)\)</span>. If some samples have
the same accuracy, the first sample will be chosen.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies the top-k categorical accuracy to compute.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>k</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>k</cite> is less than 1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TopKCategoricalAccuracy</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">topk</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.TopKCategoricalAccuracy.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/topk.html#TopKCategoricalAccuracy.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TopKCategoricalAccuracy.clear" title="Permalink to this definition"></a></dt>
<dd><p>Clear the internal evaluation result.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.TopKCategoricalAccuracy.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/topk.html#TopKCategoricalAccuracy.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TopKCategoricalAccuracy.eval" title="Permalink to this definition"></a></dt>
<dd><p>Computes the top-k categorical accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Float, computed result.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.TopKCategoricalAccuracy.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics/topk.html#TopKCategoricalAccuracy.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TopKCategoricalAccuracy.update" title="Permalink to this definition"></a></dt>
<dd><p>Updates the internal evaluation result y_pred and y.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> – Input y_pred and y. y_pred and y are Tensor, list or numpy.ndarray.
y_pred is in most cases (not strictly) a list of floating numbers in range <span class="math notranslate nohighlight">\([0, 1]\)</span>
and the shape is <span class="math notranslate nohighlight">\((N, C)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of cases and <span class="math notranslate nohighlight">\(C\)</span>
is the number of categories. y contains values of integers. The shape is <span class="math notranslate nohighlight">\((N, C)\)</span>
if one-hot encoding is used. Shape can also be <span class="math notranslate nohighlight">\((N,)\)</span> if category index is used.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.TrainOneStepCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">TrainOneStepCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#TrainOneStepCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TrainOneStepCell" title="Permalink to this definition"></a></dt>
<dd><p>Network training package class.</p>
<p>Wraps the network with an optimizer. The resulting Cell is trained with input <a href="#id8"><span class="problematic" id="id9">*</span></a>inputs.
The backward graph will be created in the construct function to update the parameter. Different
parallel modes are available for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
<li><p><strong>sens</strong> (<em>Number</em>) – The scaling number to be filled as the input of backpropagation. Default value is 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>(*inputs)</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a scalar Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) Using the WithLossCell existing provide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Using user-defined WithLossCell</span>
<span class="go">&gt;&gt;&gt;class MyWithLossCell(nn.cell):</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="nb">super</span><span class="p">(</span><span class="n">WithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.TrainOneStepWithLossScaleCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">TrainOneStepWithLossScaleCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_update_cell</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/loss_scale.html#TrainOneStepWithLossScaleCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.TrainOneStepWithLossScaleCell" title="Permalink to this definition"></a></dt>
<dd><p>Network training with loss scaling.</p>
<p>This is a training step with loss scaling. It takes a network, an optimizer and possibly a scale update
Cell as args. The loss scale value can be updated in both host side or device side. The
TrainOneStepWithLossScaleCell will be compiled to be graph which takes <cite>data</cite>, <cite>label</cite>, <cite>sens</cite> as input
data. The <cite>sens</cite> is acting as loss scaling value. If you want to update it on host side, the value should
be provided. If <cite>sens</cite> is not given, the loss scale update logic should be provied by <cite>scale_update_cell</cite>.
If <cite>scale_update_cell</cite> is not None and <cite>sens</cite> is provided, the <cite>scale_update_cell</cite> will be ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
<li><p><strong>scale_update_cell</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The loss scaling update logic cell. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
<li><p><strong>label</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
<li><p><strong>scaling_sens</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the loss, overflow flag and current loss scaling value.</p>
<ul class="simple">
<li><p><strong>loss</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>overflow</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>, type is bool.</p></li>
<li><p><strong>loss_scale</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manager</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DynamicLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_window</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_update_cell</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.Unfold">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">Unfold</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ksizes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rates</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/basic.html#Unfold"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.Unfold" title="Permalink to this definition"></a></dt>
<dd><p>Extract patches from images.
The input tensor must be a 4-D tensor and the data format is NCHW.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ksizes</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of sliding window, should be a tuple or a list of integers,
and the format is [1, ksize_row, ksize_col, 1].</p></li>
<li><p><strong>strides</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – Distance between the centers of the two consecutive patches,
should be a tuple or list of int, and the format is [1, stride_row, stride_col, 1].</p></li>
<li><p><strong>rates</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – In each extracted patch, the gap between the corresponding dimension
pixel positions, should be a tuple or a list of integers, and the format is [1, rate_row, rate_col, 1].</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The type of padding algorithm, is a string whose value is “same” or “valid”,
not case sensitive. Default: “valid”.</p>
<ul>
<li><p>same: Means that the patch can take the part beyond the original image, and this part is filled with 0.</p></li>
<li><p>valid: Means that the taken patch area must be completely covered in the original image.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - A 4-D tensor whose shape is [in_batch, in_depth, in_row, in_col] and
data type is number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a 4-D tensor whose data type is same as ‘input_x’,
and the shape is [out_batch, out_depth, out_row, out_col], the out_batch is the same as the in_batch.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Unfold</span><span class="p">(</span><span class="n">ksizes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">rates</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="go">Tensor ([[[[1, 1] [1, 1]] [[1, 1], [1, 1]] [[1, 1] [1, 1]], [[1, 1], [1, 1]]]],</span>
<span class="go">        shape=(1, 4, 2, 2), dtype=mstype.float16)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.VirtualDatasetCellTriple">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">VirtualDatasetCellTriple</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backbone</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#VirtualDatasetCellTriple"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.VirtualDatasetCellTriple" title="Permalink to this definition"></a></dt>
<dd><p>Wrap the network with virtual dataset to convert data parallel layout to model parallel layout.</p>
<p>VirtualDatasetCellTriple is a virtual Primitive, it does not exist in the final executing graph. Inputs and outputs
of VirtualDatasetCellTriple are distributed in data parallel pattern, tensor redistribution Primitives is inserted
dynamically during the graph compile process.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only used in semi auto parallel and auto parallel mode. There are three inputs, as contrary to two inputs in
_VirtualDatasetCell.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>backbone</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The target network to wrap.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">VirtualDatasetCellTriple</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.WithEvalCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">WithEvalCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_cast_fp32</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#WithEvalCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.WithEvalCell" title="Permalink to this definition"></a></dt>
<dd><p>Cell that returns loss, output and label for evaluation.</p>
<p>This Cell accepts a network and loss function as arguments and computes loss for model.
It returns loss, output and label to calculate the metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The network Cell.</p></li>
<li><p><strong>loss_fn</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The loss Cell.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
<li><p><strong>label</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, containing a scalar loss Tensor, a network output Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>
and a label Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># For a defined network Net without loss function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eval_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithEvalCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.WithGradCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">WithGradCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#WithGradCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.WithGradCell" title="Permalink to this definition"></a></dt>
<dd><p>Cell that returns the gradients.</p>
<p>Wraps the network with backward cell to compute gradients. A network with a loss function is necessary
as argument. If loss function in None, the network must be a wrapper of network and loss function. This
Cell accepts ‘<a href="#id10"><span class="problematic" id="id11">*</span></a>inputs’ as inputs and returns gradients for each trainable parameter.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Run in PyNative mode.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The target network to wrap.</p></li>
<li><p><strong>loss_fn</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Primitive loss function used to compute gradients. Default: None.</p></li>
<li><p><strong>sens</strong> (<em>Union</em><em>[</em><em>None</em><em>, </em><a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Scalar</em><em>, </em><em>Tuple </em><em>...</em><em>]</em>) – The sensitive for backpropagation, the type and shape
should be same as the <cite>network</cite> output. If None, we will fill one to a same type shape of
output value. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>(*inputs)</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>list, a list of Tensors with identical shapes as trainable weights.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># For a defined network Net without loss function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithGradCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># For a network wrapped with loss function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithGradCell</span><span class="p">(</span><span class="n">net_with_criterion</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.WithLossCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">WithLossCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">backbone</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/wrap/cell_wrapper.html#WithLossCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.WithLossCell" title="Permalink to this definition"></a></dt>
<dd><p>Cell with loss function.</p>
<p>Wraps the network with loss function. This Cell accepts data and label as inputs and
the computed loss will be returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>backbone</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The target network to wrap.</p></li>
<li><p><strong>loss_fn</strong> (<a class="reference internal" href="#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The loss function used to compute loss.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
<li><p><strong>label</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a scalar tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_criterion</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="mindspore.nn.WithLossCell.backbone_network">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backbone_network</span></span><a class="headerlink" href="#mindspore.nn.WithLossCell.backbone_network" title="Permalink to this definition"></a></dt>
<dd><p>Returns the backbone network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Cell, the backbone network.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.nn.get_activation">
<span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">get_activation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/layer/activation.html#get_activation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.get_activation" title="Permalink to this definition"></a></dt>
<dd><p>Gets the activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The name of the activation function.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Function, the activation function.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_activation</span><span class="p">(</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.nn.get_metric_fn">
<span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">get_metric_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics.html#get_metric_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.get_metric_fn" title="Permalink to this definition"></a></dt>
<dd><p>Gets the metric method based on the input name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The name of metric method. Refer to the ‘__factory__’
object for the currently supported metrics.</p></li>
<li><p><strong>args</strong> – Arguments for the metric function.</p></li>
<li><p><strong>kwargs</strong> – Keyword arguments for the metric function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Metric object, class instance of the metric method.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metric</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">get_metric_fn</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="n">eval_type</span><span class="o">=</span><span class="s1">&#39;classification&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.nn.names">
<span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">names</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/mindspore/nn/metrics.html#names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.names" title="Permalink to this definition"></a></dt>
<dd><p>Get the names of the metric methods.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>List, the name list of metric methods.</p>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.hub.html" class="btn btn-neutral float-left" title="mindspore.hub" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.nn.dynamic_lr.html" class="btn btn-neutral float-right" title="mindspore.nn.dynamic_lr" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>