<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations.nn_ops &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.dynamic_lr.html">mindspore.nn.dynamic_lr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.operations.nn_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.operations.nn_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for nn.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">...common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">..primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">prim_attr_register</span>


<span class="k">def</span> <span class="nf">_check_positive_int_or_tuple</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an argument is a positive int or tuple with 2 or 4(when allow_four is True) positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be an positive int number or a tuple of two &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;or four &#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">allow_four</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_return_value</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="n">arg_value</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_four</span><span class="p">:</span>
                <span class="n">_raise_message</span><span class="p">()</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_raise_message</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">ret_value</span> <span class="o">=</span> <span class="n">_get_return_value</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ret_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret_value</span>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Flatten">[docs]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)` to be flattened.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, X)`, where :math:`X` is</span>
<span class="sd">        the product of the remaining dimension.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; flatten = P.Flatten()</span>
<span class="sd">        &gt;&gt;&gt; output = flatten(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; assert output.shape == (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;input_x rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">prod</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prod</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Softmax">[docs]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax operation.</span>

<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given aixs :math:`x`, then for each element :math:`x_i`,</span>
<span class="sd">    the Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple]): The axis to perform the Softmax operation. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input of Softmax, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax = P.Softmax()</span>
<span class="sd">        &gt;&gt;&gt; softmax(input_x)</span>
<span class="sd">        [0.01165623, 0.03168492, 0.08612854, 0.23412167, 0.6364086]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of axis&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;length of axis&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">axis_v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>


<div class="viewcode-block" id="LogSoftmax"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LogSoftmax">[docs]</a><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log Softmax activation function.</span>

<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given aixs, :math:`x` for each element :math:`x_i`,</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The axis to perform the Log softmax operation. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input of Log Softmax, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log_softmax = P.LogSoftmax()</span>
<span class="sd">        &gt;&gt;&gt; log_softmax(input_x)</span>
<span class="sd">        [-4.4519143, -3.4519143, -2.4519143, -1.4519144, -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>


<div class="viewcode-block" id="Softplus"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Softplus">[docs]</a><span class="k">class</span> <span class="nc">Softplus</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus activation function.</span>

<span class="sd">    Softplus is a smooth approximation to the ReLU function.</span>
<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \log(1 + \exp(\text{input_x})),</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose data type must be float.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softplus = P.Softplus()</span>
<span class="sd">        &gt;&gt;&gt; softplus(input_x)</span>
<span class="sd">        [1.3132615, 2.126928, 3.0485873, 4.01815, 5.0067153]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softplus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="Softsign"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Softsign">[docs]</a><span class="k">class</span> <span class="nc">Softsign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softsign activation function.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac{\text{input_x}}{1 + \left| \text{input_x} \right|},</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softsign = P.Softsign()</span>
<span class="sd">        &gt;&gt;&gt; softsign(input_x)</span>
<span class="sd">        [0. -0.5 0.6666667 0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softsign&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU(Rectified Linear Unit) of input tensor element-wise.</span>

<span class="sd">    It returns :math:`\max(x,\  0)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu = P.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; result = relu(input_x)</span>
<span class="sd">        [[0, 4.0, 0.0], [2.0, 0.0, 9.0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReLU6">[docs]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU(Rectified Linear Unit) upper bounded by 6 of input tensor element-wise.</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu6 = P.ReLU6()</span>
<span class="sd">        &gt;&gt;&gt; result = relu6(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLU6&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="ReLUV2"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReLUV2">[docs]</a><span class="k">class</span> <span class="nc">ReLUV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU(Rectified Linear Unit) of input tensor element-wise.</span>

<span class="sd">    It returns :math:`\max(x,\  0)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor must be a 4-D tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - Has the same type and shape as the `input_x`.</span>
<span class="sd">        - **mask** (Tensor) - A tensor whose data type must be uint8.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, -2], [-3, 4]], [[-5, 6], [7, -8]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu_v2 = P.ReLUV2()</span>
<span class="sd">        &gt;&gt;&gt; output = relu_v2(input_x)</span>
<span class="sd">        ([[[[1., 0.], [0., 4.]], [[0., 6.], [7., 0.]]]],</span>
<span class="sd">         [[[[1, 0], [2, 0]], [[2, 0], [1, 0]]]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLUV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">mask_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `input_x` should be a 4-D tensor, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got a </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">-D tensor whose shape is </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span> <span class="ow">and</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
                    <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">//</span> <span class="mi">32</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span> <span class="o">//</span> <span class="mi">16</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span> <span class="ow">and</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">mask_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">mask_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">mask_dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">output_shape</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">output_dtype</span><span class="p">,</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="Elu"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Elu">[docs]</a><span class="k">class</span> <span class="nc">Elu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes exponential linear: `alpha * (exp(x) - 1)` if x &lt; 0, `x` otherwise.</span>
<span class="sd">    The data type of input tensor must be float.</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (float): The coefficient of negative factor whose type is float,</span>
<span class="sd">            only support &#39;1.0&#39; currently. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose data type must be float.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; elu = P.Elu()</span>
<span class="sd">        &gt;&gt;&gt; result = elu(input_x)</span>
<span class="sd">        Tensor([[-0.632  4.0   -0.999]</span>
<span class="sd">                [2.0    -0.993  9.0  ]], shape=(2, 3), dtype=mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Elu&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="HSwish"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.HSwish">[docs]</a><span class="k">class</span> <span class="nc">HSwish</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard swish activation function.</span>

<span class="sd">    Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard swish is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6},</span>

<span class="sd">    where :math:`x_{i}` is the :math:`i`-th slice in the given dimension of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The input of HSwish, data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_data`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; hswish = P.HSwish()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hswish(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xshape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">xshape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sigmoid activation function.</span>

<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + exp(-x_i)},</span>

<span class="sd">    where :math:`x_i` is the element of the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Sigmoid, data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = P.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; sigmoid(input_x)</span>
<span class="sd">        [0.73105866, 0.880797, 0.9525742, 0.98201376, 0.9933071]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="HSigmoid"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.HSigmoid">[docs]</a><span class="k">class</span> <span class="nc">HSigmoid</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard sigmoid is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),</span>

<span class="sd">    where :math:`x_{i}` is the :math:`i`-th slice in the given dimension of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The input of HSigmoid, data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_data`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; hsigmoid = P.HSigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hsigmoid(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tanh activation function.</span>

<span class="sd">    Computes hyperbolic tangent of input element-wise. The Tanh function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Tanh.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tanh = P.Tanh()</span>
<span class="sd">        &gt;&gt;&gt; tanh(input_x)</span>
<span class="sd">        [0.7615941, 0.9640276, 0.9950548, 0.9993293, 0.99990916]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="FusedBatchNorm"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FusedBatchNorm">[docs]</a><span class="k">class</span> <span class="nc">FusedBatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FusedBatchNorm is a BatchNorm that moving mean and moving variance will be computed instead of being loaded.</span>

<span class="sd">    Batch Normalization is widely used in convolutional networks. This operation applies</span>
<span class="sd">    Batch Normalization over input to avoid internal covariate shift as described in the</span>
<span class="sd">    paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    feature using a mini-batch of data and the learned parameters which can be described</span>
<span class="sd">    in the following formula.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (int): Mode of batch normalization, value is 0 or 1. Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.9.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensor, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the `input_x`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = P.FusedBatchNorm()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, scale, bias, mean, variance)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;running_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_inv_variance&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_parameter</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="FusedBatchNormEx"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FusedBatchNormEx">[docs]</a><span class="k">class</span> <span class="nc">FusedBatchNormEx</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FusedBatchNormEx is an extension of FusedBatchNorm, FusedBatchNormEx has one more output(output reserve)</span>
<span class="sd">    than FusedBatchNorm, reserve will be used in backpropagation phase. FusedBatchNorm is a BatchNorm that</span>
<span class="sd">    moving mean and moving variance will be computed instead of being loaded.</span>

<span class="sd">    Batch Normalization is widely used in convolutional networks. This operation applies</span>
<span class="sd">    Batch Normalization over input to avoid internal covariate shift as described in the</span>
<span class="sd">    paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    feature using a mini-batch of data and the learned parameters which can be described</span>
<span class="sd">    in the following formula.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (int): Mode of batch normalization, value is 0 or 1. Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.9.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of FusedBatchNormEx, Tensor of shape :math:`(N, C)`,</span>
<span class="sd">                                 data type: float16 or float32.</span>
<span class="sd">        - **scale** (Tensor) - Parameter scale, same with gamma above-mentioned, Tensor of shape :math:`(C,)`,</span>
<span class="sd">                               data type: float32.</span>
<span class="sd">        - **bias** (Tensor) - Parameter bias, same with beta above-mentioned, Tensor of shape :math:`(C,)`,</span>
<span class="sd">                              data type: float32.</span>
<span class="sd">        - **mean** (Tensor) - mean value, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **variance** (Tensor) - variance value, Tensor of shape :math:`(C,)`, data type: float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 6 Tensors, the normalized input, the updated parameters and reserve.</span>

<span class="sd">        - **output_x** (Tensor) - The input of FusedBatchNormEx, same type and shape as the `input_x`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Updated parameter scale, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **updated_bias** (Tensor) - Updated parameter bias, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Updated mean value, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Updated variance value, Tensor of shape :math:`(C,)`,</span>
<span class="sd">                                                 data type: float32.</span>
<span class="sd">        - **reserve** (Tensor) - reserve space, Tensor of shape :math:`(C,)`, data type: float32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = P.FusedBatchNormEx()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, scale, bias, mean, variance)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;save_scale&#39;</span><span class="p">,</span> <span class="s1">&#39;save_bias&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_inv_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_parameter</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;scale rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;bias shape&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape[0]&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_x shape[1]&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;mean rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">bias</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span></div>


<div class="viewcode-block" id="BNTrainingReduce"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BNTrainingReduce">[docs]</a><span class="k">class</span> <span class="nc">BNTrainingReduce</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For BatchNorm operator, this operator update the moving averages for training and is used in conjunction with</span>
<span class="sd">    BNTrainingUpdate.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D Tensor with float16 or float32 data type. Tensor of shape :math:`(N, C, A, B)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **sum** (Tensor) - A 1-D Tensor with float32 data type. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **square_sum** (Tensor) - A 1-D Tensor with float32 data type. Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bn_training_reduce = P.BNTrainingReduce(input_x)</span>
<span class="sd">        &gt;&gt;&gt; output = bn_training_reduce(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;square_sum&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">([</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x_type&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x_type</span><span class="p">,</span> <span class="n">x_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="BNTrainingUpdate"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BNTrainingUpdate">[docs]</a><span class="k">class</span> <span class="nc">BNTrainingUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For BatchNorm operator, this operator update the moving averages for training and is used in conjunction with</span>
<span class="sd">    BNTrainingReduce.</span>

<span class="sd">    Args:</span>
<span class="sd">        isRef (bool): If a ref. Default: True.</span>
<span class="sd">        epsilon (float): A small value added to variance avoid dividing by zero. Default: 1e-5.</span>
<span class="sd">        factor (float): A weight for updating the mean and variance. Default: 0.1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D Tensor with float16 or float32 data type. Tensor of shape :math:`(N, C, A, B)`.</span>
<span class="sd">        - **sum** (Tensor) - A 1-D Tensor with float16 or float32 data type for the output of operator BNTrainingReduce.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **square_sum** (Tensor) - A 1-D Tensor with float16 or float32 data type for the output of operator</span>
<span class="sd">          BNTrainingReduce. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **scale** (Tensor) - A 1-D Tensor with float16 or float32, for the scaling factor.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **offset** (Tensor) - A 1-D Tensor with float16 or float32, for the scaling offset.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **mean** (Tensor) - A 1-D Tensor with float16 or float32, for the scaling mean. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - A 1-D Tensor with float16 or float32, for the update variance.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Tensor, has the same shape data type as `x`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor for the updated mean, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor for the updated variance, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>
<span class="sd">        - **batch_mean** (Tensor) - Tensor for the mean of `x`, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>
<span class="sd">        - **batch_variance** (Tensor) - Tensor for the mean of `variance`, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; square_sum = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; offset = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bn_training_update = P.BNTrainingUpdate()</span>
<span class="sd">        &gt;&gt;&gt; output = bn_training_update(input_x, sum, square_sum, scale, offset, mean, variance)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">isRef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;square_sum&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;running_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_inv_variance&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;isRef&quot;</span><span class="p">,</span> <span class="n">isRef</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;factor&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;BNTrainingUpdate&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;factor&#39;</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;BNTrainingUpdate&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sum rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">sum</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;square_sum rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">square_sum</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;scale rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;mean rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;variance rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">variance</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;sum shape&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;square_sum shape&quot;</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;offset shape&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x_type&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;sum_type&quot;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;square_sum_type&quot;</span><span class="p">:</span> <span class="n">square_sum</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;scale_type&quot;</span><span class="p">:</span> <span class="n">scale</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;b_type&quot;</span><span class="p">:</span> <span class="n">b</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;mean_type&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;variance_type&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span></div>


<div class="viewcode-block" id="BatchNorm"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BatchNorm">[docs]</a><span class="k">class</span> <span class="nc">BatchNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over input to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters which can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_training (bool): If `is_training` is True, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `is_training` is False, they&#39;re loaded from checkpoint during inference. Default: False.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `mean`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensor, the normalized inputs and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the input_x. The shape is :math:`(N, C)`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_1** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_2** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batch_norm = P.BatchNorm()</span>
<span class="sd">        &gt;&gt;&gt; output = batch_norm(input_x, scale, bias, mean, variance)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_training&#39;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_1&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_2&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;scale rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;bias shape&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape[0]&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_x shape[1]&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;mean rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">bias</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Conv2D">[docs]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Applies a 2D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size and :math:`C_{in}` is channel number. For each batch of shape</span>
<span class="sd">    :math:`(C_{in}, H_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is the cross correlation operator, :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to the :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{ks_h}, \text{ks_w})`, where :math:`\text{ks_h}` and</span>
<span class="sd">    :math:`\text{ks_w}` are the height and width of the convolution kernel. The full kernel has shape</span>
<span class="sd">    :math:`(C_{out}, C_{in} // \text{group}, \text{ks_h}, \text{ks_w})`, where group is the group number</span>
<span class="sd">    to split the input in the channel dimension.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -</span>
<span class="sd">    (\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -</span>
<span class="sd">    (\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` respectively.</span>


<span class="sd">    The first introduction can be found in paper `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_. More detailed introduction can be found here:</span>
<span class="sd">    http://cs231n.github.io/convolutional-networks/.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimension of the output.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 2D convolution.</span>
<span class="sd">        mode (int): Modes for different convolutions. 0 Math convolutiuon, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 1.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the</span>
<span class="sd">                    padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the value that applied 2D convolution.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = P.Conv2D(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; conv2d(input, weight)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, padding must be zero when pad_mode is &#39;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x_shape[1] / group&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;w_shape[0]&#39;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_size_h</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">stride_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">dilation_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                <span class="o">/</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                <span class="o">/</span> <span class="n">stride_w</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">))</span>
        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="DepthwiseConv2dNative"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.DepthwiseConv2dNative">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseConv2dNative</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the depth-wise convolution value for the input.</span>

<span class="sd">    Applies depthwise conv2d for the input, which will generate more channels with channel_multiplier.</span>
<span class="sd">    Given an input tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})` where :math:`N` is the batch size and a</span>
<span class="sd">    filter tensor with kernel size :math:`(ks_{h}, ks_{w})`, containing :math:`C_{in} * \text{channel_multiplier}`</span>
<span class="sd">    convolutional filters of depth 1; it applies different filters to each input channel (channel_multiplier channels</span>
<span class="sd">    for each input channel has the default value 1), then concatenates the results together. The output has</span>
<span class="sd">    :math:`\text{in_channels} * \text{channel_multiplier}` channels.</span>

<span class="sd">    Args:</span>
<span class="sd">        channel_multiplier (int): The multipiler for the original output convolution. Its value must be greater than 0.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution kernel.</span>
<span class="sd">        mode (int): Modes for different convolutions. 0 Math convolution, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 3.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union[int, tuple[int]]): The pad value to be filled. If `pad` is an integer, the paddings of</span>
<span class="sd">            top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the padding</span>
<span class="sd">            of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly. Default: 0.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Specifies the dilation rate to be used for the dilated convolution.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **weight** (Tensor) - Set the size of kernel as :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(K, C_{in}, K_1, K_2)`, `K` must be 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in} * \text{channel_multiplier}, H_{out}, W_{out})`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([1, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; depthwise_conv2d = P.DepthwiseConv2dNative(channel_multiplier = 3, kernel_size = (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = depthwise_conv2d(input, weight)</span>
<span class="sd">        &gt;&gt;&gt; output.shape == (10, 96, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">channel_multiplier</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DepthwiseConv2dNative&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of stride should be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of dilation should be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, padding must be zero when pad_mode is &#39;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;channel_multiplier&quot;</span><span class="p">,</span> <span class="n">channel_multiplier</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_size_n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_size_h</span><span class="p">,</span> <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dilation_h</span><span class="p">,</span> <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>
        <span class="k">if</span> <span class="n">kernel_size_n</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The batch of input weight should be 1, but got </span><span class="si">{</span><span class="n">kernel_size_n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                <span class="o">/</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                <span class="o">/</span> <span class="n">stride_w</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<span class="k">class</span> <span class="nc">_Pool</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max/avg pooling operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of the kernel, that must be a tuple</span>
<span class="sd">           of two `int` for height and width. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The stride of the window, that must be</span>
<span class="sd">            a tuple of two `int` for height and width. Default: 1.</span>
<span class="sd">        padding (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;MaxPoolWithArgmax&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">input_h</span><span class="p">,</span> <span class="n">input_w</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_h</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_w</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">shape_value</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape_value</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; The kernel size is not valid, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;please check it if is larger than data&#39;s shape size.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<div class="viewcode-block" id="MaxPool"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.MaxPool">[docs]</a><span class="k">class</span> <span class="nc">MaxPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling operation.</span>

<span class="sd">    Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width are both ksize, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        padding (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_op = P.MaxPool(padding=&quot;VALID&quot;, ksize=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = maxpool_op(input_tensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxPoolWithArgmax"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.MaxPoolWithArgmax">[docs]</a><span class="k">class</span> <span class="nc">MaxPoolWithArgmax</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform max pooling on the input Tensor and return both max values and indices.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg value,</span>
<span class="sd">            is an int number that represents height and width are both ksize, or a tuple of</span>
<span class="sd">            two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        padding (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>


<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</span>

<span class="sd">        - **output** (Tensor) -  Maxpooling result, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        - **mask** (Tensor) -  Max values&#39; index represented by the mask.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_arg_op = P.MaxPoolWithArgmax(padding=&quot;VALID&quot;, ksize=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = maxpool_arg_op(input_tensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPoolWithArgmax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_gpu</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">_Pool</span><span class="o">.</span><span class="n">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span> <span class="o">=</span> <span class="n">out_shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span>

        <span class="n">argmax_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">dim</span> <span class="o">=</span> <span class="n">kernel_h</span> <span class="o">*</span> <span class="n">kernel_w</span>
                    <span class="n">argmax_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                    <span class="n">dim</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">out_h</span> <span class="o">*</span> <span class="n">out_w</span> <span class="o">/</span> <span class="mi">16</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">argmax_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">argmax_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">argmax_shape</span> <span class="o">=</span> <span class="n">out_shape</span>

        <span class="k">return</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">argmax_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">out_dtype</span> <span class="o">=</span> <span class="n">x_dtype</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">argmax_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_gpu</span><span class="p">:</span>
            <span class="n">argmax_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">return</span> <span class="n">out_dtype</span><span class="p">,</span> <span class="n">argmax_dtype</span></div>


<div class="viewcode-block" id="AvgPool"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.AvgPool">[docs]</a><span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, AvgPool2d outputs</span>
<span class="sd">    regional average in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents height and width are both ksize, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        padding (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.avgpool_op = P.AvgPool(padding=&quot;VALID&quot;, ksize=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">        &gt;&gt;&gt;         result = self.avgpool_op(x)</span>
<span class="sd">        &gt;&gt;&gt;         return result</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; result = net(input_x)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[ 14.5  15.5  16.5]</span>
<span class="sd">           [ 18.5  19.5  20.5]]</span>
<span class="sd">          [[ 26.5  27.5  28.5]</span>
<span class="sd">           [ 30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;GPU&quot;</span>
        <span class="k">elif</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;GE&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;OTHER&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DBackpropInput"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Conv2DBackpropInput">[docs]</a><span class="k">class</span> <span class="nc">Conv2DBackpropInput</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of convolution with respect to the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimensionality of the output space.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution window.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the</span>
<span class="sd">                    padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</span>
<span class="sd">        mode (int): Modes for different convolutions. 0 Math convolutiuon, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 1.</span>
<span class="sd">        stride (Union[int. tuple[int]]): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union[int. tuple[int]]): Specifies the dilation rate to be used for the dilated convolution.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the gradients of convolution.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([10, 32, 30, 30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]))</span>
<span class="sd">        &gt;&gt;&gt; conv2d_backprop_input = P.Conv2DBackpropInput(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; conv2d_backprop_input(dout, weight, F.shape(x))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;input_sizes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, padding must be zero when pad_mode is &#39;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;element of pad_list&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad_list</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doutput</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_size</span><span class="p">):</span>
        <span class="n">x_size_v</span> <span class="o">=</span> <span class="n">x_size</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;x_size&#39;</span><span class="p">,</span> <span class="n">x_size_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_size_v</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;x_size[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;doutput&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># infer shape</span>
        <span class="n">dout_shape</span> <span class="o">=</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">kernel_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">stride_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">dilation_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="c1"># default pad mode is valid</span>
        <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">:</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="n">pad_list</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_size_v</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="BiasAdd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BiasAdd">[docs]</a><span class="k">class</span> <span class="nc">BiasAdd</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sum of input and bias tensor.</span>

<span class="sd">    Adds the 1-D bias tensor to the input tensor, and broadcasts the shape on all axis</span>
<span class="sd">    except for the channel axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape can be 2-4 dimensions.</span>
<span class="sd">        - **bias** (Tensor) - The bias tensor, with shape :math:`(C)`.</span>
<span class="sd">          The shape of `bias` must be the same as `input_x` in the second dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3,)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias_add = P.BiasAdd()</span>
<span class="sd">        &gt;&gt;&gt; bias_add(input_x, bias)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;bias rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">b_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">b_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="TopK"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.TopK">[docs]</a><span class="k">class</span> <span class="nc">TopK</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values and indices of the `k` largest entries along the last dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        sorted (bool): If true, the obtained elements will</span>
<span class="sd">            be sorted by the values in descending order. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to be computed, data type must be float16, float32 or int32.</span>
<span class="sd">        - **k** (int) - The number of top elements to be computed along the last dimension, constant input is needed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the values and the indices.</span>

<span class="sd">        - **values** (Tensor) - The `k` largest elements in each slice of the last dimensional.</span>
<span class="sd">        - **indices** (Tensor) - The indices of values within the last dimension of input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; topk = P.TopK(sorted=True)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 5], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; k = 3</span>
<span class="sd">        &gt;&gt;&gt; values, indices = topk(input_x, k)</span>
<span class="sd">        &gt;&gt;&gt; assert values == Tensor(np.array([5, 4, 3]), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; assert indices == Tensor(np.array([4, 3, 2]), mstype.int32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sorted&quot;</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">k_v</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">k_v</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x_shape</span><span class="p">[</span><span class="n">ndim</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_v</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">),</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the softmax cross-entropy value between logits and labels with one-hot encoding.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = -\sum_j{Y_{ij} * ln(p_{ij})}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N, C)`, has the same data type with `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the `loss` shape is `(N,)`, and the `dlogits` with the same shape as `logits`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 4, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax_cross = P.SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss, backprop = softmax_cross(logits, labels)</span>
<span class="sd">        ([0.5899297, 0.52374405], [[0.02760027, 0.20393994, 0.01015357, 0.20393994, -0.44563377],</span>
<span class="sd">        [0.08015892, 0.02948882, 0.08015892, -0.4077012, 0.21789455]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;logits_shape&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="s2">&quot;labels_shape&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">dlogits_shape</span> <span class="o">=</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss_shape</span><span class="p">,</span> <span class="n">dlogits_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_type</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">labels_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">logits_type</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseSoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SparseSoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr -ln(1 - p_{ij}), &amp; j \neq y_i \end{cases}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss = \sum_{ij} loss_{ij}</span>

<span class="sd">    Args:</span>
<span class="sd">        is_grad (bool): If true, this operation returns the computed gradient. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if `is_grad` is False, the output tensor is the value of loss which is a scalar tensor;</span>
<span class="sd">        if `is_grad` is True, the output tensor is the gradient of input with the same shape as `logits`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage in nn.SoftmaxCrossEntropyWithLogits source code.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span> <span class="o">=</span> <span class="n">is_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;sens&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;logits_shape[0]&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;labels_shape[0]&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="n">loss_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_type</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">labels_type</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits_type</span></div>


<div class="viewcode-block" id="ApplyMomentum"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyMomentum">[docs]</a><span class="k">class</span> <span class="nc">ApplyMomentum</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Momentum algorithm.</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Inputs of `variable`, `accumulation` and `gradient` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    Data type conversion of Parameter is not supported. RuntimeError exception will be thrown.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumlation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>
<span class="sd">        use_nesterov (bool): Enable Nesterov momentum. Default: False.</span>
<span class="sd">        gradient_scale (float): The scale of the gradient. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - Weights to be updated. data type must be float.</span>
<span class="sd">        - **accumulation** (Parameter) - Accumulated gradient value by moment weight.</span>
<span class="sd">          Has the same data type with `variable`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same data type as `variable`.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum, must be a float number or</span>
<span class="sd">          a scalar tensor with float data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage in nn.ApplyMomentum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">a_shape</span><span class="p">,</span> <span class="n">l_shape</span><span class="p">,</span> <span class="n">g_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">v_shape</span>
        <span class="k">return</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">a_dtype</span><span class="p">,</span> <span class="n">l_dtype</span><span class="p">,</span> <span class="n">g_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">v_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_refkey</span> <span class="ow">and</span> <span class="n">a_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_refkey</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;a&quot;</span><span class="p">:</span> <span class="n">a_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l_dtype&quot;</span><span class="p">:</span> <span class="n">l_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;g_dtype&quot;</span><span class="p">:</span> <span class="n">g_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;m_dtype&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">g_dtype</span><span class="p">,</span> <span class="n">g_dtype</span>
        <span class="k">return</span> <span class="n">g_dtype</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SmoothL1Loss">[docs]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes smooth L1 loss, a robust L1 loss.</span>

<span class="sd">    SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the</span>
<span class="sd">    `Fast R-CNN &lt;https://arxiv.org/abs/1504.08083&gt;`_ by Ross Girshick.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input prediction as `X`, input target as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            \text{SmoothL1Loss} = \begin{cases} \frac{0.5 x^{2}}{\text{beta}}, &amp;if \left |x \right | &lt; \text{beta} \cr</span>
<span class="sd">            \left |x \right|-0.5 \text{beta}, &amp;\text{otherwise}\end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        beta (float): A parameter used to control the point where the function will change from</span>
<span class="sd">            quadratic to linear. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **prediction** (Tensor) - Predict data. Data type must be float16 or float32.</span>
<span class="sd">        - **target** (Tensor) - Ground truth data, with the same type and shape as `prediction`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `prediction`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = P.SmoothL1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target_data = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss(input_data, target_data)</span>
<span class="sd">        [0, 0, 0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;prediction shape&#39;</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="s1">&#39;target shape&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prediction&quot;</span><span class="p">:</span> <span class="n">prediction</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">target</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span></div>


<div class="viewcode-block" id="L2Loss"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.L2Loss">[docs]</a><span class="k">class</span> <span class="nc">L2Loss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates half of the L2 norm of a tensor without using the `sqrt`.</span>

<span class="sd">    Set `input_x` as x and output as loss.</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss = sum(x ** 2) / nelement(x)</span>

<span class="sd">    :math:`nelement(x)` represents the number of `input_x`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A input Tensor. Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `input_x`. The output tensor is the value of loss which is a scalar tensor.</span>

<span class="sd">    Examples</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; l2_loss = P.L2Loss()</span>
<span class="sd">        &gt;&gt;&gt; l2_loss(input_x)</span>
<span class="sd">        7.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L2Loss&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">loss_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x_type&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x_type&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="DataFormatDimMap"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.DataFormatDimMap">[docs]</a><span class="k">class</span> <span class="nc">DataFormatDimMap</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the dimension index in the destination data format given in the source data format.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_format (string): An optional value for source data format. Default: &#39;NHWC&#39;.</span>
<span class="sd">        dst_format (string): An optional value for destination data format. Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor with each element as a dimension index in source data format.</span>
<span class="sd">          The suggested values is in the range [-4, 4). It&#39;s type is int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([0, 1, 2, 3], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dfdm = P.DataFormatDimMap()</span>
<span class="sd">        &gt;&gt;&gt; dfdm(x)</span>
<span class="sd">        [0 3 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="n">dst_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;src_format&quot;</span><span class="p">,</span> <span class="n">src_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;dst_format&quot;</span><span class="p">,</span> <span class="n">dst_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="RNNTLoss"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.RNNTLoss">[docs]</a><span class="k">class</span> <span class="nc">RNNTLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the RNNTLoss and its gradient with respect to the softmax outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank_label (int): blank label. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **acts** (Tensor) - Tensor of shape :math:`(B, T, U, V)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor[int32]) - Tensor of shape :math:`(B, U-1)`.</span>
<span class="sd">        - **input_lengths** (Tensor[int32]) - Tensor of shape :math:`(B,)`.</span>
<span class="sd">        - **label_lengths** (Tensor[int32]) - Tensor of shape :math:`(B,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **costs** (Tensor[int32]) - Tensor of shape :math:`(B,)`.</span>
<span class="sd">        - **grads** (Tensor[int32]) - Has the same shape as `acts`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; B, T, U, V = 1, 2, 3, 5</span>
<span class="sd">        &gt;&gt;&gt; acts = np.random.random((B, T, U, V)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = np.array([[1, 2]]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_length = np.array([T] * B).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; label_length = np.array([len(l) for l in labels]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; rnnt_loss = P.RNNTLoss(blank_label=blank)</span>
<span class="sd">        &gt;&gt;&gt; costs, grads = rnnt_loss(Tensor(acts), Tensor(labels), Tensor(input_length), Tensor(label_length))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank_label</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;blank_label&#39;</span><span class="p">,</span> <span class="n">blank_label</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acts&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;input_length&#39;</span><span class="p">,</span> <span class="s1">&#39;label_length&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;costs&#39;</span><span class="p">,</span> <span class="s1">&#39;grads&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;acts_rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acts_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;labels_rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;input_length_rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;label_length_rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[0]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[1]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;acts shape[2]-1&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;input_length size&#39;</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;label_length size&#39;</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">costs_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">costs_shape</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;acts_type&quot;</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;labels_type&quot;</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_length_type&quot;</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;label_length_type&quot;</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;acts_type&quot;</span><span class="p">:</span> <span class="n">acts_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;labels_type&quot;</span><span class="p">:</span> <span class="n">labels_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_length_type&quot;</span><span class="p">:</span> <span class="n">input_length_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;label_length_type&quot;</span><span class="p">:</span> <span class="n">label_length_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">acts_type</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SGD">[docs]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes stochastic gradient descent (optionally with momentum).</span>

<span class="sd">    Nesterov momentum is based on the formula from On the importance of</span>
<span class="sd">    initialization and momentum in deep learning.</span>

<span class="sd">    Note:</span>
<span class="sd">        For details, please refer to `nn.SGD` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        dampening (float): The dampening for momentum. Default: 0.0.</span>
<span class="sd">        weight_decay (float): Weight decay (L2 penalty). Default: 0.0.</span>
<span class="sd">        nesterov (bool): Enable Nesterov momentum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **parameters** (Tensor) - Parameters to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, with float16 or float32 data type.</span>
<span class="sd">        - **learning_rate** (Tensor) - Learning rate, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32)</span>
<span class="sd">        - **accum** (Tensor) - Accum(velocity) to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **momentum** (Tensor) - Momentum, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32).</span>
<span class="sd">        - **stat** (Tensor) - States to be updated with the same shape as gradient, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; sgd = P.SGD()</span>
<span class="sd">        &gt;&gt;&gt; parameters = Tensor(np.array([2, -0.5, 1.7, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([1, -1, 0.5, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.01, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; accum = Tensor(np.array([0.1, 0.3, -0.2, -0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stat = Tensor(np.array([1.5, -0.3, 0.2, -0.7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = sgd(parameters, gradient, learning_rate, accum, momentum, stat)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nesterov&quot;</span><span class="p">,</span> <span class="n">nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nesterov</span> <span class="ow">and</span> <span class="n">dampening</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nesterov need zero dampening!&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;stat&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span>
                    <span class="n">accum_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;parameters rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;gradient rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">gradient_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;learning rate rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;accumulation rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">accum_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;momentum rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">momentum_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;stat rank&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">stat_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="s2">&quot;stat shape&quot;</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">parameters_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span>
                    <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="n">parameters_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;gradient&quot;</span><span class="p">:</span> <span class="n">gradient_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;accum&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;momentum&quot;</span><span class="p">:</span> <span class="n">momentum_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;stat&quot;</span><span class="p">:</span> <span class="n">stat_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">parameters_dtype</span></div>


<div class="viewcode-block" id="ApplyRMSProp"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyRMSProp">[docs]</a><span class="k">class</span> <span class="nc">ApplyRMSProp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Root Mean Square prop(RMSProp) algorithm.</span>
<span class="sd">    Please refer to the usage in source code of `nn.RMSProp`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Update `var` according to the RMSProp algorithm.</span>

<span class="sd">        ..  math::</span>
<span class="sd">            s_{t} = \\rho s_{t-1} + (1 - \\rho)(\\nabla Q_{i}(w))^2</span>

<span class="sd">        ..  math::</span>
<span class="sd">            m_{t} = \\beta m_{t-1} + \\frac{\\eta} {\\sqrt{s_{t} + \\epsilon}} \\nabla Q_{i}(w)</span>

<span class="sd">        ..  math::</span>
<span class="sd">            w = w - m_{t}</span>

<span class="sd">        where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">        :math:`s_{t}` represents `mean_square`, :math:`s_{t-1}` is the last momentent of :math:`s_{t}`,</span>
<span class="sd">        :math:`m_{t}` represents `moment`, :math:`m_{t-1}` is the last momentent of :math:`m_{t}`.</span>
<span class="sd">        :math:`\\rho` represents `decay`. :math:`\\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">        :math:`\\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">        :math:`\\eta` represents `learning_rate`. :math:`\\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumlation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be update.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must have the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must have the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must have the same type as `var`.</span>
<span class="sd">        - **decay** (float) - Decay rate. Only constant value is allowed.</span>
<span class="sd">        - **momentum** (float) - Momentum. Only constant value is allowed.</span>
<span class="sd">        - **epsilon** (float) - Ridge term. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be update.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; apply_rms = P.ApplyRMSProp()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(1., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(2., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(1., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(2., mindspore.float32 )</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; decay = 0.0</span>
<span class="sd">        &gt;&gt;&gt; momentum = 1e-10</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 0.001</span>
<span class="sd">        &gt;&gt;&gt; result = apply_rms(input_x, mean_square, moment, learning_rate, grad, decay, momentum, epsilon)</span>
<span class="sd">        (-2.9977674, 0.80999994, 1.9987665)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_square&#39;</span><span class="p">,</span> <span class="s1">&#39;moment&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_d</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">decay_shape</span><span class="p">,</span>
                    <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;mean_square_shape&quot;</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;moment_shape&quot;</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_d</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">var_shape</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">decay_dtype</span><span class="p">,</span>
                    <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;mean_square&quot;</span><span class="p">:</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="s2">&quot;moment&quot;</span><span class="p">:</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args_decay</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;decay&quot;</span><span class="p">:</span> <span class="n">decay_dtype</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args_decay</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_lr</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="s2">&quot;decay&quot;</span><span class="p">:</span> <span class="n">decay_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args_lr</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_mix</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_d</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">var_dtype</span>
        <span class="k">return</span> <span class="n">var_dtype</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">mean_square</span><span class="p">,</span> <span class="n">moment</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">decay</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">momentum</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">epsilon</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, decay, momentum, epsilon must be const.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyCenteredRMSProp"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyCenteredRMSProp">[docs]</a><span class="k">class</span> <span class="nc">ApplyCenteredRMSProp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the centered RMSProp algorithm.</span>
<span class="sd">    Please refer to the usage in source code of `nn.RMSProp`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Update `var` according to the centered RMSProp algorithm.</span>

<span class="sd">        ..  math::</span>
<span class="sd">            g_{t} = \\rho g_{t-1} + (1 - \\rho)\\nabla Q_{i}(w)</span>

<span class="sd">        ..  math::</span>
<span class="sd">            s_{t} = \\rho s_{t-1} + (1 - \\rho)(\\nabla Q_{i}(w))^2</span>

<span class="sd">        ..  math::</span>
<span class="sd">            m_{t} = \\beta m_{t-1} + \\frac{\\eta} {\\sqrt{s_{t} - g_{t}^2 + \\epsilon}} \\nabla Q_{i}(w)</span>

<span class="sd">        ..  math::</span>
<span class="sd">            w = w - m_{t}</span>

<span class="sd">        where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">        :math:`g_{t}` represents `mean_gradient`, :math:`g_{t-1}` is the last momentent of :math:`g_{t}`.</span>
<span class="sd">        :math:`s_{t}` represents `mean_square`, :math:`s_{t-1}` is the last momentent of :math:`s_{t}`,</span>
<span class="sd">        :math:`m_{t}` represents `moment`, :math:`m_{t-1}` is the last momentent of :math:`m_{t}`.</span>
<span class="sd">        :math:`\\rho` represents `decay`. :math:`\\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">        :math:`\\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">        :math:`\\eta` represents `learning_rate`. :math:`\\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumlation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be update.</span>
<span class="sd">        - **mean_gradient** (Tensor) - Mean gradients, must have the same type as `var`.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must have the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must have the same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must have the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **decay** (float) - Decay rate.</span>
<span class="sd">        - **momentum** (float) - Momentum.</span>
<span class="sd">        - **epsilon** (float) - Ridge term.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be update.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; centered_rms_prop = P.ApplyCenteredRMSProp()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(-6, 6).astype(np.float32).reshape(2, 3, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean_grad = Tensor(np.arange(12).astype(np.float32).reshape(2, 3, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(np.arange(-8, 4).astype(np.float32).reshape(2, 3, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(np.arange(12).astype(np.float32).reshape(2, 3, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.arange(12).astype(np.float32).reshape(2, 3, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; decay = 0.0</span>
<span class="sd">        &gt;&gt;&gt; momentum = 1e-10</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 0.05</span>
<span class="sd">        &gt;&gt;&gt; result = centered_rms_prop(input_x, mean_grad, mean_square, moment, grad,</span>
<span class="sd">        &gt;&gt;&gt;                            learning_rate, decay, momentum, epsilon)</span>
<span class="sd">        [[[ -6.        -9.024922]</span>
<span class="sd">          [-12.049845 -15.074766]</span>
<span class="sd">          [-18.09969  -21.124613]]</span>
<span class="sd">         [[-24.149532 -27.174456]</span>
<span class="sd">          [-30.199379 -33.2243  ]</span>
<span class="sd">          [-36.249226 -39.274143]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">mean_gradient_shape</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span>
                    <span class="n">learning_rate_shape</span><span class="p">,</span> <span class="n">decay_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;mean_gradient_shape&quot;</span><span class="p">,</span> <span class="n">mean_gradient_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;mean_square_shape&quot;</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;moment_shape&quot;</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">mean_gradient_shape</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">moment_shape</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">mean_gradient_dtype</span><span class="p">,</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span>
                    <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">rho_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;mean_gradient&quot;</span><span class="p">:</span> <span class="n">mean_gradient_dtype</span><span class="p">,</span>
                <span class="s2">&quot;mean_square&quot;</span><span class="p">:</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="s2">&quot;moment&quot;</span><span class="p">:</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args_rho</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="n">rho_dtype</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_same</span><span class="p">(</span><span class="n">args_rho</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_lr</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="n">rho_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args_lr</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_mix</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_ascend</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">mean_gradient_dtype</span><span class="p">,</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="n">moment_dtype</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LayerNorm">[docs]</a><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_norm_axis (int): The begin axis of the `input_x` to apply LayerNorm,</span>
<span class="sd">            the value must be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        begin_params_axis (int): The begin axis of the parameter input (`gamma`, `beta`) to</span>
<span class="sd">            apply LayerNorm, the value must be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        epsilon (float): A value added to the denominator for numerical stability. Default: 1e-7.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          The input of LayerNorm.</span>
<span class="sd">        - **gamma** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `gamma` as the scale on norm.</span>
<span class="sd">        - **beta** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `beta` as the scale on norm.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.</span>
<span class="sd">          The shape is :math:`(N, C)`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; layer_norm = P.LayerNorm()</span>
<span class="sd">        &gt;&gt;&gt; output = layer_norm(input_x, gamma, beta)</span>
<span class="sd">        ([[-0.22474492, 1., 2.2247488], [-0.22474492, 1., 2.2247488]],</span>
<span class="sd">         [[2.], [2.]], [[0.6666667], [0.6666667]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;begin_norm_axis&#39;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;begin_params_axis&#39;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Normalize"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.L2Normalize">[docs]</a><span class="k">class</span> <span class="nc">L2Normalize</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L2 normalization Operator.</span>

<span class="sd">    This operator will normalize the input using the given axis. The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac{x}{\sqrt{\text{max}(\text{sum} (\text{input_x}^2), \epsilon)}},</span>

<span class="sd">    where :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The starting axis for the input to apply the L2 normalization. Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-4.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the normalization. Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; l2_normalize = P.L2Normalize()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(-256, 256, (2, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = l2_normalize(input_x)</span>
<span class="sd">        [[[-0.47247353   -0.30934513   -0.4991462   0.8185567 ]</span>
<span class="sd">          [-0.08070751   -0.9961299    -0.5741758   0.09262337]</span>
<span class="sd">          [-0.9916556    -0.3049123     0.5730487  -0.40579924]</span>
<span class="sd">         [[-0.88134485    0.9509498    -0.86651784  0.57442576]</span>
<span class="sd">          [ 0.99673784    0.08789381   -0.8187321   0.9957012 ]</span>
<span class="sd">          [ 0.12891524   -0.9523804    -0.81952125  0.91396334]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s1">&#39;axis value&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="DropoutGenMask"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.DropoutGenMask">[docs]</a><span class="k">class</span> <span class="nc">DropoutGenMask</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates the mask value for the input shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        Seed0 (int): Seed0 value for random generating. Default: 0.</span>
<span class="sd">        Seed1 (int): Seed1 value for random generating. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (tuple[int]) - The shape of target mask.</span>
<span class="sd">        - **keep_prob** (Tensor) - The keep rate, greater than 0 and less equal than 1, e.g. keep_prob = 0.9,</span>
<span class="sd">          means dropping out 10% of input units.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value of generated mask for input shape.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout_gen_mask = P.DropoutGenMask()</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; keep_prob = Tensor(0.5, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = dropout_gen_mask(shape, keep_prob)</span>
<span class="sd">        [249, 11, 134, 133, 143, 246, 89, 52, 169, 15, 94, 63, 146, 103, 7, 101]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;keep_prob&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;_random_effect&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="DropoutDoMask"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.DropoutDoMask">[docs]</a><span class="k">class</span> <span class="nc">DropoutDoMask</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies dropout mask on the input tensor.</span>

<span class="sd">    Take the mask output of DropoutGenMask as input, and apply dropout on the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">        - **mask** (Tensor) - The mask to be applied on `input_x`, which is the output of `DropoutGenMask`. And the</span>
<span class="sd">          shape of `input_x` must be the same as the value of `DropoutGenMask`&#39;s input `shape`. If input wrong `mask`,</span>
<span class="sd">          the output of `DropoutDoMask` are unpredictable.</span>
<span class="sd">        - **keep_prob** (Union[Tensor, float]) - The keep rate, greater than 0 and less equal than 1, e.g. keep_prob =</span>
<span class="sd">          0.9, means dropping out 10% of input units. The value of `keep_prob` is the same as the input `keep_prob` of</span>
<span class="sd">          `DropoutGenMask`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied dropout on.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; keep_prob = Tensor(0.5, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; dropout_gen_mask = P.DropoutGenMask()</span>
<span class="sd">        &gt;&gt;&gt; dropout_do_mask = P.DropoutDoMask()</span>
<span class="sd">        &gt;&gt;&gt; mask = dropout_gen_mask(shape, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; output = dropout_do_mask(x, mask, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; assert output.shape == (2, 2, 3)</span>
<span class="sd">        [[[2.0, 0.0, 0.0],</span>
<span class="sd">          [0.0, 0.0, 0.0]],</span>
<span class="sd">         [[0.0, 0.0, 0.0],</span>
<span class="sd">          [2.0, 2.0, 2.0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
        <span class="n">input_x_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">mask_shape</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">keep_prob_shape</span> <span class="o">=</span> <span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;keep_prob&#39;s dim&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_prob_shape</span><span class="p">),</span> <span class="s1">&#39;0(scalar)&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">size_x</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DropoutDoMask mask shape should be 1-dimension.&quot;</span><span class="p">)</span>
        <span class="n">size_y</span> <span class="o">=</span> <span class="n">mask_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span>
        <span class="k">if</span> <span class="n">size_x</span> <span class="o">&gt;</span> <span class="n">size_y</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DropoutDoMask y mask do not math input input_x shape:&quot;</span>
                             <span class="s2">&quot;</span><span class="si">{input_x_shape}</span><span class="s2">, mask shape: </span><span class="si">{mask_shape}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_mask&quot;</span><span class="p">:</span> <span class="n">mask</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">keep_prob_v</span> <span class="o">=</span> <span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">keep_prob_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="n">keep_prob_v</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="n">keep_prob_v</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">input_x_shape</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="ResizeBilinear"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ResizeBilinear">[docs]</a><span class="k">class</span> <span class="nc">ResizeBilinear</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the image to certain size using bilinear interpolation.</span>

<span class="sd">    The resizing only affects the lower two dimensions which represent the height and width. The input images</span>
<span class="sd">    can be represented by different data types, but the data types of output images are always float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (tuple[int]): A tuple of 2 int elements `(new_height, new_width)`, the new size of the images.</span>
<span class="sd">        align_corners (bool): If true, rescale input by `(new_height - 1) / (height - 1)`,</span>
<span class="sd">                       which exactly aligns the 4 corners of images and resized images. If false,</span>
<span class="sd">                       rescale by `new_height / height`. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Image to be resized. Input images must be a 4-D tensor with shape</span>
<span class="sd">          :math:`(batch, channels, height, width)`, with data type of float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, resized image. 4-D with shape [batch, channels, new_height, new_width] in `float32`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; resize_bilinear = P.ResizeBilinear((5, 5))</span>
<span class="sd">        &gt;&gt;&gt; result = resize_bilinear(tensor)</span>
<span class="sd">        &gt;&gt;&gt; assert result.shape == (1, 1, 5, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;input shape rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_dtype&#39;</span><span class="p">:</span> <span class="n">input_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="OneHot"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.OneHot">[docs]</a><span class="k">class</span> <span class="nc">OneHot</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    Makes a new tensor, whose locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Position to insert the value. e.g. If `indices` shape is [n, c], and `axis` is `-1` the output shape</span>
<span class="sd">            will be [n, c, depth], If `axis` is `0` the output shape will be [depth, n, c]. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">          Data type must be int32.</span>
<span class="sd">        - **depth** (int) - A scalar defining the depth of the one hot dimension.</span>
<span class="sd">        - **on_value** (Tensor) - A value to fill in output when `indices[j] = i`. With data type of float16 or float32.</span>
<span class="sd">        - **off_value** (Tensor) - A value to fill in output when `indices[j] != i`.</span>
<span class="sd">          Has the same data type with as `on_value`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; onehot = P.OneHot()</span>
<span class="sd">        &gt;&gt;&gt; result = onehot(indices, depth, on_value, off_value)</span>
<span class="sd">        [[1, 0, 0], [0, 1, 0], [0, 0, 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;depth&#39;</span><span class="p">,</span> <span class="s1">&#39;on_value&#39;</span><span class="p">,</span> <span class="s1">&#39;off_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">):</span>
        <span class="c1"># check type</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;indices&quot;</span><span class="p">:</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;depth&quot;</span><span class="p">,</span> <span class="n">depth</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;on_value&quot;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s2">&quot;off_value&quot;</span><span class="p">:</span> <span class="n">off_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># check shape</span>
        <span class="n">indices_shp</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shp</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">depth_val</span> <span class="o">=</span> <span class="n">depth</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;depth&quot;</span><span class="p">,</span> <span class="n">depth_val</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="c1"># create new dimension at end if self.axis is -1</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">indices_shp</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">depth_val</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">indices_shp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">depth_val</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">indices_shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="Gelu"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Gelu">[docs]</a><span class="k">class</span> <span class="nc">Gelu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    Gelu is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = 0.5 * x * (1 + erf(x / \sqrt{2})),</span>

<span class="sd">    where :math:`erf` is the &quot;Gauss error function&quot; .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the Gelu with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gelu = P.Gelu()</span>
<span class="sd">        &gt;&gt;&gt; result = gelu(tensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="GetNext"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.GetNext">[docs]</a><span class="k">class</span> <span class="nc">GetNext</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next element in the dataset queue.</span>

<span class="sd">    Note:</span>
<span class="sd">        The GetNext operation needs to be associated with network and it also depends on the init_dataset interface,</span>
<span class="sd">        it can&#39;t be used directly as a single operation.</span>
<span class="sd">        For details, please refer to `connect_network_with_dataset` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        types (list[:class:`mindspore.dtype`]): The type of the outputs.</span>
<span class="sd">        shapes (list[tuple[int]]): The dimensionality of the outputs.</span>
<span class="sd">        output_num (int): The output number, length of `types` and `shapes`.</span>
<span class="sd">        shared_name (str): The queue name of `init_dataset` interface.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        No inputs.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the output of Dataset. The shape is described in `shapes`</span>
<span class="sd">        and the type is described is `types`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; get_next = P.GetNext([mindspore.float32, mindspore.int32], [[32, 1, 28, 28], [10]], 2, &#39;shared_name&#39;)</span>
<span class="sd">        &gt;&gt;&gt; feature, label = get_next()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;types&quot;</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shapes&quot;</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;types length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">),</span> <span class="s2">&quot;shapes length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shapes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">types</span><span class="p">)</span></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.PReLU">[docs]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of an channel of the input.</span>

<span class="sd">    Note:</span>
<span class="sd">        1-dimensional input_x is not supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Float tensor, representing the output of the preview layer.</span>
<span class="sd">          With data type of float16 or float32.</span>
<span class="sd">        - **weight** (Tensor) -  Float Tensor, w &gt; 0, there are only two shapes are legitimate,</span>
<span class="sd">          1 or the number of channels of the input. With data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as `input_x`.</span>

<span class="sd">    For detailed information, please refer to `nn.PReLU`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.prelu = P.PReLU()</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, input_x, weight):</span>
<span class="sd">        &gt;&gt;&gt;         result = self.prelu(input_x, weight)</span>
<span class="sd">        &gt;&gt;&gt;         return result</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(-3, 3, (2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; result = net(input_x, weight)</span>
<span class="sd">        [[[-0.1, 1.0],</span>
<span class="sd">          [0.0, 2.0],</span>
<span class="sd">          [0.0, 0.0]],</span>
<span class="sd">         [[-0.2, -0.1],</span>
<span class="sd">          [2.0, -1.8],</span>
<span class="sd">          [0.6, 0.6]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">):</span>
        <span class="n">input_x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x_shape</span><span class="p">)</span>
        <span class="n">weight_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_x_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> input_x rank 1 is not supported.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_dim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> weight_dim must be 1, while weight_dim is </span><span class="si">{</span><span class="n">weight_dim</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">input_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> channel of input_x and weight must be matched,&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39; while channel of input_x is </span><span class="si">{</span><span class="n">input_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">,&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39; weight_shape[0] is </span><span class="si">{</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="n">weight_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x_dtype</span></div>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LSTM">[docs]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the long short term memory(LSTM) on the input.</span>

<span class="sd">    For detailed information, please refer to `nn.LSTM`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;num_layers&quot;</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="c1"># (seq, batch_size, feature)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x[2]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># h and c should be same shape</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># (num_layers * num_directions, batch, hidden_size)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h[2]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">)</span>

        <span class="c1"># set arbitrary shape for reserved space</span>
        <span class="n">type_size</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="n">gates_ws_ld</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_good_ld</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">type_size</span><span class="p">)</span>
        <span class="n">states_ws_ld</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_good_ld</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">),</span> <span class="n">type_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ws_gates_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">gates_ws_ld</span> <span class="o">*</span> <span class="n">type_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ws_states_size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span>
            <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">states_ws_ld</span> <span class="o">*</span> <span class="n">type_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ws_c_states_size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span>
            <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">states_ws_ld</span> <span class="o">*</span> <span class="n">type_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ws_diff_states_size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span>
            <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">states_ws_ld</span> <span class="o">*</span> <span class="n">type_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ws_grid_comp_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span> <span class="o">=</span> <span class="mi">4096</span>
        <span class="n">current_offset</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">current_offset</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws_gates_size</span>
        <span class="n">current_offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnd_up</span><span class="p">(</span><span class="n">current_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">)</span>
        <span class="n">current_offset</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws_states_size</span>
        <span class="n">current_offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnd_up</span><span class="p">(</span><span class="n">current_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">)</span>
        <span class="n">current_offset</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws_c_states_size</span>
        <span class="n">current_offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnd_up</span><span class="p">(</span><span class="n">current_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">)</span>
        <span class="n">current_offset</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws_diff_states_size</span>
        <span class="n">current_offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnd_up</span><span class="p">(</span><span class="n">current_offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">page_size</span><span class="p">)</span>
        <span class="n">current_offset</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ws_grid_comp_size</span>
        <span class="n">reserved_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_offset</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">y_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">reserved_shape</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">rnd_up</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_offset</span><span class="p">,</span> <span class="n">page_size</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">current_offset</span> <span class="o">+</span> <span class="n">page_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">page_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">page_size</span>

    <span class="k">def</span> <span class="nf">get_good_ld</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">type_size</span><span class="p">):</span>
        <span class="n">ld</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnd_up</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="mi">64</span> <span class="o">//</span> <span class="n">type_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ld</span> <span class="o">*</span> <span class="mi">256</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ld</span> <span class="o">+</span> <span class="mi">64</span> <span class="o">//</span> <span class="n">type_size</span>
        <span class="k">return</span> <span class="n">ld</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SigmoidCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SigmoidCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the given logits to compute sigmoid cross entropy.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = -[Y_{ij} * ln(p_{ij}) + (1 - Y_{ij})ln(1 - p_{ij})]</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as input `logits`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.random.randn(2, 3).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.random.randn(2, 3).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = P.SigmoidCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; sigmoid(logits, labels)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SigmoidCrossEntropyWithLogits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;y_shape&quot;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y_dtype&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Pad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Pad">[docs]</a><span class="k">class</span> <span class="nc">Pad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads input tensor according to the paddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        paddings (tuple): The shape of parameter `paddings` is (N, 2). N is the rank of input data. All elements of</span>
<span class="sd">            paddings are int type. For the input in `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">            extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">            be extended behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_op = P.Pad(((1, 2), (2, 1)))</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = pad_op(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; assert output_tensor == Tensor(np.array([[ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. , -0.1,  0.3,  3.6,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. ,  0.4,  0.5, -3.2,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ],</span>
<span class="sd">        &gt;&gt;&gt;                                          [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ]]), mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Paddings must be tuple type.&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">paddings</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The shape of paddings must be (n, 2).&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paddings</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;paddings.shape&#39;</span><span class="p">,</span> <span class="n">paddings</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All elements of paddings must be &gt;= 0.&#39;</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">paddings</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="n">y_shape</span> <span class="o">+=</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),)</span>
        <span class="k">return</span> <span class="n">y_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="MirrorPad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.MirrorPad">[docs]</a><span class="k">class</span> <span class="nc">MirrorPad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (str): Specifies the padding mode. The optional values are &quot;REFLECT&quot; and &quot;SYMMETRIC&quot;.</span>
<span class="sd">            Default: &quot;REFLECT&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">        - **paddings** (Tensor) - The paddings tensor. The value of `paddings` is a matrix(list),</span>
<span class="sd">          and its shape is (N, 2). N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">          extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">          be extended behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is &quot;REFLECT&quot;, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3],[4,5,6],[7,8,9]] and `paddings` is [[1,1],[2,2]], then the</span>
<span class="sd">          Outputs is [[6,5,4,5,6,5,4],[3,2,1,2,3,2,1],[6,5,4,5,6,5,4],[9,8,7,8,9,8,7],[6,5,4,5,6,5,4]].</span>
<span class="sd">        - If `mode` is &quot;SYMMETRIC&quot;, the filling method is similar to the &quot;REFLECT&quot;. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3],[4,5,6],[7,8,9]] and `paddings` is [[1,1],[2,2]], then the Outputs is</span>
<span class="sd">          [[2,1,1,2,3,3,2],[2,1,1,2,3,3,2],[5,4,4,5,6,6,5],[8,7,7,8,9,9,8],[8,7,7,8,9,9,8]].</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.pad = P.MirrorPad(mode=&quot;REFLECT&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x, paddings):</span>
<span class="sd">        &gt;&gt;&gt;         return self.pad(x, paddings)</span>
<span class="sd">        &gt;&gt;&gt; x = np.random.random(size=(2, 3)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; paddings = Tensor([[1,1],[2,2]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net()</span>
<span class="sd">        &gt;&gt;&gt; ms_output = pad(Tensor(x), paddings)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">,</span> <span class="s1">&#39;SYMMETRIC&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_const_input_indexes</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;paddings&quot;</span><span class="p">,</span> <span class="n">paddings</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">paddings_value</span> <span class="o">=</span> <span class="n">paddings</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="n">paddings_size</span> <span class="o">=</span> <span class="n">paddings_value</span><span class="o">.</span><span class="n">size</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s1">&#39;paddings.shape&#39;</span><span class="p">,</span> <span class="n">paddings_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">paddings_value</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All elements of paddings must be &gt;= 0.&#39;</span><span class="p">)</span>
        <span class="n">adjust</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;SYMMETRIC&#39;</span><span class="p">:</span>
            <span class="n">adjust</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">paddings_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">adjust</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">adjust</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;At least one dim has too high a padding value for this input and mode&#39;</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">paddings_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="n">y_shape</span> <span class="o">+=</span> <span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">y_shape</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="ROIAlign"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ROIAlign">[docs]</a><span class="k">class</span> <span class="nc">ROIAlign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Region of Interest (RoI) Align operator.</span>

<span class="sd">    The operator computes the value of each sampling point by bilinear interpolation from the nearby grid points on the</span>
<span class="sd">    feature map. No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling</span>
<span class="sd">    points. The details of (RoI) Align operator are described in `Mask R-CNN &lt;https://arxiv.org/abs/1703.06870&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooled_height (int): The output features&#39; height.</span>
<span class="sd">        pooled_width (int): The output features&#39; width.</span>
<span class="sd">        spatial_scale (float): A scaling factor that maps the raw image coordinates to the input</span>
<span class="sd">            feature map coordinates. Suppose the height of a RoI is `ori_h` in the raw image and `fea_h` in the</span>
<span class="sd">            input feature map, the `spatial_scale` must be `fea_h / ori_h`.</span>
<span class="sd">        sample_num (int): Number of sampling points. Default: 2.</span>
<span class="sd">        roi_end_mode (int): Number must be 0 or 1. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape must be `(N, C, H, W)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is `(rois_n, 5)`. With data type of float16 or float32.</span>
<span class="sd">          `rois_n` represents the number of RoI. The size of the second dimension must be `5` and the `5` colunms</span>
<span class="sd">          are `(image_index, top_left_x, top_left_y, bottom_right_x, bottom_right_y)`. `image_index` represents the</span>
<span class="sd">          index of image. `top_left_x` and `top_left_y` represent the `x, y` coordinates of the top left corner</span>
<span class="sd">          of corresponding RoI, respectively. `bottom_right_x` and `bottom_right_y` represent the `x, y`</span>
<span class="sd">          coordinates of the bottom right corner of corresponding RoI, respectively.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is `(rois_n, C, pooled_height, pooled_width)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[[[1., 2.], [3., 4.]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor(np.array([[0, 0.2, 0.3, 0.2, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; roi_align = P.ROIAlign(2, 2, 0.5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor = roi_align(input_tensor, rois)</span>
<span class="sd">        &gt;&gt;&gt; assert output_tensor == Tensor(np.array([[[[2.15]]]]), mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">sample_num</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ROIAlign&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_height&quot;</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_width&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sample_num&quot;</span><span class="p">,</span> <span class="n">sample_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span> <span class="o">=</span> <span class="n">pooled_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span> <span class="o">=</span> <span class="n">pooled_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_num</span> <span class="o">=</span> <span class="n">sample_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roi_end_mode</span> <span class="o">=</span> <span class="n">roi_end_mode</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">,</span> <span class="n">rois_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">rois_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_type</span><span class="p">,</span> <span class="n">rois_type</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;inputs_type&quot;</span><span class="p">:</span> <span class="n">inputs_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;rois_type&quot;</span><span class="p">:</span> <span class="n">rois_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs_type</span></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by Adaptive Moment Estimation (Adam) algorithm.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If true, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated.</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula, has the same type as `var`.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula.</span>
<span class="sd">          Mean square gradients with the same type as `var`.</span>
<span class="sd">        - **beta1_power** (float) - :math:`beta_1^t` in the updating formula.</span>
<span class="sd">        - **beta2_power** (float) - :math:`beta_2^t` in the updating formula.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations.</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations.</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_adam = P.Adam()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.m = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.v = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        &gt;&gt;&gt;                               epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.random.rand(3, 3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(0.9, 0.999, 0.001, 0.9, 0.999, 1e-8, gradient)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="FusedSparseAdam"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FusedSparseAdam">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseAdam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If true, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same type as `var` with</span>
<span class="sd">          float32 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients, has the same type as</span>
<span class="sd">          `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula. With float32 data type.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient value with float32 data type.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_adam = P.FusedSparseAdam()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        &gt;&gt;&gt;                                      epsilon, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.random.rand(2, 1, 2), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">grad_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span> <span class="o">+</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of updates should be [] or &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;grad_shape = indices_shape + var_shape[1:], but got var_shape: </span><span class="si">{</span><span class="n">var_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, grad_shape: </span><span class="si">{</span><span class="n">grad_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="FusedSparseLazyAdam"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FusedSparseLazyAdam">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseLazyAdam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse. The behavior is not equivalent to the</span>
<span class="sd">    original Adam algorithm, as only the current indices parameters will be updated.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If true, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same type as `var` with</span>
<span class="sd">          float32 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients, has the same type as</span>
<span class="sd">          `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula with float32 data type.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient value with float32 data type.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_lazyadam = P.FusedSparseLazyAdam()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_lazyadam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1,</span>
<span class="sd">        &gt;&gt;&gt;                                          beta2, epsilon, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.random.rand(2, 1, 2), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">grad_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span> <span class="o">+</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of updates should be [] or &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;grad_shape = indices_shape + var_shape[1:], but got var_shape: </span><span class="si">{</span><span class="n">var_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, grad_shape: </span><span class="si">{</span><span class="n">grad_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="FusedSparseFtrl"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FusedSparseFtrl">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseFtrl</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`. The shape</span>
<span class="sd">          of `indices` must be the same as `grad` in first dimension. The type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **linear** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_ftrl = P.FusedSparseFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.linear = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(2, 1, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span></div>


<div class="viewcode-block" id="FusedSparseProximalAdagrad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FusedSparseProximalAdagrad">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the proximal adagrad</span>
<span class="sd">    algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the variable and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float32.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Tensor) - The learning rate value. The data type must be float32.</span>
<span class="sd">        - **l1** (Tensor) - l1 regularization strength. The data type must be float32.</span>
<span class="sd">        - **l2** (Tensor) - l2 regularization strength. The data type must be float32.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient. The data type must be float32.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`. The data type</span>
<span class="sd">          must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_proximal_adagrad = P.FusedSparseProximalAdagrad()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.lr = Tensor(0.01, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt;         self.l1 = Tensor(0.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt;         self.l2 = Tensor(0.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        &gt;&gt;&gt;                                                  self.l2, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(2, 1, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                       <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;indices&#39;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="KLDivLoss"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.KLDivLoss">[docs]</a><span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the target and the output.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input as :math:`x`, input label as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">        Let,</span>

<span class="sd">        .. math::</span>
<span class="sd">            L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">            l_n = y_n \cdot (\log y_n - x_n)</span>

<span class="sd">        Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            \ell(x, y) = \begin{cases}</span>
<span class="sd">            L, &amp; \text{if reduction} = \text{`none&#39;;}\\</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor. The data type must be float32.</span>
<span class="sd">        - **input_y** (Tensor) - The label Tensor which has the same shape as `input_x`. The data type must be float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `input_x`.</span>
<span class="sd">        Otherwise it is a scalar.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.kldiv_loss = P.KLDivLoss()</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x, y):</span>
<span class="sd">        &gt;&gt;&gt;         result = self.kldiv_loss(x, y)</span>
<span class="sd">        &gt;&gt;&gt;         return result</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(input_x, input_y)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x_shape&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_type</span><span class="p">}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="BinaryCrossEntropy"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BinaryCrossEntropy">[docs]</a><span class="k">class</span> <span class="nc">BinaryCrossEntropy</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Binary Cross Entropy between the target and the output.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input as :math:`x`, input label as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">        Let,</span>

<span class="sd">        .. math::</span>
<span class="sd">            L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">            l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">        Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            \ell(x, y) = \begin{cases}</span>
<span class="sd">            L, &amp; \text{if reduction} = \text{`none&#39;;}\\</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor. The data type must be float16 or float32.</span>
<span class="sd">        - **input_y** (Tensor) - The label Tensor which has same shape and data type as `input_x`.</span>
<span class="sd">        - **weight** (Tensor, optional) - A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">          And it must have same shape and data type as `input_x`. Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `input_x`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.binary_cross_entropy = P.BinaryCrossEntropy()</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x, y, weight):</span>
<span class="sd">        &gt;&gt;&gt;         result = self.binary_cross_entropy(x, y, weight)</span>
<span class="sd">        &gt;&gt;&gt;         return result</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(input_x, input_y, weight)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x_shape&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="s1">&#39;weight_shape&#39;</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span><span class="p">,</span> <span class="n">weight_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_type</span><span class="p">}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_type</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">weight_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="ApplyAdaMax"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyAdaMax">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdaMax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adamax scheme.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t} = \beta_1 * m_{t-1} + (1 - \beta_1) * g \\</span>
<span class="sd">            v_{t} = \max(\beta_2 * v_{t-1}, \left| g \right|) \\</span>
<span class="sd">            var = var - \frac{l}{1 - \beta_1^t} * \frac{m_{t}}{v_{t} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`m_{t}`, :math:`v` represents the 2nd moment vector, :math:`v_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`v_{t}`, :math:`l` represents scaling factor `lr`,</span>
<span class="sd">    :math:`g` represents `grad`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`beta_1^t` represents `beta1_power`, :math:`var` represents the variable to be updated,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `m`, `v` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients</span>
<span class="sd">          with the same shape and type as `var`. With float32 or float16 data type.</span>
<span class="sd">        - **beta1_power** (Union[Number, Tensor]) - :math:`beta_1^t` in the updating formula, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, :math:`l` in the updating formula, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **beta1** (Union[Number, Tensor]) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta2** (Union[Number, Tensor]) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_ada_max = P.ApplyAdaMax()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.m = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.v = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, beta1_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_ada_max(self.var, self.m, self.v, beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power =Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.99, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-10, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T5</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdaMax&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta1_power_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta1_power_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta1 power&#39;s rank&quot;</span><span class="p">,</span> <span class="n">beta1_power_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta1_power_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta1_power_shape[0]&quot;</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta1_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta1_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta1&#39;s rank&quot;</span><span class="p">,</span> <span class="n">beta1_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta1_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta1_shape[0]&quot;</span><span class="p">,</span> <span class="n">beta1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta2_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta2_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta2&#39;s rank&quot;</span><span class="p">,</span> <span class="n">beta2_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta2_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta2_shape[0]&quot;</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">epsilon_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;epsilon&#39;s rank&quot;</span><span class="p">,</span> <span class="n">epsilon_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epsilon_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;epsilon_shape[0]&quot;</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="ApplyAdadelta"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyAdadelta">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdadelta</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adadelta scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum = \rho * accum + (1 - \rho) * grad^2</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{update} = \sqrt{\text{accum_update} + \epsilon} * \frac{grad}{\sqrt{accum + \epsilon}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * update^2</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * update</span>

<span class="sd">    Inputs of `var`, `accum`, `accum_update` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **accum_update** (Parameter) - Accum_update to be updated, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **rho** (Union[Number, Tensor]) - Decay rate, must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradients, has the same shape and type as `var`. With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>
<span class="sd">        - **accum_update** (Tensor) - The same shape and data type as `accum_update`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_adadelta = P.ApplyAdadelta()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum_update = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum_update&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, lr, rho, epsilon, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_adadelta(self.var, self.accum, self.accum_update, lr, rho, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; rho = Tensor(0.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-6, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(lr, rho, epsilon, grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum_update&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdadelta&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">accum_update_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">rho_shape</span><span class="p">,</span>
                    <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;accum_shape&quot;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;accum_update_shape&quot;</span><span class="p">,</span> <span class="n">accum_update_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">rho_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rho_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;rho&#39;s rank&quot;</span><span class="p">,</span> <span class="n">rho_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rho_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;rho_shape[0]&quot;</span><span class="p">,</span> <span class="n">rho_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">epsilon_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lepsilon&#39;s rank&quot;</span><span class="p">,</span> <span class="n">epsilon_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epsilon_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;epsilon_shape[0]&quot;</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">accum_update_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">accum_update_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">rho_dtype</span><span class="p">,</span>
                    <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_update&quot;</span><span class="p">:</span> <span class="n">accum_update_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="n">rho_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">accum_update_dtype</span></div>


<div class="viewcode-block" id="ApplyAdagrad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyAdagrad">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum}}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent..</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_adagrad = P.ApplyAdagrad()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, lr, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_adagrad(self.var, self.accum, lr, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(lr, grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="ApplyAdagradV2"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyAdagradV2">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdagradV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagradv2 scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float16 or float32 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_adagrad_v2 = P.ApplyAdagradV2(epsilon=1e-6)</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, lr, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_adagrad_v2(self.var, self.accum, lr, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(lr, grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;grad shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="SparseApplyAdagrad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SparseApplyAdagrad">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * (1 / sqrt(accum))</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>
<span class="sd">        use_locking (bool): If true, the `var` and `accumulation` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The shape must be the same as `var`&#39;s shape except the first dimension.</span>
<span class="sd">          Gradients has the same data type as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in first dimension, the type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_adagrad = P.SparseApplyAdagrad(lr=1e-8)</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_adagrad(self.var, self.accum, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1, 2], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;len of var shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">),</span> <span class="s1">&#39;len of grad shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;indices&#39;</span><span class="p">:</span> <span class="n">indices_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span></div>


<div class="viewcode-block" id="SparseApplyAdagradV2"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SparseApplyAdagradV2">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyAdagradV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        use_locking (bool): If `True`, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        update_slots (bool): If `True`, the computation logic will be different to `False`. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The shape must be the same as `var`&#39;s shape except the first dimension.</span>
<span class="sd">          Gradients has the same data type as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in first dimension, the type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_adagrad_v2 = P.SparseApplyAdagradV2(lr=1e-8, epsilon=1e-6)</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.ones([3, 3, 3]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_adagrad_v2(self.var, self.accum, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1, 2], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_slots</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;len of var shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">),</span> <span class="s1">&#39;len of grad shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;indices&#39;</span><span class="p">:</span> <span class="n">indices_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span></div>


<div class="viewcode-block" id="ApplyProximalAdagrad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyProximalAdagrad">[docs]</a><span class="k">class</span> <span class="nc">ApplyProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. Must has the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **grad** (Tensor) - Gradient with the same shape and dtype as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_proximal_adagrad = P.ApplyProximalAdagrad()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.lr = 0.01</span>
<span class="sd">        &gt;&gt;&gt;         self.l1 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;         self.l2 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1, self.l2, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l1_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l1&#39;s rank&quot;</span><span class="p">,</span> <span class="n">l1_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l1_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l1_shape[0]&quot;</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l2_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l2_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l2&#39;s rank&quot;</span><span class="p">,</span> <span class="n">l2_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l2_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l2_shape[0]&quot;</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="SparseApplyProximalAdagrad"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SparseApplyProximalAdagrad">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm. Compared with ApplyProximalAdagrad,</span>
<span class="sd">    an additional index tensor is input.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type..</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices in the first dimension of `var` and `accum`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_proximal_adagrad = P.SparseApplyProximalAdagrad()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.lr = 0.01</span>
<span class="sd">        &gt;&gt;&gt;         self.l1 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;         self.l2 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">                                                             self.l2, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones((3,), np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                       <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;indices&#39;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAddSign"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyAddSign">[docs]</a><span class="k">class</span> <span class="nc">ApplyAddSign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t} = \beta * m_{t-1} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = (\alpha + \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`m_{t}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_add_sign = P.ApplyAddSign()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.m = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.lr = 0.001</span>
<span class="sd">        &gt;&gt;&gt;         self.alpha = 1.0</span>
<span class="sd">        &gt;&gt;&gt;         self.sign_decay = 0.99</span>
<span class="sd">        &gt;&gt;&gt;         self.beta = 0.9</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_add_sign(self.var, self.m, self.lr, self.alpha, self.sign_decay, self.beta, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyAddSign&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">,</span> <span class="n">sign_decay_shape</span><span class="p">,</span> <span class="n">beta_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;m_shape&#39;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">alpha_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;alpha&#39;s rank&quot;</span><span class="p">,</span> <span class="n">alpha_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;alpha_shape[0]&quot;</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">sign_decay_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sign_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sign_decay&#39;s rank&quot;</span><span class="p">,</span> <span class="n">sign_decay_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sign_decay_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sign_decay_shape[0]&quot;</span><span class="p">,</span> <span class="n">sign_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta&#39;s rank&quot;</span><span class="p">,</span> <span class="n">beta_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta_shape[0]&quot;</span><span class="p">,</span> <span class="n">beta_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">alpha_dtype</span><span class="p">,</span> <span class="n">sign_decay_dtype</span><span class="p">,</span> <span class="n">beta_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;sign_decay&quot;</span><span class="p">:</span> <span class="n">sign_decay_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span></div>


<div class="viewcode-block" id="ApplyPowerSign"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyPowerSign">[docs]</a><span class="k">class</span> <span class="nc">ApplyPowerSign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t} = \beta * m_{t-1} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`m_{t}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`.</span>

<span class="sd">    All of inputs comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If `lr`, `logbase`, `sign_decay` or `beta` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>
<span class="sd">    If inputs are tensors and have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">          If data type of `var` is float16, all inputs must have the same data type as `var`.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **logbase** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_power_sign = P.ApplyPowerSign()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.m = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.lr = 0.001</span>
<span class="sd">        &gt;&gt;&gt;         self.logbase = np.e</span>
<span class="sd">        &gt;&gt;&gt;         self.sign_decay = 0.99</span>
<span class="sd">        &gt;&gt;&gt;         self.beta = 0.9</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_power_sign(self.var, self.m, self.lr, self.logbase,</span>
<span class="sd">                                                self.sign_decay, self.beta, grad)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;logbase&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyPowerSign&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">logbase_shape</span><span class="p">,</span> <span class="n">sign_decay_shape</span><span class="p">,</span> <span class="n">beta_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;m_shape&#39;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="n">lr_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">logbase_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logbase_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;logbase&#39;s rank&quot;</span><span class="p">,</span> <span class="n">logbase_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logbase_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;logbase_shape[0]&quot;</span><span class="p">,</span> <span class="n">logbase_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">sign_decay_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sign_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sign_decay&#39;s rank&quot;</span><span class="p">,</span> <span class="n">sign_decay_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sign_decay_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sign_decay_shape[0]&quot;</span><span class="p">,</span> <span class="n">sign_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta&#39;s rank&quot;</span><span class="p">,</span> <span class="n">beta_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;beta_shape[0]&quot;</span><span class="p">,</span> <span class="n">beta_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">logbase_dtype</span><span class="p">,</span> <span class="n">sign_decay_dtype</span><span class="p">,</span> <span class="n">beta_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;logbase&quot;</span><span class="p">:</span> <span class="n">logbase_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;sign_decay&quot;</span><span class="p">:</span> <span class="n">sign_decay_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span></div>


<div class="viewcode-block" id="ApplyGradientDescent"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyGradientDescent">[docs]</a><span class="k">class</span> <span class="nc">ApplyGradientDescent</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the following formula.</span>

<span class="sd">    .. math::</span>
<span class="sd">        var = var - \alpha * \delta</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change, has the same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_gradient_descent = P.ApplyGradientDescent()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.alpha = 0.001</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, delta):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_gradient_descent(self.var, self.alpha, delta)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyGradientDescent&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;delta shape&#39;</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">alpha_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;alpha&#39;s rank&quot;</span><span class="p">,</span> <span class="n">alpha_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;alpha_shape[0]&quot;</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">alpha_dtype</span><span class="p">,</span> <span class="n">delta_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="n">delta_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="ApplyProximalGradientDescent"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyProximalGradientDescent">[docs]</a><span class="k">class</span> <span class="nc">ApplyProximalGradientDescent</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FOBOS(Forward Backward Splitting) algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{prox_v} = var - \alpha * \delta</span>
<span class="sd">    .. math::</span>
<span class="sd">        var = \frac{sign(\text{prox_v})}{1 + \alpha * l2} * \max(\left| \text{prox_v} \right| - alpha * l1, 0)</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Saling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change, has the same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_proximal_gradient_descent = P.ApplyProximalGradientDescent()</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.alpha = 0.001</span>
<span class="sd">        &gt;&gt;&gt;         self.l1 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;         self.l2 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, delta):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_proximal_gradient_descent(self.var, self.alpha, self.l1, self.l2, delta)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyGradientDescent&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;delta shape&#39;</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">alpha_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;alpha&#39;s rank&quot;</span><span class="p">,</span> <span class="n">alpha_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;alpha_shape[0]&quot;</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l1_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l1&#39;s rank&quot;</span><span class="p">,</span> <span class="n">l1_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l1_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l1_shape[0]&quot;</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l2_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l2_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l2&#39;s rank&quot;</span><span class="p">,</span> <span class="n">l2_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l2_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;l2_shape[0]&quot;</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">alpha_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span> <span class="n">delta_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="n">delta_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="LARSUpdate"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LARSUpdate">[docs]</a><span class="k">class</span> <span class="nc">LARSUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Conducts lars (layer-wise adaptive rate scaling) update on the sum of squares of gradient.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): Term added to the denominator to improve numerical stability. Default: 1e-05.</span>
<span class="sd">        hyperpara (float): Trust coefficient for calculating the local learning rate. Default: 0.001.</span>
<span class="sd">        use_clip (bool): Whether to use clip operation for calculating the local learning rate. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - The weight to be updated.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of weight, which has the same shape and dtype with weight.</span>
<span class="sd">        - **norm_weight** (Tensor) - A scalar tensor, representing the sum of squares of weight.</span>
<span class="sd">        - **norm_gradient** (Tensor) - A scalar tensor, representing the sum of squares of gradient.</span>
<span class="sd">        - **weight_decay** (Union[Number, Tensor]) - Weight decay. It must be a scalar tensor or number.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. It must be a scalar tensor or number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the new gradient.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import functional as F</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.lars = P.LARSUpdate()</span>
<span class="sd">        &gt;&gt;&gt;         self.reduce = P.ReduceSum()</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, weight, gradient):</span>
<span class="sd">        &gt;&gt;&gt;         w_square_sum = self.reduce(F.square(weight))</span>
<span class="sd">        &gt;&gt;&gt;         grad_square_sum = self.reduce(F.square(gradient))</span>
<span class="sd">        &gt;&gt;&gt;         grad_t = self.lars(weight, gradient, w_square_sum, grad_square_sum, 0.0, 1.0)</span>
<span class="sd">        &gt;&gt;&gt;         return grad_t</span>
<span class="sd">        &gt;&gt;&gt; weight = np.random.random(size=(2, 3)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = np.random.random(size=(2, 3)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; ms_output = net(Tensor(weight), Tensor(gradient))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">hyperpara</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">use_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;hyperpara&quot;</span><span class="p">,</span> <span class="n">hyperpara</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_clip&quot;</span><span class="p">,</span> <span class="n">use_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">norm_weight_shape</span><span class="p">,</span> <span class="n">norm_gradient_shape</span><span class="p">,</span> <span class="n">weight_decay_shape</span><span class="p">,</span>
                    <span class="n">learning_rate_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight shape&quot;</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;norm weight shape&quot;</span><span class="p">,</span> <span class="n">norm_weight_shape</span><span class="p">,</span> <span class="s2">&quot;norm gradient shape&quot;</span><span class="p">,</span> <span class="n">norm_gradient_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;weight decay&#39;s rank&quot;</span><span class="p">,</span> <span class="n">shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;weight_decay_shape[0]&quot;</span><span class="p">,</span> <span class="n">weight_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;learning rate&#39;s rank&quot;</span><span class="p">,</span> <span class="n">shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;learning_rate_shape[0]&quot;</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">norm_weight_dtype</span><span class="p">,</span> <span class="n">norm_gradient_dtype</span><span class="p">,</span>
                    <span class="n">weight_decay_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Weight dtype&quot;</span><span class="p">:</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="s2">&quot;gradient dtype&quot;</span><span class="p">:</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="s2">&quot;norm weight dtype&quot;</span><span class="p">:</span> <span class="n">norm_weight_dtype</span><span class="p">,</span>
                <span class="s2">&quot;norm gradient dtype&quot;</span><span class="p">:</span> <span class="n">norm_gradient_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">weight_decay_dtype</span><span class="p">},</span>
                                                   <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">},</span>
                                                   <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight_dtype</span></div>


<div class="viewcode-block" id="ApplyFtrl"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApplyFtrl">[docs]</a><span class="k">class</span> <span class="nc">ApplyFtrl</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL scheme.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The data type must be float16 or float32.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be positive. Default: 0.001.</span>
<span class="sd">          It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **lr_power** (Union[Number, Tensor]) - Learning rate power controls how the learning rate decreases</span>
<span class="sd">          during training, must be less than or equal to zero. Use fixed learning rate if lr_power is zero.</span>
<span class="sd">          Default: -0.5. It must be a float number or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class ApplyFtrlNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(ApplyFtrlNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.apply_ftrl = P.ApplyFtrl()</span>
<span class="sd">        &gt;&gt;&gt;         self.lr = 0.001</span>
<span class="sd">        &gt;&gt;&gt;         self.l1 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;         self.l2 = 0.0</span>
<span class="sd">        &gt;&gt;&gt;         self.lr_power = -0.5</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.linear = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.apply_ftrl(self.var, self.accum, self.linear, grad, self.lr, self.l1, self.l2,</span>
<span class="sd">        &gt;&gt;&gt;                               self.lr_power)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(-4, 4, (3, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = net(input_x)</span>
<span class="sd">        [[0.67455846   0.14630564   0.160499  ]</span>
<span class="sd">         [0.16329421   0.00415689   0.05202988]</span>
<span class="sd">         [0.18672481   0.17418946   0.36420345]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_power&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span>
                    <span class="n">lr_power_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">var_shape</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">linear_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">lr_type</span><span class="p">,</span> <span class="n">l1_type</span><span class="p">,</span> <span class="n">l2_type</span><span class="p">,</span> <span class="n">lr_power_type</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span> <span class="n">linear_type</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;lr_power&quot;</span><span class="p">:</span> <span class="n">lr_power_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_tbe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">var_type</span>
        <span class="k">return</span> <span class="n">var_type</span></div>


<div class="viewcode-block" id="SparseApplyFtrl"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SparseApplyFtrl">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyFtrl</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be the same data type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in the first dimension. The type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_ftrl = P.SparseApplyFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.linear = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyFtrlV2"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SparseApplyFtrlV2">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyFtrlV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2_shrinkage (float): L2 shrinkage regularization.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): If `True`, the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in the first dimension. The type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as P</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlV2Net(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(SparseApplyFtrlV2Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.sparse_apply_ftrl_v2 = P.SparseApplyFtrlV2(lr=0.01, l1=0.0, l2=0.0,</span>
<span class="sd">                                                                    l2_shrinkage=0.0, lr_power=-0.5)</span>
<span class="sd">        &gt;&gt;&gt;         self.var = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.accum = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.linear = Parameter(Tensor(np.random.rand(3, 3).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, grad, indices):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.sparse_apply_ftrl_v2(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlV2Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(3, 3).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2_shrinkage</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2_shrinkage&quot;</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">:</span> <span class="n">indices_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span></div>


<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Dropout">[docs]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor with probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): The keep rate, between 0 and 1, e.g. keep_prob = 0.9,</span>
<span class="sd">          means dropping out 10% of input units.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (tuple[int]) - The shape of target mask.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value of generated mask for input shape.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout = P.Dropout(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; in = Tensor((20, 16, 50, 50))</span>
<span class="sd">        &gt;&gt;&gt; out = dropout(in)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">mask_shape</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">mask_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="CTCLoss"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.CTCLoss">[docs]</a><span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    Args:</span>
<span class="sd">        preprocess_collapse_repeated (bool): If true, repeated labels will be collapsed prior to the CTC calculation.</span>
<span class="sd">                                             Default: False.</span>
<span class="sd">        ctc_merge_repeated (bool): If false, during CTC calculation, repeated non-blank labels will not be merged</span>
<span class="sd">                                   and these labels will be interpreted as individual ones. This is a simplfied</span>
<span class="sd">                                   version of CTC. Default: True.</span>
<span class="sd">        ignore_longer_outputs_than_inputs (bool): If true, sequences with longer outputs than inputs will be ignored.</span>
<span class="sd">                                                  Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - The input Tensor must be a `3-D` tensor whose shape is</span>
<span class="sd">          (`max_time`, `batch_size`, `num_classes`). `num_classes` must be `num_labels + 1` classes, `num_labels`</span>
<span class="sd">          indicates the number of actual labels. Blank labels are reserved. Default blank label is `num_classes - 1`.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels_indices** (Tensor) - The indices of labels. `labels_indices[i, :] == [b, t]` means `labels_values[i]`</span>
<span class="sd">          stores the id for `(batch b, time t)`. The type must be int64 and rank must be 2.</span>
<span class="sd">        - **labels_values** (Tensor) - A `1-D` input tensor. The values are associated with the given batch and time.</span>
<span class="sd">          The type must be int32. `labels_values[i]` must in the range of `[0, num_classes)`.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of (`batch_size`).</span>
<span class="sd">          The type must be int32. Each value in the tensor must not be greater than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - A tensor containing log-probabilities, the shape is (`batch_size`). The tensor has</span>
<span class="sd">          the same type with `inputs`.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of `loss`, has the same type and shape with `inputs`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.random.random((2, 2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels_indices = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; labels_values = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = P.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = ctc_loss(inputs, labels_indices, labels_values, sequence_length)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">ignore_longer_outputs_than_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_values&quot;</span><span class="p">,</span> <span class="s2">&quot;sequence_length&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;preprocess_collapse_repeated&quot;</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_collapse_repeated_</span> <span class="o">=</span> <span class="n">preprocess_collapse_repeated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ctc_merge_repeated_</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ctc_merge_repeated&quot;</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="p">,</span>
                                                              <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_longer_outputs_than_inputs&quot;</span><span class="p">,</span>
                                   <span class="n">ignore_longer_outputs_than_inputs</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_longer_outputs_than_inputs_</span> <span class="o">=</span> <span class="n">ignore_longer_outputs_than_inputs</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">,</span> <span class="n">labels_values</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;inputs rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;labels_indices rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_indices</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;labels_indices dim one&quot;</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;labels_values rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_values</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sequence_length rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels_indices size&#39;</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;labels_values size&#39;</span><span class="p">,</span>
                        <span class="n">labels_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;inputs batch_size&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;sequence_length batch_size&#39;</span><span class="p">,</span>
                        <span class="n">sequence_length</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_size</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">,</span> <span class="n">labels_values</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="n">valid_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">double</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;inputs_dtype&quot;</span><span class="p">:</span> <span class="n">inputs</span><span class="p">},</span> <span class="n">valid_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;labels_indices_dtype&quot;</span><span class="p">:</span> <span class="n">labels_indices</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;labels_values_dtype&quot;</span><span class="p">:</span> <span class="n">labels_values</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;sequence_length_dtype&quot;</span><span class="p">:</span> <span class="n">sequence_length</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span></div>


<div class="viewcode-block" id="CTCGreedyDecoder"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.CTCGreedyDecoder">[docs]</a><span class="k">class</span> <span class="nc">CTCGreedyDecoder</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        merge_repeated (bool): If true, merge repeated classes in output. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - The input Tensor must be a `3-D` tensor whose shape is</span>
<span class="sd">          (`max_time`, `batch_size`, `num_classes`). `num_classes` must be `num_labels + 1` classes,</span>
<span class="sd">          `num_labels` indicates the number of actual labels. Blank labels are reserved.</span>
<span class="sd">          Default blank label is `num_classes - 1`. Data type must be float32 or float64.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of (`batch_size`).</span>
<span class="sd">          The type must be int32. Each value in the tensor must not greater than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **decoded_indices** (Tensor) - A tensor with shape of (`total_decoded_outputs`, 2).</span>
<span class="sd">          Data type is int64.</span>
<span class="sd">        - **decoded_values** (Tensor) - A tensor with shape of (`total_decoded_outputs`),</span>
<span class="sd">          it stores the decoded classes. Data type is int64.</span>
<span class="sd">        - **decoded_shape** (Tensor) - The value of tensor is [`batch_size`, `max_decoded_legth`].</span>
<span class="sd">          Data type is int64.</span>
<span class="sd">        - **log_probability** (Tensor) - A tensor with shape of (`batch_size`, 1),</span>
<span class="sd">          containing sequence log-probability, has the same type as `inputs`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt;    class CTCGreedyDecoderNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;        def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;            super(CTCGreedyDecoderNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;            self.ctc_greedy_decoder = P.CTCGreedyDecoder()</span>
<span class="sd">        &gt;&gt;&gt;            self.assert_op = P.Assert(300)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;        def construct(self, inputs, sequence_length):</span>
<span class="sd">        &gt;&gt;&gt;            out = self.ctc_greedy_decoder(inputs,sequence_length)</span>
<span class="sd">        &gt;&gt;&gt;            self.assert_op(True, (out[0], out[1], out[2], out[3]))</span>
<span class="sd">        &gt;&gt;&gt;            return out[2]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.random.random((2, 2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = CTCGreedyDecoderNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(inputs, sequence_length)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merge_repeated</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;merge_repeated&quot;</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">,</span> <span class="n">sequence_length_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;inputs rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;sequence_length rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequence_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;inputs batch_size&#39;</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;sequence_length batch_size&#39;</span><span class="p">,</span>
                        <span class="n">sequence_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">total_decoded_outputs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">decoded_indices_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">total_decoded_outputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">decoded_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">total_decoded_outputs</span><span class="p">]</span>
        <span class="n">decoded_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">log_probability_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">decoded_indices_shape</span><span class="p">,</span> <span class="n">decoded_values</span><span class="p">,</span> <span class="n">decoded_shape</span><span class="p">,</span> <span class="n">log_probability_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_dtype</span><span class="p">,</span> <span class="n">sequence_length_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;inputs_dtype&quot;</span><span class="p">:</span> <span class="n">inputs_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">double</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;sequence_length_dtype&quot;</span><span class="p">:</span> <span class="n">sequence_length_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">decoded_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoded_type</span><span class="p">,</span> <span class="n">decoded_type</span><span class="p">,</span> <span class="n">decoded_type</span><span class="p">,</span> <span class="n">inputs_dtype</span></div>


<div class="viewcode-block" id="BasicLSTMCell"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BasicLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">BasicLSTMCell</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the long short-term memory (LSTM) to the input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            i_t = \sigma(W_{ix} x_t + b_{ix} + W_{ih} h_{(t-1)} + b_{ih}) \\</span>
<span class="sd">            f_t = \sigma(W_{fx} x_t + b_{fx} + W_{fh} h_{(t-1)} + b_{fh}) \\</span>
<span class="sd">            \tilde{c}_t = \tanh(W_{cx} x_t + b_{cx} + W_{ch} h_{(t-1)} + b_{ch}) \\</span>
<span class="sd">            o_t = \sigma(W_{ox} x_t + b_{ox} + W_{oh} h_{(t-1)} + b_{oh}) \\</span>
<span class="sd">            c_t = f_t * c_{(t-1)} + i_t * \tilde{c}_t \\</span>
<span class="sd">            h_t = o_t * \tanh(c_t) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Here :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product. :math:`W, b`</span>
<span class="sd">    are learnable weights between the output and the input in the formula. For instance,</span>
<span class="sd">    :math:`W_{ix}, b_{ix}` are the weight and bias used to transform from input :math:`x` to :math:`i`.</span>
<span class="sd">    Details can be found in paper `LONG SHORT-TERM MEMORY</span>
<span class="sd">    &lt;https://www.bioinf.jku.at/publications/older/2604.pdf&gt;`_ and</span>
<span class="sd">    `Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</span>
<span class="sd">    &lt;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43905.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): If not 1.0, append `Dropout` layer on the outputs of each</span>
<span class="sd">            LSTM layer except the last layer. Default 1.0. The range of dropout is [0.0, 1.0].</span>
<span class="sd">        forget_bias (float): Add forget bias to forget gate biases in order to decrease former scale. Default: 1.0.</span>
<span class="sd">        state_is_tuple (bool): If true, the state is a tuple of 2 tensors, containing h and c; If false, the state is</span>
<span class="sd">        a tensor and it needs to be split first. Default: True.</span>
<span class="sd">        activation (str): Activation. Default: &quot;tanh&quot;. Only &quot;tanh&quot; is currently supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words. Tensor of shape (`batch_size`, `input_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **h** (Tensor) - Hidden state last moment. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **c** (Tensor) - Cell state last moment. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **w** (Tensor) - Weight. Tensor of shape (`input_size + hidden_size`, `4 x hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **b** (Tensor) - Bias. Tensor of shape (`4 x hidden_size`).</span>
<span class="sd">          The data type must be the same as `c`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **ct** (Tensor) - Forward :math:`c_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **ht** (Tensor) - Cell output. Tensor of shape (`batch_size`, `hidden_size`). With data type of float16.</span>
<span class="sd">        - **it** (Tensor) - Forward :math:`i_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **jt** (Tensor) - Forward :math:`j_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **ft** (Tensor) - Forward :math:`f_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **ot** (Tensor) - Forward :math:`o_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **tanhct** (Tensor) - Forward :math:`tanh c_t` cache at moment `t`.</span>
<span class="sd">          Tensor of shape (`batch_size`, `hidden_size`), has the same type with input `c`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(1, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; h = Tensor(np.random.rand(1, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; c = Tensor(np.random.rand(1, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.rand(96, 256).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.random.rand(256, ).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; lstm = P.BasicLSTMCell(keep_prob=1.0, forget_bias=1.0, state_is_tuple=True, activation=&#39;tanh&#39;)</span>
<span class="sd">        &gt;&gt;&gt; lstm(x, h, c, w, b)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_is_tuple</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;state_is_tuple&quot;</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;io_format&quot;</span><span class="p">,</span> <span class="s2">&quot;ND&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;c rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[0]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[1]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]+h_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">ct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ht_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">it_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">jt_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ft_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ot_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">tanhct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">ct_shape</span><span class="p">,</span> <span class="n">ht_shape</span><span class="p">,</span> <span class="n">it_shape</span><span class="p">,</span> <span class="n">jt_shape</span><span class="p">,</span> <span class="n">ft_shape</span><span class="p">,</span> <span class="n">ot_shape</span><span class="p">,</span> <span class="n">tanhct_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;h_dtype&quot;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;w_dtype&quot;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;c_dtype&quot;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s2">&quot;b_dtype&quot;</span><span class="p">:</span> <span class="n">b_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">c_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DynamicRNN"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.DynamicRNN">[docs]</a><span class="k">class</span> <span class="nc">DynamicRNN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DynamicRNN Operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_type (str): An string identifying the cell type in the op. Default: &#39;LSTM&#39;.</span>
<span class="sd">            Only &#39;LSTM&#39; is currently supported.</span>
<span class="sd">        direction (str): An string identifying the direction in the op. Default: &#39;UNIDIRECTIONAL&#39;.</span>
<span class="sd">            Only &#39;UNIDIRECTIONAL&#39; is currently supported.</span>
<span class="sd">        cell_depth (int): An integer identifying the cell depth in the op. Default: 1.</span>
<span class="sd">        use_peephole (bool): An bool identifying if use peephole in the op. Default: False.</span>
<span class="sd">        keep_prob (float): An float identifying the keep prob in the op. Default: 1.0.</span>
<span class="sd">        cell_clip (float): An float identifying the cell clip in the op. Default: -1.0.</span>
<span class="sd">        num_proj (int): An integer identifying the num proj in the op. Default: 0.</span>
<span class="sd">        time_major (bool): An bool identifying the time major in the op. Default: True.</span>
<span class="sd">            Only `True` is currently supported.</span>
<span class="sd">        activation (str): An string identifying the type of activation function in the op. Default: &#39;tanh&#39;.</span>
<span class="sd">            Only &#39;tanh&#39; is currently supported.</span>
<span class="sd">        forget_bias (float): An float identifying the forget bias in the op. Default: 0.0.</span>
<span class="sd">        is_training (bool): An bool identifying is training in the op. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words. Tensor of shape (`num_step`, `batch_size`, `input_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **w** (Tensor) - Weight. Tensor of shape (`input_size + hidden_size`, `4 x hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **b** (Tensor) - Bias. Tensor of shape (`4 x hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape (`batch_size`).</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time. Tensor of shape (1, `batch_size`, `hidden_size`).</span>
<span class="sd">        - **init_c** (Tensor) - Cell state of initial time. Tensor of shape (1, `batch_size`, `hidden_size`).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          With data type of float16.</span>
<span class="sd">        - **output_c** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **i** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **j** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **f** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **o** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **tanhct** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 16, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.rand(96, 128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.random.rand(128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_c = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_rnn = P.DynamicRNN()</span>
<span class="sd">        &gt;&gt;&gt; output = lstm(x, w, b, None, init_h, init_c)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cell_type</span><span class="o">=</span><span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">use_peephole</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_peephole</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_peephole&quot;</span><span class="p">,</span> <span class="n">use_peephole</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;cell_type&quot;</span><span class="p">,</span> <span class="n">cell_type</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;io_format&quot;</span><span class="p">,</span> <span class="s2">&quot;ND&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">,</span> <span class="n">seq_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seq_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, seq_shape should be None.&quot;</span><span class="p">)</span>

        <span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[-1]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[-1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, w_shape[-1] should multiple of 4.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_size + hidden_size&quot;</span><span class="p">,</span>
                        <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape[2]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">seq_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x dtype&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;w dtype&quot;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;b dtype&quot;</span><span class="p">:</span> <span class="n">b_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;h dtype&quot;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;c dtype&quot;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span></div>


<div class="viewcode-block" id="InTopK"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.InTopK">[docs]</a><span class="k">class</span> <span class="nc">InTopK</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Whether the targets are in the top `k` predictions.</span>

<span class="sd">    Args:</span>
<span class="sd">        k (int): Specifies the number of top elements to be used for computing precision.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - A 2D Tensor defines the predictions of a batch of samples with float16 or float32 data type.</span>
<span class="sd">        - **x2** (Tensor) - A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of x2</span>
<span class="sd">          must be equal to x1&#39;s first dimension. The values of `x2` can not be negative and</span>
<span class="sd">          must be equal to or less than index of x1&#39;s second dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor has 1 dimension of type bool and the same shape with `x2`. For labeling sample `i` in `x2`,</span>
<span class="sd">        if the label in the first `k` predictions for sample `i` is in `x1`, then the value is True, otherwise False.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; in_top_k = P.InTopK(3)</span>
<span class="sd">        &gt;&gt;&gt; result = in_top_k(x1, x2)</span>
<span class="sd">        [True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InTopK&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_dtype</span><span class="p">,</span> <span class="n">x2_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x1&quot;</span><span class="p">:</span> <span class="n">x1_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x2&quot;</span><span class="p">:</span> <span class="n">x2_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x1 shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x2 shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;size of x2&quot;</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x1&#39;s first dimension&quot;</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x2_shape</span></div>


<div class="viewcode-block" id="LRN"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LRN">[docs]</a><span class="k">class</span> <span class="nc">LRN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D.</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0).</span>
<span class="sd">        alpha (float): A scale factor, usually positive.</span>
<span class="sd">        beta (float): An exponent.</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: &quot;ACROSS_CHANNELS&quot;. Default: &quot;ACROSS_CHANNELS&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4D Tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as the input tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(1, 10, 4, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lrn = P.LRN()</span>
<span class="sd">        &gt;&gt;&gt; lrn(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LRN&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="n">depth_radius</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;norm_region&quot;</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s1">&#39;norm_region&#39;</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ACROSS_CHANNELS&#39;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="n">depth_radius</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>