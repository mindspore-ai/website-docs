

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.nn.loss.loss &mdash; MindSpore r1.1 documentation</title>
  

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>
<p class="caption"><span class="caption-text">MindSpore Serving Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore_serving/mindspore_serving.html">mindspore_serving</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.nn.loss.loss</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.nn.loss.loss</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;loss&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">_selected_ops</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.cell</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="n">context</span>


<span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for other losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">reduction</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span>

        <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;reduction method for {reduction.lower()} is not supported&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_mean</span> <span class="o">=</span> <span class="n">_selected_ops</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">perm</span>

    <span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<div class="viewcode-block" id="L1Loss"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.L1Loss.html#mindspore.nn.L1Loss">[docs]</a><span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L1Loss creates a criterion to measure the mean absolute error (MAE) between :math:`x` and :math:`y` element-wise,</span>
<span class="sd">    where :math:`x` is the input Tensor and :math:`y` is the target Tensor.</span>

<span class="sd">    For simplicity, let :math:`x` and :math:`y` be 1-dimensional Tensor with length :math:`N`,</span>
<span class="sd">    the unreduced loss (i.e. with argument reduction set to &#39;none&#39;) of :math:`x` and :math:`y` is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(x, y) = \{l_1,\dots,l_N\}, \quad \text{with } l_n = \left| x_n - y_n \right|</span>

<span class="sd">    When argument reduction is &#39;mean&#39;, the mean value of :math:`L(x, y)` will be returned.</span>
<span class="sd">    When argument reduction is &#39;sum&#39;, the sum of :math:`L(x, y)` will be returned. :math:`N` is the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &quot;mean&quot;, &quot;sum&quot;, and &quot;none&quot;.</span>
<span class="sd">            Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - Tensor of shape :math:`(x_1, x_2, ..., x_R)`. The data type must be float16 or</span>
<span class="sd">          float32.</span>
<span class="sd">        - **target_data** (Tensor) - Tensor of shape :math:`(y_1, y_2, ..., y_S)`. The data type must be float16 or</span>
<span class="sd">          float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, loss float tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.L1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target_data = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input_data, target_data)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.33333334</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="MSELoss"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.MSELoss.html#mindspore.nn.MSELoss">[docs]</a><span class="k">class</span> <span class="nc">MSELoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MSELoss creates a criterion to measure the mean squared error (squared L2-norm) between :math:`x` and :math:`y`</span>
<span class="sd">    element-wise, where :math:`x` is the input and :math:`y` is the target.</span>

<span class="sd">    For simplicity, let :math:`x` and :math:`y` be 1-dimensional Tensor with length :math:`N`,</span>
<span class="sd">    the unreduced loss (i.e. with argument reduction set to &#39;none&#39;) of :math:`x` and :math:`y` is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(x, y) = \{l_1,\dots,l_N\}, \quad \text{with} \quad l_n = (x_n - y_n)^2.</span>

<span class="sd">    When argument reduction is &#39;mean&#39;, the mean value of :math:`L(x, y)` will be returned.</span>
<span class="sd">    When argument reduction is &#39;sum&#39;, the sum of :math:`L(x, y)` will be returned. :math:`N` is the batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &quot;mean&quot;, &quot;sum&quot;, and &quot;none&quot;.</span>
<span class="sd">            Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - Tensor of shape :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **target_data** (Tensor) - Tensor of shape :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, weighted loss float tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target_data = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input_data, target_data)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.33333334</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">base</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.SmoothL1Loss.html#mindspore.nn.SmoothL1Loss">[docs]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A loss class for learning region proposals.</span>

<span class="sd">    SmoothL1Loss can be regarded as modified version of L1Loss or a combination of L1Loss and L2Loss.</span>
<span class="sd">    L1Loss computes the element-wise absolute difference between two input Tensor while L2Loss computes the</span>
<span class="sd">    squared difference between two input Tensor. L2Loss often leads to faster convergence but it is less</span>
<span class="sd">    robust to outliers.</span>

<span class="sd">    Given two input :math:`x,\  y` of length :math:`N`, the unreduced SmoothL1Loss can be described</span>
<span class="sd">    as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{i} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        \frac{0.5 (x_i - y_i)^{2}}{\text{beta}}, &amp; \text{if } |x_i - y_i| &lt; \text{beta} \\</span>
<span class="sd">        |x_i - y_i| - 0.5 \text{beta}, &amp; \text{otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Here :math:`\text{beta}` controls the point where the loss function changes from quadratic to linear.</span>
<span class="sd">    Its default value is 1.0. :math:`N` is the batch size. This function returns an</span>
<span class="sd">    unreduced loss Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta (float): A parameter used to control the point where the function will change from</span>
<span class="sd">            quadratic to linear. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - Tensor of shape :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **target_data** (Tensor) - Tensor of shape :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, loss float tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SmoothL1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target_data = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input_data, target_data)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SmoothL1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth_l1_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.SoftmaxCrossEntropyWithLogits.html#mindspore.nn.SoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes softmax cross entropy between logits and labels.</span>

<span class="sd">    Measures the distribution error between the probabilities of the input (computed with softmax function) and the</span>
<span class="sd">    target where the classes are mutually exclusive (only one class is positive) using cross entropy loss.</span>

<span class="sd">    Typical input into this function is unnormalized scores and target of each class.</span>
<span class="sd">    Scores Tensor :math:`x` is of shape :math:`(N, C)` and target Tensor :math:`t` is a</span>
<span class="sd">    Tensor of shape :math:`(N, C)` which contains one-hot labels of length :math:`C`.</span>

<span class="sd">    For each instance :math:`N_i`, the loss is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x_i, t_i) = - \log\left(\frac{\exp(x_{t_i})}{\sum_j \exp(x_j)}\right)</span>
<span class="sd">        =  -x_{t_i} + \log\left(\sum_j \exp(x_j)\right)</span>

<span class="sd">    where :math:`x_i` is a 1D score Tensor, :math:`t_i` is a scalar.</span>

<span class="sd">    Note:</span>
<span class="sd">        While the target classes are mutually exclusive, i.e., only one class is positive in the target, the predicted</span>
<span class="sd">        probabilities need not to be exclusive. It is only required that the predicted probability distribution</span>
<span class="sd">        of entry is a valid one.</span>

<span class="sd">    Args:</span>
<span class="sd">        sparse (bool): Specifies whether labels use sparse format or not. Default: False.</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &quot;mean&quot;, &quot;sum&quot;, and &quot;none&quot;.</span>
<span class="sd">            If &quot;none&quot;, do not perform reduction. Default: &quot;none&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape (N, C).</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape (N, ). If `sparse` is True, The type of</span>
<span class="sd">          `labels` is mindspore.int32. If `sparse` is False, the type of `labels` is the same as the type of `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor of the same shape as logits with the component-wise</span>
<span class="sd">        logistic losses.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)</span>
<span class="sd">        &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.random.randint(0, 9, [1, 10]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels_np = np.ones([1,]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(labels_np)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [7.868383]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">sparse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_cross_entropy</span> <span class="o">=</span> <span class="n">_selected_ops</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_cpugpu</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">x</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logits</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>

<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_label_dtype</span><span class="p">(</span><span class="n">labels_dtype</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="n">labels_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>


<div class="viewcode-block" id="SampledSoftmaxLoss"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.SampledSoftmaxLoss.html#mindspore.nn.SampledSoftmaxLoss">[docs]</a><span class="k">class</span> <span class="nc">SampledSoftmaxLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sampled softmax training loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_sampled (int): The number of classes to randomly sample per batch.</span>
<span class="sd">        num_classes (int): The number of possible classes.</span>
<span class="sd">        num_true (int): The number of target classes per training example.</span>
<span class="sd">        sampled_values (Tuple):  Tuple of (`sampled_candidates`, `true_expected_count`,</span>
<span class="sd">            `sampled_expected_count`) returned by a `*CandidateSampler` function.</span>
<span class="sd">            Default to None, `UniformCandidateSampler` is applied.</span>
<span class="sd">        remove_accidental_hits (bool): Whether to remove &quot;accidental hits&quot;</span>
<span class="sd">            where a sampled class equals one of the target classes.  Default is True.</span>
<span class="sd">        seed (int): Random seed for candidate sampling. Default: 0</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &quot;mean&quot;, &quot;sum&quot;, and &quot;none&quot;.</span>
<span class="sd">            If &quot;none&quot;, do not perform reduction. Default: &quot;none&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weights** (Tensor) - Tensor of shape (C, dim).</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape (C).  The class biases.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape (N, num_true), type `int64, int32`. The</span>
<span class="sd">          target classes.</span>
<span class="sd">        - **inputs** (Tensor) - Tensor of shape (N, dim). The forward activations of</span>
<span class="sd">          the input network.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor of shape (N) with the per-example sampled softmax losses.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; mindspore.set_seed(1)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SampledSoftmaxLoss(num_sampled=4, num_classes=7, num_true=1)</span>
<span class="sd">        &gt;&gt;&gt; weights = Tensor(np.random.randint(0, 9, [7, 10]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; biases = Tensor(np.random.randint(0, 9, [7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1, 2])</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.random.randint(0, 9, [3, 10]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(weights, biases, labels, inputs)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4.6051701e+01 1.4000047e+01 6.1989022e-06]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_sampled</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">sampled_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">remove_accidental_hits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SampledSoftmaxLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_true</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;num_true </span><span class="si">{num_true}</span><span class="s2"> is less than 1.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;seed </span><span class="si">{seed}</span><span class="s2"> is less than 0.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_sampled</span> <span class="o">&gt;</span> <span class="n">num_classes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;num_sampled </span><span class="si">{num_sampled}</span><span class="s2"> is great than num_classes </span><span class="si">{num_classes}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_true</span> <span class="o">&gt;</span> <span class="n">num_classes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;num_true </span><span class="si">{num_true}</span><span class="s2"> is great than num_classes </span><span class="si">{num_classes}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sampled_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampled_values</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sampled_values </span><span class="si">{sampled_values}</span><span class="s2"> is not a list.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sampled_values</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sampled_values size {len(sampled_values)} is not 3.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_sampled</span> <span class="o">=</span> <span class="n">num_sampled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span> <span class="o">=</span> <span class="n">num_true</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampled_values</span> <span class="o">=</span> <span class="n">sampled_values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_accidental_hits</span> <span class="o">=</span> <span class="n">remove_accidental_hits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UniformCandidateSampler</span><span class="p">(</span>
            <span class="n">num_true</span><span class="p">,</span>
            <span class="n">num_sampled</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">,</span>
            <span class="n">seed</span><span class="p">,</span>
            <span class="n">remove_accidental_hits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gather_v2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_max_true</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum_true</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim0</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">_check_label_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sampled_logits</span><span class="p">(</span>
            <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span>
            <span class="n">biases</span><span class="o">=</span><span class="n">biases</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">num_true</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_true</span><span class="p">,</span>
            <span class="n">sampled_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampled_values</span><span class="p">,</span>
            <span class="n">subtract_log_q</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_softmax_cross_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">stable_exp_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_max_true</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">stable_exp_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum_true</span><span class="p">(</span><span class="n">stable_exp_logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">targets</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span> <span class="o">+</span> <span class="mf">1.0e-20</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_sampled_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span>
                                <span class="n">biases</span><span class="p">,</span>
                                <span class="n">labels</span><span class="p">,</span>
                                <span class="n">inputs</span><span class="p">,</span>
                                <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">sampled_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">subtract_log_q</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper function for SampledSoftmaxLoss functions.</span>

<span class="sd">        Computes sampled output training logits and labels suitable</span>

<span class="sd">        Note: In the case where num_true &gt; 1, we assign to each target class</span>
<span class="sd">        the target probability 1 / num_true so that the target probabilities</span>
<span class="sd">        sum to 1 per-example.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights (Tensor): Tensor of shape `[num_classes, dim]`.</span>
<span class="sd">            biases (Tensor): Tensor of shape `[num_classes]`.</span>
<span class="sd">            labels (Tensor): Tensor of shape `[batch_size, num_true]`. The target classes.</span>
<span class="sd">            inputs (Tensor): Tensor of shape `[batch_size, dim]`.  The forward</span>
<span class="sd">                activations of the input network.</span>
<span class="sd">            num_true (int): The number of target classes per training example.</span>
<span class="sd">            sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`,</span>
<span class="sd">                `sampled_expected_count`) returned by a `UniformCandidateSampler` function.</span>
<span class="sd">            subtract_log_q: A `bool`.  whether to subtract the log expected count of</span>
<span class="sd">                the labels in the sample to get the logits of the true labels.</span>
<span class="sd">                Default is True.</span>
<span class="sd">        Returns:</span>
<span class="sd">            out_logits: `Tensor` object with shape</span>
<span class="sd">                `[batch_size, num_true + num_sampled]`</span>
<span class="sd">            out_labels: A Tensor object with the same shape as `out_logits`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">))</span>
        <span class="n">labels_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>

        <span class="c1"># Sample the negative labels.</span>
        <span class="c1">#   sampled shape: [num_sampled] tensor</span>
        <span class="c1">#   true_expected_count shape is [batch_size, 1] tensor</span>
        <span class="c1">#   sampled_expected_count shape is [num_sampled] tensor</span>
        <span class="k">if</span> <span class="n">sampled_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampled_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

        <span class="p">(</span><span class="n">sampled</span><span class="p">,</span> <span class="n">true_expected_count</span><span class="p">,</span> <span class="n">sampled_expected_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">sampled_values</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">sampled</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">sampled</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">all_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim0</span><span class="p">((</span><span class="n">labels_flat</span><span class="p">,</span> <span class="n">sampled</span><span class="p">))</span>
        <span class="n">all_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_v2</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">all_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">n_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">labels_flat</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">sampled</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">all_w</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># true_w shape is [batch_size * num_true, dim]</span>
        <span class="n">true_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_w</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">n_true</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">])</span>
        <span class="n">sampled_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_w</span><span class="p">,</span> <span class="p">[</span><span class="n">n_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">n_sampled</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">])</span>
        <span class="n">sampled_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sampled_w</span><span class="p">)</span>

        <span class="n">all_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_v2</span><span class="p">(</span><span class="n">biases</span><span class="p">,</span> <span class="n">all_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">true_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_b</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">n_true</span><span class="p">])</span>
        <span class="n">sampled_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_b</span><span class="p">,</span> <span class="p">[</span><span class="n">n_true</span><span class="p">],</span> <span class="p">[</span><span class="n">n_sampled</span><span class="p">])</span>

        <span class="c1"># inputs shape is [batch_size, dim]</span>
        <span class="c1"># true_w shape is [batch_size * num_true, dim]</span>
        <span class="c1"># row_wise_dots is [batch_size, num_true, dim]</span>
        <span class="n">new_true_w_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">)</span>
        <span class="n">row_wise_dots</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">true_w</span><span class="p">,</span> <span class="n">new_true_w_shape</span><span class="p">))</span>

        <span class="c1"># We want the row-wise dot plus biases which yields a</span>
        <span class="c1"># [batch_size, num_true] tensor of true_logits.</span>
        <span class="n">dots_as_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">row_wise_dots</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">))</span>
        <span class="n">true_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">dots_as_matrix</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">))</span>
        <span class="n">true_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">true_b</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">))</span>
        <span class="n">true_logits</span> <span class="o">+=</span> <span class="n">true_b</span>
        <span class="n">sampled_logits</span> <span class="o">+=</span> <span class="n">sampled_b</span>

        <span class="k">if</span> <span class="n">subtract_log_q</span><span class="p">:</span>
            <span class="c1"># Subtract log of Q(l), prior probability that l appears in sampled.</span>
            <span class="n">true_logits</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">true_expected_count</span><span class="p">)</span>
            <span class="n">sampled_logits</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sampled_expected_count</span><span class="p">)</span>

        <span class="c1"># Construct output logits and labels. The true labels/logits start at col 0.</span>
        <span class="n">out_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim1</span><span class="p">((</span><span class="n">true_logits</span><span class="p">,</span> <span class="n">sampled_logits</span><span class="p">))</span>

        <span class="c1"># true_logits is a float tensor, ones_like(true_logits) is a float</span>
        <span class="c1"># tensor of ones. We then divide by num_true to ensure the per-example</span>
        <span class="c1"># labels sum to 1.0, i.e. form a proper probability distribution.</span>
        <span class="n">out_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim1</span><span class="p">((</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">true_logits</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_true</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sampled_logits</span><span class="p">)</span>
        <span class="p">))</span>
        <span class="k">return</span> <span class="n">out_logits</span><span class="p">,</span> <span class="n">out_labels</span></div>


<div class="viewcode-block" id="BCELoss"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.BCELoss.html#mindspore.nn.BCELoss">[docs]</a><span class="k">class</span> <span class="nc">BCELoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    BCELoss creates a criterion to measure the binary cross entropy between the true labels and predicted labels.</span>

<span class="sd">    Set the predicted labels as :math:`x`, true labels as :math:`y`, the output loss as :math:`\ell(x, y)`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{`none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        Note that the predicted labels should always be the output of sigmoid and the true labels should be numbers</span>
<span class="sd">        between 0 and 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            And it must have same shape and data type as `inputs`. Default: None</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &#39;none&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - The input Tensor. The data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - The label Tensor which has same shape and data type as `inputs`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `inputs`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 3.3, 2.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.BCELoss(weight=weight, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.array([[0.1, 0.2, 0.3], [0.5, 0.7, 0.9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[0, 1, 0], [0, 0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(inputs, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.8952923</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_one</span> <span class="o">=</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_one</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ones</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_one</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_reduced_shape_valid</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">,</span> <span class="n">reduced_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_reduce_shape</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">,</span> <span class="n">reduced_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>

<div class="viewcode-block" id="CosineEmbeddingLoss"><a class="viewcode-back" href="../../../../mindspore/nn/mindspore.nn.CosineEmbeddingLoss.html#mindspore.nn.CosineEmbeddingLoss">[docs]</a><span class="k">class</span> <span class="nc">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the similarity between two tensors using cosine distance.</span>

<span class="sd">    Given two tensors `x1`, `x2`, and a Tensor label `y` with values 1 or -1:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss(x_1, x_2, y) = \begin{cases}</span>
<span class="sd">        1-cos(x_1, x_2), &amp; \text{if } y = 1\\</span>
<span class="sd">        max(0, cos(x_1, x_2)-margin), &amp; \text{if } y = -1\\</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float): Should be in [-1.0, 1.0]. Default 0.0.</span>
<span class="sd">        reduction (str): Specifies which reduction to be applied to the output. It must be one of</span>
<span class="sd">          &quot;none&quot;, &quot;mean&quot;, and &quot;sum&quot;, meaning no reduction, reduce mean and sum on output, respectively. Default &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x1** (Tensor) - Input tensor.</span>
<span class="sd">        - **input_x2** (Tensor) - Its shape and data type must be the same as `input_x1`&#39;s shape and data type.</span>
<span class="sd">        - **y** (Tensor) - Contains value 1 or -1. Suppose the shape of `input_x1` is</span>
<span class="sd">          :math:`(x_1, x_2, x_3,..., x_R)`, then the shape of `target` must be :math:`(x_1, x_3, x_4, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - If `reduction` is &quot;none&quot;, its shape is the same as `y`&#39;s shape, otherwise a scalar value</span>
<span class="sd">          will be returned.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[0.3, 0.8], [0.4, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[0.4, 1.2], [-0.4, -0.9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1,-1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; cosine_embedding_loss = nn.CosineEmbeddingLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = cosine_embedding_loss(x1, x2, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.0003426075</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">F</span><span class="o">.</span><span class="n">same_type_shape</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">_check_reduced_shape_valid</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="c1"># if target &gt; 0, 1-cosine(x1, x2)</span>
        <span class="c1"># else, max(0, cosine(x1, x2)-margin)</span>
        <span class="n">prod_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">square1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">square2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">square1</span> <span class="o">*</span> <span class="n">square2</span><span class="p">)</span>
        <span class="n">cosine</span> <span class="o">=</span> <span class="n">prod_sum</span> <span class="o">/</span> <span class="n">denom</span>

        <span class="n">pos_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">cosine</span>
        <span class="n">neg_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">cosine</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">zeros</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cosine</span><span class="p">)</span>
        <span class="n">pos_part</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pos_value</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
        <span class="n">neg_part</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">neg_value</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
        <span class="n">output_unreduced</span> <span class="o">=</span> <span class="n">pos_part</span> <span class="o">+</span> <span class="n">neg_part</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">output_unreduced</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, MindSpore

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>