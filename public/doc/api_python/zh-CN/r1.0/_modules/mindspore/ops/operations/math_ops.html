

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.operations.math_ops &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.dynamic_lr.html">mindspore.nn.dynamic_lr</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.ops.operations.math_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.ops.operations.math_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for math.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">...common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">...common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">.._utils</span> <span class="kn">import</span> <span class="n">get_broadcast_shape</span>
<span class="kn">from</span> <span class="nn">..primitive</span> <span class="kn">import</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">prim_attr_register</span><span class="p">,</span> <span class="n">_run_op</span>


<span class="k">def</span> <span class="nf">_infer_shape_reduce</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Common infer for reduce operator&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">reduce_one_axis</span><span class="p">(</span><span class="n">one_axis</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">one_axis</span><span class="p">,</span> <span class="o">-</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">one_axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">one_axis</span> <span class="o">+=</span> <span class="n">dim</span>
        <span class="n">axis_reduce</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">one_axis</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">axis_reduce</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">reduce_one_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">axis</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">keep_dims</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim</span>
            <span class="k">return</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_axis</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span> <span class="n">one_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
            <span class="n">reduce_one_axis</span><span class="p">(</span><span class="n">one_axis</span><span class="p">)</span>

    <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axis_reduce</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">keep_dims</span><span class="p">:</span>
                <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">out_shape</span>


<span class="k">class</span> <span class="nc">_BinaryOp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize _BinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">get_broadcast_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_MathBinaryOp</span><span class="p">(</span><span class="n">_BinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define math binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args_type</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args_type</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_BitwiseBinaryOp</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define bitwise binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize _BitwiseBinaryOp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_check_bitwise_op_input_type</span><span class="p">(</span><span class="n">x1_type</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">,</span> <span class="n">prim</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x1&#39;</span><span class="p">:</span> <span class="n">x1_type</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">:</span> <span class="n">x2_type</span><span class="p">}</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint_type</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="n">prim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x1_type</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_type</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_BitwiseBinaryOp</span><span class="o">.</span><span class="n">_check_bitwise_op_input_type</span><span class="p">(</span><span class="n">x1_type</span><span class="p">,</span> <span class="n">x2_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="TensorAdd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.TensorAdd">[docs]</a><span class="k">class</span> <span class="nc">TensorAdd</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds two input tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number,  or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; add = P.TensorAdd()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1,2,3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4,5,6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; add(input_x, input_y)</span>
<span class="sd">        [5,7,9]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="AssignAdd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.AssignAdd">[docs]</a><span class="k">class</span> <span class="nc">AssignAdd</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates a `Parameter` by adding a value to it.</span>

<span class="sd">    Inputs of `variable` and `value` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    If `value` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - The `Parameter`.</span>
<span class="sd">        - **value** (Union[numbers.Number, Tensor]) - The value to be added to the `variable`.</span>
<span class="sd">          It must have the same shape as `variable` if it is a Tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.AssignAdd = P.AssignAdd()</span>
<span class="sd">        &gt;&gt;&gt;         self.variable = mindspore.Parameter(initializer(1, [1], mindspore.int64), name=&quot;global_step&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">        &gt;&gt;&gt;         self.AssignAdd(self.variable, x)</span>
<span class="sd">        &gt;&gt;&gt;         return self.variable</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones([1]).astype(np.int64)*100)</span>
<span class="sd">        &gt;&gt;&gt; net(value)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize AssignAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;ref&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;variable&quot;</span><span class="p">:</span> <span class="n">variable</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="AssignSub"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.AssignSub">[docs]</a><span class="k">class</span> <span class="nc">AssignSub</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates a `Parameter` by subtracting a value from it.</span>

<span class="sd">    Inputs of `variable` and `value` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    If `value` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - The `Parameter`.</span>
<span class="sd">        - **value** (Union[numbers.Number, Tensor]) - The value to be subtracted from the `variable`.</span>
<span class="sd">          It must have the same shape as `variable` if it is a Tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(Net, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.AssignSub = P.AssignSub()</span>
<span class="sd">        &gt;&gt;&gt;         self.variable = mindspore.Parameter(initializer(1, [1], mindspore.int32), name=&quot;global_step&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">        &gt;&gt;&gt;         self.AssignSub(self.variable, x)</span>
<span class="sd">        &gt;&gt;&gt;         return self.variable</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones([1]).astype(np.int32)*100)</span>
<span class="sd">        &gt;&gt;&gt; net(value)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize AssignSub&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;variable&quot;</span><span class="p">:</span> <span class="n">variable</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span></div>


<span class="k">class</span> <span class="nc">_Reduce</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Definition of base class of reduction class operators.</span>

<span class="sd">    Args:</span>
<span class="sd">         keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                           If false, don&#39;t keep these dimensions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">())</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Reduce&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_dims&#39;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;io_format&quot;</span><span class="p">,</span> <span class="s2">&quot;ND&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">do_infer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; return meta infos of input parameters &quot;&quot;&quot;</span>
        <span class="n">axis_v</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">input_shp</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">axis_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{self.name}</span><span class="s2">, axis must be const.&quot;</span><span class="p">)</span>
        <span class="n">input_shp</span> <span class="o">=</span> <span class="n">_infer_shape_reduce</span><span class="p">(</span><span class="n">input_shp</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prim_map</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;ReduceSum&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">,</span>
                <span class="s1">&#39;ReduceMax&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">,</span>
                <span class="s1">&#39;ReduceMin&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="n">np_reduce_func</span> <span class="o">=</span> <span class="n">prim_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">np_reduce_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">axis_v</span><span class="p">:</span>
                    <span class="n">axis_v</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]))]</span>
                    <span class="n">axis_v</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">axis_v</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">np_reduce_func</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">input_shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_infer</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="ReduceMean"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceMean">[docs]</a><span class="k">class</span> <span class="nc">ReduceMean</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Reduce a dimension of a tensor by averaging all elements in the dimension.</span>

<span class="sd">     The dtype of the tensor to be reduced is number.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the mean of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceMean(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ReduceSum"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceSum">[docs]</a><span class="k">class</span> <span class="nc">ReduceSum</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce a dimension of a tensor by summing all elements in the dimension.</span>

<span class="sd">    The dtype of the tensor to be reduced is number.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">         - **input_x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">         - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">           Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the sum of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceSum(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize ReduceSum&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReduceSum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">keep_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="ReduceAll"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceAll">[docs]</a><span class="k">class</span> <span class="nc">ReduceAll</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce a dimension of a tensor by the &quot;logical and&quot; of all elements in the dimension.</span>

<span class="sd">    The dtype of the tensor to be reduced is bool.</span>

<span class="sd">    Args:</span>
<span class="sd">       keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                         If false, don&#39;t keep these dimensions.</span>
<span class="sd">                         Default : False, don&#39;t keep these reduced dimensions.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[bool]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical and&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceAll(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_infer</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,))</span></div>


<div class="viewcode-block" id="ReduceAny"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceAny">[docs]</a><span class="k">class</span> <span class="nc">ReduceAny</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce a dimension of a tensor by the &quot;logical OR&quot; of all elements in the dimension.</span>

<span class="sd">    The dtype of the tensor to be reduced is bool.</span>

<span class="sd">    Args:</span>
<span class="sd">       keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                         If false, don&#39;t keep these dimensions.</span>
<span class="sd">                         Default : False, don&#39;t keep these reduced dimensions.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[bool]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical or&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceAny(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">        [[True],</span>
<span class="sd">         [True]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_infer</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,))</span></div>


<div class="viewcode-block" id="ReduceMax"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceMax">[docs]</a><span class="k">class</span> <span class="nc">ReduceMax</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce a dimension of a tensor by the maximum value in this dimension.</span>

<span class="sd">    The dtype of the tensor to be reduced is number.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions.</span>
<span class="sd">                          Default : False, don&#39;t keep these reduced dimensions.</span>

<span class="sd">    Inputs:</span>
<span class="sd">         - **input_x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">         - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">           Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the maximum of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceMax(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;ReduceMax&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ReduceMax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">keep_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="ReduceMin"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceMin">[docs]</a><span class="k">class</span> <span class="nc">ReduceMin</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce a dimension of a tensor by the minimum value in the dimension.</span>

<span class="sd">    The dtype of the tensor to be reduced is number.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions.</span>
<span class="sd">                          Default : False, don&#39;t keep these reduced dimensions.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the minimum of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceMin(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ReduceProd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ReduceProd">[docs]</a><span class="k">class</span> <span class="nc">ReduceProd</span><span class="p">(</span><span class="n">_Reduce</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduce a dimension of a tensor by multiplying all elements in the dimension.</span>

<span class="sd">    The dtype of the tensor to be reduced is number.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions.</span>
<span class="sd">                          Default : False, don&#39;t keep these reduced dimensions.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = P.ReduceProd(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="CumProd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.CumProd">[docs]</a><span class="k">class</span> <span class="nc">CumProd</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the cumulative product of the tensor x along axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool): If true, perform exclusive cumulative product. Default: False.</span>
<span class="sd">        reverse (bool): If true, reverse the result along axis. Default: False</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (int) - The dimensions to compute the cumulative product.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([a, b, c]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op0 = P.CumProd()</span>
<span class="sd">        &gt;&gt;&gt; output = op0(input_x, 0) # output=[a, a * b, a * b * c]</span>
<span class="sd">        &gt;&gt;&gt; op1 = P.CumProd(exclusive=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op1(input_x, 0) # output=[1, a, a * b]</span>
<span class="sd">        &gt;&gt;&gt; op2 = P.CumProd(reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op2(input_x, 0) # output=[a * b * c, b * c, c]</span>
<span class="sd">        &gt;&gt;&gt; op3 = P.CumProd(exclusive=True, reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op3(input_x, 0) # output=[b * c, c, 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exclusive</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">axis_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">axis_type</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{self.name}</span><span class="s2">, axis must be const.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="MatMul"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.MatMul">[docs]</a><span class="k">class</span> <span class="nc">MatMul</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `a` by matrix `b`.</span>

<span class="sd">    The rank of input tensors must be `2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        transpose_a (bool): If true, `a` is transposed before multiplication. Default: False.</span>
<span class="sd">        transpose_b (bool): If true, `b` is transposed before multiplication. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The first tensor to be multiplied. The shape of the tensor is :math:`(N, C)`. If</span>
<span class="sd">          `transpose_a` is True, its shape must be :math:`(N, C)` after transposing.</span>
<span class="sd">        - **input_y** (Tensor) - The second tensor to be multiplied. The shape of the tensor is :math:`(C, M)`. If</span>
<span class="sd">          `transpose_b` is True, its shape must be :math:`(C, M)` after transpose.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, M)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[1, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.ones(shape=[3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matmul = P.MatMul()</span>
<span class="sd">        &gt;&gt;&gt; output = matmul(input_x, input_y)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">,</span> <span class="n">transpose_a</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_b&quot;</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;io_format&quot;</span><span class="p">,</span> <span class="s2">&quot;ND&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;MatMul input x, y should be the same dimension size and should be &#39;</span>
                             <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;equal to 2, while x size = {len(x)}, y size= {len(y)}&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">check_shape_size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="c1"># expected dimension of x, y, x:[...,a,b] y:[..., c,d], the dim size should be the same except the last two</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{cls_name}</span><span class="se">\&#39;</span><span class="s1"> shape in dim[</span><span class="si">{i}</span><span class="s1">] not the same, while x is </span><span class="si">{x[i]}</span><span class="s1">, y is </span><span class="si">{y[i]}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># validate whether last two dims satifing matrix multiply</span>
        <span class="n">x_last</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">y_last</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

        <span class="n">x_col</span> <span class="o">=</span> <span class="n">x_last</span><span class="p">[</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_a</span><span class="p">]</span>  <span class="c1"># x_col = x_last[1] if (not transpose_a) else x_last[0]</span>
        <span class="n">y_row</span> <span class="o">=</span> <span class="n">y_last</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_b</span><span class="p">]</span>  <span class="c1"># y_row = y_last[0] if (not transpose_b) else y_last[1]</span>
        <span class="k">if</span> <span class="n">x_col</span> <span class="o">!=</span> <span class="n">y_row</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{cls_name}</span><span class="se">\&#39;</span><span class="s1"> evaluator shapes of inputs can not do this operator,&#39;</span>
                             <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39; got </span><span class="si">{x_col}</span><span class="s1"> and </span><span class="si">{y_row}</span><span class="s1">, with x shape </span><span class="si">{x}</span><span class="s1">(transpose_a=</span><span class="si">{self.transpose_a}</span><span class="s1">)&#39;</span>
                             <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;, y shape </span><span class="si">{y}</span><span class="s1">(transpose_b=</span><span class="si">{self.transpose_b}</span><span class="s1">).&#39;</span><span class="p">)</span>
        <span class="c1"># set attribute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;transpose_x1&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_a</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;transpose_x2&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_b</span><span class="p">)</span>

        <span class="n">ret_dims</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">x_last</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">transpose_a</span><span class="p">],</span> <span class="n">y_last</span><span class="p">[</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_b</span><span class="p">]]</span>
        <span class="k">return</span> <span class="n">ret_dims</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="BatchMatMul"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BatchMatMul">[docs]</a><span class="k">class</span> <span class="nc">BatchMatMul</span><span class="p">(</span><span class="n">MatMul</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes matrix multiplication between two tensors by batch</span>

<span class="sd">    `result[..., :, :] = tensor(a[..., :, :]) * tensor(b[..., :, :])`.</span>

<span class="sd">    The two input tensors must have the same rank and the rank must be not less than `3`.</span>

<span class="sd">    Args:</span>
<span class="sd">        transpose_a (bool): If true, the last two dimensions of `a` is transposed before multiplication.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        transpose_b (bool): If true, the last two dimensions of `b` is transposed before multiplication.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The first tensor to be multiplied. The shape of the tensor is :math:`(*B, N, C)`,</span>
<span class="sd">          where :math:`*B` represents the batch size which can be multidimensional, :math:`N` and :math:`C` are the</span>
<span class="sd">          size of the last two dimensions. If `transpose_a` is True, its shape must be :math:`(*B, C, N)`.</span>
<span class="sd">        - **input_y** (Tensor) - The second tensor to be multiplied. The shape of the tensor is :math:`(*B, C, M)`. If</span>
<span class="sd">          `transpose_b` is True, its shape must be :math:`(*B, M, C)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(*B, N, M)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[2, 4, 1, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.ones(shape=[2, 4, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batmatmul = P.BatchMatMul()</span>
<span class="sd">        &gt;&gt;&gt; output = batmatmul(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[2, 4, 3, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.ones(shape=[2, 4, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batmatmul = P.BatchMatMul(transpose_a=True)</span>
<span class="sd">        &gt;&gt;&gt; output = batmatmul(input_x, input_y)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_a&quot;</span><span class="p">,</span> <span class="n">transpose_a</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;transpose_b&quot;</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="s1">BatchMatMul</span><span class="se">\&#39;</span><span class="s1"> input x, y should be the same dimension size and should be &#39;</span>
                             <span class="s1">&#39;greater or equal to 3,&#39;</span> <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39; while x size = {len(x)}, y size= {len(y)}&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="CumSum"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.CumSum">[docs]</a><span class="k">class</span> <span class="nc">CumSum</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum of input tensor along axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool): If true, perform exclusive mode. Default: False.</span>
<span class="sd">        reverse (bool): If true, perform inverse cumulative sum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor to accumulate.</span>
<span class="sd">        - **axis**  (int) - The axis to accumulate the tensor&#39;s value. Only constant value is allowed.</span>
<span class="sd">          Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is consistent with the input tensor&#39;s.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[3, 4, 6, 10],[1, 6, 7, 9],[4, 3, 8, 7],[1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; cumsum = P.CumSum()</span>
<span class="sd">        &gt;&gt;&gt; output = cumsum(input, 1)</span>
<span class="sd">        [[ 3.  7. 13. 23.]</span>
<span class="sd">         [ 1.  7. 14. 23.]</span>
<span class="sd">         [ 4.  7. 15. 22.]</span>
<span class="sd">         [ 1.  4. 11. 20.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize cumsum&quot;&quot;&quot;</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;exclusive&#39;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;reverse&#39;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">x_shp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{self.name}</span><span class="s2">, axis must be const.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="AddN"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.AddN">[docs]</a><span class="k">class</span> <span class="nc">AddN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes addition of all input tensors element-wise.</span>

<span class="sd">    All input tensors must have the same shape.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union(tuple[Tensor], list[Tensor])) - The input tuple or list</span>
<span class="sd">          is made up of multiple tensors whose dtype is number or bool to be added together.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as each entry of the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class NetAddN(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(NetAddN, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.addN = P.AddN()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, *z):</span>
<span class="sd">        &gt;&gt;&gt;         return self.addN(z)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = NetAddN()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net(input_x, input_y, input_x, input_y)</span>
<span class="sd">        [10.0, 14.0, 18.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_elim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expecting Tensor, got : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="n">shp0</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">shp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shape of inputs[</span><span class="si">{i}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">shp</span><span class="p">,</span> <span class="s1">&#39;shape of inputs[0]&#39;</span><span class="p">,</span> <span class="n">shp0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">shp0</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">contains_undetermined</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">args</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;inputs[</span><span class="si">{i}</span><span class="s2">]&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtype</span>
            <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">undetermined</span><span class="p">:</span>
                <span class="n">contains_undetermined</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">contains_undetermined</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">inputs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">None</span>

        <span class="n">added</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">added</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">added</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span></div>


<div class="viewcode-block" id="AccumulateNV2"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.AccumulateNV2">[docs]</a><span class="k">class</span> <span class="nc">AccumulateNV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes accumulation of all input tensors element-wise.</span>

<span class="sd">    AccumulateNV2 is similar to AddN, but there is a significant difference</span>
<span class="sd">    among them: AccumulateNV2 will not wait for all of its inputs to be ready</span>
<span class="sd">    before summing. That is to say, AccumulateNV2 is able to save</span>
<span class="sd">    memory when inputs are ready at different time since the minimum temporary</span>
<span class="sd">    storage is proportional to the output size rather than the input size.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union(tuple[Tensor], list[Tensor])) - The input tuple or list</span>
<span class="sd">          is made up of multiple tensors whose dtype is number to be added together.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as each entry of the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class NetAccumulateNV2(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(NetAccumulateNV2, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.accumulateNV2 = P.AccumulateNV2()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, *z):</span>
<span class="sd">        &gt;&gt;&gt;         return self.accumulateNV2(z)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = NetAccumulateNV2()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net(input_x, input_y, input_x, input_y)</span>
<span class="sd">        Tensor([10., 14., 18.], shape=(3,), dtype=mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="n">shp0</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">shp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;shape of inputs[</span><span class="si">{i}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">shp</span><span class="p">,</span> <span class="s1">&#39;shape of inputs[0]&#39;</span><span class="p">,</span> <span class="n">shp0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">shp0</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
            <span class="n">args</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;inputs[</span><span class="si">{i}</span><span class="s2">]&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dtype</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="Neg"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Neg">[docs]</a><span class="k">class</span> <span class="nc">Neg</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with negative values of the input tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; neg = P.Neg()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, -1, 2, 0, -3.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = neg(input_x)</span>
<span class="sd">        [-1.  -2.   1.  -2.   0.   3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Neg&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">input_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">-</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="InplaceAdd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.InplaceAdd">[docs]</a><span class="k">class</span> <span class="nc">InplaceAdd</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds v into specified rows of x. Computes y = x; y[i,] += v.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of x, and determines which rows of x</span>
<span class="sd">            to add with v. It is an integer or a tuple, whose value is in [0, the first dimension size of x).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The first input is a tensor whose data type is float16, float32 or int32.</span>
<span class="sd">        - **input_v** (Tensor) - The second input is a tensor that has the same dimension sizes as x except</span>
<span class="sd">          the first dimension, which must be the same as indices&#39;s size. It has the same data type with `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as input_x.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; inplaceAdd = P.InplaceAdd(indices)</span>
<span class="sd">        &gt;&gt;&gt; inplaceAdd(input_x, input_v)</span>
<span class="sd">        [[1.5 3.]</span>
<span class="sd">         [4. 5.5]</span>
<span class="sd">         [5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize InplaceAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of indices&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">}</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;size of indices&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">),</span> <span class="s2">&quot;v&#39;s first dimension&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The value of indices must be in [0, </span><span class="si">{x_shape[0]}</span><span class="s1">), but got </span><span class="si">{i}</span><span class="s1">.&#39;</span><span class="p">)</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_rank</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;v dim </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="s2">&quot;x dim </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_shape</span></div>


<div class="viewcode-block" id="InplaceSub"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.InplaceSub">[docs]</a><span class="k">class</span> <span class="nc">InplaceSub</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts v into specified rows of x. Computes y = x; y[i, :] -= v.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of x, and determines which rows of x</span>
<span class="sd">            to subtract with v. It is a int or tuple, whose value is in [0, the first dimension size of x).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The first input is a tensor whose data type is float16, float32 or int32.</span>
<span class="sd">        - **input_v** (Tensor) - The second input is a tensor who has the same dimension sizes as x except</span>
<span class="sd">          the first dimension, which must be the same as indices&#39;s size. It has the same data type with `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as input_x.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; inplaceSub = P.InplaceSub(indices)</span>
<span class="sd">        &gt;&gt;&gt; inplaceSub(input_x, input_v)</span>
<span class="sd">        [[0.5 1.]</span>
<span class="sd">         [2. 2.5]</span>
<span class="sd">         [5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize InplaceSub&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span><span class="p">,)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of indices&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">}</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">v_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;size of indices&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">),</span> <span class="s2">&quot;v&#39;s first dimension&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                        <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The value of indices must be in [0, </span><span class="si">{x_shape[0]}</span><span class="s1">), but got </span><span class="si">{i}</span><span class="s1">.&#39;</span><span class="p">)</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_rank</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;v dim </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="s2">&quot;x dim </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_shape</span></div>


<div class="viewcode-block" id="Sub"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Sub">[docs]</a><span class="k">class</span> <span class="nc">Sub</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts the second input tensor from the first input tensor element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4, 5, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sub = P.Sub()</span>
<span class="sd">        &gt;&gt;&gt; sub(input_x, input_y)</span>
<span class="sd">        [-3, -3, -3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Mul"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Mul">[docs]</a><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mul = P.Mul()</span>
<span class="sd">        &gt;&gt;&gt; mul(input_x, input_y)</span>
<span class="sd">        [4, 10, 18]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="SquaredDifference"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SquaredDifference">[docs]</a><span class="k">class</span> <span class="nc">SquaredDifference</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts the second input tensor from the first input tensor element-wise and returns square of it.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is float16, float32, int32 or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor or a tensor whose data type isfloat16, float32, int32 or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([2.0, 4.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; squared_difference = P.SquaredDifference()</span>
<span class="sd">        &gt;&gt;&gt; squared_difference(input_x, input_y)</span>
<span class="sd">        [1.0, 4.0, 9.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Square"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Square">[docs]</a><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; square = P.Square()</span>
<span class="sd">        &gt;&gt;&gt; square(input_x)</span>
<span class="sd">        [1.0, 4.0, 9.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Square&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Rsqrt"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Rsqrt">[docs]</a><span class="k">class</span> <span class="nc">Rsqrt</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes reciprocal of square root of input tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Rsqrt. Each element must be a non-negative number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor([[4, 4], [9, 9]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rsqrt = P.Rsqrt()</span>
<span class="sd">        &gt;&gt;&gt; rsqrt(input_tensor)</span>
<span class="sd">        [[0.5, 0.5], [0.333333, 0.333333]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Rsqrt&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Sqrt"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Sqrt">[docs]</a><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square root of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sqrt = P.Sqrt()</span>
<span class="sd">        &gt;&gt;&gt; sqrt(input_x)</span>
<span class="sd">        [1.0, 2.0, 3.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Sqrt&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Reciprocal"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Reciprocal">[docs]</a><span class="k">class</span> <span class="nc">Reciprocal</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns reciprocal of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; reciprocal = P.Reciprocal()</span>
<span class="sd">        &gt;&gt;&gt; reciprocal(input_x)</span>
<span class="sd">        [1.0, 0.5, 0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Reciprocal&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;GPU&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;OTHER&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">x</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Pow"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Pow">[docs]</a><span class="k">class</span> <span class="nc">Pow</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a tensor to the power of the second input.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = 3.0</span>
<span class="sd">        &gt;&gt;&gt; pow = P.Pow()</span>
<span class="sd">        &gt;&gt;&gt; pow(input_x, input_y)</span>
<span class="sd">        [1.0, 8.0, 64.0]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pow = P.Pow()</span>
<span class="sd">        &gt;&gt;&gt; pow(input_x, input_y)</span>
<span class="sd">        [1.0, 16.0, 64.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">power</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">power</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">power</span> <span class="o">=</span> <span class="n">power</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">power</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Exp"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Exp">[docs]</a><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The data type mast be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; exp = P.Exp()</span>
<span class="sd">        &gt;&gt;&gt; exp(input_x)</span>
<span class="sd">        [ 2.71828183,  7.3890561 , 54.59815003]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Exp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Expm1"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Expm1">[docs]</a><span class="k">class</span> <span class="nc">Expm1</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential then minus 1 of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. With float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.0, 1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; expm1 = P.Expm1()</span>
<span class="sd">        &gt;&gt;&gt; expm1(input_x)</span>
<span class="sd">        [ 0.,  1.71828183,  6.3890561 , 53.59815003]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Exp&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="HistogramFixedWidth"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.HistogramFixedWidth">[docs]</a><span class="k">class</span> <span class="nc">HistogramFixedWidth</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a rank 1 histogram counting the number of entries in values that fall into every bin. The bins are equal</span>
<span class="sd">    width and determined by the arguments range and nbins.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype (str): An optional attribute. Must be one of the following types: &quot;int32&quot;, &quot;int64&quot;. Default: &quot;int32&quot;.</span>
<span class="sd">        nbins (int): The number of histogram bins, the type is a positive integer.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Numeric Tensor. Must be one of the following types: int32, float32, float16.</span>
<span class="sd">        - **range** (Tensor) - Must has the same data type as `x`, and the shape is [2].</span>
<span class="sd">          x &lt;= range[0] will be mapped to hist[0], x &gt;= range[1] will be mapped to hist[-1].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the type is int32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([-1.0, 0.0, 1.5, 2.0, 5.0, 15], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; range = Tensor([0.0, 5.0], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; hist = P.HistogramFixedWidth(5)</span>
<span class="sd">        &gt;&gt;&gt; hist(x, range)</span>
<span class="sd">        [2 1 1 0 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;int32&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nbins</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nbins&quot;</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;nbins&quot;</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;int32&#39;</span><span class="p">,</span> <span class="s1">&#39;int64&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;range&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">range_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nbins</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">range_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;range&quot;</span><span class="p">:</span> <span class="n">range_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">y_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">return</span> <span class="n">y_dtype</span></div>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Log">[docs]</a><span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. With float16 or float32 data type. The value must be greater than 0.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log = P.Log()</span>
<span class="sd">        &gt;&gt;&gt; log(input_x)</span>
<span class="sd">        [0.0, 0.69314718, 1.38629436]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Log1p"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Log1p">[docs]</a><span class="k">class</span> <span class="nc">Log1p</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. With float16 or float32 data type. The value must be greater than -1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log1p = P.Log1p()</span>
<span class="sd">        &gt;&gt;&gt; log1p(input_x)</span>
<span class="sd">        [0.6931472, 1.0986123, 1.609438]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="Erf"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Erf">[docs]</a><span class="k">class</span> <span class="nc">Erf</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Gauss error function of `input_x` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; erf = P.Erf()</span>
<span class="sd">        &gt;&gt;&gt; erf(input_x)</span>
<span class="sd">        [-0.8427168, 0., 0.8427168, 0.99530876, 0.99997765]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Erf&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="Erfc"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Erfc">[docs]</a><span class="k">class</span> <span class="nc">Erfc</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the complementary error function of `input_x` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; erfc = P.Erfc()</span>
<span class="sd">        &gt;&gt;&gt; erfc(input_x)</span>
<span class="sd">        [1.8427168, 0., 0.1572832, 0.00469124, 0.00002235]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Erfc&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="Minimum"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Minimum">[docs]</a><span class="k">class</span> <span class="nc">Minimum</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; minimum = P.Minimum()</span>
<span class="sd">        &gt;&gt;&gt; minimum(input_x, input_y)</span>
<span class="sd">        [1.0, 2.0, 3.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Maximum"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Maximum">[docs]</a><span class="k">class</span> <span class="nc">Maximum</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maximum = P.Maximum()</span>
<span class="sd">        &gt;&gt;&gt; maximum(input_x, input_y)</span>
<span class="sd">        [4.0, 5.0, 6.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="RealDiv"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.RealDiv">[docs]</a><span class="k">class</span> <span class="nc">RealDiv</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the first input tensor by the second input tensor in floating-point type element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; realdiv = P.RealDiv()</span>
<span class="sd">        &gt;&gt;&gt; realdiv(input_x, input_y)</span>
<span class="sd">        [0.25, 0.4, 0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">y</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Div"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Div">[docs]</a><span class="k">class</span> <span class="nc">Div</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the quotient of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - When the first input is a tensor, The second input</span>
<span class="sd">          could be a number, a bool, or a tensor whose data type is number or bool. When the first input</span>
<span class="sd">          is a number or a bool, the second input must be a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([3.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; div = P.Div()</span>
<span class="sd">        &gt;&gt;&gt; div(input_x, input_y)</span>
<span class="sd">        [-1.3, 2.5, 2.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="DivNoNan"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.DivNoNan">[docs]</a><span class="k">class</span> <span class="nc">DivNoNan</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a safe divide which returns 0 if the y is zero.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1.0, 0., 1.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([0., 0., 0., 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; div_no_nan = P.DivNoNan()</span>
<span class="sd">        &gt;&gt;&gt; div_no_nan(input_x, input_y)</span>
<span class="sd">        [0., 0., 0., 2.5, 2.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">errstate</span><span class="p">(</span><span class="n">divide</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">invalid</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">):</span>
                <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
                <span class="n">out</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">out</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">out</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="FloorDiv"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FloorDiv">[docs]</a><span class="k">class</span> <span class="nc">FloorDiv</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the first input tensor by the second input tensor element-wise and round down to the closest integer.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; floor_div = P.FloorDiv()</span>
<span class="sd">        &gt;&gt;&gt; floor_div(input_x, input_y)</span>
<span class="sd">        [0, 1, -1]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="TruncateDiv"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.TruncateDiv">[docs]</a><span class="k">class</span> <span class="nc">TruncateDiv</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the first input tensor by the second input tensor element-wise for integer types, negative numbers will</span>
<span class="sd">    round fractional quantities towards zero.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; truncate_div = P.TruncateDiv()</span>
<span class="sd">        &gt;&gt;&gt; truncate_div(input_x, input_y)</span>
<span class="sd">        [0, 1, 0]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="TruncateMod"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.TruncateMod">[docs]</a><span class="k">class</span> <span class="nc">TruncateMod</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns element-wise remainder of division.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input</span>
<span class="sd">          is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; truncate_mod = P.TruncateMod()</span>
<span class="sd">        &gt;&gt;&gt; truncate_mod(input_x, input_y)</span>
<span class="sd">        [2, 1, -1]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Mod"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Mod">[docs]</a><span class="k">class</span> <span class="nc">Mod</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors,</span>
<span class="sd">    both dtypes cannot be bool, and the shapes of them could be broadcast. When the inputs are one tensor</span>
<span class="sd">    and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number]) - The first input is a number or a tensor whose data type is number.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number]) - When the first input is a tensor, The second input</span>
<span class="sd">          could be a number or a tensor whose data type is number. When the first input is a number,</span>
<span class="sd">          the second input must be a tensor whose data type is number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: When `input_x` and `input_y` are not the same dtype.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([3.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mod = P.Mod()</span>
<span class="sd">        &gt;&gt;&gt; mod(input_x, input_y)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fmod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Floor"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Floor">[docs]</a><span class="k">class</span> <span class="nc">Floor</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Round a tensor down to the closest integer element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. Its element data type must be float.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; floor = P.Floor()</span>
<span class="sd">        &gt;&gt;&gt; floor(input_x)</span>
<span class="sd">        [1.0, 2.0, -2.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="FloorMod"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FloorMod">[docs]</a><span class="k">class</span> <span class="nc">FloorMod</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the remainder of division element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool , and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; floor_mod = P.FloorMod()</span>
<span class="sd">        &gt;&gt;&gt; floor_mod(input_x, input_y)</span>
<span class="sd">        [2, 1, 2]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Ceil"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Ceil">[docs]</a><span class="k">class</span> <span class="nc">Ceil</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Round a tensor up to the closest integer element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. It&#39;s element data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; ceil_op = P.Ceil()</span>
<span class="sd">        &gt;&gt;&gt; ceil_op(input_x)</span>
<span class="sd">        [2.0, 3.0, -1.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Xdivy"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Xdivy">[docs]</a><span class="k">class</span> <span class="nc">Xdivy</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the first input tensor by the second input tensor element-wise. Returns zero when `x` is zero.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number, or a bool,</span>
<span class="sd">          or a tensor whose data type is float16, float32 or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number,</span>
<span class="sd">          or a bool when the first input is a tensor, or a tensor whose data type is float16, float32 or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2, 4, -1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; xdivy = P.Xdivy()</span>
<span class="sd">        &gt;&gt;&gt; xdivy(input_x, input_y)</span>
<span class="sd">        [1.0, 2.0, -0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Xlogy"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Xlogy">[docs]</a><span class="k">class</span> <span class="nc">Xlogy</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes first input tensor multiplied by the logarithm of second input tensor element-wise.</span>
<span class="sd">    Returns zero when `x` is zero.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is float16, float32 or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is float16, float32 or bool.</span>
<span class="sd">          The value must be positive.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-5, 0, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; xlogy = P.Xlogy()</span>
<span class="sd">        &gt;&gt;&gt; xlogy(input_x, input_y)</span>
<span class="sd">        [-3.465736, 0.0, 2.7725887]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_MathBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Acosh"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Acosh">[docs]</a><span class="k">class</span> <span class="nc">Acosh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute inverse hyperbolic cosine of the input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; acosh = P.Acosh()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = acosh(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Acosh&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Cosh"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Cosh">[docs]</a><span class="k">class</span> <span class="nc">Cosh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic cosine of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; cosh = P.Cosh()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = cosh(input_x)</span>
<span class="sd">        [1.0289385 1.364684 1.048436 1.4228927]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Cosh&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Asinh"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Asinh">[docs]</a><span class="k">class</span> <span class="nc">Asinh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; asinh = P.Asinh()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = asinh(input_x)</span>
<span class="sd">        [-2.3212, 1.1976, 1.8184, 5.2983]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Asinh&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Sinh"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Sinh">[docs]</a><span class="k">class</span> <span class="nc">Sinh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic sine of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; sinh = P.Sinh()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sinh(input_x)</span>
<span class="sd">        [0.6604918 0.28367308 0.44337422 0.6604918]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Sinh&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<span class="k">class</span> <span class="nc">_LogicBinaryOp</span><span class="p">(</span><span class="n">_BinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define logic binary operators.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">valid_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args_dtype</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args_dtype</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LogicBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Equal"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Equal">[docs]</a><span class="k">class</span> <span class="nc">Equal</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the equivalence between two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number]) - The first input is a number or</span>
<span class="sd">          a tensor whose data type is number.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number]) - The second input is a number</span>
<span class="sd">          when the first input is a tensor or a tensor whose data type is number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equal = P.Equal()</span>
<span class="sd">        &gt;&gt;&gt; equal(input_x, 2.0)</span>
<span class="sd">        [False, True, False]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; equal = P.Equal()</span>
<span class="sd">        &gt;&gt;&gt; equal(input_x, input_y)</span>
<span class="sd">        [True, True, False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LogicBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApproximateEqual"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ApproximateEqual">[docs]</a><span class="k">class</span> <span class="nc">ApproximateEqual</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns true if abs(x1-x2) is smaller than tolerance element-wise, otherwise false.</span>

<span class="sd">    Inputs of `x1` and `x2` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        tolerance (float): The maximum deviation that two elements can be considered equal. Default: 1e-05.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - A tensor. Must be one of the following types: float32, float16.</span>
<span class="sd">        - **x2** (Tensor) - A tensor of the same type and shape as &#39;x1&#39;.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the shape of &#39;x1&#39;, and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([2, 4, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; approximate_equal = P.ApproximateEqual(2.)</span>
<span class="sd">        &gt;&gt;&gt; result = approximate_equal(x1, x2)</span>
<span class="sd">        [True  True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize ApproximateEqual&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;tolerance&quot;</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;y_shape&quot;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="n">args_dtype</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args_dtype</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span></div>


<div class="viewcode-block" id="EqualCount"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.EqualCount">[docs]</a><span class="k">class</span> <span class="nc">EqualCount</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the number of the same elements of two tensors.</span>

<span class="sd">    The two input tensors must have the same data type and shape.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The first input tensor.</span>
<span class="sd">        - **input_y** (Tensor) - The second input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the type same as input tensor and size as (1,).</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; equal_count = P.EqualCount()</span>
<span class="sd">        &gt;&gt;&gt; equal_count(input_x, input_y)</span>
<span class="sd">        [2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize EqualCount&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;y_shape&quot;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
        <span class="k">return</span> <span class="n">output_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="NotEqual"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.NotEqual">[docs]</a><span class="k">class</span> <span class="nc">NotEqual</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the non-equivalence of two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; not_equal = P.NotEqual()</span>
<span class="sd">        &gt;&gt;&gt; not_equal(input_x, 2.0)</span>
<span class="sd">        [True, False, True]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; not_equal = P.NotEqual()</span>
<span class="sd">        &gt;&gt;&gt; not_equal(input_x, input_y)</span>
<span class="sd">        [False, False, True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LogicBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Greater"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Greater">[docs]</a><span class="k">class</span> <span class="nc">Greater</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &gt; y` element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater = P.Greater()</span>
<span class="sd">        &gt;&gt;&gt; greater(input_x, input_y)</span>
<span class="sd">        [False, True, False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">greater</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="GreaterEqual"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.GreaterEqual">[docs]</a><span class="k">class</span> <span class="nc">GreaterEqual</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &gt;= y` element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater_equal = P.GreaterEqual()</span>
<span class="sd">        &gt;&gt;&gt; greater_equal(input_x, input_y)</span>
<span class="sd">        [True, True, False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Less"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Less">[docs]</a><span class="k">class</span> <span class="nc">Less</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt; y` element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; less = P.Less()</span>
<span class="sd">        &gt;&gt;&gt; less(input_x, input_y)</span>
<span class="sd">        [False, False, True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="LessEqual"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LessEqual">[docs]</a><span class="k">class</span> <span class="nc">LessEqual</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt;= y` element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool , and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; less_equal = P.LessEqual()</span>
<span class="sd">        &gt;&gt;&gt; less_equal(input_x, input_y)</span>
<span class="sd">        [True, False, True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">y</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="LogicalNot"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LogicalNot">[docs]</a><span class="k">class</span> <span class="nc">LogicalNot</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical NOT&quot; of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `input_x`, and the dtype is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_not = P.LogicalNot()</span>
<span class="sd">        &gt;&gt;&gt; logical_not(input_x)</span>
<span class="sd">        [False, True, False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize LogicalNot&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogicalAnd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LogicalAnd">[docs]</a><span class="k">class</span> <span class="nc">LogicalAnd</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical AND&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type is bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type is bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_and = P.LogicalAnd()</span>
<span class="sd">        &gt;&gt;&gt; logical_and(input_x, input_y)</span>
<span class="sd">        [True, False, False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LogicBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogicalOr"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.LogicalOr">[docs]</a><span class="k">class</span> <span class="nc">LogicalOr</span><span class="p">(</span><span class="n">_LogicBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical OR&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type is bool.</span>
<span class="sd">        - **input_y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type is bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_or = P.LogicalOr()</span>
<span class="sd">        &gt;&gt;&gt; logical_or(input_x, input_y)</span>
<span class="sd">        [True, True, True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_LogicBinaryOp</span><span class="o">.</span><span class="n">do_infer_dtype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="IsNan"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.IsNan">[docs]</a><span class="k">class</span> <span class="nc">IsNan</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Judge which elements are nan for each position.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; is_nan = P.IsNan()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = is_nan(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize IsNan&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span></div>


<div class="viewcode-block" id="IsInf"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.IsInf">[docs]</a><span class="k">class</span> <span class="nc">IsInf</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Judging which elements are inf or -inf for each position</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; is_inf = P.IsInf()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = is_inf(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize IsInf&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span></div>


<div class="viewcode-block" id="IsFinite"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.IsFinite">[docs]</a><span class="k">class</span> <span class="nc">IsFinite</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Judge which elements are finite for each position.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; is_finite = P.IsFinite()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = is_finite(input_x)</span>
<span class="sd">        [False   True   False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize IsFinite&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span></div>


<div class="viewcode-block" id="FloatStatus"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.FloatStatus">[docs]</a><span class="k">class</span> <span class="nc">FloatStatus</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine if the elements contain Not a Number(NaN), infinite or negative infinite. 0 for normal, 1 for overflow.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the shape of `(1,)`, and has the same dtype of input `mindspore.dtype.float32` or</span>
<span class="sd">        `mindspore.dtype.float16`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; float_status = P.FloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = float_status(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize FloatStatus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="NPUAllocFloatStatus"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.NPUAllocFloatStatus">[docs]</a><span class="k">class</span> <span class="nc">NPUAllocFloatStatus</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Allocates a flag to store the overflow status.</span>

<span class="sd">    The flag is a tensor whose shape is `(8,)` and data type is `mindspore.dtype.float32`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Examples: see `NPUGetFloatStatus`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the shape of `(8,)`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; alloc_status = P.NPUAllocFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; init = alloc_status()</span>
<span class="sd">        Tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], shape=(8,), dtype=mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize NPUAllocFloatStatus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;_side_effect_flag&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span></div>


<div class="viewcode-block" id="NPUGetFloatStatus"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.NPUGetFloatStatus">[docs]</a><span class="k">class</span> <span class="nc">NPUGetFloatStatus</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the flag which is the output tensor of `NPUAllocFloatStatus` with latest overflow status.</span>

<span class="sd">    The flag is a tensor whose shape is `(8,)` and data type is `mindspore.dtype.float32`.</span>
<span class="sd">    If the sum of the flag equals to 0, there is no overflow happened. If the sum of the flag is bigger than 0, there</span>
<span class="sd">    is overflow happened.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The output tensor of `NPUAllocFloatStatus`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`. All the elements in the tensor will be zero.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; alloc_status = P.NPUAllocFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; get_status = P.NPUGetFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; init = alloc_status()</span>
<span class="sd">        &gt;&gt;&gt; flag = get_status(init)</span>
<span class="sd">        Tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], shape=(8,), dtype=mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize NPUGetFloatStatus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;_side_effect_flag&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;len(x_shape)&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">8</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span></div>


<div class="viewcode-block" id="NPUClearFloatStatus"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.NPUClearFloatStatus">[docs]</a><span class="k">class</span> <span class="nc">NPUClearFloatStatus</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Clear the flag which stores the overflow status.</span>

<span class="sd">    Note:</span>
<span class="sd">        The flag is in the register on the `Ascend` device. It will be reset and can not be reused again after the</span>
<span class="sd">        `NPUClearFloatStatus` is called.</span>

<span class="sd">        Examples: see `NPUGetFloatStatus`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The output tensor of `NPUAllocFloatStatus`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`. All the elements in the tensor will be zero.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; alloc_status = P.NPUAllocFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; get_status = P.NPUGetFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; clear_status = P.NPUClearFloatStatus()</span>
<span class="sd">        &gt;&gt;&gt; init = alloc_status()</span>
<span class="sd">        &gt;&gt;&gt; flag = get_status(init)</span>
<span class="sd">        &gt;&gt;&gt; clear = clear_status(init)</span>
<span class="sd">        Tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], shape=(8,), dtype=mindspore.float32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize NPUClearFloatStatus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;_side_effect_flag&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;len(x_shape)&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;x_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">8</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">8</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span></div>


<div class="viewcode-block" id="Cos"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Cos">[docs]</a><span class="k">class</span> <span class="nc">Cos</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cosine of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; cos = P.Cos()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = cos(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Cos&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="ACos"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.ACos">[docs]</a><span class="k">class</span> <span class="nc">ACos</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arccosine of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; acos = P.ACos()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = acos(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize ACos&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Sin"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Sin">[docs]</a><span class="k">class</span> <span class="nc">Sin</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sine of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; sin = P.Sin()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sin(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Sin.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Asin"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Asin">[docs]</a><span class="k">class</span> <span class="nc">Asin</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arcsine of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; asin = P.Asin()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = asin(input_x)</span>
<span class="sd">        [0.8331, 0.0400, 0.3047, 0.5944]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Asin&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="NMSWithMask"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.NMSWithMask">[docs]</a><span class="k">class</span> <span class="nc">NMSWithMask</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Select some bounding boxes in descending order of score.</span>

<span class="sd">    Args:</span>
<span class="sd">        iou_threshold (float): Specifies the threshold of overlap boxes with respect to</span>
<span class="sd">            IOU. Default: 0.5.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the iou_threshold is not a float number, or if the first dimension</span>
<span class="sd">            of input Tensor is less than or equal to 0, or if the data type of the input</span>
<span class="sd">            Tensor is not float16 or float32.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **bboxes** (Tensor) - The shape of tensor is :math:`(N, 5)`. Input bounding boxes.</span>
<span class="sd">          `N` is the number of input bounding boxes. Every bounding box</span>
<span class="sd">          contains 5 values, the first 4 values are the coordinates of bounding</span>
<span class="sd">          box, and the last value is the score of this bounding box.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of three tensors, they are selected_boxes, selected_idx and selected_mask.</span>

<span class="sd">        - **selected_boxes** (Tensor) - The shape of tensor is :math:`(N, 5)`. The list of bounding boxes</span>
<span class="sd">          after non-max suppression calculation.</span>
<span class="sd">        - **selected_idx** (Tensor) - The shape of tensor is :math:`(N,)`. The indexes list of</span>
<span class="sd">          valid input bounding boxes.</span>
<span class="sd">        - **selected_mask** (Tensor) - The shape of tensor is :math:`(N,)`. A mask list of</span>
<span class="sd">          valid output bounding boxes.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; bbox = np.random.rand(128, 5)</span>
<span class="sd">        &gt;&gt;&gt; bbox[:, 2] += bbox[:, 0]</span>
<span class="sd">        &gt;&gt;&gt; bbox[:, 3] += bbox[:, 1]</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(bbox, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; nms = P.NMSWithMask(0.5)</span>
<span class="sd">        &gt;&gt;&gt; output_boxes, indices, mask = nms(inputs)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize NMSWithMask&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;iou_threshold&quot;</span><span class="p">,</span> <span class="n">iou_threshold</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bboxes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;selected_boxes&#39;</span><span class="p">,</span> <span class="s1">&#39;selected_idx&#39;</span><span class="p">,</span> <span class="s1">&#39;selected_mask&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_ge</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;enable_ge&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bboxes_shape</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;bboxes rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bboxes_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;bboxes.shape[0]&quot;</span><span class="p">,</span> <span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;bboxes.shape[1]&quot;</span><span class="p">,</span> <span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">5</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
        <span class="n">num</span> <span class="o">=</span> <span class="n">bboxes_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">bboxes_shape</span><span class="p">,</span> <span class="p">(</span><span class="n">num</span><span class="p">,),</span> <span class="p">(</span><span class="n">num</span><span class="p">,))</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bboxes_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;bboxes&quot;</span><span class="p">:</span> <span class="n">bboxes_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">bboxes_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span></div>


<div class="viewcode-block" id="Abs"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Abs">[docs]</a><span class="k">class</span> <span class="nc">Abs</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns absolute value of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x = Tensor(np.array([-1.0, 1.0, 0.0]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; abs = P.Abs()</span>
<span class="sd">         &gt;&gt;&gt; abs(input_x)</span>
<span class="sd">         [1.0, 1.0, 0.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Abs&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Sign"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Sign">[docs]</a><span class="k">class</span> <span class="nc">Sign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform :math:`sign` on tensor element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        .. math::</span>
<span class="sd">            sign(x) = \begin{cases} -1, &amp;if\ x &lt; 0 \cr</span>
<span class="sd">            0, &amp;if\ x = 0 \cr</span>
<span class="sd">            1, &amp;if\ x &gt; 0\end{cases}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x = Tensor(np.array([[2.0, 0.0, -1.0]]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; sign = P.Sign()</span>
<span class="sd">         &gt;&gt;&gt; output = sign(input_x)</span>
<span class="sd">         [[1.0, 0.0, -1.0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Round"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Round">[docs]</a><span class="k">class</span> <span class="nc">Round</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns half to even of a tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as the `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; round = P.Round()</span>
<span class="sd">         &gt;&gt;&gt; round(input_x)</span>
<span class="sd">         [1.0, 2.0, 2.0, 2.0, -4.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Round&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="Tan"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Tan">[docs]</a><span class="k">class</span> <span class="nc">Tan</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes tangent of `input_x` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`. Data type must be</span>
<span class="sd">          float16, float32 or int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tan = P.Tan()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1.0, 0.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = tan(input_x)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Tan&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="Atan"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Atan">[docs]</a><span class="k">class</span> <span class="nc">Atan</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor): The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.047, 0.785]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tan = P.Tan()</span>
<span class="sd">        &gt;&gt;&gt; output_y = tan(input_x)</span>
<span class="sd">        &gt;&gt;&gt; atan = P.Atan()</span>
<span class="sd">        &gt;&gt;&gt; atan(output_y)</span>
<span class="sd">        [[1.047, 07850001]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="Atanh"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Atanh">[docs]</a><span class="k">class</span> <span class="nc">Atanh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor): The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1.047, 0.785]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; atanh = P.Atanh()</span>
<span class="sd">        &gt;&gt;&gt; atanh(input_x)</span>
<span class="sd">        [[1.8869909 1.058268]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="Atan2"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Atan2">[docs]</a><span class="k">class</span> <span class="nc">Atan2</span><span class="p">(</span><span class="n">_MathBinaryOp</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns arctangent of input_x/input_y element-wise.</span>

<span class="sd">    It returns :math:`\theta\ \in\ [-\pi, \pi]`</span>
<span class="sd">    such that :math:`x = r*\sin(\theta), y = r*\cos(\theta)`, where :math:`r = \sqrt{x^2 + y^2}`.</span>

<span class="sd">    Inputs of `input_x` and `input_y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">        - **input_y** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is same as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1]]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; input_y = Tensor(np.array([[1, 1]]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; atan2 = P.Atan2()</span>
<span class="sd">         &gt;&gt;&gt; atan2(input_x, input_y)</span>
<span class="sd">         [[0. 0.7853982]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="SquareSumAll"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.SquareSumAll">[docs]</a><span class="k">class</span> <span class="nc">SquareSumAll</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square sum all of a tensor element-wise</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x1** (Tensor) - The input tensor. The data type must be float16 or float32.</span>
<span class="sd">        - **input_x2** (Tensor) - The input tensor has the same type and shape as the `input_x1`.</span>

<span class="sd">    Note:</span>
<span class="sd">        SquareSumAll only supports float16 and float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output_y1** (Tensor) - The same type as the `input_x1`.</span>
<span class="sd">        - **output_y2** (Tensor) - The same type as the `input_x1`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x1 = Tensor(np.random.randint([3, 2, 5, 7]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; input_x2 = Tensor(np.random.randint([3, 2, 5, 7]), mindspore.float32)</span>
<span class="sd">         &gt;&gt;&gt; square_sum_all = P.SquareSumAll()</span>
<span class="sd">         &gt;&gt;&gt; square_sum_all(input_x1, input_x2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize SquareSumAll&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x1_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;x2_shape&quot;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x1_type&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x2_type&#39;</span><span class="p">:</span> <span class="n">y_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span></div>


<div class="viewcode-block" id="BitwiseAnd"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BitwiseAnd">[docs]</a><span class="k">class</span> <span class="nc">BitwiseAnd</span><span class="p">(</span><span class="n">_BitwiseBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `and` of two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x1` and `input_x2` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x1** (Tensor) - The input tensor with int16, int32 or uint16 data type.</span>
<span class="sd">        - **input_x2** (Tensor) - The input tensor with same type as the `input_x1`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - The same type as the `input_x1`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x1 = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mstype.int16)</span>
<span class="sd">         &gt;&gt;&gt; input_x2 = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mstype.int16)</span>
<span class="sd">         &gt;&gt;&gt; bitwise_and = P.BitwiseAnd()</span>
<span class="sd">         &gt;&gt;&gt; bitwise_and(input_x1, input_x2)</span>
<span class="sd">         [0, 0, 1, -1, 1, 0, 1]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BitwiseOr"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BitwiseOr">[docs]</a><span class="k">class</span> <span class="nc">BitwiseOr</span><span class="p">(</span><span class="n">_BitwiseBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `or` of two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x1` and `input_x2` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x1** (Tensor) - The input tensor with int16, int32 or uint16 data type.</span>
<span class="sd">        - **input_x2** (Tensor) - The input tensor with same type as the `input_x1`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - The same type as the `input_x1`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x1 = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mstype.int16)</span>
<span class="sd">         &gt;&gt;&gt; input_x2 = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mstype.int16)</span>
<span class="sd">         &gt;&gt;&gt; bitwise_or = P.BitwiseOr()</span>
<span class="sd">         &gt;&gt;&gt; bitwise_or(input_x1, input_x2)</span>
<span class="sd">         [0, 1, 1, -1, -1, 3, 3]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BitwiseXor"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BitwiseXor">[docs]</a><span class="k">class</span> <span class="nc">BitwiseXor</span><span class="p">(</span><span class="n">_BitwiseBinaryOp</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `xor` of two tensors element-wise.</span>

<span class="sd">    Inputs of `input_x1` and `input_x2` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x1** (Tensor) - The input tensor with int16, int32 or uint16 data type.</span>
<span class="sd">        - **input_x2** (Tensor) - The input tensor with same type as the `input_x1`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - The same type as the `input_x1`.</span>

<span class="sd">    Examples:</span>
<span class="sd">         &gt;&gt;&gt; input_x1 = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mstype.int16)</span>
<span class="sd">         &gt;&gt;&gt; input_x2 = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mstype.int16)</span>
<span class="sd">         &gt;&gt;&gt; bitwise_xor = P.BitwiseXor()</span>
<span class="sd">         &gt;&gt;&gt; bitwise_xor(input_x1, input_x2)</span>
<span class="sd">         [0, 1, 0, 0, -2, 3, 2]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="BesselI0e"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BesselI0e">[docs]</a><span class="k">class</span> <span class="nc">BesselI0e</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselI0e of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`. Data type must be float16 or float32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; bessel_i0e = P.BesselI0e()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_i0e(input_x)</span>
<span class="sd">        [0.7979961, 0.5144438, 0.75117415, 0.9157829]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize BesselI0e&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="BesselI1e"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.BesselI1e">[docs]</a><span class="k">class</span> <span class="nc">BesselI1e</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes BesselI1e of input element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`. Data type must be float16 or float32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; bessel_i1e = P.BesselI1e()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = bessel_i1e(input_x)</span>
<span class="sd">        [0.09507662, 0.19699717, 0.11505538, 0.04116856]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize BesselI1e&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="Inv"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Inv">[docs]</a><span class="k">class</span> <span class="nc">Inv</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Inv(Reciprocal) of input tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          Must be one of the following types: float16, float32, int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inv = P.Inv()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.25, 0.4, 0.31, 0.52]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = inv(input_x)</span>
<span class="sd">        [4., 2.5, 3.2258065, 1.923077]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x_dtype&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                                                                <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Invert"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Invert">[docs]</a><span class="k">class</span> <span class="nc">Invert</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips all bits of input tensor element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[int16], Tensor[uint16]) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `input_x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; invert = P.Invert()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([25, 4, 13, 9]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = invert(input_x)</span>
<span class="sd">        [-26, -5, -14, -10]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;x_dtype&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Eps"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.Eps">[docs]</a><span class="k">class</span> <span class="nc">Eps</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor filled with `input_x` dtype minimum val.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor. The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as `input_x`, but filled with `input_x` dtype minimum val.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([4, 1, 2, 3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = P.Eps()(input_x)</span>
<span class="sd">        [1.52587891e-05, 1.52587891e-05, 1.52587891e-05, 1.52587891e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize Eps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s1">&#39;input_x&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">x_nptype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">element_type</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">x_nptype</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">14</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="mi">16</span><span class="p">)</span>

        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">x_nptype</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">res</span><span class="p">),</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="IFMR"><a class="viewcode-back" href="../../../../mindspore/mindspore.ops.html#mindspore.ops.IFMR">[docs]</a><span class="k">class</span> <span class="nc">IFMR</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The TFMR(Input Feature Map Reconstruction).</span>

<span class="sd">    Args:</span>
<span class="sd">        min_percentile (float): Min init percentile.</span>
<span class="sd">        max_percentile (float): Max init percentile.</span>
<span class="sd">        search_range Union[list(float), tuple(float)]: Range of searching.</span>
<span class="sd">        search_step (float): Step size of searching.</span>
<span class="sd">        with_offset (bool): Whether using offset.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **data** (Tensor) - A Tensor of feature map. With float16 or float32 data type.</span>
<span class="sd">        - **data_min** (Tensor) - A Tensor of min value of feature map, the shape is :math:`(1)`.</span>
<span class="sd">          With float16 or float32 data type.</span>
<span class="sd">        - **data_max** (Tensor) - A Tensor of max value of feature map, the shape is :math:`(1)`.</span>
<span class="sd">          With float16 or float32 data type.</span>
<span class="sd">        - **cumsum** (Tensor) - A `1-D` Tensor of cumsum bin of data. With int32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **scale** (Tensor) - A tensor of optimal scale, the shape is :math:`(1)`. Data dtype is float32.</span>
<span class="sd">        - **offset** (Tensor) - A tensor of optimal offset, the shape is :math:`(1)`. Data dtype is float32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.random.rand(1, 3, 6, 4).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; data_min = Tensor([0.1], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; data_max = Tensor([0.5], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; cumsum = Tensor(np.random.rand(4).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; ifmr = P.IFMR(min_percentile=0.2, max_percentile=0.9, search_range=(1.0, 2.0),</span>
<span class="sd">                          search_step=1.0, with_offset=False)</span>
<span class="sd">        &gt;&gt;&gt; output = ifmr(data, data_min, data_max, cumsum)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_percentile</span><span class="p">,</span> <span class="n">max_percentile</span><span class="p">,</span> <span class="n">search_range</span><span class="p">,</span> <span class="n">search_step</span><span class="p">,</span> <span class="n">with_offset</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;min_percentile&quot;</span><span class="p">,</span> <span class="n">min_percentile</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;max_percentile&quot;</span><span class="p">,</span> <span class="n">max_percentile</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;search_range&quot;</span><span class="p">,</span> <span class="n">search_range</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">search_range</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_float_positive</span><span class="p">(</span><span class="s2">&quot;item of search_range&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;search_range[1]&#39;</span><span class="p">,</span> <span class="n">search_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;search_range[0]&#39;</span><span class="p">,</span> <span class="n">search_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;search_step&quot;</span><span class="p">,</span> <span class="n">search_step</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;offset_flag&quot;</span><span class="p">,</span> <span class="n">with_offset</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_shape</span><span class="p">,</span> <span class="n">data_min_shape</span><span class="p">,</span> <span class="n">data_max_shape</span><span class="p">,</span> <span class="n">cumsum_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;dims of data_min&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_min_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;data_min[0]&quot;</span><span class="p">,</span> <span class="n">data_min_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;dims of data_max&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_max_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;data_max[0]&quot;</span><span class="p">,</span> <span class="n">data_max_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_integer</span><span class="p">(</span><span class="s2">&quot;dims of cumsum&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">cumsum_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dtype</span><span class="p">,</span> <span class="n">data_min_dtype</span><span class="p">,</span> <span class="n">data_max_dtype</span><span class="p">,</span> <span class="n">cumsum_dtype</span><span class="p">):</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_value&quot;</span><span class="p">:</span> <span class="n">data_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_min&quot;</span><span class="p">:</span> <span class="n">data_min_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_max&quot;</span><span class="p">:</span> <span class="n">data_max_dtype</span><span class="p">},</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_type_same</span><span class="p">({</span><span class="s2">&quot;input_bins&quot;</span><span class="p">:</span> <span class="n">cumsum_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, MindSpore

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>