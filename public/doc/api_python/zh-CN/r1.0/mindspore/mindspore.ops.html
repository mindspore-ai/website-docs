<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore.profiler" href="mindspore.profiler.html" />
    <link rel="prev" title="mindspore.nn.probability" href="mindspore.nn.probability.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.dynamic_lr.html">mindspore.nn.dynamic_lr</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.ops</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/mindspore/mindspore.ops.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-mindspore.ops">
<span id="mindspore-ops"></span><h1>mindspore.ops<a class="headerlink" href="#module-mindspore.ops" title="Permalink to this headline"></a></h1>
<p>Operators can be used in the construct function of Cell.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The Primitive operators in operations need to be used after instantiation.</p></li>
<li><p>The composite operators are the pre-defined combination of operators.</p></li>
<li><p>The functional operators are the pre-instantiated Primitive operators, which can be used directly as a function.</p></li>
<li><p>For functional operators usage, please refer to
<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/ops/functional.py">https://gitee.com/mindspore/mindspore/blob/master/mindspore/ops/functional.py</a></p></li>
</ul>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ACos">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ACos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ACos"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ACos" title="Permalink to this definition"></a></dt>
<dd><p>Computes arccosine of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">acos</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ACos</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.74</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.56</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">acos</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Abs">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Abs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Abs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Abs" title="Permalink to this definition"></a></dt>
<dd><p>Returns absolute value of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">abs</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">abs</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0, 1.0, 0.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AccumulateNV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AccumulateNV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#AccumulateNV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AccumulateNV2" title="Permalink to this definition"></a></dt>
<dd><p>Computes accumulation of all input tensors element-wise.</p>
<p>AccumulateNV2 is similar to AddN, but there is a significant difference
among them: AccumulateNV2 will not wait for all of its inputs to be ready
before summing. That is to say, AccumulateNV2 is able to save
memory when inputs are ready at different time since the minimum temporary
storage is proportional to the output size rather than the input size.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union(tuple[Tensor], list[Tensor])) - The input tuple or list
is made up of multiple tensors whose dtype is number to be added together.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as each entry of the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">NetAccumulateNV2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">NetAccumulateNV2</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accumulateNV2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AccumulateNV2</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">z</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulateNV2</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">NetAccumulateNV2</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">Tensor([10., 14., 18.], shape=(3,), dtype=mindspore.float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Acosh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Acosh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Acosh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Acosh" title="Permalink to this definition"></a></dt>
<dd><p>Compute inverse hyperbolic cosine of the input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">acosh</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Acosh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">acosh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Adam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Adam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Adam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Adam" title="Permalink to this definition"></a></dt>
<dd><p>Updates gradients by Adaptive Moment Estimation (Adam) algorithm.</p>
<p>The Adam algorithm is proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m = \beta_1 * m + (1 - \beta_1) * g \\
    v = \beta_2 * v + (1 - \beta_2) * g * g \\
    l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\
    w = w - l * \frac{m}{\sqrt{v} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector, <span class="math notranslate nohighlight">\(g\)</span> represents
<cite>gradient</cite>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent <cite>beta1</cite> and <cite>beta2</cite>,
<span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(beta_1^t\)</span> and <span class="math notranslate nohighlight">\(beta_2^t\)</span> represent <cite>beta1_power</cite> and
<cite>beta2_power</cite>, <span class="math notranslate nohighlight">\(\alpha\)</span> represents <cite>learning_rate</cite>, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>var</cite>, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents
<cite>epsilon</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect variable tensors from being updated.
If true, updates of the var, m, and v tensors will be protected by a lock.
If false, the result is unpredictable. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.
If true, update the gradients using NAG.
If true, update the gradients without using NAG. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Tensor) - Weights to be updated.</p></li>
<li><p><strong>m</strong> (Tensor) - The 1st moment vector in the updating formula, has the same type as <cite>var</cite>.</p></li>
<li><p><strong>v</strong> (Tensor) - the 2nd moment vector in the updating formula.
Mean square gradients with the same type as <cite>var</cite>.</p></li>
<li><p><strong>beta1_power</strong> (float) - <span class="math notranslate nohighlight">\(beta_1^t\)</span> in the updating formula.</p></li>
<li><p><strong>beta2_power</strong> (float) - <span class="math notranslate nohighlight">\(beta_2^t\)</span> in the updating formula.</p></li>
<li><p><strong>lr</strong> (float) - <span class="math notranslate nohighlight">\(l\)</span> in the updating formula.</p></li>
<li><p><strong>beta1</strong> (float) - The exponential decay rate for the 1st moment estimations.</p></li>
<li><p><strong>beta2</strong> (float) - The exponential decay rate for the 2nd moment estimations.</p></li>
<li><p><strong>epsilon</strong> (float) - Term added to the denominator to improve numerical stability.</p></li>
<li><p><strong>gradient</strong> (Tensor) - Gradient, has the same type as <cite>var</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>m</strong> (Tensor) - The same shape and data type as <cite>m</cite>.</p></li>
<li><p><strong>v</strong> (Tensor) - The same shape and data type as <cite>v</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_adam</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                              <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">gradient</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AddN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AddN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#AddN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AddN" title="Permalink to this definition"></a></dt>
<dd><p>Computes addition of all input tensors element-wise.</p>
<p>All input tensors must have the same shape.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union(tuple[Tensor], list[Tensor])) - The input tuple or list
is made up of multiple tensors whose dtype is number or bool to be added together.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as each entry of the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">NetAddN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">NetAddN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">addN</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AddN</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">z</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">addN</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">NetAddN</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[10.0, 14.0, 18.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AiCPURegOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AiCPURegOp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#AiCPURegOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AiCPURegOp" title="Permalink to this definition"></a></dt>
<dd><p>Class for AiCPU op info register</p>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.AiCPURegOp.attr">
<span class="sig-name descname"><span class="pre">attr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#AiCPURegOp.attr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AiCPURegOp.attr" title="Permalink to this definition"></a></dt>
<dd><p>Register AiCPU op attribute information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the attribute. Default: None.</p></li>
<li><p><strong>value_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Value type of the attribute. Default: None.</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Value of the attribute. Default: None.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Other information of the attribute.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.AiCPURegOp.input">
<span class="sig-name descname"><span class="pre">input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#AiCPURegOp.input"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AiCPURegOp.input" title="Permalink to this definition"></a></dt>
<dd><p>Register AiCPU op input information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Order of the input. Default: None.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the input. Default: None.</p></li>
<li><p><strong>param_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Param type of the input. Default: None.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Other information of the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.AiCPURegOp.output">
<span class="sig-name descname"><span class="pre">output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#AiCPURegOp.output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AiCPURegOp.output" title="Permalink to this definition"></a></dt>
<dd><p>Register AiCPU op output information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Order of the output. Default: None.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the output. Default: None.</p></li>
<li><p><strong>param_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Param type of the output. Default: None.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Other information of the output.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AllGather">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AllGather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/comm_ops.html#AllGather"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AllGather" title="Permalink to this definition"></a></dt>
<dd><p>Gathers tensors from the specified communication group.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The tensors must have the same shape and format in all processes of the collection.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The communication group to work on. Default: “hccl_world_group”.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If group is not a string.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the local rank id of the calling process in the group
    is larger than the group’s rank size.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. If the number of devices in the group is N,
then the shape of output is <span class="math notranslate nohighlight">\((N, x_1, x_2, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">allgather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AllGather</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="s2">&quot;nccl_world_group&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">allgather</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AllReduce">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AllReduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/comm_ops.html#AllReduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AllReduce" title="Permalink to this definition"></a></dt>
<dd><p>Reduces the tensor data across all devices in such a way that all devices will get the same final result.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The operation of AllReduce does not support “prod” currently.
The tensors must have the same shape and format in all processes of the collection.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies an operation used for element-wise reductions,
like sum, max, and min. Default: ReduceOp.SUM.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The communication group to work on. Default: “hccl_world_group”.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If any of operation and group is not a string,
    or fusion is not an integer, or the input’s dtype is bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the operation is “prod”.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape of the input, i.e., <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
The contents depend on the specified operation.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops.operations.comm_ops</span> <span class="kn">import</span> <span class="n">ReduceOp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">(</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&quot;nccl_world_group&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyAdaMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyAdaMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyAdaMax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyAdaMax" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the adamax scheme.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m_{t} = \beta_1 * m_{t-1} + (1 - \beta_1) * g \\
    v_{t} = \max(\beta_2 * v_{t-1}, \left| g \right|) \\
    var = var - \frac{l}{1 - \beta_1^t} * \frac{m_{t}}{v_{t} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(m_{t-1}\)</span>
is the last momentent of <span class="math notranslate nohighlight">\(m_{t}\)</span>, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector, <span class="math notranslate nohighlight">\(v_{t-1}\)</span>
is the last momentent of <span class="math notranslate nohighlight">\(v_{t}\)</span>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>,
<span class="math notranslate nohighlight">\(g\)</span> represents <cite>grad</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent <cite>beta1</cite> and <cite>beta2</cite>,
<span class="math notranslate nohighlight">\(beta_1^t\)</span> represents <cite>beta1_power</cite>, <span class="math notranslate nohighlight">\(var\)</span> represents the variable to be updated,
<span class="math notranslate nohighlight">\(\epsilon\)</span> represents <cite>epsilon</cite>.</p>
<p>Inputs of <cite>var</cite>, <cite>m</cite>, <cite>v</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable to be updated. With float32 or float16 data type.</p></li>
<li><p><strong>m</strong> (Parameter) - The 1st moment vector in the updating formula, has the same shape and type as <cite>var</cite>.
With float32 or float16 data type.</p></li>
<li><p><strong>v</strong> (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients
with the same shape and type as <cite>var</cite>. With float32 or float16 data type.</p></li>
<li><p><strong>beta1_power</strong> (Union[Number, Tensor]) - <span class="math notranslate nohighlight">\(beta_1^t\)</span> in the updating formula, must be scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - Learning rate, <span class="math notranslate nohighlight">\(l\)</span> in the updating formula, must be scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>beta1</strong> (Union[Number, Tensor]) - The exponential decay rate for the 1st moment estimations,
must be scalar. With float32 or float16 data type.</p></li>
<li><p><strong>beta2</strong> (Union[Number, Tensor]) - The exponential decay rate for the 2nd moment estimations,
must be scalar. With float32 or float16 data type.</p></li>
<li><p><strong>epsilon</strong> (Union[Number, Tensor]) - A small value added for numerical stability, must be scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor for gradient, has the same shape and type as <cite>var</cite>.
With float32 or float16 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>m</strong> (Tensor) - The same shape and data type as <cite>m</cite>.</p></li>
<li><p><strong>v</strong> (Tensor) - The same shape and data type as <cite>v</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_ada_max</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyAdaMax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_ada_max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1_power</span> <span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">beta1_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyAdadelta">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyAdadelta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyAdadelta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyAdadelta" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the adadelta scheme.</p>
<div class="math notranslate nohighlight">
\[accum = \rho * accum + (1 - \rho) * grad^2\]</div>
<div class="math notranslate nohighlight">
\[\text{update} = \sqrt{\text{accum_update} + \epsilon} * \frac{grad}{\sqrt{accum + \epsilon}}\]</div>
<div class="math notranslate nohighlight">
\[\text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * update^2\]</div>
<div class="math notranslate nohighlight">
\[var -= lr * update\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite>, <cite>accum_update</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Weights to be updated. With float32 or float16 data type.</p></li>
<li><p><strong>accum</strong> (Parameter) - Accumulation to be updated, has the same shape and type as <cite>var</cite>.
With float32 or float16 data type.</p></li>
<li><p><strong>accum_update</strong> (Parameter) - Accum_update to be updated, has the same shape and type as <cite>var</cite>.
With float32 or float16 data type.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - Learning rate, must be scalar. With float32 or float16 data type.</p></li>
<li><p><strong>rho</strong> (Union[Number, Tensor]) - Decay rate, must be scalar. With float32 or float16 data type.</p></li>
<li><p><strong>epsilon</strong> (Union[Number, Tensor]) - A small value added for numerical stability, must be scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradients, has the same shape and type as <cite>var</cite>. With float32 or float16 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>accum</cite>.</p></li>
<li><p><strong>accum_update</strong> (Tensor) - The same shape and data type as <cite>accum_update</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_adadelta</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyAdadelta</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum_update</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum_update&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_adadelta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum_update</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rho</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyAdagrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyAdagrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyAdagrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyAdagrad" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the adagrad scheme.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[var -= lr * grad * \frac{1}{\sqrt{accum}}\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent..
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>update_slots</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>True</cite>, <cite>accum</cite> will be updated. Default: True.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable to be updated. With float32 or float16 data type.</p></li>
<li><p><strong>accum</strong> (Parameter) - Accumulation to be updated. The shape and dtype must be the same as <cite>var</cite>.
With float32 or float16 data type.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be scalar. With float32 or float16 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor for gradient. The shape and dtype must be the same as <cite>var</cite>.
With float32 or float16 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>accum</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_adagrad</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyAdagrad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_adagrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyAdagradV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyAdagradV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyAdagradV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyAdagradV2" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the adagradv2 scheme.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added for numerical stability.</p></li>
<li><p><strong>update_slots</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>True</cite>, <cite>accum</cite> will be updated. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable to be updated. With float16 or float32 data type.</p></li>
<li><p><strong>accum</strong> (Parameter) - Accumulation to be updated. The shape and dtype must be the same as <cite>var</cite>.
With float16 or float32 data type.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be a float number or
a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor for gradient. The shape and dtype must be the same as <cite>var</cite>.
With float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>m</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_adagrad_v2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyAdagradV2</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_adagrad_v2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyAddSign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyAddSign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyAddSign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyAddSign" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the AddSign algorithm.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m_{t} = \beta * m_{t-1} + (1 - \beta) * g \\
    \text{update} = (\alpha + \text{sign_decay} * sign(g) * sign(m)) * g \\
    var = var - lr_{t} * \text{update}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(m_{t-1}\)</span>
is the last momentent of <span class="math notranslate nohighlight">\(m_{t}\)</span>, <span class="math notranslate nohighlight">\(lr\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(g\)</span> represents <cite>grad</cite>.</p>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</p></li>
<li><p><strong>m</strong> (Parameter) - Variable tensor to be updated, has the same dtype as <cite>var</cite>.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be a scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>alpha</strong> (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</p></li>
<li><p><strong>sign_decay</strong> (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</p></li>
<li><p><strong>beta</strong> (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>m</strong> (Tensor) - The same shape and data type as <cite>m</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_add_sign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyAddSign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sign_decay</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_add_sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sign_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyCenteredRMSProp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyCenteredRMSProp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyCenteredRMSProp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyCenteredRMSProp" title="Permalink to this definition"></a></dt>
<dd><p>Optimizer that implements the centered RMSProp algorithm.
Please refer to the usage in source code of <cite>nn.RMSProp</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Update <cite>var</cite> according to the centered RMSProp algorithm.</p>
<div class="math notranslate nohighlight">
\[g_{t} = \rho g_{t-1} + (1 - \rho)\nabla Q_{i}(w)\]</div>
<div class="math notranslate nohighlight">
\[s_{t} = \rho s_{t-1} + (1 - \rho)(\nabla Q_{i}(w))^2\]</div>
<div class="math notranslate nohighlight">
\[m_{t} = \beta m_{t-1} + \frac{\eta} {\sqrt{s_{t} - g_{t}^2 + \epsilon}} \nabla Q_{i}(w)\]</div>
<div class="math notranslate nohighlight">
\[w = w - m_{t}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> represents <cite>var</cite>, which will be updated.
<span class="math notranslate nohighlight">\(g_{t}\)</span> represents <cite>mean_gradient</cite>, <span class="math notranslate nohighlight">\(g_{t-1}\)</span> is the last momentent of <span class="math notranslate nohighlight">\(g_{t}\)</span>.
<span class="math notranslate nohighlight">\(s_{t}\)</span> represents <cite>mean_square</cite>, <span class="math notranslate nohighlight">\(s_{t-1}\)</span> is the last momentent of <span class="math notranslate nohighlight">\(s_{t}\)</span>,
<span class="math notranslate nohighlight">\(m_{t}\)</span> represents <cite>moment</cite>, <span class="math notranslate nohighlight">\(m_{t-1}\)</span> is the last momentent of <span class="math notranslate nohighlight">\(m_{t}\)</span>.
<span class="math notranslate nohighlight">\(\rho\)</span> represents <cite>decay</cite>. <span class="math notranslate nohighlight">\(\beta\)</span> is the momentum term, represents <cite>momentum</cite>.
<span class="math notranslate nohighlight">\(\epsilon\)</span> is a smoothing term to avoid division by zero, represents <cite>epsilon</cite>.
<span class="math notranslate nohighlight">\(\eta\)</span> represents <cite>learning_rate</cite>. <span class="math notranslate nohighlight">\(\nabla Q_{i}(w)\)</span> represents <cite>grad</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect the variable and accumlation tensors
from being updated. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Tensor) - Weights to be update.</p></li>
<li><p><strong>mean_gradient</strong> (Tensor) - Mean gradients, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>mean_square</strong> (Tensor) - Mean square gradients, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>moment</strong> (Tensor) - Delta of <cite>var</cite>, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradient, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>learning_rate</strong> (Union[Number, Tensor]) - Learning rate. Must be a float number or
a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>decay</strong> (float) - Decay rate.</p></li>
<li><p><strong>momentum</strong> (float) - Momentum.</p></li>
<li><p><strong>epsilon</strong> (float) - Ridge term.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, parameters to be update.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">centered_rms_prop</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyCenteredRMSProp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_square</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">moment</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decay</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">momentum</span> <span class="o">=</span> <span class="mf">1e-10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">centered_rms_prop</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mean_grad</span><span class="p">,</span> <span class="n">mean_square</span><span class="p">,</span> <span class="n">moment</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                           <span class="n">learning_rate</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="go">[[[ -6.        -9.024922]</span>
<span class="go">  [-12.049845 -15.074766]</span>
<span class="go">  [-18.09969  -21.124613]]</span>
<span class="go"> [[-24.149532 -27.174456]</span>
<span class="go">  [-30.199379 -33.2243  ]</span>
<span class="go">  [-36.249226 -39.274143]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyFtrl">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyFtrl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyFtrl"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyFtrl" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the FTRL scheme.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use locks for updating operation if true . Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - The variable to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - The accumulation to be updated, must be same type and shape as <cite>var</cite>.</p></li>
<li><p><strong>linear</strong> (Parameter) - the linear coefficient to be updated, must be same type and shape as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradient. The data type must be float16 or float32.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be positive. Default: 0.001.
It must be a float number or a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>l1</strong> (Union[Number, Tensor]) - l1 regularization strength, must be greater than or equal to zero.
Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>l2</strong> (Union[Number, Tensor]) - l2 regularization strength, must be greater than or equal to zero.
Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>lr_power</strong> (Union[Number, Tensor]) - Learning rate power controls how the learning rate decreases
during training, must be less than or equal to zero. Use fixed learning rate if lr_power is zero.
Default: -0.5. It must be a float number or a scalar tensor with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, represents the updated <cite>var</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ApplyFtrlNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">ApplyFtrlNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_ftrl</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyFtrl</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_ftrl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                              <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">ApplyFtrlNet</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[0.67455846   0.14630564   0.160499  ]</span>
<span class="go"> [0.16329421   0.00415689   0.05202988]</span>
<span class="go"> [0.18672481   0.17418946   0.36420345]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyGradientDescent">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyGradientDescent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyGradientDescent"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyGradientDescent" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the following formula.</p>
<div class="math notranslate nohighlight">
\[var = var - \alpha * \delta\]</div>
<p>Inputs of <cite>var</cite> and <cite>delta</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</p></li>
<li><p><strong>alpha</strong> (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</p></li>
<li><p><strong>delta</strong> (Tensor) - A tensor for the change, has the same type as <cite>var</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, represents the updated <cite>var</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_gradient_descent</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyGradientDescent</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyMomentum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyMomentum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyMomentum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyMomentum" title="Permalink to this definition"></a></dt>
<dd><p>Optimizer that implements the Momentum algorithm.</p>
<p>Refer to the paper <a class="reference external" href="https://dl.acm.org/doi/10.5555/3042817.3043064">On the importance of initialization and momentum in deep
learning</a>  for more details.</p>
<p>Inputs of <cite>variable</cite>, <cite>accumulation</cite> and <cite>gradient</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
Data type conversion of Parameter is not supported. RuntimeError exception will be thrown.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect the variable and accumlation tensors
from being updated. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enable Nesterov momentum. Default: False.</p></li>
<li><p><strong>gradient_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The scale of the gradient. Default: 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>variable</strong> (Parameter) - Weights to be updated. data type must be float.</p></li>
<li><p><strong>accumulation</strong> (Parameter) - Accumulated gradient value by moment weight.
Has the same data type with <cite>variable</cite>.</p></li>
<li><p><strong>learning_rate</strong> (Union[Number, Tensor]) - The learning rate value, must be a float number or
a scalar tensor with float data type.</p></li>
<li><p><strong>gradient</strong> (Tensor) - Gradient, has the same data type as <cite>variable</cite>.</p></li>
<li><p><strong>momentum</strong> (Union[Number, Tensor]) - Momentum, must be a float number or
a scalar tensor with float data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, parameters to be updated.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Please refer to the usage in nn.ApplyMomentum.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyPowerSign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyPowerSign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyPowerSign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyPowerSign" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the AddSign algorithm.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m_{t} = \beta * m_{t-1} + (1 - \beta) * g \\
    \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\
    var = var - lr_{t} * \text{update}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(m_{t-1}\)</span>
is the last momentent of <span class="math notranslate nohighlight">\(m_{t}\)</span>, <span class="math notranslate nohighlight">\(lr\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(g\)</span> represents <cite>grad</cite>.</p>
<p>All of inputs comply with the implicit type conversion rules to make the data types consistent.
If <cite>lr</cite>, <cite>logbase</cite>, <cite>sign_decay</cite> or <cite>beta</cite> is a number, the number is automatically converted to Tensor,
and the data type is consistent with the Tensor data type involved in the operation.
If inputs are tensors and have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. With float32 or float16 data type.
If data type of <cite>var</cite> is float16, all inputs must have the same data type as <cite>var</cite>.</p></li>
<li><p><strong>m</strong> (Parameter) - Variable tensor to be updated, has the same dtype as <cite>var</cite>.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be a scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>logbase</strong> (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</p></li>
<li><p><strong>sign_decay</strong> (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</p></li>
<li><p><strong>beta</strong> (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>m</strong> (Tensor) - The same shape and data type as <cite>m</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_power_sign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyPowerSign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">logbase</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">e</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sign_decay</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_power_sign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">logbase</span><span class="p">,</span>
<span class="go">                                        self.sign_decay, self.beta, grad)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyProximalAdagrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyProximalAdagrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyProximalAdagrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyProximalAdagrad" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the proximal adagrad algorithm.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[\text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}\]</div>
<div class="math notranslate nohighlight">
\[var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the var and accumulation tensors will be protected from being updated.
Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - Accumulation to be updated. Must has the same shape and dtype as <cite>var</cite>.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be scalar. The data type must be
float16 or float32.</p></li>
<li><p><strong>l1</strong> (Union[Number, Tensor]) - l1 regularization strength, must be scalar. The data type must be
float16 or float32.</p></li>
<li><p><strong>l2</strong> (Union[Number, Tensor]) - l2 regularization strength, must be scalar. The data type must be
float16 or float32.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradient with the same shape and dtype as <cite>var</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>accum</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_proximal_adagrad</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyProximalAdagrad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_proximal_adagrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyProximalGradientDescent">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyProximalGradientDescent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyProximalGradientDescent"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyProximalGradientDescent" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the FOBOS(Forward Backward Splitting) algorithm.</p>
<div class="math notranslate nohighlight">
\[\text{prox_v} = var - \alpha * \delta\]</div>
<div class="math notranslate nohighlight">
\[var = \frac{sign(\text{prox_v})}{1 + \alpha * l2} * \max(\left| \text{prox_v} \right| - alpha * l1, 0)\]</div>
<p>Inputs of <cite>var</cite> and <cite>delta</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</p></li>
<li><p><strong>alpha</strong> (Union[Number, Tensor]) - Saling factor, must be a scalar. With float32 or float16 data type.</p></li>
<li><p><strong>l1</strong> (Union[Number, Tensor]) - l1 regularization strength, must be scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>l2</strong> (Union[Number, Tensor]) - l2 regularization strength, must be scalar.
With float32 or float16 data type.</p></li>
<li><p><strong>delta</strong> (Tensor) - A tensor for the change, has the same type as <cite>var</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, represents the updated <cite>var</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_proximal_gradient_descent</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyProximalGradientDescent</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_proximal_gradient_descent</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">delta</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApplyRMSProp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApplyRMSProp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ApplyRMSProp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApplyRMSProp" title="Permalink to this definition"></a></dt>
<dd><p>Optimizer that implements the Root Mean Square prop(RMSProp) algorithm.
Please refer to the usage in source code of <cite>nn.RMSProp</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Update <cite>var</cite> according to the RMSProp algorithm.</p>
<div class="math notranslate nohighlight">
\[s_{t} = \rho s_{t-1} + (1 - \rho)(\nabla Q_{i}(w))^2\]</div>
<div class="math notranslate nohighlight">
\[m_{t} = \beta m_{t-1} + \frac{\eta} {\sqrt{s_{t} + \epsilon}} \nabla Q_{i}(w)\]</div>
<div class="math notranslate nohighlight">
\[w = w - m_{t}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> represents <cite>var</cite>, which will be updated.
<span class="math notranslate nohighlight">\(s_{t}\)</span> represents <cite>mean_square</cite>, <span class="math notranslate nohighlight">\(s_{t-1}\)</span> is the last momentent of <span class="math notranslate nohighlight">\(s_{t}\)</span>,
<span class="math notranslate nohighlight">\(m_{t}\)</span> represents <cite>moment</cite>, <span class="math notranslate nohighlight">\(m_{t-1}\)</span> is the last momentent of <span class="math notranslate nohighlight">\(m_{t}\)</span>.
<span class="math notranslate nohighlight">\(\rho\)</span> represents <cite>decay</cite>. <span class="math notranslate nohighlight">\(\beta\)</span> is the momentum term, represents <cite>momentum</cite>.
<span class="math notranslate nohighlight">\(\epsilon\)</span> is a smoothing term to avoid division by zero, represents <cite>epsilon</cite>.
<span class="math notranslate nohighlight">\(\eta\)</span> represents <cite>learning_rate</cite>. <span class="math notranslate nohighlight">\(\nabla Q_{i}(w)\)</span> represents <cite>grad</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect the variable and accumlation tensors
from being updated. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Tensor) - Weights to be update.</p></li>
<li><p><strong>mean_square</strong> (Tensor) - Mean square gradients, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>moment</strong> (Tensor) - Delta of <cite>var</cite>, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>learning_rate</strong> (Union[Number, Tensor]) - Learning rate. Must be a float number or
a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradient, must have the same type as <cite>var</cite>.</p></li>
<li><p><strong>decay</strong> (float) - Decay rate. Only constant value is allowed.</p></li>
<li><p><strong>momentum</strong> (float) - Momentum. Only constant value is allowed.</p></li>
<li><p><strong>epsilon</strong> (float) - Ridge term. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, parameters to be update.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">apply_rms</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApplyRMSProp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_square</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">moment</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decay</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">momentum</span> <span class="o">=</span> <span class="mf">1e-10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">apply_rms</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mean_square</span><span class="p">,</span> <span class="n">moment</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="go">(-2.9977674, 0.80999994, 1.9987665)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ApproximateEqual">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ApproximateEqual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ApproximateEqual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ApproximateEqual" title="Permalink to this definition"></a></dt>
<dd><p>Returns true if abs(x1-x2) is smaller than tolerance element-wise, otherwise false.</p>
<p>Inputs of <cite>x1</cite> and <cite>x2</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tolerance</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The maximum deviation that two elements can be considered equal. Default: 1e-05.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x1</strong> (Tensor) - A tensor. Must be one of the following types: float32, float16.</p></li>
<li><p><strong>x2</strong> (Tensor) - A tensor of the same type and shape as ‘x1’.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the shape of ‘x1’, and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">approximate_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ApproximateEqual</span><span class="p">(</span><span class="mf">2.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">approximate_equal</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="go">[True  True  False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ArgMaxWithValue">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ArgMaxWithValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ArgMaxWithValue"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ArgMaxWithValue" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the maximum value with the corresponding index.</p>
<p>Calculates the maximum value along with the given axis for the input tensor. It returns the maximum values and
indices.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension to reduce. Default: 0.</p></li>
<li><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to reduce dimension, if true, the output will keep same dimension with the input,
the output will reduce dimension if false. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor, can be any dimension. Set the shape of input tensor as
<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the maximum value of the input
tensor.
- index (Tensor) - The index for the maximum value of the input tensor. If <cite>keep_dims</cite> is true, the shape of
output tensors is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)\)</span>. Otherwise, the shape is
<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)\)</span>.
- output_x (Tensor) - The maximum value of input tensor, with the same shape as index.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ArgMaxWithValue</span><span class="p">()(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ArgMinWithValue">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ArgMinWithValue</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ArgMinWithValue"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ArgMinWithValue" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the minimum value with corresponding index, return indices and values.</p>
<p>Calculates the minimum value along with the given axis for the input tensor. It returns the minimum values and
indices.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension to reduce. Default: 0.</p></li>
<li><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to reduce dimension, if true the output will keep the same dimension as the input,
the output will reduce dimension if false. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor, can be any dimension. Set the shape of input tensor as
<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the minimum value of the input
tensor.
- index (Tensor) - The index for the maximum value of the input tensor. If <cite>keep_dims</cite> is true, the shape of
output tensors is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)\)</span>. Otherwise, the shape is
<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)\)</span>.
- output_x (Tensor) - The minimum value of input tensor, with the same shape as index.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ArgMinWithValue</span><span class="p">()(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Argmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Argmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Argmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Argmax" title="Permalink to this definition"></a></dt>
<dd><p>Returns the indices of the max value of a tensor across the axis.</p>
<p>If the shape of input tensor is <span class="math notranslate nohighlight">\((x_1, ..., x_N)\)</span>, the shape of the output tensor will be
<span class="math notranslate nohighlight">\((x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Axis where the Argmax operation applies to. Default: -1.</p></li>
<li><p><strong>output_type</strong> (<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – An optional data type of <cite>mindspore.dtype.int32</cite>.
Default: <cite>mindspore.dtype.int32</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, indices of the max value of input tensor across the axis.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Argmax</span><span class="p">(</span><span class="n">output_type</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Argmin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Argmin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Argmin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Argmin" title="Permalink to this definition"></a></dt>
<dd><p>Returns the indices of the min value of a tensor across the axis.</p>
<p>If the shape of input tensor is <span class="math notranslate nohighlight">\((x_1, ..., x_N)\)</span>, the shape of the output tensor is
<span class="math notranslate nohighlight">\((x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Axis where the Argmin operation applies to. Default: -1.</p></li>
<li><p><strong>output_type</strong> (<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – An optional data type of <cite>mindspore.dtype.int32</cite>.
Default: <cite>mindspore.dtype.int32</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, indices of the min value of input tensor across the axis.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">index</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Argmin</span><span class="p">()(</span><span class="n">input_x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">index</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Asin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Asin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Asin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Asin" title="Permalink to this definition"></a></dt>
<dd><p>Computes arcsine of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">asin</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Asin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.74</span><span class="p">,</span> <span class="mf">0.04</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.56</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">asin</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.8331, 0.0400, 0.3047, 0.5944]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Asinh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Asinh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Asinh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Asinh" title="Permalink to this definition"></a></dt>
<dd><p>Compute inverse hyperbolic sine of the input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">asinh</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Asinh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">asinh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-2.3212, 1.1976, 1.8184, 5.2983]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Assert">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Assert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#Assert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Assert" title="Permalink to this definition"></a></dt>
<dd><p>Asserts that the given condition is true.
If input condition evaluates to false, print the list of tensor in data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>summarize</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Print this many entries of each tensor.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>condition</strong> [Union[Tensor[bool], bool]] - The condition to evaluate.</p></li>
<li><p><strong>input_data</strong> (Union(tuple[Tensor], list[Tensor])) - The tensors to print out when condition is false.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">AssertDemo</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">AssertDemo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="k">assert</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assert</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="k">assert</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="p">[</span><span class="n">data</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Assign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Assign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#Assign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Assign" title="Permalink to this definition"></a></dt>
<dd><p>Assigns <cite>Parameter</cite> with a value.</p>
<p>Inputs of <cite>variable</cite> and <cite>value</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>variable</strong> (Parameter) - The <cite>Parameter</cite>.</p></li>
<li><p><strong>value</strong> (Tensor) - The value to be assigned.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type as original <cite>variable</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AssignAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AssignAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#AssignAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AssignAdd" title="Permalink to this definition"></a></dt>
<dd><p>Updates a <cite>Parameter</cite> by adding a value to it.</p>
<p>Inputs of <cite>variable</cite> and <cite>value</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
If <cite>value</cite> is a number, the number is automatically converted to Tensor,
and the data type is consistent with the Tensor data type involved in the operation.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>variable</strong> (Parameter) - The <cite>Parameter</cite>.</p></li>
<li><p><strong>value</strong> (Union[numbers.Number, Tensor]) - The value to be added to the <cite>variable</cite>.
It must have the same shape as <cite>variable</cite> if it is a Tensor.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">AssignAdd</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AssignAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">variable</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_step&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">AssignAdd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variable</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AssignSub">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AssignSub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#AssignSub"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AssignSub" title="Permalink to this definition"></a></dt>
<dd><p>Updates a <cite>Parameter</cite> by subtracting a value from it.</p>
<p>Inputs of <cite>variable</cite> and <cite>value</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
If <cite>value</cite> is a number, the number is automatically converted to Tensor,
and the data type is consistent with the Tensor data type involved in the operation.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>variable</strong> (Parameter) - The <cite>Parameter</cite>.</p></li>
<li><p><strong>value</strong> (Union[numbers.Number, Tensor]) - The value to be subtracted from the <cite>variable</cite>.
It must have the same shape as <cite>variable</cite> if it is a Tensor.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">AssignSub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AssignSub</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">variable</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;global_step&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">AssignSub</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">variable</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">variable</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Atan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Atan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Atan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Atan" title="Permalink to this definition"></a></dt>
<dd><p>Computes the trigonometric inverse tangent of the input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor): The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>A Tensor, has the same type as the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.047</span><span class="p">,</span> <span class="mf">0.785</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tan</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_y</span> <span class="o">=</span> <span class="n">tan</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atan</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atan</span><span class="p">(</span><span class="n">output_y</span><span class="p">)</span>
<span class="go">[[1.047, 07850001]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Atan2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Atan2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Atan2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Atan2" title="Permalink to this definition"></a></dt>
<dd><p>Returns arctangent of input_x/input_y element-wise.</p>
<p>It returns <span class="math notranslate nohighlight">\(\theta\ \in\ [-\pi, \pi]\)</span>
such that <span class="math notranslate nohighlight">\(x = r*\sin(\theta), y = r*\cos(\theta)\)</span>, where <span class="math notranslate nohighlight">\(r = \sqrt{x^2 + y^2}\)</span>.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is same as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atan2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atan2</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atan2</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[[0. 0.7853982]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Atanh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Atanh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Atanh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Atanh" title="Permalink to this definition"></a></dt>
<dd><p>Computes inverse hyperbolic tangent of the input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor): The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>A Tensor, has the same type as the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.047</span><span class="p">,</span> <span class="mf">0.785</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atanh</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">atanh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[1.8869909 1.058268]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.AvgPool">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">AvgPool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#AvgPool"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.AvgPool" title="Permalink to this definition"></a></dt>
<dd><p>Average pooling operation.</p>
<p>Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.
Typically the input is of shape <span class="math notranslate nohighlight">\((N_{in}, C_{in}, H_{in}, W_{in})\)</span>, AvgPool2d outputs
regional average in the <span class="math notranslate nohighlight">\((H_{in}, W_{in})\)</span>-dimension. Given kernel size
<span class="math notranslate nohighlight">\(ks = (h_{ker}, w_{ker})\)</span> and stride <span class="math notranslate nohighlight">\(s = (s_0, s_1)\)</span>, the operation is as follows.</p>
<div class="math notranslate nohighlight">
\[\text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}
\text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ksize</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of kernel used to take the average value,
is an int number that represents height and width are both ksize, or a tuple
of two int numbers that represent height and width respectively. Default: 1.</p></li>
<li><p><strong>strides</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The optional value for pad mode, is “same” or “valid”, not case sensitive.
Default: “valid”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible.
Otherwise, the last extra padding will be done from the bottom and the right side.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output
will be returned without padding. Extra pixels will be discarded.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">avgpool_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AvgPool</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">avgpool_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">result</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[[[ 2.5   3.5   4.5]</span>
<span class="go">   [ 6.5   7.5   8.5]]</span>
<span class="go">  [[ 14.5  15.5  16.5]</span>
<span class="go">   [ 18.5  19.5  20.5]]</span>
<span class="go">  [[ 26.5  27.5  28.5]</span>
<span class="go">   [ 30.5  31.5  32.5]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BNTrainingReduce">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BNTrainingReduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#BNTrainingReduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BNTrainingReduce" title="Permalink to this definition"></a></dt>
<dd><p>For BatchNorm operator, this operator update the moving averages for training and is used in conjunction with
BNTrainingUpdate.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - A 4-D Tensor with float16 or float32 data type. Tensor of shape <span class="math notranslate nohighlight">\((N, C, A, B)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>sum</strong> (Tensor) - A 1-D Tensor with float32 data type. Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>square_sum</strong> (Tensor) - A 1-D Tensor with float32 data type. Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bn_training_reduce</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BNTrainingReduce</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">bn_training_reduce</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BNTrainingUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BNTrainingUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#BNTrainingUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BNTrainingUpdate" title="Permalink to this definition"></a></dt>
<dd><p>For BatchNorm operator, this operator update the moving averages for training and is used in conjunction with
BNTrainingReduce.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>isRef</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If a ref. Default: True.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added to variance avoid dividing by zero. Default: 1e-5.</p></li>
<li><p><strong>factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A weight for updating the mean and variance. Default: 0.1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - A 4-D Tensor with float16 or float32 data type. Tensor of shape <span class="math notranslate nohighlight">\((N, C, A, B)\)</span>.</p></li>
<li><p><strong>sum</strong> (Tensor) - A 1-D Tensor with float16 or float32 data type for the output of operator BNTrainingReduce.
Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>square_sum</strong> (Tensor) - A 1-D Tensor with float16 or float32 data type for the output of operator
BNTrainingReduce. Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>scale</strong> (Tensor) - A 1-D Tensor with float16 or float32, for the scaling factor.
Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>offset</strong> (Tensor) - A 1-D Tensor with float16 or float32, for the scaling offset.
Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>mean</strong> (Tensor) - A 1-D Tensor with float16 or float32, for the scaling mean. Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>variance</strong> (Tensor) - A 1-D Tensor with float16 or float32, for the update variance.
Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>y</strong> (Tensor) - Tensor, has the same shape data type as <cite>x</cite>.</p></li>
<li><p><strong>mean</strong> (Tensor) - Tensor for the updated mean, with float32 data type.
Has the same shape as <cite>variance</cite>.</p></li>
<li><p><strong>variance</strong> (Tensor) - Tensor for the updated variance, with float32 data type.
Has the same shape as <cite>variance</cite>.</p></li>
<li><p><strong>batch_mean</strong> (Tensor) - Tensor for the mean of <cite>x</cite>, with float32 data type.
Has the same shape as <cite>variance</cite>.</p></li>
<li><p><strong>batch_variance</strong> (Tensor) - Tensor for the mean of <cite>variance</cite>, with float32 data type.
Has the same shape as <cite>variance</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">sum</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square_sum</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variance</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bn_training_update</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BNTrainingUpdate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">bn_training_update</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BasicLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BasicLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#BasicLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BasicLSTMCell" title="Permalink to this definition"></a></dt>
<dd><p>Applies the long short-term memory (LSTM) to the input.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    i_t = \sigma(W_{ix} x_t + b_{ix} + W_{ih} h_{(t-1)} + b_{ih}) \\
    f_t = \sigma(W_{fx} x_t + b_{fx} + W_{fh} h_{(t-1)} + b_{fh}) \\
    \tilde{c}_t = \tanh(W_{cx} x_t + b_{cx} + W_{ch} h_{(t-1)} + b_{ch}) \\
    o_t = \sigma(W_{ox} x_t + b_{ox} + W_{oh} h_{(t-1)} + b_{oh}) \\
    c_t = f_t * c_{(t-1)} + i_t * \tilde{c}_t \\
    h_t = o_t * \tanh(c_t) \\
\end{array}\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\sigma\)</span> is the sigmoid function, and <span class="math notranslate nohighlight">\(*\)</span> is the Hadamard product. <span class="math notranslate nohighlight">\(W, b\)</span>
are learnable weights between the output and the input in the formula. For instance,
<span class="math notranslate nohighlight">\(W_{ix}, b_{ix}\)</span> are the weight and bias used to transform from input <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(i\)</span>.
Details can be found in paper <a class="reference external" href="https://www.bioinf.jku.at/publications/older/2604.pdf">LONG SHORT-TERM MEMORY</a> and
<a class="reference external" href="https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43905.pdf">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>keep_prob</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – If not 1.0, append <cite>Dropout</cite> layer on the outputs of each
LSTM layer except the last layer. Default 1.0. The range of dropout is [0.0, 1.0].</p></li>
<li><p><strong>forget_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Add forget bias to forget gate biases in order to decrease former scale. Default: 1.0.</p></li>
<li><p><strong>state_is_tuple</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the state is a tuple of 2 tensors, containing h and c; If false, the state is</p></li>
<li><p><strong>Default</strong> (<em>a tensor and it needs to be split first.</em>) – True.</p></li>
<li><p><strong>activation</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Activation. Default: “tanh”. Only “tanh” is currently supported.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Current words. Tensor of shape (<cite>batch_size</cite>, <cite>input_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>h</strong> (Tensor) - Hidden state last moment. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>c</strong> (Tensor) - Cell state last moment. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>w</strong> (Tensor) - Weight. Tensor of shape (<cite>input_size + hidden_size</cite>, <cite>4 x hidden_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>b</strong> (Tensor) - Bias. Tensor of shape (<cite>4 x hidden_size</cite>).
The data type must be the same as <cite>c</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>ct</strong> (Tensor) - Forward <span class="math notranslate nohighlight">\(c_t\)</span> cache at moment <cite>t</cite>. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>c</cite>.</p></li>
<li><p><strong>ht</strong> (Tensor) - Cell output. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>). With data type of float16.</p></li>
<li><p><strong>it</strong> (Tensor) - Forward <span class="math notranslate nohighlight">\(i_t\)</span> cache at moment <cite>t</cite>. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>c</cite>.</p></li>
<li><p><strong>jt</strong> (Tensor) - Forward <span class="math notranslate nohighlight">\(j_t\)</span> cache at moment <cite>t</cite>. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>c</cite>.</p></li>
<li><p><strong>ft</strong> (Tensor) - Forward <span class="math notranslate nohighlight">\(f_t\)</span> cache at moment <cite>t</cite>. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>c</cite>.</p></li>
<li><p><strong>ot</strong> (Tensor) - Forward <span class="math notranslate nohighlight">\(o_t\)</span> cache at moment <cite>t</cite>. Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>c</cite>.</p></li>
<li><p><strong>tanhct</strong> (Tensor) - Forward <span class="math notranslate nohighlight">\(tanh c_t\)</span> cache at moment <cite>t</cite>.
Tensor of shape (<cite>batch_size</cite>, <cite>hidden_size</cite>), has the same type with input <cite>c</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">h</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lstm</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BatchMatMul">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BatchMatMul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#BatchMatMul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BatchMatMul" title="Permalink to this definition"></a></dt>
<dd><p>Computes matrix multiplication between two tensors by batch</p>
<p><cite>result[…, :, :] = tensor(a[…, :, :]) * tensor(b[…, :, :])</cite>.</p>
<p>The two input tensors must have the same rank and the rank must be not less than <cite>3</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transpose_a</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the last two dimensions of <cite>a</cite> is transposed before multiplication.
Default: False.</p></li>
<li><p><strong>transpose_b</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the last two dimensions of <cite>b</cite> is transposed before multiplication.
Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The first tensor to be multiplied. The shape of the tensor is <span class="math notranslate nohighlight">\((*B, N, C)\)</span>,
where <span class="math notranslate nohighlight">\(*B\)</span> represents the batch size which can be multidimensional, <span class="math notranslate nohighlight">\(N\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are the
size of the last two dimensions. If <cite>transpose_a</cite> is True, its shape must be <span class="math notranslate nohighlight">\((*B, C, N)\)</span>.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The second tensor to be multiplied. The shape of the tensor is <span class="math notranslate nohighlight">\((*B, C, M)\)</span>. If
<cite>transpose_b</cite> is True, its shape must be <span class="math notranslate nohighlight">\((*B, M, C)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of the output tensor is <span class="math notranslate nohighlight">\((*B, N, M)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batmatmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">batmatmul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batmatmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">(</span><span class="n">transpose_a</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">batmatmul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BatchNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BatchNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#BatchNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BatchNorm" title="Permalink to this definition"></a></dt>
<dd><p>Batch Normalization for input data and updated parameters.</p>
<p>Batch Normalization is widely used in convolutional neural networks. This operation
applies Batch Normalization over input to avoid internal covariate shift as described
in the paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal
Covariate Shift</a>. It rescales and recenters the
features using a mini-batch of data and the learned parameters which can be described
in the following formula,</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is scale, <span class="math notranslate nohighlight">\(\beta\)</span> is bias, <span class="math notranslate nohighlight">\(\epsilon\)</span> is epsilon.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>is_training</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>is_training</cite> is True, <cite>mean</cite> and <cite>variance</cite> are computed during training.
If <cite>is_training</cite> is False, they’re loaded from checkpoint during inference. Default: False.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added for numerical stability. Default: 1e-5.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C)\)</span>, with float16 or float32 data type.</p></li>
<li><p><strong>scale</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, with float16 or float32 data type.</p></li>
<li><p><strong>bias</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, has the same data type with <cite>scale</cite>.</p></li>
<li><p><strong>mean</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, with float16 or float32 data type.</p></li>
<li><p><strong>variance</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, has the same data type with <cite>mean</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 5 Tensor, the normalized inputs and the updated parameters.</p>
<ul class="simple">
<li><p><strong>output_x</strong> (Tensor) - The same type and shape as the input_x. The shape is <span class="math notranslate nohighlight">\((N, C)\)</span>.</p></li>
<li><p><strong>updated_scale</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>updated_bias</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>reserve_space_1</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>reserve_space_2</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variance</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchNorm</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">batch_norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BatchToSpace">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BatchToSpace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#BatchToSpace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BatchToSpace" title="Permalink to this definition"></a></dt>
<dd><p>Divides batch dimension with blocks and interleaves these blocks back into spatial dimensions.</p>
<p>This operation will divide batch dimension N into blocks with block_size, the output tensor’s N dimension
is the corresponding number of blocks after division. The output tensor’s H, W dimension is product of original H, W
dimension and block_size with given amount to crop from dimension, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>block_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The block size of division, has the value not less than 2.</p></li>
<li><p><strong>crops</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>]</em>) – The crop value for H and W dimension, containing 2 subtraction lists.
Each list contains 2 integers.
All values must be not less than 0. crops[i] specifies the crop values for the spatial dimension i, which
corresponds to the input dimension i+2. It is required that
input_shape[i+2]*block_size &gt;= crops[i][0]+crops[i][1].</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. It must be a 4-D tensor, dimension 0 must be divisible by
product of <cite>block_shape</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the output tensor with the same type as input. Assume input shape is (n, c, h, w) with block_size
and crops. The output shape will be (n’, c’, h’, w’), where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(n' = n//(block\_size*block\_size)\)</span></p>
<p><span class="math notranslate nohighlight">\(c' = c\)</span></p>
<p><span class="math notranslate nohighlight">\(h' = h*block\_size-crops[0][0]-crops[0][1]\)</span></p>
<p><span class="math notranslate nohighlight">\(w' = w*block\_size-crops[1][0]-crops[1][1]\)</span></p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crops</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchToSpace</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">crops</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">2</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">3</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">4</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[[[1., 2.], [3., 4.]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BatchToSpaceND">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BatchToSpaceND</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#BatchToSpaceND"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BatchToSpaceND" title="Permalink to this definition"></a></dt>
<dd><p>Divides batch dimension with blocks and interleave these blocks back into spatial dimensions.</p>
<p>This operation will divide batch dimension N into blocks with block_shape, the output tensor’s N dimension
is the corresponding number of blocks after division. The output tensor’s H, W dimension is product of original H, W
dimension and block_shape with given amount to crop from dimension, respectively.B</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>block_shape</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>]</em>) – The block shape of dividing block with all value &gt;= 1.
The length of block_shape is M correspoding to the number of spatial dimensions. M must be 2.</p></li>
<li><p><strong>crops</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>]</em>) – The crop value for H and W dimension, containing 2 subtraction list,
each containing 2 int value.
All values must be &gt;= 0. crops[i] specifies the crop values for spatial dimension i, which corresponds to
input dimension i+2. It is required that input_shape[i+2]*block_shape[i] &gt; crops[i][0]+crops[i][1].</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. It must be a 4-D tensor, dimension 0 must be divisible by
product of <cite>block_shape</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the output tensor with the same type as input. Assume input shape is (n, c, h, w) with block_shape
and crops. The output shape will be (n’, c’, h’, w’), where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(n' = n//(block\_shape[0]*block\_shape[1])\)</span></p>
<p><span class="math notranslate nohighlight">\(c' = c\)</span></p>
<p><span class="math notranslate nohighlight">\(h' = h*block\_shape[0]-crops[0][0]-crops[0][1]\)</span></p>
<p><span class="math notranslate nohighlight">\(w' = w*block\_shape[1]-crops[1][0]-crops[1][1]\)</span></p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crops</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_to_space_nd</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchToSpaceND</span><span class="p">(</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">2</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">3</span><span class="p">]]],</span> <span class="p">[[[</span><span class="mi">4</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">batch_to_space_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[[[1., 2.], [3., 4.]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BesselI0e">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BesselI0e</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#BesselI0e"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BesselI0e" title="Permalink to this definition"></a></dt>
<dd><p>Computes BesselI0e of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>. Data type must be float16 or float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bessel_i0e</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BesselI0e</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.24</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">bessel_i0e</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.7979961, 0.5144438, 0.75117415, 0.9157829]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BesselI1e">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BesselI1e</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#BesselI1e"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BesselI1e" title="Permalink to this definition"></a></dt>
<dd><p>Computes BesselI1e of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>. Data type must be float16 or float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bessel_i1e</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BesselI1e</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.24</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">bessel_i1e</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.09507662, 0.19699717, 0.11505538, 0.04116856]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BiasAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BiasAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#BiasAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BiasAdd" title="Permalink to this definition"></a></dt>
<dd><p>Returns sum of input and bias tensor.</p>
<p>Adds the 1-D bias tensor to the input tensor, and broadcasts the shape on all axis
except for the channel axis.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The shape can be 2-4 dimensions.</p></li>
<li><p><strong>bias</strong> (Tensor) - The bias tensor, with shape <span class="math notranslate nohighlight">\((C)\)</span>.
The shape of <cite>bias</cite> must be the same as <cite>input_x</cite> in the second dimension.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same shape and type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BinaryCrossEntropy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BinaryCrossEntropy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#BinaryCrossEntropy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BinaryCrossEntropy" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Binary Cross Entropy between the target and the output.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sets input as <span class="math notranslate nohighlight">\(x\)</span>, input label as <span class="math notranslate nohighlight">\(y\)</span>, output as <span class="math notranslate nohighlight">\(\ell(x, y)\)</span>.
Let,</p>
<div class="math notranslate nohighlight">
\[L = \{l_1,\dots,l_N\}^\top, \quad
l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
L, &amp; \text{if reduction} = \text{`none';}\\
\operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean';}\\
\operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the reduction to be applied to the output.
Its value must be one of ‘none’, ‘mean’, ‘sum’. Default: ‘mean’.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input Tensor. The data type must be float16 or float32.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The label Tensor which has same shape and data type as <cite>input_x</cite>.</p></li>
<li><p><strong>weight</strong> (Tensor, optional) - A rescaling weight applied to the loss of each batch element.
And it must have same shape and data type as <cite>input_x</cite>. Default: None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor or Scalar, if <cite>reduction</cite> is ‘none’, then output is a tensor and has the same shape as <cite>input_x</cite>.
Otherwise, the output is a scalar.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">result</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="go">0.38240486</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BitwiseAnd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BitwiseAnd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#BitwiseAnd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BitwiseAnd" title="Permalink to this definition"></a></dt>
<dd><p>Returns bitwise <cite>and</cite> of two tensors element-wise.</p>
<p>Inputs of <cite>input_x1</cite> and <cite>input_x2</cite> comply with the implicit type conversion rules to
make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x1</strong> (Tensor) - The input tensor with int16, int32 or uint16 data type.</p></li>
<li><p><strong>input_x2</strong> (Tensor) - The input tensor with same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>y</strong> (Tensor) - The same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bitwise_and</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseAnd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bitwise_and</span><span class="p">(</span><span class="n">input_x1</span><span class="p">,</span> <span class="n">input_x2</span><span class="p">)</span>
<span class="go">[0, 0, 1, -1, 1, 0, 1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BitwiseOr">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BitwiseOr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#BitwiseOr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BitwiseOr" title="Permalink to this definition"></a></dt>
<dd><p>Returns bitwise <cite>or</cite> of two tensors element-wise.</p>
<p>Inputs of <cite>input_x1</cite> and <cite>input_x2</cite> comply with the implicit type conversion rules to
make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x1</strong> (Tensor) - The input tensor with int16, int32 or uint16 data type.</p></li>
<li><p><strong>input_x2</strong> (Tensor) - The input tensor with same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>y</strong> (Tensor) - The same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bitwise_or</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseOr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bitwise_or</span><span class="p">(</span><span class="n">input_x1</span><span class="p">,</span> <span class="n">input_x2</span><span class="p">)</span>
<span class="go">[0, 1, 1, -1, -1, 3, 3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BitwiseXor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BitwiseXor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#BitwiseXor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BitwiseXor" title="Permalink to this definition"></a></dt>
<dd><p>Returns bitwise <cite>xor</cite> of two tensors element-wise.</p>
<p>Inputs of <cite>input_x1</cite> and <cite>input_x2</cite> comply with the implicit type conversion rules to
make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x1</strong> (Tensor) - The input tensor with int16, int32 or uint16 data type.</p></li>
<li><p><strong>input_x2</strong> (Tensor) - The input tensor with same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>y</strong> (Tensor) - The same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bitwise_xor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseXor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bitwise_xor</span><span class="p">(</span><span class="n">input_x1</span><span class="p">,</span> <span class="n">input_x2</span><span class="p">)</span>
<span class="go">[0, 1, 0, 0, -2, 3, 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BoundingBoxDecode">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BoundingBoxDecode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#BoundingBoxDecode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BoundingBoxDecode" title="Permalink to this definition"></a></dt>
<dd><p>Decodes bounding boxes locations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>means</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The means of deltas calculation. Default: (0.0, 0.0, 0.0, 0.0).</p></li>
<li><p><strong>stds</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The standard deviations of deltas calculation. Default: (1.0, 1.0, 1.0, 1.0).</p></li>
<li><p><strong>max_shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The max size limit for decoding box calculation.</p></li>
<li><p><strong>wh_ratio_clip</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The limit of width and height ratio for decoding box calculation. Default: 0.016.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>anchor_box</strong> (Tensor) - Anchor boxes. The shape of <cite>anchor_box</cite> must be (n, 4).</p></li>
<li><p><strong>deltas</strong> (Tensor) - Delta of boxes. Which has the same shape with <cite>anchor_box</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, decoded boxes.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_box</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deltas</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]],</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boundingbox_decode</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BoundingBoxDecode</span><span class="p">(</span><span class="n">means</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">stds</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
<span class="gp">&gt;&gt;&gt; </span>                                         <span class="n">max_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">1280</span><span class="p">),</span> <span class="n">wh_ratio_clip</span><span class="o">=</span><span class="mf">0.016</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boundingbox_decode</span><span class="p">(</span><span class="n">anchor_box</span><span class="p">,</span> <span class="n">deltas</span><span class="p">)</span>
<span class="go">[[4.1953125  0.  0.  5.1953125]</span>
<span class="go"> [2.140625  0.  3.859375  60.59375]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BoundingBoxEncode">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BoundingBoxEncode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#BoundingBoxEncode"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BoundingBoxEncode" title="Permalink to this definition"></a></dt>
<dd><p>Encodes bounding boxes locations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>means</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – Means for encoding bounding boxes calculation. Default: (0.0, 0.0, 0.0, 0.0).</p></li>
<li><p><strong>stds</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The standard deviations of deltas calculation. Default: (1.0, 1.0, 1.0, 1.0).</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>anchor_box</strong> (Tensor) - Anchor boxes. The shape of anchor_box must be (n, 4).</p></li>
<li><p><strong>groundtruth_box</strong> (Tensor) - Ground truth boxes. Which has the same shape with anchor_box.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, encoded bounding boxes.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_box</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]],</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">groundtruth_box</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">]],</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boundingbox_encode</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BoundingBoxEncode</span><span class="p">(</span><span class="n">means</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="n">stds</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boundingbox_encode</span><span class="p">(</span><span class="n">anchor_box</span><span class="p">,</span> <span class="n">groundtruth_box</span><span class="p">)</span>
<span class="go">[[5.0000000e-01  5.0000000e-01  -6.5504000e+04  6.9335938e-01]</span>
<span class="go"> [-1.0000000e+00  2.5000000e-01  0.0000000e+00  4.0551758e-01]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Broadcast">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Broadcast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/comm_ops.html#Broadcast"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Broadcast" title="Permalink to this definition"></a></dt>
<dd><p>Broadcasts the tensor to the whole group.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The tensors must have the same shape and format in all processes of the collection.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root_rank</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Source rank. Required in all processes except the one
that is sending the data.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The communication group to work on. Default: “hccl_world_group”.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape of the input, i.e., <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
The contents depend on the data of the <cite>root_rank</cite> device.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If root_rank is not a integer or group is not a string.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">broadcast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Broadcast</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">broadcast</span><span class="p">((</span><span class="n">x</span><span class="p">,))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.BroadcastTo">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">BroadcastTo</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#BroadcastTo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.BroadcastTo" title="Permalink to this definition"></a></dt>
<dd><p>Broadcasts input tensor to a given shape.
Input shape can be broadcast to target shape if for each dimension pair they are either equal or input is one.
When input shape is broadcast to target shape, it starts with the trailing dimensions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The target shape to broadcast.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the given <cite>shape</cite> and the same data type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">broadcast_to</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CTCGreedyDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CTCGreedyDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#CTCGreedyDecoder"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CTCGreedyDecoder" title="Permalink to this definition"></a></dt>
<dd><p>Performs greedy decoding on the logits given in inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>merge_repeated</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, merge repeated classes in output. Default: True.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (Tensor) - The input Tensor must be a <cite>3-D</cite> tensor whose shape is
(<cite>max_time</cite>, <cite>batch_size</cite>, <cite>num_classes</cite>). <cite>num_classes</cite> must be <cite>num_labels + 1</cite> classes,
<cite>num_labels</cite> indicates the number of actual labels. Blank labels are reserved.
Default blank label is <cite>num_classes - 1</cite>. Data type must be float32 or float64.</p></li>
<li><p><strong>sequence_length</strong> (Tensor) - A tensor containing sequence lengths with the shape of (<cite>batch_size</cite>).
The type must be int32. Each value in the tensor must not greater than <cite>max_time</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>decoded_indices</strong> (Tensor) - A tensor with shape of (<cite>total_decoded_outputs</cite>, 2).
Data type is int64.</p></li>
<li><p><strong>decoded_values</strong> (Tensor) - A tensor with shape of (<cite>total_decoded_outputs</cite>),
it stores the decoded classes. Data type is int64.</p></li>
<li><p><strong>decoded_shape</strong> (Tensor) - The value of tensor is [<cite>batch_size</cite>, <cite>max_decoded_legth</cite>].
Data type is int64.</p></li>
<li><p><strong>log_probability</strong> (Tensor) - A tensor with shape of (<cite>batch_size</cite>, 1),
containing sequence log-probability, has the same type as <cite>inputs</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span>   <span class="k">class</span> <span class="nc">CTCGreedyDecoderNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="nb">super</span><span class="p">(</span><span class="n">CTCGreedyDecoderNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="bp">self</span><span class="o">.</span><span class="n">ctc_greedy_decoder</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CTCGreedyDecoder</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="bp">self</span><span class="o">.</span><span class="n">assert_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assert</span><span class="p">(</span><span class="mi">300</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>       <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctc_greedy_decoder</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="n">sequence_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="bp">self</span><span class="o">.</span><span class="n">assert_op</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span>           <span class="k">return</span> <span class="n">out</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">CTCGreedyDecoderNet</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CTCLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CTCLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#CTCLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CTCLoss" title="Permalink to this definition"></a></dt>
<dd><p>Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>preprocess_collapse_repeated</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, repeated labels will be collapsed prior to the CTC calculation.
Default: False.</p></li>
<li><p><strong>ctc_merge_repeated</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If false, during CTC calculation, repeated non-blank labels will not be merged
and these labels will be interpreted as individual ones. This is a simplfied
version of CTC. Default: True.</p></li>
<li><p><strong>ignore_longer_outputs_than_inputs</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, sequences with longer outputs than inputs will be ignored.
Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (Tensor) - The input Tensor must be a <cite>3-D</cite> tensor whose shape is
(<cite>max_time</cite>, <cite>batch_size</cite>, <cite>num_classes</cite>). <cite>num_classes</cite> must be <cite>num_labels + 1</cite> classes, <cite>num_labels</cite>
indicates the number of actual labels. Blank labels are reserved. Default blank label is <cite>num_classes - 1</cite>.
Data type must be float16, float32 or float64.</p></li>
<li><p><strong>labels_indices</strong> (Tensor) - The indices of labels. <cite>labels_indices[i, :] == [b, t]</cite> means <cite>labels_values[i]</cite>
stores the id for <cite>(batch b, time t)</cite>. The type must be int64 and rank must be 2.</p></li>
<li><p><strong>labels_values</strong> (Tensor) - A <cite>1-D</cite> input tensor. The values are associated with the given batch and time.
The type must be int32. <cite>labels_values[i]</cite> must in the range of <cite>[0, num_classes)</cite>.</p></li>
<li><p><strong>sequence_length</strong> (Tensor) - A tensor containing sequence lengths with the shape of (<cite>batch_size</cite>).
The type must be int32. Each value in the tensor must not be greater than <cite>max_time</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - A tensor containing log-probabilities, the shape is (<cite>batch_size</cite>). The tensor has
the same type with <cite>inputs</cite>.</p></li>
<li><p><strong>gradient</strong> (Tensor) - The gradient of <cite>loss</cite>, has the same type and shape with <cite>inputs</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_values</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ctc_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CTCLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ctc_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">,</span> <span class="n">labels_values</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Cast">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Cast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Cast"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Cast" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor with the new specified data type.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number]) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
The tensor to be cast.</p></li>
<li><p><strong>type</strong> (dtype.Number) - The valid data type of the output tensor. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is the same as <cite>input_x</cite>, <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">input_np</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">type_dst</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">type_dst</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Ceil">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Ceil</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Ceil"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Ceil" title="Permalink to this definition"></a></dt>
<dd><p>Round a tensor up to the closest integer element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. It’s element data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ceil_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ceil</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ceil_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[2.0, 3.0, -1.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CheckBprop">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CheckBprop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#CheckBprop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CheckBprop" title="Permalink to this definition"></a></dt>
<dd><p>Checks whether the data type and the shape of corresponding elements from tuples x and y are the same.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If tuples x and y are not the same.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (tuple[Tensor]) - The <cite>input_x</cite> contains the outputs of bprop to be checked.</p></li>
<li><p><strong>input_y</strong> (tuple[Tensor]) - The <cite>input_y</cite> contains the inputs of bprop to check against.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>(tuple[Tensor]), the <cite>input_x</cite>,
if data type and shape of corresponding elements from <cite>input_x</cite> and <cite>input_y</cite> are the same.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CheckBprop</span><span class="p">()(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CheckValid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CheckValid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#CheckValid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CheckValid" title="Permalink to this definition"></a></dt>
<dd><p>Checks bounding box.</p>
<p>Checks whether the bounding box cross data and data border are valid.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>bboxes</strong> (Tensor) - Bounding boxes tensor with shape (N, 4). Data type must be float16 or float32.</p></li>
<li><p><strong>img_metas</strong> (Tensor) - Raw image size information with the format of (height, width, ratio).
Data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the valided tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">check_valid</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CheckValid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">valid_result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_valid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">valid_result</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bboxes</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">img_metas</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">bboxes</span><span class="p">,</span> <span class="n">img_metas</span><span class="p">)</span>
<span class="go">[True   False   False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Concat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Concat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Concat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Concat" title="Permalink to this definition"></a></dt>
<dd><p>Concats tensor in specified axis.</p>
<p>Concats input tensors along with the given axis.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input data is a tuple of tensors. These tensors have the same rank <cite>R</cite>. Set the given axis as <cite>m</cite>, and
<span class="math notranslate nohighlight">\(0 \le m &lt; R\)</span>. Set the number of input tensors as <cite>N</cite>. For the <span class="math notranslate nohighlight">\(i\)</span>-th tensor <span class="math notranslate nohighlight">\(t_i\)</span>, it has
the shape of <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{mi}, ..., x_R)\)</span>. <span class="math notranslate nohighlight">\(x_{mi}\)</span> is the <span class="math notranslate nohighlight">\(m\)</span>-th dimension of the
<span class="math notranslate nohighlight">\(i\)</span>-th tensor. Then, the shape of the output tensor is</p>
<div class="math notranslate nohighlight">
\[(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)\]</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The specified axis. Default: 0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (tuple, list) - A tuple or a list of input tensors.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">((</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">))</span>
<span class="go">[[0, 1],</span>
<span class="go"> [2, 1],</span>
<span class="go"> [0, 1],</span>
<span class="go"> [2, 1]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ControlDepend">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ControlDepend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/control_ops.html#ControlDepend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ControlDepend" title="Permalink to this definition"></a></dt>
<dd><p>Adds control dependency relation between source and destination operation.</p>
<p>In many cases, we need to control the execution order of operations. ControlDepend is designed for this.
ControlDepend will instruct the execution engine to run the operations in a specific order. ControlDepend
tells the engine that the destination operations must depend on the source operation which means the source
operations must be executed before the destination.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This operation does not work in <cite>PYNATIVE_MODE</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>depend_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Use 0 for a normal dependency relation. Use 1 to depends on operations which using Parameter</p></li>
<li><p><strong>Default</strong> (<em>as its input.</em>) – <ol class="arabic simple" start="0">
<li></li>
</ol>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>src</strong> (Any) - The source input. It can be a tuple of operations output or a single operation output. We do
not concern about the input data, but concern about the operation that generates the input data.
If <cite>depend_mode</cite> is 1 and the source input is Parameter, we will try to find the operations that
used the parameter as input.</p></li>
<li><p><strong>dst</strong> (Any) - The destination input. It can be a tuple of operations output or a single operation output.
We do not concern about the input data, but concern about the operation that generates the input data.
If <cite>depend_mode</cite> is 1 and the source input is Parameter, we will try to find the operations that
used the parameter as input.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Bool. This operation has no actual data output, it will be used to setup the order of relative operations.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">control_depend</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ControlDepend</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">mul</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">softmax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">control_depend</span><span class="p">(</span><span class="n">mul</span><span class="p">,</span> <span class="n">softmax</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">ret</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Conv2D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Conv2D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Conv2D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Conv2D" title="Permalink to this definition"></a></dt>
<dd><p>2D convolution layer.</p>
<p>Applies a 2D convolution over an input tensor which is typically of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>,
where <span class="math notranslate nohighlight">\(N\)</span> is batch size and <span class="math notranslate nohighlight">\(C_{in}\)</span> is channel number. For each batch of shape
<span class="math notranslate nohighlight">\((C_{in}, H_{in}, W_{in})\)</span>, the formula is defined as:</p>
<div class="math notranslate nohighlight">
\[out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,\]</div>
<p>where <span class="math notranslate nohighlight">\(ccor\)</span> is the cross correlation operator, <span class="math notranslate nohighlight">\(C_{in}\)</span> is the input channel number, <span class="math notranslate nohighlight">\(j\)</span> ranges
from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(C_{out} - 1\)</span>, <span class="math notranslate nohighlight">\(W_{ij}\)</span> corresponds to the <span class="math notranslate nohighlight">\(i\)</span>-th channel of the <span class="math notranslate nohighlight">\(j\)</span>-th
filter and <span class="math notranslate nohighlight">\(out_{j}\)</span> corresponds to the <span class="math notranslate nohighlight">\(j\)</span>-th channel of the output. <span class="math notranslate nohighlight">\(W_{ij}\)</span> is a slice
of kernel and it has shape <span class="math notranslate nohighlight">\((\text{ks_h}, \text{ks_w})\)</span>, where <span class="math notranslate nohighlight">\(\text{ks_h}\)</span> and
<span class="math notranslate nohighlight">\(\text{ks_w}\)</span> are the height and width of the convolution kernel. The full kernel has shape
<span class="math notranslate nohighlight">\((C_{out}, C_{in} // \text{group}, \text{ks_h}, \text{ks_w})\)</span>, where group is the group number
to split the input in the channel dimension.</p>
<p>If the ‘pad_mode’ is set to be “valid”, the output height and width will be
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -
(\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> and
<span class="math notranslate nohighlight">\(\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -
(\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor\)</span> respectively.</p>
<p>The first introduction can be found in paper <a class="reference external" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">Gradient Based Learning Applied to Document Recognition</a>. More detailed introduction can be found here:
<a class="reference external" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of the output.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The kernel size of the 2D convolution.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Modes for different convolutions. 0 Math convolutiuon, 1 cross-correlation convolution ,
2 deconvolution, 3 depthwise convolution. Default: 1.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Modes to fill padding. It could be “valid”, “same”, or “pad”. Default: “valid”.</p></li>
<li><p><strong>pad</strong> (<em>Union</em><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>)</em>) – The pad value to be filled. Default: 0. If <cite>pad</cite> is an integer, the paddings of
top, bottom, left and right are the same, equal to pad. If <cite>pad</cite> is a tuple of four integers, the
padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>)</em>) – The stride to be applied to the convolution filter. Default: 1.</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>)</em>) – Specifies the space to use between kernel elements. Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Splits input into groups. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor, the value that applied 2D convolution.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
<li><p><strong>weight</strong> (Tensor) - Set size of kernel is <span class="math notranslate nohighlight">\((K_1, K_2)\)</span>, then the shape is
<span class="math notranslate nohighlight">\((C_{out}, C_{in}, K_1, K_2)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Conv2DBackpropInput">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Conv2DBackpropInput</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Conv2DBackpropInput"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Conv2DBackpropInput" title="Permalink to this definition"></a></dt>
<dd><p>Computes the gradients of convolution with respect to the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimensionality of the output space.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of the convolution window.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Modes to fill padding. It could be “valid”, “same”, or “pad”. Default: “valid”.</p></li>
<li><p><strong>pad</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The pad value to be filled. Default: 0. If <cite>pad</cite> is an integer, the paddings of
top, bottom, left and right are the same, equal to pad. If <cite>pad</cite> is a tuple of four integers, the
padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Modes for different convolutions. 0 Math convolutiuon, 1 cross-correlation convolution ,
2 deconvolution, 3 depthwise convolution. Default: 1.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><em>int. tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The stride to be applied to the convolution filter. Default: 1.</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>[</em><em>int. tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – Specifies the dilation rate to be used for the dilated convolution.
Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Splits input into groups. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor, the gradients of convolution.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dout</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d_backprop_input</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2DBackpropInput</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv2d_backprop_input</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Cos">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Cos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Cos"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Cos" title="Permalink to this definition"></a></dt>
<dd><p>Computes cosine of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cos</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.24</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cos</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Cosh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Cosh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Cosh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Cosh" title="Permalink to this definition"></a></dt>
<dd><p>Computes hyperbolic cosine of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cosh</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cosh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.24</span><span class="p">,</span> <span class="mf">0.83</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">,</span> <span class="mf">0.09</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cosh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0289385 1.364684 1.048436 1.4228927]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CropAndResize">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CropAndResize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/image_ops.html#CropAndResize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CropAndResize" title="Permalink to this definition"></a></dt>
<dd><p>Extracts crops from the input image tensor and resizes them.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In case that the output shape depends on crop_size, the crop_size must be constant.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – An optional string that specifies the sampling method for resizing.
It can be “bilinear”, “nearest” or “bilinear_v2”. The option “bilinear” stands for standard bilinear
interpolation algorithm, while “bilinear_v2” may result in better result in some cases. Default: “bilinear”</p></li>
<li><p><strong>extrapolation_value</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – An optional float value used extrapolation, if applicable. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input image must be a 4-D tensor of shape [batch, image_height, image_width, depth].
Types allowed: int8, int16, int32, int64, float16, float32, float64, uint8, uint16.</p></li>
<li><p><strong>boxes</strong> (Tensor) - A 2-D tensor of shape [num_boxes, 4].
The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image
and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to
the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is
mapped to [0, image_height - 1] in image height coordinates. We do allow y1 &gt; y2, in which case the sampled
crop is an up-down flipped version of the original image. The width dimension is treated similarly.
Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to
extrapolate the input image values. Types allowd: float32.</p></li>
<li><p><strong>box_index</strong> (Tensor) - A 1-D tensor of shape [num_boxes] with int32 values in [0, batch).
The value of box_ind[i] specifies the image that the i-th box refers to. Types allowd: int32.</p></li>
<li><p><strong>crop_size</strong> (Tuple[int]) - A tuple of two int32 elements: (crop_height, crop_width).
Only constant value is allowed. All cropped image patches are resized to this size.
The aspect ratio of the image content is not preserved. Both crop_height and crop_width need to be positive.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth] with type: float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">CropAndResizeNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">crop_size</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">CropAndResizeNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">crop_and_resize</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CropAndResize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">crop_size</span> <span class="o">=</span> <span class="n">crop_size</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@ms_function</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">box_index</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_and_resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">box_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">crop_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">NUM_BOXES</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">IMAGE_HEIGHT</span> <span class="o">=</span> <span class="mi">256</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">IMAGE_WIDTH</span> <span class="o">=</span> <span class="mi">256</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">CHANNELS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">IMAGE_HEIGHT</span><span class="p">,</span> <span class="n">IMAGE_WIDTH</span><span class="p">,</span> <span class="n">CHANNELS</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boxes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">NUM_BOXES</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">box_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">NUM_BOXES</span><span class="p">],</span> <span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crop_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crop_and_resize</span> <span class="o">=</span> <span class="n">CropAndResizeNet</span><span class="p">(</span><span class="n">crop_size</span><span class="o">=</span><span class="n">crop_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">crop_and_resize</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">boxes</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">box_index</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CumProd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CumProd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#CumProd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CumProd" title="Permalink to this definition"></a></dt>
<dd><p>Compute the cumulative product of the tensor x along axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>exclusive</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, perform exclusive cumulative product. Default: False.</p></li>
<li><p><strong>reverse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, reverse the result along axis. Default: False</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (int) - The dimensions to compute the cumulative product.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op0</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op0</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># output=[a, a * b, a * b * c]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">(</span><span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op1</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># output=[1, a, a * b]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op2</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># output=[a * b * c, b * c, c]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op3</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">(</span><span class="n">exclusive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op3</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># output=[b * c, c, 1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.CumSum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">CumSum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#CumSum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.CumSum" title="Permalink to this definition"></a></dt>
<dd><p>Computes the cumulative sum of input tensor along axis.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>exclusive</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, perform exclusive mode. Default: False.</p></li>
<li><p><strong>reverse</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, perform inverse cumulative sum. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - The input tensor to accumulate.</p></li>
<li><p><strong>axis</strong>  (int) - The axis to accumulate the tensor’s value. Only constant value is allowed.
Must be in the range [-rank(input), rank(input)).</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of the output tensor is consistent with the input tensor’s.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cumsum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumSum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">cumsum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">[[ 3.  7. 13. 23.]</span>
<span class="go"> [ 1.  7. 14. 23.]</span>
<span class="go"> [ 4.  7. 15. 22.]</span>
<span class="go"> [ 1.  4. 11. 20.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DType">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DType</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#DType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DType" title="Permalink to this definition"></a></dt>
<dd><p>Returns the data type of input tensor as mindspore.dtype.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>mindspore.dtype, the data type of a tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DataFormatDimMap">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DataFormatDimMap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#DataFormatDimMap"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DataFormatDimMap" title="Permalink to this definition"></a></dt>
<dd><p>Returns the dimension index in the destination data format given in the source data format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src_format</strong> (<em>string</em>) – An optional value for source data format. Default: ‘NHWC’.</p></li>
<li><p><strong>dst_format</strong> (<em>string</em>) – An optional value for destination data format. Default: ‘NCHW’.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - A Tensor with each element as a dimension index in source data format.
The suggested values is in the range [-4, 4). It’s type is int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dfdm</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DataFormatDimMap</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dfdm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[0 3 1 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DataType">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DataType</span></span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#DataType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DataType" title="Permalink to this definition"></a></dt>
<dd><p>Various combinations of dtype and format.</p>
<p>The current list below may be incomplete. Please add it if necessary.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Depend">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Depend</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#Depend"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Depend" title="Permalink to this definition"></a></dt>
<dd><p>Depend is used for processing side-effect operations.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>value</strong> (Tensor) - the real value to return for depend operator.</p></li>
<li><p><strong>expr</strong> (Expression) - the expression to execute with no outputs.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the value passed by last operator.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DepthToSpace">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DepthToSpace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#DepthToSpace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DepthToSpace" title="Permalink to this definition"></a></dt>
<dd><p>Rearranges blocks of depth data into spatial dimensions.</p>
<p>This is the reverse operation of SpaceToDepth.</p>
<p>The depth of output tensor is <span class="math notranslate nohighlight">\(input\_depth / (block\_size * block\_size)\)</span>.</p>
<p>The output tensor’s <cite>height</cite> dimension is <span class="math notranslate nohighlight">\(height * block\_size\)</span>.</p>
<p>The output tensor’s <cite>weight</cite> dimension is <span class="math notranslate nohighlight">\(weight * block\_size\)</span>.</p>
<p>The input tensor’s depth must be divisible by <cite>block_size * block_size</cite>.
The data format is “NCHW”.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>block_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The block size used to divide depth data. It must be &gt;= 2.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The target tensor. It must be a 4-D tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the ‘x’.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DepthToSpace</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DepthwiseConv2dNative">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DepthwiseConv2dNative</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#DepthwiseConv2dNative"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DepthwiseConv2dNative" title="Permalink to this definition"></a></dt>
<dd><p>Returns the depth-wise convolution value for the input.</p>
<p>Applies depthwise conv2d for the input, which will generate more channels with channel_multiplier.
Given an input tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span> where <span class="math notranslate nohighlight">\(N\)</span> is the batch size and a
filter tensor with kernel size <span class="math notranslate nohighlight">\((ks_{h}, ks_{w})\)</span>, containing <span class="math notranslate nohighlight">\(C_{in} * \text{channel_multiplier}\)</span>
convolutional filters of depth 1; it applies different filters to each input channel (channel_multiplier channels
for each input channel has the default value 1), then concatenates the results together. The output has
<span class="math notranslate nohighlight">\(\text{in_channels} * \text{channel_multiplier}\)</span> channels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>channel_multiplier</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The multipiler for the original output convolution. Its value must be greater than 0.</p></li>
<li><p><strong>kernel_size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of the convolution kernel.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Modes for different convolutions. 0 Math convolution, 1 cross-correlation convolution ,
2 deconvolution, 3 depthwise convolution. Default: 3.</p></li>
<li><p><strong>pad_mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Modes to fill padding. It could be “valid”, “same”, or “pad”. Default: “valid”.</p></li>
<li><p><strong>pad</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The pad value to be filled. If <cite>pad</cite> is an integer, the paddings of
top, bottom, left and right are the same, equal to pad. If <cite>pad</cite> is a tuple of four integers, the padding
of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly. Default: 0.</p></li>
<li><p><strong>stride</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The stride to be applied to the convolution filter. Default: 1.</p></li>
<li><p><strong>dilation</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – Specifies the dilation rate to be used for the dilated convolution.
Default: 1.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Splits input into groups. Default: 1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
<li><p><strong>weight</strong> (Tensor) - Set the size of kernel as <span class="math notranslate nohighlight">\((K_1, K_2)\)</span>, then the shape is
<span class="math notranslate nohighlight">\((K, C_{in}, K_1, K_2)\)</span>, <cite>K</cite> must be 1.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in} * \text{channel_multiplier}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">depthwise_conv2d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DepthwiseConv2dNative</span><span class="p">(</span><span class="n">channel_multiplier</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">depthwise_conv2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Diag">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Diag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Diag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Diag" title="Permalink to this definition"></a></dt>
<dd><p>Constructs a diagonal tensor with a given diagonal values.</p>
<p>Assume <cite>input_x</cite> has dimensions <span class="math notranslate nohighlight">\([D_1,... D_k]\)</span>, the output is a tensor of
rank 2k with dimensions <span class="math notranslate nohighlight">\([D_1,..., D_k, D_1,..., D_k]\)</span> where:
<span class="math notranslate nohighlight">\(output[i_1,..., i_k, i_1,..., i_k] = input_x[i_1,..., i_k]\)</span> and 0 everywhere else.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The input shape must be less than 5d.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diag</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Diag</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diag</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[1, 0, 0, 0],</span>
<span class="go"> [0, 2, 0, 0],</span>
<span class="go"> [0, 0, 3, 0],</span>
<span class="go"> [0, 0, 0, 4]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DiagPart">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DiagPart</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#DiagPart"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DiagPart" title="Permalink to this definition"></a></dt>
<dd><p>Extracts the diagonal part from given tensor.</p>
<p>Assume input has dimensions <span class="math notranslate nohighlight">\([D_1,..., D_k, D_1,..., D_k]\)</span>, the output is a tensor
of rank k with dimensions <span class="math notranslate nohighlight">\([D_1,..., D_k]\)</span> where:
<span class="math notranslate nohighlight">\(output[i_1,..., i_k] = input[i_1,..., i_k, i_1,..., i_k]\)</span>.</p>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input Tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor.</p>
</dd>
<dt>Examples</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diag_part</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DiagPart</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diag_part</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1, 2, 3, 4]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Div">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Div</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Div"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Div" title="Permalink to this definition"></a></dt>
<dd><p>Computes the quotient of dividing the first input tensor by the second input tensor element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - When the first input is a tensor, The second input
could be a number, a bool, or a tensor whose data type is number or bool. When the first input
is a number or a bool, the second input must be a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Div</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[-1.3, 2.5, 2.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DivNoNan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DivNoNan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#DivNoNan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DivNoNan" title="Permalink to this definition"></a></dt>
<dd><p>Computes a safe divide which returns 0 if the y is zero.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">div_no_nan</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DivNoNan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">div_no_nan</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[0., 0., 0., 2.5, 2.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Dropout">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Dropout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Dropout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Dropout" title="Permalink to this definition"></a></dt>
<dd><p>During training, randomly zeroes some of the elements of the input tensor with probability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_prob</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The keep rate, between 0 and 1, e.g. keep_prob = 0.9,
means dropping out 10% of input units.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple[int]) - The shape of target mask.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the value of generated mask for input shape.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dropout</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="ow">in</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">((</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="ow">in</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DropoutDoMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DropoutDoMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#DropoutDoMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DropoutDoMask" title="Permalink to this definition"></a></dt>
<dd><p>Applies dropout mask on the input tensor.</p>
<p>Take the mask output of DropoutGenMask as input, and apply dropout on the input.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
<li><p><strong>mask</strong> (Tensor) - The mask to be applied on <cite>input_x</cite>, which is the output of <cite>DropoutGenMask</cite>. And the
shape of <cite>input_x</cite> must be the same as the value of <cite>DropoutGenMask</cite>’s input <cite>shape</cite>. If input wrong <cite>mask</cite>,
the output of <cite>DropoutDoMask</cite> are unpredictable.</p></li>
<li><p><strong>keep_prob</strong> (Union[Tensor, float]) - The keep rate, greater than 0 and less equal than 1, e.g. keep_prob =
0.9, means dropping out 10% of input units. The value of <cite>keep_prob</cite> is the same as the input <cite>keep_prob</cite> of
<cite>DropoutGenMask</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the value that applied dropout on.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_gen_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DropoutGenMask</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_do_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DropoutDoMask</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">dropout_gen_mask</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">dropout_do_mask</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">[[[2.0, 0.0, 0.0],</span>
<span class="go">  [0.0, 0.0, 0.0]],</span>
<span class="go"> [[0.0, 0.0, 0.0],</span>
<span class="go">  [2.0, 2.0, 2.0]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DropoutGenMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DropoutGenMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#DropoutGenMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DropoutGenMask" title="Permalink to this definition"></a></dt>
<dd><p>Generates the mask value for the input shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Seed0</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed0 value for random generating. Default: 0.</p></li>
<li><p><strong>Seed1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed1 value for random generating. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple[int]) - The shape of target mask.</p></li>
<li><p><strong>keep_prob</strong> (Tensor) - The keep rate, greater than 0 and less equal than 1, e.g. keep_prob = 0.9,
means dropping out 10% of input units.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the value of generated mask for input shape.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dropout_gen_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DropoutGenMask</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">dropout_gen_mask</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
<span class="go">[249, 11, 134, 133, 143, 246, 89, 52, 169, 15, 94, 63, 146, 103, 7, 101]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DynamicRNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DynamicRNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#DynamicRNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DynamicRNN" title="Permalink to this definition"></a></dt>
<dd><p>DynamicRNN Operator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cell_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – An string identifying the cell type in the op. Default: ‘LSTM’.
Only ‘LSTM’ is currently supported.</p></li>
<li><p><strong>direction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – An string identifying the direction in the op. Default: ‘UNIDIRECTIONAL’.
Only ‘UNIDIRECTIONAL’ is currently supported.</p></li>
<li><p><strong>cell_depth</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An integer identifying the cell depth in the op. Default: 1.</p></li>
<li><p><strong>use_peephole</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – An bool identifying if use peephole in the op. Default: False.</p></li>
<li><p><strong>keep_prob</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – An float identifying the keep prob in the op. Default: 1.0.</p></li>
<li><p><strong>cell_clip</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – An float identifying the cell clip in the op. Default: -1.0.</p></li>
<li><p><strong>num_proj</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An integer identifying the num proj in the op. Default: 0.</p></li>
<li><p><strong>time_major</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – An bool identifying the time major in the op. Default: True.
Only <cite>True</cite> is currently supported.</p></li>
<li><p><strong>activation</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – An string identifying the type of activation function in the op. Default: ‘tanh’.
Only ‘tanh’ is currently supported.</p></li>
<li><p><strong>forget_bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – An float identifying the forget bias in the op. Default: 0.0.</p></li>
<li><p><strong>is_training</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – An bool identifying is training in the op. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Current words. Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>input_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>w</strong> (Tensor) - Weight. Tensor of shape (<cite>input_size + hidden_size</cite>, <cite>4 x hidden_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>b</strong> (Tensor) - Bias. Tensor of shape (<cite>4 x hidden_size</cite>).
The data type must be float16 or float32.</p></li>
<li><p><strong>seq_length</strong> (Tensor) - The length of each batch. Tensor of shape (<cite>batch_size</cite>).
Only <cite>None</cite> is currently supported.</p></li>
<li><p><strong>init_h</strong> (Tensor) - Hidden state of initial time. Tensor of shape (1, <cite>batch_size</cite>, <cite>hidden_size</cite>).</p></li>
<li><p><strong>init_c</strong> (Tensor) - Cell state of initial time. Tensor of shape (1, <cite>batch_size</cite>, <cite>hidden_size</cite>).</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>y</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
<li><p><strong>output_h</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
With data type of float16.</p></li>
<li><p><strong>output_c</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
<li><p><strong>i</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
<li><p><strong>j</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
<li><p><strong>f</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
<li><p><strong>o</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
<li><p><strong>tanhct</strong> (Tensor) - A Tensor of shape (<cite>num_step</cite>, <cite>batch_size</cite>, <cite>hidden_size</cite>).
Has the same type with input <cite>b</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">w</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_h</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_c</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dynamic_rnn</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DynamicRNN</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">init_h</span><span class="p">,</span> <span class="n">init_c</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.DynamicShape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">DynamicShape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#DynamicShape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.DynamicShape" title="Permalink to this definition"></a></dt>
<dd><p>Returns the shape of input tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[int], 1-dim Tensor of type int32</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DynamicShape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.EditDistance">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">EditDistance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#EditDistance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.EditDistance" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Levebshtein Edit Distance. It is used to measure the similarity of two sequences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>normalize</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, edit distances are normalized by length of truth. Default: True.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>hypothesis_indices</strong> (Tensor) - The indices of the hypothesis list SparseTensor. With int64 data type.
The shape of tensor is <span class="math notranslate nohighlight">\((N, R)\)</span>.</p></li>
<li><p><strong>hypothesis_values</strong> (Tensor) - The values of the hypothesis list SparseTensor.
Must be 1-D vector with length of N.</p></li>
<li><p><strong>hypothesis_shape</strong> (Tensor) - The shape of the hypothesis list SparseTensor.
Must be R-length vector with int64 data type. Only constant value is allowed.</p></li>
<li><p><strong>truth_indices</strong> (Tensor) - The indices of the truth list SparseTensor. With int64 data type.
The shape of tensor is <span class="math notranslate nohighlight">\((M, R)\)</span>.</p></li>
<li><p><strong>truth_values</strong> (Tensor) - The values of the truth list SparseTensor. Must be 1-D vector with length of M.</p></li>
<li><p><strong>truth_shape</strong> (Tensor) - The shape of the truth list SparseTensor.
Must be R-length vector with int64 data type. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a dense tensor with rank <cite>R-1</cite> and float32 data type.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">EditDistance</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hypothesis_shape</span><span class="p">,</span> <span class="n">truth_shape</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">EditDistance</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">edit_distance</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">EditDistance</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">hypothesis_shape</span> <span class="o">=</span> <span class="n">hypothesis_shape</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">truth_shape</span> <span class="o">=</span> <span class="n">truth_shape</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hypothesis_indices</span><span class="p">,</span> <span class="n">hypothesis_values</span><span class="p">,</span> <span class="n">truth_indices</span><span class="p">,</span> <span class="n">truth_values</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">edit_distance</span><span class="p">(</span><span class="n">hypothesis_indices</span><span class="p">,</span> <span class="n">hypothesis_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hypothesis_shape</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                  <span class="n">truth_indices</span><span class="p">,</span> <span class="n">truth_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">truth_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hypothesis_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hypothesis_values</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hypothesis_shape</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truth_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truth_values</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truth_shape</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">edit_distance</span> <span class="o">=</span> <span class="n">EditDistance</span><span class="p">(</span><span class="n">hypothesis_shape</span><span class="p">,</span> <span class="n">truth_shape</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">edit_distance</span><span class="p">(</span><span class="n">hypothesis_indices</span><span class="p">,</span> <span class="n">hypothesis_values</span><span class="p">,</span> <span class="n">truth_indices</span><span class="p">,</span> <span class="n">truth_values</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Elu">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Elu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Elu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Elu" title="Permalink to this definition"></a></dt>
<dd><p>Computes exponential linear: <cite>alpha * (exp(x) - 1)</cite> if x &lt; 0, <cite>x</cite> otherwise.
The data type of input tensor must be float.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The coefficient of negative factor whose type is float,
only support ‘1.0’ currently. Default: 1.0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose data type must be float.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and data type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">elu</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Elu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">elu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">Tensor([[-0.632  4.0   -0.999]</span>
<span class="go">        [2.0    -0.993  9.0  ]], shape=(2, 3), dtype=mindspore.float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.EmbeddingLookup">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">EmbeddingLookup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#EmbeddingLookup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.EmbeddingLookup" title="Permalink to this definition"></a></dt>
<dd><p>Returns a slice of input tensor based on the specified indices.</p>
<p>This Primitive has the similar functionality as GatherV2 operating on <cite>axis = 0</cite>, but has one more inputs:
<cite>offset</cite>.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_params</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
This represents a Tensor slice, instead of the entire Tensor. Currently, the dimension is restricted to be 2.</p></li>
<li><p><strong>input_indices</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.
Specifies the indices of elements of the original Tensor. Values can be out of range of <cite>input_params</cite>,
and the exceeding part will be filled with 0 in the output.</p></li>
<li><p><strong>offset</strong> (int) - Specifies the offset value of this <cite>input_params</cite> slice. Thus the real indices
are equal to <cite>input_indices</cite> minus <cite>offset</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((z_1, z_2, ..., z_N)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_params</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">],</span> <span class="p">[</span><span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">offset</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">EmbeddingLookup</span><span class="p">()(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
<span class="go">[[[10, 11], [0 ,0]], [[0, 0], [10, 11]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Eps">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Eps</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Eps"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Eps" title="Permalink to this definition"></a></dt>
<dd><p>Creates a tensor filled with <cite>input_x</cite> dtype minimum val.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input tensor. The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type and shape as <cite>input_x</cite>, but filled with <cite>input_x</cite> dtype minimum val.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Eps</span><span class="p">()(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.52587891e-05, 1.52587891e-05, 1.52587891e-05, 1.52587891e-05]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Equal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Equal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Equal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Equal" title="Permalink to this definition"></a></dt>
<dd><p>Computes the equivalence between two tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors, the shapes of them could be broadcast.
When the inputs are one tensor and one scalar, the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number]) - The first input is a number or
a tensor whose data type is number.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number]) - The second input is a number
when the first input is a tensor or a tensor whose data type is number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equal</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="go">[False, True, False]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equal</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[True, True, False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.EqualCount">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">EqualCount</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#EqualCount"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.EqualCount" title="Permalink to this definition"></a></dt>
<dd><p>Computes the number of the same elements of two tensors.</p>
<p>The two input tensors must have the same data type and shape.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The first input tensor.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The second input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the type same as input tensor and size as (1,).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equal_count</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">EqualCount</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">equal_count</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Erf">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Erf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Erf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Erf" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Gauss error function of <cite>input_x</cite> element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">erf</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erf</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">erf</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-0.8427168, 0., 0.8427168, 0.99530876, 0.99997765]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Erfc">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Erfc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Erfc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Erfc" title="Permalink to this definition"></a></dt>
<dd><p>Computes the complementary error function of <cite>input_x</cite> element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">erfc</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erfc</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">erfc</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.8427168, 0., 0.1572832, 0.00469124, 0.00002235]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Exp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Exp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Exp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Exp" title="Permalink to this definition"></a></dt>
<dd><p>Returns exponential of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The data type mast be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">exp</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[ 2.71828183,  7.3890561 , 54.59815003]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ExpandDims">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ExpandDims</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ExpandDims"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ExpandDims" title="Permalink to this definition"></a></dt>
<dd><p>Adds an additional dimension at the given axis.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the specified axis is a negative number, the index is counted
backward from the end and starts at 1.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If axis is not an integer or not in the valid range.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>axis</strong> (int) - Specifies the dimension index at which to expand
the shape of <cite>input_x</cite>. The value of axis must be in the range
<cite>[-input_x.dim()-1, input_x.dim()]</cite>. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((1, x_1, x_2, ..., x_R)\)</span> if the
value of <cite>axis</cite> is 0.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="go">[[[2.0, 2.0],</span>
<span class="go">  [2.0, 2.0]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Expm1">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Expm1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Expm1"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Expm1" title="Permalink to this definition"></a></dt>
<dd><p>Returns exponential then minus 1 of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. With float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expm1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Expm1</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">expm1</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[ 0.,  1.71828183,  6.3890561 , 53.59815003]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Eye">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Eye</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Eye"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Eye" title="Permalink to this definition"></a></dt>
<dd><p>Creates a tensor with ones on the diagonal and zeros the rest.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>n</strong> (int) - The number of rows of returned tensor</p></li>
<li><p><strong>m</strong> (int) - The number of columns of returned tensor</p></li>
<li><p><strong>t</strong> (mindspore.dtype) - MindSpore’s dtype, The data type of the returned tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a tensor with ones on the diagonal and the rest of elements are zero.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eye</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Eye</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out_tensor</span> <span class="o">=</span> <span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">[[1, 0],</span>
<span class="go"> [0, 1]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Fill">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Fill</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Fill"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Fill" title="Permalink to this definition"></a></dt>
<dd><p>Creates a tensor filled with a scalar value.</p>
<p>Creates a tensor with shape described by the first argument and fills it with values in the second argument.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>type</strong> (mindspore.dtype) - The specified type of output tensor. Only constant value is allowed.</p></li>
<li><p><strong>shape</strong> (tuple) - The specified shape of output tensor. Only constant value is allowed.</p></li>
<li><p><strong>value</strong> (scalar) - Value to fill the returned tensor. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type and shape as input value.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fill</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fill</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">[[1.0, 1.0],</span>
<span class="go"> [1.0, 1.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Flatten" title="Permalink to this definition"></a></dt>
<dd><p>Flattens a tensor without changing its batch size on the 0-th axis.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span> to be flattened.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of the output tensor is <span class="math notranslate nohighlight">\((N, X)\)</span>, where <span class="math notranslate nohighlight">\(X\)</span> is
the product of the remaining dimension.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flatten</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FloatStatus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FloatStatus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#FloatStatus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FloatStatus" title="Permalink to this definition"></a></dt>
<dd><p>Determine if the elements contain Not a Number(NaN), infinite or negative infinite. 0 for normal, 1 for overflow.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the shape of <cite>(1,)</cite>, and has the same dtype of input <cite>mindspore.dtype.float32</cite> or
<cite>mindspore.dtype.float16</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">float_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">)]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">float_status</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Floor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Floor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Floor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Floor" title="Permalink to this definition"></a></dt>
<dd><p>Round a tensor down to the closest integer element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. Its element data type must be float.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">floor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Floor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">floor</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0, 2.0, -2.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FloorDiv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FloorDiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#FloorDiv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FloorDiv" title="Permalink to this definition"></a></dt>
<dd><p>Divide the first input tensor by the second input tensor element-wise and round down to the closest integer.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">floor_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloorDiv</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">floor_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[0, 1, -1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FloorMod">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FloorMod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#FloorMod"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FloorMod" title="Permalink to this definition"></a></dt>
<dd><p>Compute the remainder of division element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool , and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">floor_mod</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloorMod</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">floor_mod</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[2, 1, 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FusedBatchNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FusedBatchNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#FusedBatchNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FusedBatchNorm" title="Permalink to this definition"></a></dt>
<dd><p>FusedBatchNorm is a BatchNorm that moving mean and moving variance will be computed instead of being loaded.</p>
<p>Batch Normalization is widely used in convolutional networks. This operation applies
Batch Normalization over input to avoid internal covariate shift as described in the
paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal
Covariate Shift</a>. It rescales and recenters the
feature using a mini-batch of data and the learned parameters which can be described
in the following formula.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is scale, <span class="math notranslate nohighlight">\(\beta\)</span> is bias, <span class="math notranslate nohighlight">\(\epsilon\)</span> is epsilon.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Mode of batch normalization, value is 0 or 1. Default: 0.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added for numerical stability. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The hyper parameter to compute moving average for running_mean and running_var
(e.g. <span class="math notranslate nohighlight">\(new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean\)</span>).
Momentum value must be [0, 1]. Default: 0.9.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C)\)</span>.</p></li>
<li><p><strong>scale</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>bias</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>mean</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>variance</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 5 Tensor, the normalized input and the updated parameters.</p>
<ul class="simple">
<li><p><strong>output_x</strong> (Tensor) - The same type and shape as the <cite>input_x</cite>.</p></li>
<li><p><strong>updated_scale</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>updated_bias</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>updated_moving_mean</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>updated_moving_variance</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variance</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FusedBatchNorm</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FusedBatchNormEx">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FusedBatchNormEx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#FusedBatchNormEx"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FusedBatchNormEx" title="Permalink to this definition"></a></dt>
<dd><p>FusedBatchNormEx is an extension of FusedBatchNorm, FusedBatchNormEx has one more output(output reserve)
than FusedBatchNorm, reserve will be used in backpropagation phase. FusedBatchNorm is a BatchNorm that
moving mean and moving variance will be computed instead of being loaded.</p>
<p>Batch Normalization is widely used in convolutional networks. This operation applies
Batch Normalization over input to avoid internal covariate shift as described in the
paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal
Covariate Shift</a>. It rescales and recenters the
feature using a mini-batch of data and the learned parameters which can be described
in the following formula.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is scale, <span class="math notranslate nohighlight">\(\beta\)</span> is bias, <span class="math notranslate nohighlight">\(\epsilon\)</span> is epsilon.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Mode of batch normalization, value is 0 or 1. Default: 0.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added for numerical stability. Default: 1e-5.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The hyper parameter to compute moving average for running_mean and running_var
(e.g. <span class="math notranslate nohighlight">\(new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean\)</span>).
Momentum value must be [0, 1]. Default: 0.9.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><strong>input_x</strong> (Tensor) - The input of FusedBatchNormEx, Tensor of shape <span class="math notranslate nohighlight">\((N, C)\)</span>,</dt><dd><p>data type: float16 or float32.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>scale</strong> (Tensor) - Parameter scale, same with gamma above-mentioned, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>,</dt><dd><p>data type: float32.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>bias</strong> (Tensor) - Parameter bias, same with beta above-mentioned, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>,</dt><dd><p>data type: float32.</p>
</dd>
</dl>
</li>
<li><p><strong>mean</strong> (Tensor) - mean value, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, data type: float32.</p></li>
<li><p><strong>variance</strong> (Tensor) - variance value, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, data type: float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 6 Tensors, the normalized input, the updated parameters and reserve.</p>
<ul class="simple">
<li><p><strong>output_x</strong> (Tensor) - The input of FusedBatchNormEx, same type and shape as the <cite>input_x</cite>.</p></li>
<li><p><strong>updated_scale</strong> (Tensor) - Updated parameter scale, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, data type: float32.</p></li>
<li><p><strong>updated_bias</strong> (Tensor) - Updated parameter bias, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, data type: float32.</p></li>
<li><p><strong>updated_moving_mean</strong> (Tensor) - Updated mean value, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, data type: float32.</p></li>
<li><dl class="simple">
<dt><strong>updated_moving_variance</strong> (Tensor) - Updated variance value, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>,</dt><dd><p>data type: float32.</p>
</dd>
</dl>
</li>
<li><p><strong>reserve</strong> (Tensor) - reserve space, Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>, data type: float32.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scale</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">variance</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">64</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FusedBatchNormEx</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FusedSparseAdam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FusedSparseAdam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#FusedSparseAdam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FusedSparseAdam" title="Permalink to this definition"></a></dt>
<dd><p>Merges the duplicate value of the gradient and then updates parameters by Adaptive Moment Estimation (Adam)
algorithm. This operator is used when the gradient is sparse.</p>
<p>The Adam algorithm is proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m = \beta_1 * m + (1 - \beta_1) * g \\
    v = \beta_2 * v + (1 - \beta_2) * g * g \\
    l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\
    w = w - l * \frac{m}{\sqrt{v} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector, <span class="math notranslate nohighlight">\(g\)</span> represents
<cite>gradient</cite>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent <cite>beta1</cite> and <cite>beta2</cite>,
<span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(beta_1^t\)</span> and <span class="math notranslate nohighlight">\(beta_2^t\)</span> represent <cite>beta1_power</cite> and
<cite>beta2_power</cite>, <span class="math notranslate nohighlight">\(\alpha\)</span> represents <cite>learning_rate</cite>, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>var</cite>, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents
<cite>epsilon</cite>.</p>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect variable tensors from being updated.
If true, updates of the var, m, and v tensors will be protected by a lock.
If false, the result is unpredictable. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.
If true, update the gradients using NAG.
If true, update the gradients without using NAG. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Parameters to be updated with float32 data type.</p></li>
<li><p><strong>m</strong> (Parameter) - The 1st moment vector in the updating formula, has the same type as <cite>var</cite> with
float32 data type.</p></li>
<li><p><strong>v</strong> (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients, has the same type as
<cite>var</cite> with float32 data type.</p></li>
<li><p><strong>beta1_power</strong> (Tensor) - <span class="math notranslate nohighlight">\(beta_1^t\)</span> in the updating formula with float32 data type.</p></li>
<li><p><strong>beta2_power</strong> (Tensor) - <span class="math notranslate nohighlight">\(beta_2^t\)</span> in the updating formula with float32 data type.</p></li>
<li><p><strong>lr</strong> (Tensor) - <span class="math notranslate nohighlight">\(l\)</span> in the updating formula. With float32 data type.</p></li>
<li><p><strong>beta1</strong> (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</p></li>
<li><p><strong>beta2</strong> (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</p></li>
<li><p><strong>epsilon</strong> (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</p></li>
<li><p><strong>gradient</strong> (Tensor) - Gradient value with float32 data type.</p></li>
<li><p><strong>indices</strong> (Tensor) - Gradient indices with int32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>m</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>v</strong> (Tensor) - A Tensor with shape (1,).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adam</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FusedSparseAdam</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                     <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1_power</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2_power</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FusedSparseFtrl">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FusedSparseFtrl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#FusedSparseFtrl"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FusedSparseFtrl" title="Permalink to this definition"></a></dt>
<dd><p>Merges the duplicate value of the gradient and then updates relevant entries according to the FTRL-proximal scheme.</p>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The learning rate value, must be positive.</p></li>
<li><p><strong>l1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l1 regularization strength, must be greater than or equal to zero.</p></li>
<li><p><strong>l2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l2 regularization strength, must be greater than or equal to zero.</p></li>
<li><p><strong>lr_power</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Learning rate power controls how the learning rate decreases during training,
must be less than or equal to zero. Use fixed learning rate if <cite>lr_power</cite> is zero.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use locks for updating operation if true . Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - The variable to be updated. The data type must be float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - The accumulation to be updated, must be same type and shape as <cite>var</cite>.</p></li>
<li><p><strong>linear</strong> (Parameter) - the linear coefficient to be updated, must be same type and shape as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices into the first dimension of <cite>var</cite> and <cite>accum</cite>. The shape
of <cite>indices</cite> must be the same as <cite>grad</cite> in first dimension. The type must be int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, this operator will update the input parameters directly, the outputs are useless.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>accum</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>linear</strong> (Tensor) - A Tensor with shape (1,).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SparseApplyFtrlNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SparseApplyFtrlNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_ftrl</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FusedSparseFtrl</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">l1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">lr_power</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_ftrl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">SparseApplyFtrlNet</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FusedSparseLazyAdam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FusedSparseLazyAdam</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#FusedSparseLazyAdam"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FusedSparseLazyAdam" title="Permalink to this definition"></a></dt>
<dd><p>Merges the duplicate value of the gradient and then updates parameters by Adaptive Moment Estimation (Adam)
algorithm. This operator is used when the gradient is sparse. The behavior is not equivalent to the
original Adam algorithm, as only the current indices parameters will be updated.</p>
<p>The Adam algorithm is proposed in <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a>.</p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    m = \beta_1 * m + (1 - \beta_1) * g \\
    v = \beta_2 * v + (1 - \beta_2) * g * g \\
    l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\
    w = w - l * \frac{m}{\sqrt{v} + \epsilon}
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(m\)</span> represents the 1st moment vector, <span class="math notranslate nohighlight">\(v\)</span> represents the 2nd moment vector, <span class="math notranslate nohighlight">\(g\)</span> represents
<cite>gradient</cite>, <span class="math notranslate nohighlight">\(l\)</span> represents scaling factor <cite>lr</cite>, <span class="math notranslate nohighlight">\(\beta_1, \beta_2\)</span> represent <cite>beta1</cite> and <cite>beta2</cite>,
<span class="math notranslate nohighlight">\(t\)</span> represents updating step while <span class="math notranslate nohighlight">\(beta_1^t\)</span> and <span class="math notranslate nohighlight">\(beta_2^t\)</span> represent <cite>beta1_power</cite> and
<cite>beta2_power</cite>, <span class="math notranslate nohighlight">\(\alpha\)</span> represents <cite>learning_rate</cite>, <span class="math notranslate nohighlight">\(w\)</span> represents <cite>var</cite>, <span class="math notranslate nohighlight">\(\epsilon\)</span> represents
<cite>epsilon</cite>.</p>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable a lock to protect variable tensors from being updated.
If true, updates of the var, m, and v tensors will be protected by a lock.
If false, the result is unpredictable. Default: False.</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.
If true, update the gradients using NAG.
If true, update the gradients without using NAG. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Parameters to be updated with float32 data type.</p></li>
<li><p><strong>m</strong> (Parameter) - The 1st moment vector in the updating formula, has the same type as <cite>var</cite> with
float32 data type.</p></li>
<li><p><strong>v</strong> (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients, has the same type as
<cite>var</cite> with float32 data type.</p></li>
<li><p><strong>beta1_power</strong> (Tensor) - <span class="math notranslate nohighlight">\(beta_1^t\)</span> in the updating formula with float32 data type.</p></li>
<li><p><strong>beta2_power</strong> (Tensor) - <span class="math notranslate nohighlight">\(beta_2^t\)</span> in the updating formula with float32 data type.</p></li>
<li><p><strong>lr</strong> (Tensor) - <span class="math notranslate nohighlight">\(l\)</span> in the updating formula with float32 data type.</p></li>
<li><p><strong>beta1</strong> (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</p></li>
<li><p><strong>beta2</strong> (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</p></li>
<li><p><strong>epsilon</strong> (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</p></li>
<li><p><strong>gradient</strong> (Tensor) - Gradient value with float32 data type.</p></li>
<li><p><strong>indices</strong> (Tensor) - Gradient indices with int32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>m</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>v</strong> (Tensor) - A Tensor with shape (1,).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_lazyadam</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FusedSparseLazyAdam</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_lazyadam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">,</span> <span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                         <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1_power</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2_power</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">beta1_power</span><span class="p">,</span> <span class="n">beta2_power</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.FusedSparseProximalAdagrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">FusedSparseProximalAdagrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#FusedSparseProximalAdagrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.FusedSparseProximalAdagrad" title="Permalink to this definition"></a></dt>
<dd><p>Merges the duplicate value of the gradient and then updates relevant entries according to the proximal adagrad
algorithm.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[\text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}\]</div>
<div class="math notranslate nohighlight">
\[var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)\]</div>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the variable and accumulation tensors will be protected from being updated.
Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. The data type must be float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - Variable tensor to be updated, has the same dtype as <cite>var</cite>.</p></li>
<li><p><strong>lr</strong> (Tensor) - The learning rate value. The data type must be float32.</p></li>
<li><p><strong>l1</strong> (Tensor) - l1 regularization strength. The data type must be float32.</p></li>
<li><p><strong>l2</strong> (Tensor) - l2 regularization strength. The data type must be float32.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient. The data type must be float32.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices into the first dimension of <cite>var</cite> and <cite>accum</cite>. The data type
must be int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, this operator will update the input parameters directly, the outputs are useless.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - A Tensor with shape (1,).</p></li>
<li><p><strong>accum</strong> (Tensor) - A Tensor with shape (1,).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_proximal_adagrad</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FusedSparseProximalAdagrad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_proximal_adagrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                                 <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Gamma">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Gamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#Gamma"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Gamma" title="Permalink to this definition"></a></dt>
<dd><p>Produces random positive floating-point values x, distributed according to probability density function:</p>
<div class="math notranslate nohighlight">
\[\text{P}(x|α,β) = \frac{\exp(-x/β)}{{β^α}\cdot{\Gamma(α)}}\cdot{x^{α-1}},\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed, must be non-negative. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2, must be non-negative. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple) - The shape of random tensor to be generated. Only constant value is allowed.</p></li>
<li><p><strong>alpha</strong> (Tensor) - The α distribution parameter. It must be greater than 0.
It is also known as the shape parameter with float32 data type.</p></li>
<li><p><strong>beta</strong> (Tensor) - The β distribution parameter. It must be greater than 0.
It is also known as the scale parameter with float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. The shape must be the broadcasted shape of Input “shape” and shapes of alpha and beta.
The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GatherNd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GatherNd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#GatherNd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GatherNd" title="Permalink to this definition"></a></dt>
<dd><p>Gathers slices from a tensor by indices.</p>
<p>Using given indices to gather slices from a tensor with a specified shape.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The target tensor to gather values.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index tensor, with int data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type as <cite>input_x</cite> and the shape is indices_shape[:-1] + x_shape[indices_shape[-1]:].</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherNd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="go">[-0.1, 0.5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GatherV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GatherV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#GatherV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GatherV2" title="Permalink to this definition"></a></dt>
<dd><p>Returns a slice of input tensor based on the specified indices and axis.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_params</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
The original Tensor.</p></li>
<li><p><strong>input_indices</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.
Specifies the indices of elements of the original Tensor. Must be in the range
<cite>[0, input_param.shape[axis])</cite>.</p></li>
<li><p><strong>axis</strong> (int) - Specifies the dimension index to gather indices.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((z_1, z_2, ..., z_N)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_params</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">42</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">22</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherV2</span><span class="p">()(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
<span class="go">[[2.0, 7.0],</span>
<span class="go"> [4.0, 54.0],</span>
<span class="go"> [2.0, 55.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GeSwitch">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GeSwitch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/control_ops.html#GeSwitch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GeSwitch" title="Permalink to this definition"></a></dt>
<dd><p>Adds control switch to data.</p>
<p>Switch data flows into false or true branch depending on the condition. If the condition is true,
the true branch will be activated, or vise verse.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong> (Union[Tensor, Number]) - The data to be used for switch control.</p></li>
<li><p><strong>pred</strong> (Tensor) - It must be a scalar whose type is bool and shape is <cite>()</cite>, It is used as condition for
switch control.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple. Output is tuple(false_output, true_output). The Elements in the tuple has the same shape of input data.
The false_output connects with the false_branch and the true_output connects with the true_branch.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">square</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="mi">3</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">switch</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GeSwitch</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">merge</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Merge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">less</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">st1</span><span class="p">,</span> <span class="n">sf1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cond</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">st2</span><span class="p">,</span> <span class="n">sf2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">cond</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">add_ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">st1</span><span class="p">,</span> <span class="n">st2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">st3</span><span class="p">,</span> <span class="n">sf3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">cond</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">sq_ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">sf3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merge</span><span class="p">((</span><span class="n">add_ret</span><span class="p">,</span> <span class="n">sq_ret</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Gelu">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Gelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Gelu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Gelu" title="Permalink to this definition"></a></dt>
<dd><p>Gaussian Error Linear Units activation function.</p>
<p>GeLU is described in the paper <a class="reference external" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>.
And also please refer to <a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>.</p>
<p>Gelu is defined as follows:</p>
<div class="math notranslate nohighlight">
\[\text{output} = 0.5 * x * (1 + erf(x / \sqrt{2})),\]</div>
<p>where <span class="math notranslate nohighlight">\(erf\)</span> is the “Gauss error function” .</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input to compute the Gelu with data type of float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gelu</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gelu</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GetNext">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GetNext</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#GetNext"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GetNext" title="Permalink to this definition"></a></dt>
<dd><p>Returns the next element in the dataset queue.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The GetNext operation needs to be associated with network and it also depends on the init_dataset interface,
it can’t be used directly as a single operation.
For details, please refer to <cite>connect_network_with_dataset</cite> source code.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>types</strong> (list[<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>]) – The type of the outputs.</p></li>
<li><p><strong>shapes</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The dimensionality of the outputs.</p></li>
<li><p><strong>output_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The output number, length of <cite>types</cite> and <cite>shapes</cite>.</p></li>
<li><p><strong>shared_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The queue name of <cite>init_dataset</cite> interface.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><p>No inputs.</p>
</dd>
<dt>Outputs:</dt><dd><p>tuple[Tensor], the output of Dataset. The shape is described in <cite>shapes</cite>
and the type is described is <cite>types</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">get_next</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GetNext</span><span class="p">([</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="p">[[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]],</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;shared_name&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">get_next</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GradOperation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GradOperation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">get_all</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_by_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens_param</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/base.html#GradOperation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GradOperation" title="Permalink to this definition"></a></dt>
<dd><p>A higher-order function which is used to generate the gradient function for the input function.</p>
<p>The gradient function generated by <cite>GradOperation</cite> higher-order function can be customized by
construction arguments.</p>
<p>Given an input function <cite>net = Net()</cite> that takes <cite>x</cite> and <cite>y</cite> as inputs, and has a parameter <cite>z</cite>,
see <cite>Net</cite> in Examples.</p>
<p>To generate a gradient function that returns gradients with respect to the first input
(see <cite>GradNetWrtX</cite> in Examples).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with default arguments:
<cite>grad_op = GradOperation()</cite>.</p></li>
<li><p>Call it with input function as argument to get the gradient function: <cite>gradient_function = grad_op(net)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs to get the gradients with respect to the first input:
<cite>grad_op(net)(x, y)</cite>.</p></li>
</ol>
</div></blockquote>
<p>To generate a gradient function that returns gradients with respect to all inputs (see <cite>GradNetWrtXY</cite> in Examples).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_all=True</cite> which
indicates getting gradients with respect to all inputs, they are <cite>x</cite> and <cite>y</cite> in example function <cite>Net()</cite>:
<cite>grad_op = GradOperation(get_all=True)</cite>.</p></li>
<li><p>Call it with input function as argument to get the gradient function: <cite>gradient_function = grad_op(net)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs to get the gradients with respect to all inputs:
<cite>gradient_function(x, y)</cite>.</p></li>
</ol>
</div></blockquote>
<p>To generate a gradient function that returns gradients with respect to given parameters
(see <cite>GradNetWithWrtParams</cite> in Examples).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_by_list=True</cite>:
<cite>grad_op = GradOperation(get_by_list=True)</cite>.</p></li>
<li><p>Construct a <cite>ParameterTuple</cite> that will be passed to the input function when constructing
<cite>GradOperation</cite> higher-order function, it will be used as a parameter filter that determine
which gradient to return: <cite>params = ParameterTuple(net.trainable_params())</cite>.</p></li>
<li><p>Call it with input function and <cite>params</cite> as arguments to get the gradient function:
<cite>gradient_function = grad_op(net, params)</cite>.</p></li>
</ol>
<p>4. Call the gradient function with input function’s inputs to get the gradients with
respect to given parameters: <cite>gradient_function(x, y)</cite>.</p>
</div></blockquote>
<p>To generate a gradient function that returns gradients with respect to all inputs and given parameters
in the format of ((dx, dy), (dz))(see <cite>GradNetWrtInputsAndParams</cite> in Examples).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_all=True</cite> and <cite>get_by_list=True</cite>:
<cite>grad_op = GradOperation(get_all=True, get_by_list=True)</cite>.</p></li>
<li><p>Construct a <cite>ParameterTuple</cite> that will be passed along input function when constructing
<cite>GradOperation</cite> higher-order function: <cite>params = ParameterTuple(net.trainable_params())</cite>.</p></li>
<li><p>Call it with input function and <cite>params</cite> as arguments to get the gradient function:
<cite>gradient_function = grad_op(net, params)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs
to get the gradients with respect to all inputs and given parameters: <cite>gradient_function(x, y)</cite>.</p></li>
</ol>
</div></blockquote>
<p>We can configure the sensitivity(gradient with respect to output) by setting <cite>sens_param</cite> as True and
passing an extra sensitivity input to the gradient function, the sensitivity input should has the
same shape and type with input function’s output(see <cite>GradNetWrtXYWithSensParam</cite> in Examples).</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_all=True</cite> and <cite>sens_param=True</cite>:
<cite>grad_op = GradOperation(get_all=True, sens_param=True)</cite>.</p></li>
<li><p>Define <cite>grad_wrt_output</cite> as <cite>sens_param</cite> which works as the gradient with respect to output:
<cite>grad_wrt_output = Tensor(np.ones([2, 2]).astype(np.float32))</cite>.</p></li>
<li><p>Call it with input function as argument to get the gradient function:
<cite>gradient_function = grad_op(net)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs and <cite>sens_param</cite> to
get the gradients with respect to all inputs:
<cite>gradient_function(x, y, grad_wrt_output)</cite>.</p></li>
</ol>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>get_all</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, get all the gradients with respect to inputs. Default: False.</p></li>
<li><p><strong>get_by_list</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, get all the gradients with respect to Parameter variables.
If get_all and get_by_list are both False, get the gradient with respect to first input.
If get_all and get_by_list are both True, get the gradients with respect to inputs and Parameter variables
at the same time in the form of ((gradients with respect to inputs),
(gradients with respect to parameters)). Default: False.</p></li>
<li><p><strong>sens_param</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to append sensitivity (gradient with respect to output) as input.
If sens_param is False, a ‘ones_like(outputs)’ sensitivity will be attached automatically.
Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The higher-order function which takes a function as argument and returns gradient function for it.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">GradNetWrtX</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Tensor(shape=[2, 3], dtype=Float32,</span>
<span class="go">[[1.4100001 1.5999999 6.6      ]</span>
<span class="go"> [1.4100001 1.5999999 6.6      ]])</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtXY</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtXY</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">GradNetWrtXY</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 3], dtype=Float32,</span>
<span class="go">[[4.5099998 2.7       3.6000001]</span>
<span class="go"> [4.5099998 2.7       3.6000001]]), Tensor(shape=[3, 3], dtype=Float32,</span>
<span class="go">[[2.6       2.6       2.6      ]</span>
<span class="go"> [1.9       1.9       1.9      ]</span>
<span class="go"> [1.3000001 1.3000001 1.3000001]]))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtXYWithSensParam</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtXYWithSensParam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_wrt_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_wrt_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">GradNetWrtXYWithSensParam</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 3], dtype=Float32,</span>
<span class="go">[[2.211     0.51      1.4900001]</span>
<span class="go"> [5.588     2.68      4.07     ]]), Tensor(shape=[3, 3], dtype=Float32,</span>
<span class="go">[[1.52       2.82       2.14      ]</span>
<span class="go"> [1.1        2.05       1.55      ]</span>
<span class="go"> [0.90000004 1.55       1.25      ]]))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWithWrtParams</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWithWrtParams</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">GradNetWithWrtParams</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">(Tensor(shape=[1], dtype=Float32, [21.536]),)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtInputsAndParams</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtInputsAndParams</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.12</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">GradNetWrtInputsAndParams</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">((Tensor(shape=[2, 3], dtype=Float32,</span>
<span class="go">[[3.52 3.9  2.6 ]</span>
<span class="go"> [3.52 3.9  2.6 ]]), Tensor(shape=[3, 3], dtype=Float32,</span>
<span class="go">[[0.6       0.6       0.6      ]</span>
<span class="go"> [1.9       1.9       1.9      ]</span>
<span class="go"> [1.3000001 1.3000001 1.3000001]])), (Tensor(shape=[1], dtype=Float32, [12.902]),))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Greater">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Greater</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Greater"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Greater" title="Permalink to this definition"></a></dt>
<dd><p>Computes the boolean value of <span class="math notranslate nohighlight">\(x &gt; y\)</span> element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">greater</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">greater</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[False, True, False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GreaterEqual">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GreaterEqual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#GreaterEqual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GreaterEqual" title="Permalink to this definition"></a></dt>
<dd><p>Computes the boolean value of <span class="math notranslate nohighlight">\(x &gt;= y\)</span> element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">greater_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">greater_equal</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[True, True, False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.HSigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">HSigmoid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#HSigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.HSigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Hard sigmoid activation function.</p>
<p>Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</p>
<p>Hard sigmoid is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th slice in the given dimension of the input Tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of HSigmoid, data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hsigmoid</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HSigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">hsigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.HSwish">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">HSwish</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#HSwish"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.HSwish" title="Permalink to this definition"></a></dt>
<dd><p>Hard swish activation function.</p>
<p>Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</p>
<p>Hard swish is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{i}\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th slice in the given dimension of the input Tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_data</strong> (Tensor) - The input of HSwish, data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_data</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hswish</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HSwish</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">hswish</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.HistogramFixedWidth">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">HistogramFixedWidth</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#HistogramFixedWidth"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.HistogramFixedWidth" title="Permalink to this definition"></a></dt>
<dd><p>Returns a rank 1 histogram counting the number of entries in values that fall into every bin. The bins are equal
width and determined by the arguments range and nbins.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dtype</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – An optional attribute. Must be one of the following types: “int32”, “int64”. Default: “int32”.</p></li>
<li><p><strong>nbins</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of histogram bins, the type is a positive integer.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Numeric Tensor. Must be one of the following types: int32, float32, float16.</p></li>
<li><p><strong>range</strong> (Tensor) - Must has the same data type as <cite>x</cite>, and the shape is [2].
x &lt;= range[0] will be mapped to hist[0], x &gt;= range[1] will be mapped to hist[-1].</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the type is int32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">15</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">range</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HistogramFixedWidth</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">range</span><span class="p">)</span>
<span class="go">[2 1 1 0 2]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.HistogramSummary">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">HistogramSummary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#HistogramSummary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.HistogramSummary" title="Permalink to this definition"></a></dt>
<dd><p>Outputs tensor to protocol buffer through histogram summary operator.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>name</strong> (str) - The name of the input variable.</p></li>
<li><p><strong>value</strong> (Tensor) - The value of tensor, and the rank of tensor must be greater than 0.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SummaryDemo</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SummaryDemo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HistogramSummary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.HookBackward">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">HookBackward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#HookBackward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.HookBackward" title="Permalink to this definition"></a></dt>
<dd><p>This operation is used as a tag to hook gradient in intermediate variables.  Note that this function
is only supported in Pynative Mode.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The hook function must be defined like <cite>hook_fn(grad) -&gt; Tensor or None</cite>,
where grad is the gradient passed to the primitive and gradient may be
modified and passed to next primitive. The difference between a hook function and
callback of InsertGradientOf is that a hook function is executed in the python
environment while callback will be parsed and added to the graph.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hook_fn</strong> (<em>Function</em>) – Python function. hook function.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (Tensor) - The variable to hook.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">grad_out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad_all</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hook</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">HookBackward</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">hook_test</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">z</span> <span class="o">=</span> <span class="n">hook</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">grad_all</span><span class="p">(</span><span class="n">hook_test</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">backward</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.HyperMap">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">HyperMap</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ops</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/base.html#HyperMap"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.HyperMap" title="Permalink to this definition"></a></dt>
<dd><p>Hypermap will apply the set operation to input sequences.</p>
<p>Apply the operations to every elements of the sequence or nested sequence. Different
from <cite>Map</cite>, the <cite>HyperMap</cite> supports to apply on nested structure.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>ops</strong> (<em>Union</em><em>[</em><a class="reference internal" href="#mindspore.ops.MultitypeFuncGraph" title="mindspore.ops.MultitypeFuncGraph"><em>MultitypeFuncGraph</em></a><em>, </em><em>None</em><em>]</em>) – <cite>ops</cite> is the operation to apply. If <cite>ops</cite> is <cite>None</cite>,
the operations should be put in the first input of the instance.</p>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul>
<li><p><strong>args</strong> (Tuple[sequence]) - If <cite>ops</cite> is <cite>None</cite>, all the inputs should be sequences with the same length.
And each row of the sequences will be the inputs of the operation.</p>
<p>If <cite>ops</cite> is not <cite>None</cite>, the first input is the operation, and the others are inputs.</p>
</li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Sequence or nested sequence, the sequence of output after applying the function.
e.g. <cite>operation(args[0][i], args[1][i])</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nest_tensor_list</span> <span class="o">=</span> <span class="p">((</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># square all the tensor in the nested list</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@square</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">square_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">common_map</span> <span class="o">=</span> <span class="n">HyperMap</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">common_map</span><span class="p">(</span><span class="n">square</span><span class="p">,</span> <span class="n">nest_tensor_list</span><span class="p">)</span>
<span class="go">((Tensor(shape=[], dtype=Float32, 1), Tensor(shape=[], dtype=Float32, 4)),</span>
<span class="go">(Tensor(shape=[], dtype=Float32, 9), Tensor(shape=[], dtype=Float32, 16))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square_map</span> <span class="o">=</span> <span class="n">HyperMap</span><span class="p">(</span><span class="n">square</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square_map</span><span class="p">(</span><span class="n">nest_tensor_list</span><span class="p">)</span>
<span class="go">((Tensor(shape=[], dtype=Float32, 1), Tensor(shape=[], dtype=Float32, 4)),</span>
<span class="go">(Tensor(shape=[], dtype=Float32, 9), Tensor(shape=[], dtype=Float32, 16))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IFMR">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IFMR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#IFMR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IFMR" title="Permalink to this definition"></a></dt>
<dd><p>The TFMR(Input Feature Map Reconstruction).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min_percentile</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Min init percentile.</p></li>
<li><p><strong>max_percentile</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Max init percentile.</p></li>
<li><p><strong>Union</strong><strong>[</strong><strong>list</strong> (<em>search_range</em>) – Range of searching.</p></li>
<li><p><strong>search_step</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Step size of searching.</p></li>
<li><p><strong>with_offset</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether using offset.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>data</strong> (Tensor) - A Tensor of feature map. With float16 or float32 data type.</p></li>
<li><p><strong>data_min</strong> (Tensor) - A Tensor of min value of feature map, the shape is <span class="math notranslate nohighlight">\((1)\)</span>.
With float16 or float32 data type.</p></li>
<li><p><strong>data_max</strong> (Tensor) - A Tensor of max value of feature map, the shape is <span class="math notranslate nohighlight">\((1)\)</span>.
With float16 or float32 data type.</p></li>
<li><p><strong>cumsum</strong> (Tensor) - A <cite>1-D</cite> Tensor of cumsum bin of data. With int32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>scale</strong> (Tensor) - A tensor of optimal scale, the shape is <span class="math notranslate nohighlight">\((1)\)</span>. Data dtype is float32.</p></li>
<li><p><strong>offset</strong> (Tensor) - A tensor of optimal offset, the shape is <span class="math notranslate nohighlight">\((1)\)</span>. Data dtype is float32.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_min</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_max</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cumsum</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ifmr</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IFMR</span><span class="p">(</span><span class="n">min_percentile</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">max_percentile</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">search_range</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span>
<span class="go">                  search_step=1.0, with_offset=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">ifmr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data_min</span><span class="p">,</span> <span class="n">data_max</span><span class="p">,</span> <span class="n">cumsum</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IOU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IOU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#IOU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IOU" title="Permalink to this definition"></a></dt>
<dd><p>Calculates intersection over union for boxes.</p>
<p>Computes the intersection over union (IOU) or the intersection over foreground (IOF) based on the ground-truth and
predicted regions.</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{IOU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}\\\text{IOF} = \frac{\text{Area of Overlap}}{\text{Area of Ground Truth}}\end{aligned}\end{align} \]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<em>string</em>) – The mode is used to specify the calculation method,
now supporting ‘iou’ (intersection over union) or ‘iof’
(intersection over foreground) mode. Default: ‘iou’.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>anchor_boxes</strong> (Tensor) - Anchor boxes, tensor of shape (N, 4). “N” indicates the number of anchor boxes,
and the value “4” refers to “x0”, “y0”, “x1”, and “y1”. Data type must be float16 or float32.</p></li>
<li><p><strong>gt_boxes</strong> (Tensor) - Ground truth boxes, tensor of shape (M, 4). “M” indicates the number of ground
truth boxes, and the value “4” refers to “x0”, “y0”, “x1”, and “y1”. Data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the ‘iou’ values, tensor of shape (M, N), with the same data type as <cite>anchor_boxes</cite>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#KeyError" title="(in Python v3.8)"><strong>KeyError</strong></a> – When <cite>mode</cite> is not ‘iou’ or ‘iof’.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">iou</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IOU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">anchor_boxes</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gt_boxes</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iou</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_boxes</span><span class="p">)</span>
<span class="go">[[0.0, 65504, 65504],</span>
<span class="go"> [0.0, 0.0, 0.0],</span>
<span class="go"> [0.22253, 0.0, 0.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ImageSummary">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ImageSummary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#ImageSummary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ImageSummary" title="Permalink to this definition"></a></dt>
<dd><p>Outputs image tensor to protocol buffer through image summary operator.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>name</strong> (str) - The name of the input variable, it must not be an empty string.</p></li>
<li><p><strong>value</strong> (Tensor) - The value of image, the rank of tensor must be 4.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ImageSummary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;image&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.InTopK">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">InTopK</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#InTopK"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.InTopK" title="Permalink to this definition"></a></dt>
<dd><p>Whether the targets are in the top <cite>k</cite> predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>k</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Specifies the number of top elements to be used for computing precision.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x1</strong> (Tensor) - A 2D Tensor defines the predictions of a batch of samples with float16 or float32 data type.</p></li>
<li><p><strong>x2</strong> (Tensor) - A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of x2
must be equal to x1’s first dimension. The values of <cite>x2</cite> can not be negative and
must be equal to or less than index of x1’s second dimension.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor has 1 dimension of type bool and the same shape with <cite>x2</cite>. For labeling sample <cite>i</cite> in <cite>x2</cite>,
if the label in the first <cite>k</cite> predictions for sample <cite>i</cite> is in <cite>x1</cite>, then the value is True, otherwise False.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">in_top_k</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">InTopK</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">in_top_k</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="go">[True  False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.InplaceAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">InplaceAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#InplaceAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.InplaceAdd" title="Permalink to this definition"></a></dt>
<dd><p>Adds v into specified rows of x. Computes y = x; y[i,] += v.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indices</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – Indices into the left-most dimension of x, and determines which rows of x
to add with v. It is an integer or a tuple, whose value is in [0, the first dimension size of x).</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The first input is a tensor whose data type is float16, float32 or int32.</p></li>
<li><p><strong>input_v</strong> (Tensor) - The second input is a tensor that has the same dimension sizes as x except
the first dimension, which must be the same as indices’s size. It has the same data type with <cite>input_x</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as input_x.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_v</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inplaceAdd</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">InplaceAdd</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inplaceAdd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_v</span><span class="p">)</span>
<span class="go">[[1.5 3.]</span>
<span class="go"> [4. 5.5]</span>
<span class="go"> [5. 6.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.InplaceSub">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">InplaceSub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#InplaceSub"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.InplaceSub" title="Permalink to this definition"></a></dt>
<dd><p>Subtracts v into specified rows of x. Computes y = x; y[i, :] -= v.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indices</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – Indices into the left-most dimension of x, and determines which rows of x
to subtract with v. It is a int or tuple, whose value is in [0, the first dimension size of x).</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The first input is a tensor whose data type is float16, float32 or int32.</p></li>
<li><p><strong>input_v</strong> (Tensor) - The second input is a tensor who has the same dimension sizes as x except
the first dimension, which must be the same as indices’s size. It has the same data type with <cite>input_x</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as input_x.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_v</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inplaceSub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">InplaceSub</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inplaceSub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_v</span><span class="p">)</span>
<span class="go">[[0.5 1.]</span>
<span class="go"> [2. 2.5]</span>
<span class="go"> [5. 6.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.InplaceUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">InplaceUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#InplaceUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.InplaceUpdate" title="Permalink to this definition"></a></dt>
<dd><p>Updates specified rows with values in <cite>v</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>indices</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – Indices into the left-most dimension of <cite>x</cite>, and determines which rows of x
to update with v. It is a int or tuple, whose value is in [0, the first dimension size of x).</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - A tensor which to be inplace updated. It can be one of the following data types:
float32, float16 and int32.</p></li>
<li><p><strong>v</strong> (Tensor) - A tensor with the same type as <cite>x</cite> and the same dimension size as <cite>x</cite> except
the first dimension, which must be the same as the size of <cite>indices</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the input <cite>x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inplace_update</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">InplaceUpdate</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">inplace_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="go">[[0.5, 1.0],</span>
<span class="go"> [1.0, 1.5],</span>
<span class="go"> [5.0, 6.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.InsertGradientOf">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">InsertGradientOf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#InsertGradientOf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.InsertGradientOf" title="Permalink to this definition"></a></dt>
<dd><p>Attaches callback to graph node that will be invoked on the node’s gradient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>f</strong> (<em>Function</em>) – MindSpore’s Function. Callback function.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Any) - The graph node to attach to.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, returns <cite>input_x</cite> directly. <cite>InsertGradientOf</cite> does not affect the forward result.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">clip_gradient</span><span class="p">(</span><span class="n">dx</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">ret</span> <span class="o">=</span> <span class="n">dx</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">ret</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ret</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">ret</span> <span class="o">&lt;</span> <span class="mf">0.2</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ret</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">ret</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clip</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">InsertGradientOf</span><span class="p">(</span><span class="n">clip_gradient</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad_all</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">InsertGradientOfClipDemo</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">clip_test</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">y</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">c</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">c</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@ms_function</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">clip_test</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">fd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_all</span><span class="p">(</span><span class="n">clip_test</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;forward: &quot;</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;clip_gradient:&quot;</span><span class="p">,</span> <span class="n">fd</span><span class="p">(</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Inv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Inv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Inv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Inv" title="Permalink to this definition"></a></dt>
<dd><p>Computes Inv(Reciprocal) of input tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
Must be one of the following types: float16, float32, int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and data type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inv</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Inv</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.31</span><span class="p">,</span> <span class="mf">0.52</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[4., 2.5, 3.2258065, 1.923077]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Invert">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Invert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Invert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Invert" title="Permalink to this definition"></a></dt>
<dd><p>Flips all bits of input tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[int16], Tensor[uint16]) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">invert</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Invert</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">invert</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-26, -5, -14, -10]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.InvertPermutation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">InvertPermutation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#InvertPermutation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.InvertPermutation" title="Permalink to this definition"></a></dt>
<dd><p>Computes the inverse of an index permutation.</p>
<p>Given a tuple input, this operation inserts a dimension of 1 at the dimension
This operation calculates the inverse of the index replacement. It requires a
1-dimensional tuple x, which represents the array starting at zero,
and swaps each value with its index position. In other words, for the output
tuple y and the input tuple x, this operation calculates the following:
<span class="math notranslate nohighlight">\(y[x[i]] = i, \quad i \in [0, 1, \ldots, \text{len}(x)-1]\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These values must include 0. There must be no duplicate values and the
values can not be negative.</p>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union(tuple[int], list[int]) - The input is constructed by multiple
integers, i.e., <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span> representing the indices.
The values must include 0. There can be no duplicate values or negative values.
Only constant value is allowed. The maximum value msut be equal to length of input_x.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[int]. It has the same length as the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">invert</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">InvertPermutation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">invert</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IsFinite">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IsFinite</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#IsFinite"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IsFinite" title="Permalink to this definition"></a></dt>
<dd><p>Judge which elements are finite for each position.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape of input, and the dtype is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">is_finite</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsFinite</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">)]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">is_finite</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[False   True   False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IsInf">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IsInf</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#IsInf"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IsInf" title="Permalink to this definition"></a></dt>
<dd><p>Judging which elements are inf or -inf for each position</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape of input, and the dtype is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">is_inf</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsInf</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">)]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">is_inf</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IsInstance">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IsInstance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#IsInstance"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IsInstance" title="Permalink to this definition"></a></dt>
<dd><p>Checks whether an object is an instance of a target type.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inst</strong> (Any Object) - The instance to be checked. Only constant value is allowed.</p></li>
<li><p><strong>type_</strong> (mindspore.dtype) - The target type. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>bool, the check result.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsInstance</span><span class="p">()(</span><span class="n">a</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IsNan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IsNan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#IsNan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IsNan" title="Permalink to this definition"></a></dt>
<dd><p>Judge which elements are nan for each position.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape of input, and the dtype is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">is_nan</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsNan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">)]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">is_nan</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.IsSubClass">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">IsSubClass</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#IsSubClass"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.IsSubClass" title="Permalink to this definition"></a></dt>
<dd><p>Checks whether one type is subtraction class of another type.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>sub_type</strong> (mindspore.dtype) - The type to be checked. Only constant value is allowed.</p></li>
<li><p><strong>type_</strong> (mindspore.dtype) - The target type. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>bool, the check result.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsSubClass</span><span class="p">()(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>  <span class="n">mindspore</span><span class="o">.</span><span class="n">intc</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.KLDivLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">KLDivLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#KLDivLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.KLDivLoss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the Kullback-Leibler divergence between the target and the output.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sets input as <span class="math notranslate nohighlight">\(x\)</span>, input label as <span class="math notranslate nohighlight">\(y\)</span>, output as <span class="math notranslate nohighlight">\(\ell(x, y)\)</span>.
Let,</p>
<div class="math notranslate nohighlight">
\[L = \{l_1,\dots,l_N\}^\top, \quad
l_n = y_n \cdot (\log y_n - x_n)\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ell(x, y) = \begin{cases}
L, &amp; \text{if reduction} = \text{`none';}\\
\operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean';}\\
\operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum'.}
\end{cases}\end{split}\]</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the reduction to be applied to the output.
Its value must be one of ‘none’, ‘mean’, ‘sum’. Default: ‘mean’.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input Tensor. The data type must be float32.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The label Tensor which has the same shape as <cite>input_x</cite>. The data type must be float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor or Scalar, if <cite>reduction</cite> is ‘none’, then output is a tensor and has the same shape as <cite>input_x</cite>.
Otherwise it is a scalar.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">kldiv_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">KLDivLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kldiv_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">result</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.L2Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">L2Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#L2Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.L2Loss" title="Permalink to this definition"></a></dt>
<dd><p>Calculates half of the L2 norm of a tensor without using the <cite>sqrt</cite>.</p>
<p>Set <cite>input_x</cite> as x and output as loss.</p>
<div class="math notranslate nohighlight">
\[loss = sum(x ** 2) / nelement(x)\]</div>
<p><span class="math notranslate nohighlight">\(nelement(x)\)</span> represents the number of <cite>input_x</cite>.</p>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - A input Tensor. Data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as <cite>input_x</cite>. The output tensor is the value of loss which is a scalar tensor.</p>
</dd>
<dt>Examples</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l2_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l2_loss</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">7.0</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.L2Normalize">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">L2Normalize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#L2Normalize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.L2Normalize" title="Permalink to this definition"></a></dt>
<dd><p>L2 normalization Operator.</p>
<p>This operator will normalize the input using the given axis. The function is shown as follows:</p>
<div class="math notranslate nohighlight">
\[\text{output} = \frac{x}{\sqrt{\text{max}(\text{sum} (\text{input_x}^2), \epsilon)}},\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is epsilon.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The starting axis for the input to apply the L2 normalization. Default: 0.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added for numerical stability. Default: 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input to compute the normalization. Data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l2_normalize</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">L2Normalize</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">l2_normalize</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[[-0.47247353   -0.30934513   -0.4991462   0.8185567 ]</span>
<span class="go">  [-0.08070751   -0.9961299    -0.5741758   0.09262337]</span>
<span class="go">  [-0.9916556    -0.3049123     0.5730487  -0.40579924]</span>
<span class="go"> [[-0.88134485    0.9509498    -0.86651784  0.57442576]</span>
<span class="go">  [ 0.99673784    0.08789381   -0.8187321   0.9957012 ]</span>
<span class="go">  [ 0.12891524   -0.9523804    -0.81952125  0.91396334]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LARSUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LARSUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#LARSUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LARSUpdate" title="Permalink to this definition"></a></dt>
<dd><p>Conducts lars (layer-wise adaptive rate scaling) update on the sum of squares of gradient.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Term added to the denominator to improve numerical stability. Default: 1e-05.</p></li>
<li><p><strong>hyperpara</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Trust coefficient for calculating the local learning rate. Default: 0.001.</p></li>
<li><p><strong>use_clip</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to use clip operation for calculating the local learning rate. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>weight</strong> (Tensor) - The weight to be updated.</p></li>
<li><p><strong>gradient</strong> (Tensor) - The gradient of weight, which has the same shape and dtype with weight.</p></li>
<li><p><strong>norm_weight</strong> (Tensor) - A scalar tensor, representing the sum of squares of weight.</p></li>
<li><p><strong>norm_gradient</strong> (Tensor) - A scalar tensor, representing the sum of squares of gradient.</p></li>
<li><p><strong>weight_decay</strong> (Union[Number, Tensor]) - Weight decay. It must be a scalar tensor or number.</p></li>
<li><p><strong>learning_rate</strong> (Union[Number, Tensor]) - Learning rate. It must be a scalar tensor or number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, represents the new gradient.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lars</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LARSUpdate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w_square_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_square_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">gradient</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lars</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">w_square_sum</span><span class="p">,</span> <span class="n">grad_square_sum</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_t</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms_output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">gradient</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LRN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LRN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#LRN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LRN" title="Permalink to this definition"></a></dt>
<dd><p>Local Response Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>depth_radius</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Half-width of the 1-D normalization window with the shape of 0-D.</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – An offset (usually positive to avoid dividing by 0).</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A scale factor, usually positive.</p></li>
<li><p><strong>beta</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – An exponent.</p></li>
<li><p><strong>norm_region</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies normalization region. Options: “ACROSS_CHANNELS”. Default: “ACROSS_CHANNELS”.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - A 4D Tensor with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same shape and data type as the input tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lrn</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LRN</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lrn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LSTM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LSTM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#LSTM"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LSTM" title="Permalink to this definition"></a></dt>
<dd><p>Performs the long short term memory(LSTM) on the input.</p>
<p>For detailed information, please refer to <cite>nn.LSTM</cite>.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#LayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LayerNorm" title="Permalink to this definition"></a></dt>
<dd><p>Applies the Layer Normalization to the input tensor.</p>
<p>This operator will normalize the input tensor on given axis. LayerNorm is described in the paper
<a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma\)</span> is scale, <span class="math notranslate nohighlight">\(\beta\)</span> is bias, <span class="math notranslate nohighlight">\(\epsilon\)</span> is epsilon.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>begin_norm_axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The begin axis of the <cite>input_x</cite> to apply LayerNorm,
the value must be in [-1, rank(input)). Default: 1.</p></li>
<li><p><strong>begin_params_axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The begin axis of the parameter input (<cite>gamma</cite>, <cite>beta</cite>) to
apply LayerNorm, the value must be in [-1, rank(input)). Default: 1.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A value added to the denominator for numerical stability. Default: 1e-7.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.
The input of LayerNorm.</p></li>
<li><p><strong>gamma</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((P_0, \ldots, P_\text{begin_params_axis})\)</span>.
The learnable parameter <cite>gamma</cite> as the scale on norm.</p></li>
<li><p><strong>beta</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((P_0, \ldots, P_\text{begin_params_axis})\)</span>.
The learnable parameter <cite>beta</cite> as the scale on norm.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</p>
<ul class="simple">
<li><p><strong>output_x</strong> (Tensor) - The normalized input, has the same type and shape as the <cite>input_x</cite>.
The shape is <span class="math notranslate nohighlight">\((N, C)\)</span>.</p></li>
<li><p><strong>mean</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
<li><p><strong>variance</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((C,)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="go">([[-0.22474492, 1., 2.2247488], [-0.22474492, 1., 2.2247488]],</span>
<span class="go"> [[2.], [2.]], [[0.6666667], [0.6666667]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Less">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Less</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Less"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Less" title="Permalink to this definition"></a></dt>
<dd><p>Computes the boolean value of <span class="math notranslate nohighlight">\(x &lt; y\)</span> element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">less</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">less</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[False, False, True]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LessEqual">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LessEqual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#LessEqual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LessEqual" title="Permalink to this definition"></a></dt>
<dd><p>Computes the boolean value of <span class="math notranslate nohighlight">\(x &lt;= y\)</span> element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool , and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">less_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">less_equal</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[True, False, True]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Log">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Log"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Log" title="Permalink to this definition"></a></dt>
<dd><p>Returns the natural logarithm of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. With float16 or float32 data type. The value must be greater than 0.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.0, 0.69314718, 1.38629436]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Log1p">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Log1p</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Log1p"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Log1p" title="Permalink to this definition"></a></dt>
<dd><p>Returns the natural logarithm of one plus the input tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. With float16 or float32 data type. The value must be greater than -1.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log1p</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log1p</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log1p</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.6931472, 1.0986123, 1.609438]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#LogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LogSoftmax" title="Permalink to this definition"></a></dt>
<dd><p>Log Softmax activation function.</p>
<p>Applies the Log Softmax function to the input tensor on the specified axis.
Suppose a slice in the given aixs, <span class="math notranslate nohighlight">\(x\)</span> for each element <span class="math notranslate nohighlight">\(x_i\)</span>,
the Log Softmax function is shown as follows:</p>
<div class="math notranslate nohighlight">
\[\text{output}(x_i) = \log \left(\frac{exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the length of the Tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The axis to perform the Log softmax operation. Default: -1.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - The input of Log Softmax, with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the logits.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_softmax</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_softmax</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-4.4519143, -3.4519143, -2.4519143, -1.4519144, -0.4519144]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LogicalAnd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LogicalAnd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#LogicalAnd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LogicalAnd" title="Permalink to this definition"></a></dt>
<dd><p>Computes the “logical AND” of two tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one bool.
When the inputs are two tensors, the shapes of them could be broadcast,
and the data types of them must be bool.
When the inputs are one tensor and one bool, the bool object could only be a constant,
and the data type of the tensor must be bool.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type is bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or
a tensor whose data type is bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logical_and</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalAnd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logical_and</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[True, False, False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LogicalNot">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LogicalNot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#LogicalNot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LogicalNot" title="Permalink to this definition"></a></dt>
<dd><p>Computes the “logical NOT” of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose dtype is bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the <cite>input_x</cite>, and the dtype is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logical_not</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalNot</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logical_not</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[False, True, False]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.LogicalOr">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">LogicalOr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#LogicalOr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.LogicalOr" title="Permalink to this definition"></a></dt>
<dd><p>Computes the “logical OR” of two tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one bool.
When the inputs are two tensors, the shapes of them could be broadcast,
and the data types of them must be bool.
When the inputs are one tensor and one bool, the bool object could only be a constant,
and the data type of the tensor must be bool.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type is bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or
a tensor whose data type is bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logical_or</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalOr</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logical_or</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[True, True, True]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.MakeRefKey">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">MakeRefKey</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#MakeRefKey"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MakeRefKey" title="Permalink to this definition"></a></dt>
<dd><p>Makes a RefKey instance by string. RefKey stores the name of Parameter, can be passed through the functions,
and used for Assign target.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>tag</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Parameter name to make the RefKey.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><p>No inputs.</p>
</dd>
<dt>Outputs:</dt><dd><p>RefKeyType, made from the Parameter name.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">make_ref_key</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MakeRefKey</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">make_ref_key</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ref</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_ref</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">ref</span> <span class="o">*</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.MatMul">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">MatMul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#MatMul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MatMul" title="Permalink to this definition"></a></dt>
<dd><p>Multiplies matrix <cite>a</cite> by matrix <cite>b</cite>.</p>
<p>The rank of input tensors must be <cite>2</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transpose_a</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, <cite>a</cite> is transposed before multiplication. Default: False.</p></li>
<li><p><strong>transpose_b</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, <cite>b</cite> is transposed before multiplication. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The first tensor to be multiplied. The shape of the tensor is <span class="math notranslate nohighlight">\((N, C)\)</span>. If
<cite>transpose_a</cite> is True, its shape must be <span class="math notranslate nohighlight">\((N, C)\)</span> after transposing.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The second tensor to be multiplied. The shape of the tensor is <span class="math notranslate nohighlight">\((C, M)\)</span>. If
<cite>transpose_b</cite> is True, its shape must be <span class="math notranslate nohighlight">\((C, M)\)</span> after transpose.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of the output tensor is <span class="math notranslate nohighlight">\((N, M)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.MaxPool">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">MaxPool</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#MaxPool"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MaxPool" title="Permalink to this definition"></a></dt>
<dd><p>Max pooling operation.</p>
<p>Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</p>
<p>Typically the input is of shape <span class="math notranslate nohighlight">\((N_{in}, C_{in}, H_{in}, W_{in})\)</span>, MaxPool outputs
regional maximum in the <span class="math notranslate nohighlight">\((H_{in}, W_{in})\)</span>-dimension. Given kernel size
<span class="math notranslate nohighlight">\(ks = (h_{ker}, w_{ker})\)</span> and stride <span class="math notranslate nohighlight">\(s = (s_0, s_1)\)</span>, the operation is as follows.</p>
<div class="math notranslate nohighlight">
\[\text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}
\text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ksize</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of kernel used to take the maximum value,
is an int number that represents height and width are both ksize, or a tuple
of two int numbers that represent height and width respectively. Default: 1.</p></li>
<li><p><strong>strides</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The optional value for pad mode, is “same” or “valid”, not case sensitive.
Default: “valid”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible.
Otherwise, the last extra padding will be done from the bottom and the right side.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output
will be returned without padding. Extra pixels will be discarded.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maxpool_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">maxpool_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.MaxPoolWithArgmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">MaxPoolWithArgmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ksize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strides</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'valid'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#MaxPoolWithArgmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MaxPoolWithArgmax" title="Permalink to this definition"></a></dt>
<dd><p>Perform max pooling on the input Tensor and return both max values and indices.</p>
<p>Typically the input is of shape <span class="math notranslate nohighlight">\((N_{in}, C_{in}, H_{in}, W_{in})\)</span>, MaxPool outputs
regional maximum in the <span class="math notranslate nohighlight">\((H_{in}, W_{in})\)</span>-dimension. Given kernel size
<span class="math notranslate nohighlight">\(ks = (h_{ker}, w_{ker})\)</span> and stride <span class="math notranslate nohighlight">\(s = (s_0, s_1)\)</span>, the operation is as follows.</p>
<div class="math notranslate nohighlight">
\[\text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}
\text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ksize</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The size of kernel used to take the maximum value and arg value,
is an int number that represents height and width are both ksize, or a tuple of
two int numbers that represent height and width respectively. Default: 1.</p></li>
<li><p><strong>strides</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>]</em>) – The distance of kernel moving, an int number that represents
the height and width of movement are both strides, or a tuple of two int numbers that
represent height and width of movement respectively. Default: 1.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>The optional value for pad mode, is “same” or “valid”, not case sensitive.
Default: “valid”.</p>
<ul>
<li><p>same: Adopts the way of completion. The height and width of the output will be the same as
the input. The total number of padding will be calculated in horizontal and vertical
directions and evenly distributed to top and bottom, left and right if possible.
Otherwise, the last extra padding will be done from the bottom and the right side.</p></li>
<li><p>valid: Adopts the way of discarding. The possible largest height and width of output
will be returned without padding. Extra pixels will be discarded.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((N, C_{in}, H_{in}, W_{in})\)</span>.
Data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) -  Maxpooling result, with shape <span class="math notranslate nohighlight">\((N, C_{out}, H_{out}, W_{out})\)</span>.</p></li>
<li><p><strong>mask</strong> (Tensor) -  Max values’ index represented by the mask.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maxpool_arg_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MaxPoolWithArgmax</span><span class="p">(</span><span class="n">padding</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">argmax</span> <span class="o">=</span> <span class="n">maxpool_arg_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Maximum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Maximum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Maximum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Maximum" title="Permalink to this definition"></a></dt>
<dd><p>Computes the maximum of input tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maximum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maximum</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[4.0, 5.0, 6.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Merge">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Merge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/control_ops.html#Merge"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Merge" title="Permalink to this definition"></a></dt>
<dd><p>Merges all input data to one.</p>
<p>One and only one of the inputs must be selected as the output</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>inputs</strong> (Union(Tuple, List)) - The data to be merged. All tuple elements must have the same data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple. Output is tuple(<cite>data</cite>, <cite>output_index</cite>). The <cite>data</cite> has the same shape of <cite>inputs</cite> element.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">merge</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Merge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">merge</span><span class="p">((</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Minimum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Minimum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Minimum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Minimum" title="Permalink to this definition"></a></dt>
<dd><p>Computes the minimum of input tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">minimum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">minimum</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[1.0, 2.0, 3.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.MirrorPad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">MirrorPad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#MirrorPad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MirrorPad" title="Permalink to this definition"></a></dt>
<dd><p>Pads the input tensor according to the paddings and mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies the padding mode. The optional values are “REFLECT” and “SYMMETRIC”.
Default: “REFLECT”.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
<li><p><strong>paddings</strong> (Tensor) - The paddings tensor. The value of <cite>paddings</cite> is a matrix(list),
and its shape is (N, 2). N is the rank of input data. All elements of paddings
are int type. For the input in the <cite>D</cite> th dimension, paddings[D, 0] indicates how many sizes to be
extended ahead of the input tensor in the <cite>D</cite> th dimension, and paddings[D, 1] indicates how many sizes to
be extended behind the input tensor in the <cite>D</cite> th dimension.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the tensor after padding.</p>
<ul class="simple">
<li><p>If <cite>mode</cite> is “REFLECT”, it uses a way of symmetrical copying through the axis of symmetry to fill in.
If the <cite>input_x</cite> is [[1,2,3],[4,5,6],[7,8,9]] and <cite>paddings</cite> is [[1,1],[2,2]], then the
Outputs is [[6,5,4,5,6,5,4],[3,2,1,2,3,2,1],[6,5,4,5,6,5,4],[9,8,7,8,9,8,7],[6,5,4,5,6,5,4]].</p></li>
<li><p>If <cite>mode</cite> is “SYMMETRIC”, the filling method is similar to the “REFLECT”. It is also copied
according to the symmetry axis, except that it includes the symmetry axis. If the <cite>input_x</cite>
is [[1,2,3],[4,5,6],[7,8,9]] and <cite>paddings</cite> is [[1,1],[2,2]], then the Outputs is
[[2,1,1,2,3,3,2],[2,1,1,2,3,3,2],[5,4,4,5,6,6,5],[8,7,7,8,9,9,8],[8,7,7,8,9,9,8]].</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">pad</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MirrorPad</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;REFLECT&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paddings</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms_output</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">paddings</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Mod">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Mod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Mod"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Mod" title="Permalink to this definition"></a></dt>
<dd><p>Computes the remainder of dividing the first input tensor by the second input tensor element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors,
both dtypes cannot be bool, and the shapes of them could be broadcast. When the inputs are one tensor
and one scalar, the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number]) - The first input is a number or a tensor whose data type is number.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number]) - When the first input is a tensor, The second input
could be a number or a tensor whose data type is number. When the first input is a number,
the second input must be a tensor whose data type is number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – When <cite>input_x</cite> and <cite>input_y</cite> are not the same dtype.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mod</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mod</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mod</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Mul">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Mul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Mul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Mul" title="Permalink to this definition"></a></dt>
<dd><p>Multiplies two tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[4, 10, 18]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Multinomial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Multinomial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#Multinomial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Multinomial" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor sampled from the multinomial probability distribution located in the corresponding
row of tensor input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The rows of input do not need to sum to one (in which case we use the values as weights),
but must be non-negative, finite and have a non-zero sum.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed data is used as entropy source for Random number engines to generate pseudo-random numbers.
Must be non-negative. Default: 0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><strong>input</strong> (Tensor[float32]) - the input tensor containing the cumsum of probabilities, must be 1 or 2</dt><dd><p>dimensions.</p>
</dd>
</dl>
</li>
<li><p><strong>num_samples</strong> (int32) - number of samples to draw.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor with the same rows as input, each row has num_samples sampled indices.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multinomial</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Multinomial</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">multinomial</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.MultitypeFuncGraph">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">MultitypeFuncGraph</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">read_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/base.html#MultitypeFuncGraph"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MultitypeFuncGraph" title="Permalink to this definition"></a></dt>
<dd><p>Generate overloaded functions.</p>
<p>MultitypeFuncGraph is a class used to generate overloaded functions, considering different types as inputs.
Initialize an <cite>MultitypeFuncGraph</cite> object with name, and use <cite>register</cite> with input types as the decorator
for the function to be registed. And the object can be called with different types of inputs,
and work with <cite>HyperMap</cite> and <cite>Map</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Operator name.</p></li>
<li><p><strong>read_value</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If the registered function not need to set value on Parameter,
and all inputs will pass by value, set <cite>read_value</cite> to True. Default: False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If failed to find find a matching function for the given arguments.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># `add` is a metagraph object which will add two objects according to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># input type using &quot;.register&quot; decorator.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scala_add</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s1">&#39;scala_add&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@add</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Number&quot;</span><span class="p">,</span> <span class="s2">&quot;Number&quot;</span><span class="p">)</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">add_scala</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">scala_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@add</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="gp">... </span><span class="k">def</span> <span class="nf">add_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">tensor_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="go">Tensor(shape=[], dtype=Float32, 3)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.MultitypeFuncGraph.register">
<span class="sig-name descname"><span class="pre">register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">type_names</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/base.html#MultitypeFuncGraph.register"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.MultitypeFuncGraph.register" title="Permalink to this definition"></a></dt>
<dd><p>Register a function for the given type string.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>type_names</strong> (Union[str, <a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>]) – Inputs type names or types list.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>decorator, a decorator to register the function to run, when called under the
types described in <cite>type_names</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.NMSWithMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">NMSWithMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#NMSWithMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.NMSWithMask" title="Permalink to this definition"></a></dt>
<dd><p>Select some bounding boxes in descending order of score.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>iou_threshold</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Specifies the threshold of overlap boxes with respect to
IOU. Default: 0.5.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the iou_threshold is not a float number, or if the first dimension
    of input Tensor is less than or equal to 0, or if the data type of the input
    Tensor is not float16 or float32.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>bboxes</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((N, 5)\)</span>. Input bounding boxes.
<cite>N</cite> is the number of input bounding boxes. Every bounding box
contains 5 values, the first 4 values are the coordinates of bounding
box, and the last value is the score of this bounding box.
The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[Tensor], tuple of three tensors, they are selected_boxes, selected_idx and selected_mask.</p>
<ul class="simple">
<li><p><strong>selected_boxes</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((N, 5)\)</span>. The list of bounding boxes
after non-max suppression calculation.</p></li>
<li><p><strong>selected_idx</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((N,)\)</span>. The indexes list of
valid input bounding boxes.</p></li>
<li><p><strong>selected_mask</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((N,)\)</span>. A mask list of
valid output bounding boxes.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bbox</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bbox</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bbox</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bbox</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">+=</span> <span class="n">bbox</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nms</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NMSWithMask</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_boxes</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">nms</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.NPUAllocFloatStatus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">NPUAllocFloatStatus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#NPUAllocFloatStatus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.NPUAllocFloatStatus" title="Permalink to this definition"></a></dt>
<dd><p>Allocates a flag to store the overflow status.</p>
<p>The flag is a tensor whose shape is <cite>(8,)</cite> and data type is <cite>mindspore.dtype.float32</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Examples: see <cite>NPUGetFloatStatus</cite>.</p>
</div>
<dl class="simple">
<dt>Outputs:</dt><dd><p>Tensor, has the shape of <cite>(8,)</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alloc_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NPUAllocFloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span> <span class="o">=</span> <span class="n">alloc_status</span><span class="p">()</span>
<span class="go">Tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], shape=(8,), dtype=mindspore.float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.NPUClearFloatStatus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">NPUClearFloatStatus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#NPUClearFloatStatus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.NPUClearFloatStatus" title="Permalink to this definition"></a></dt>
<dd><p>Clear the flag which stores the overflow status.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The flag is in the register on the <cite>Ascend</cite> device. It will be reset and can not be reused again after the
<cite>NPUClearFloatStatus</cite> is called.</p>
<p>Examples: see <cite>NPUGetFloatStatus</cite>.</p>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The output tensor of <cite>NPUAllocFloatStatus</cite>.
The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>. All the elements in the tensor will be zero.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alloc_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NPUAllocFloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">get_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NPUGetFloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clear_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NPUClearFloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span> <span class="o">=</span> <span class="n">alloc_status</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flag</span> <span class="o">=</span> <span class="n">get_status</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clear</span> <span class="o">=</span> <span class="n">clear_status</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="go">Tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], shape=(8,), dtype=mindspore.float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.NPUGetFloatStatus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">NPUGetFloatStatus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#NPUGetFloatStatus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.NPUGetFloatStatus" title="Permalink to this definition"></a></dt>
<dd><p>Updates the flag which is the output tensor of <cite>NPUAllocFloatStatus</cite> with latest overflow status.</p>
<p>The flag is a tensor whose shape is <cite>(8,)</cite> and data type is <cite>mindspore.dtype.float32</cite>.
If the sum of the flag equals to 0, there is no overflow happened. If the sum of the flag is bigger than 0, there
is overflow happened.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The output tensor of <cite>NPUAllocFloatStatus</cite>.
The data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>. All the elements in the tensor will be zero.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">alloc_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NPUAllocFloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">get_status</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NPUGetFloatStatus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span> <span class="o">=</span> <span class="n">alloc_status</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">flag</span> <span class="o">=</span> <span class="n">get_status</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="go">Tensor([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], shape=(8,), dtype=mindspore.float32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Neg">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Neg</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Neg"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Neg" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor with negative values of the input tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose dtype is number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">neg</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">neg</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-1.  -2.   1.  -2.   0.   3.5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.NotEqual">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">NotEqual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#NotEqual"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.NotEqual" title="Permalink to this definition"></a></dt>
<dd><p>Computes the non-equivalence of two tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors, the shapes of them could be broadcast.
When the inputs are one tensor and one scalar, the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="go">[True, False, True]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[False, False, True]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.OneHot">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">OneHot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#OneHot"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.OneHot" title="Permalink to this definition"></a></dt>
<dd><p>Computes a one-hot tensor.</p>
<p>Makes a new tensor, whose locations represented by indices in <cite>indices</cite> take value <cite>on_value</cite>, while all
other locations take value <cite>off_value</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the input indices is rank <cite>N</cite>, the output will have rank <cite>N+1</cite>. The new axis is created at dimension <cite>axis</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Position to insert the value. e.g. If <cite>indices</cite> shape is [n, c], and <cite>axis</cite> is <cite>-1</cite> the output shape
will be [n, c, depth], If <cite>axis</cite> is <cite>0</cite> the output shape will be [depth, n, c]. Default: -1.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>indices</strong> (Tensor) - A tensor of indices. Tensor of shape <span class="math notranslate nohighlight">\((X_0, \ldots, X_n)\)</span>.
Data type must be int32.</p></li>
<li><p><strong>depth</strong> (int) - A scalar defining the depth of the one hot dimension.</p></li>
<li><p><strong>on_value</strong> (Tensor) - A value to fill in output when <cite>indices[j] = i</cite>. With data type of float16 or float32.</p></li>
<li><p><strong>off_value</strong> (Tensor) - A value to fill in output when <cite>indices[j] != i</cite>.
Has the same data type with as <cite>on_value</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, one-hot tensor. Tensor of shape <span class="math notranslate nohighlight">\((X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">onehot</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">onehot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">)</span>
<span class="go">[[1, 0, 0], [0, 1, 0], [0, 0, 1]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.OnesLike">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">OnesLike</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#OnesLike"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.OnesLike" title="Permalink to this definition"></a></dt>
<dd><p>Creates a new tensor. The values of all elements are 1.</p>
<p>Returns a tensor of ones with the same shape and type as the input.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as <cite>input_x</cite> but filled with ones.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">oneslike</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">oneslike</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[[1, 1],</span>
<span class="go"> [1, 1]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.PReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">PReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#PReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.PReLU" title="Permalink to this definition"></a></dt>
<dd><p>Parametric Rectified Linear Unit activation function.</p>
<p>PReLU is described in the paper <a class="reference external" href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on
ImageNet Classification</a>. Defined as follows:</p>
<div class="math notranslate nohighlight">
\[prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is an element of an channel of the input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>1-dimensional input_x is not supported.</p>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Float tensor, representing the output of the preview layer.
With data type of float16 or float32.</p></li>
<li><p><strong>weight</strong> (Tensor) -  Float Tensor, w &gt; 0, there are only two shapes are legitimate,
1 or the number of channels of the input. With data type of float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p>For detailed information, please refer to <cite>nn.PReLU</cite>.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">prelu</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">PReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">result</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="go">[[[-0.1, 1.0],</span>
<span class="go">  [0.0, 2.0],</span>
<span class="go">  [0.0, 0.0]],</span>
<span class="go"> [[-0.2, -0.1],</span>
<span class="go">  [2.0, -1.8],</span>
<span class="go">  [0.6, 0.6]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Pack">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Pack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Pack"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Pack" title="Permalink to this definition"></a></dt>
<dd><p>Packs a list of tensors in specified axis.</p>
<p>Packs the list of input tensors with the same rank <cite>R</cite>, output is a tensor of rank <cite>(R+1)</cite>.</p>
<p>Given input tensors of shape <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>. Set the number of input tensors as <cite>N</cite>.
If <span class="math notranslate nohighlight">\(0 \le axis\)</span>, the shape of the output tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{axis}, N, x_{axis+1}, ..., x_R)\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Dimension to pack. Default: 0.
Negative values wrap around. The range is [-(R+1), R+1).</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[tuple, list]) - A Tuple or list of Tensor objects with the same shape and type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. A packed Tensor with the same type as <cite>input_x</cite>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If the data types of elements in <cite>input_x</cite> are not the same.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the length of <cite>input_x</cite> is not greater than 1;
    or if axis is out of the range [-(R+1), R+1);
    or if the shapes of elements in input_x are not the same.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pack</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pack</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">pack</span><span class="p">([</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">])</span>
<span class="go">[[0, 1], [2, 3]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Pad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Pad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Pad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Pad" title="Permalink to this definition"></a></dt>
<dd><p>Pads input tensor according to the paddings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>paddings</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of parameter <cite>paddings</cite> is (N, 2). N is the rank of input data. All elements of
paddings are int type. For the input in <cite>D</cite> th dimension, paddings[D, 0] indicates how many sizes to be
extended ahead of the input tensor in the <cite>D</cite> th dimension, and paddings[D, 1] indicates how many sizes to
be extended behind the input tensor in the <cite>D</cite> th dimension.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the tensor after padding.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">pad_op</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">output_tensor</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                         <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.3</span><span class="p">,</span>  <span class="mf">3.6</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                         <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                         <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                                         <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Padding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Padding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Padding"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Padding" title="Permalink to this definition"></a></dt>
<dd><p>Extends the last dimension of input tensor from 1 to pad_dim_size, by filling with 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pad_dim_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The value of the last dimension of x to be extended, which must be positive.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>. The rank of x must be at least 2.
The last dimension of x must be 1.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((z_1, z_2, ..., z_N)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pad_dim_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Padding</span><span class="p">(</span><span class="n">pad_dim_size</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[[8, 0, 0, 0], [10, 0, 0, 0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ParallelConcat">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ParallelConcat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ParallelConcat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ParallelConcat" title="Permalink to this definition"></a></dt>
<dd><p>Concats tensor in the first dimension.</p>
<p>Concats input tensors along with the first dimension.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The input tensors are all required to have size 1 in the first dimension.</p>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>values</strong> (tuple, list) - A tuple or a list of input tensors. The data type and shape of these
tensors must be the same.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, data type is the same as <cite>values</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ParallelConcat</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">((</span><span class="n">data1</span><span class="p">,</span> <span class="n">data2</span><span class="p">))</span>
<span class="go">[[0, 1], [2, 1]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Partial">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Partial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#Partial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Partial" title="Permalink to this definition"></a></dt>
<dd><p>Makes a partial function instance, used for pynative mode.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>args</strong> (Union[FunctionType, Tensor]) - The function and bind arguments.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>FunctionType, partial function binded with arguments.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Poisson">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Poisson</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#Poisson"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Poisson" title="Permalink to this definition"></a></dt>
<dd><p>Produces random non-negative integer values i, distributed according to discrete probability function:</p>
<div class="math notranslate nohighlight">
\[\text{P}(i|μ) = \frac{\exp(-μ)μ^{i}}{i!},\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed, must be non-negative. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2, must be non-negative. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple) - The shape of random tensor to be generated. Only constant value is allowed.</p></li>
<li><p><strong>mean</strong> (Tensor) - μ parameter the distribution was constructed with. The parameter defines mean number
of occurrences of the event. It must be greater than 0. With float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. Its shape must be the broadcasted shape of <cite>shape</cite> and the shape of <cite>mean</cite>.
The dtype is int32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poisson</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.PopulationCount">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">PopulationCount</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#PopulationCount"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.PopulationCount" title="Permalink to this definition"></a></dt>
<dd><p>Calculates population count.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) -  The data type must be int16 or uint16.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the sam  shape as the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">population_count</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">PopulationCount</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">population_count</span><span class="p">(</span><span class="n">x_input</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Pow">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Pow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Pow"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Pow" title="Permalink to this definition"></a></dt>
<dd><p>Computes a tensor to the power of the second input.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="mf">3.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">pow</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">pow</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[1.0, 8.0, 64.0]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">pow</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">pow</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[1.0, 16.0, 64.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Primitive">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Primitive</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#Primitive"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Primitive" title="Permalink to this definition"></a></dt>
<dd><p>Primitive is the base class of primitives in python.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name for the current Primitive.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">Primitive</span><span class="p">(</span><span class="s1">&#39;add&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># or work with prim_attr_register:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># init a Primitive class with attr1 and attr2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@prim_attr_register</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr1</span><span class="p">,</span> <span class="n">attr2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="c1"># check attr1 and attr2 or do some initializations</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># init a Primitive obj with attr1=1 and attr2=2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">Add</span><span class="p">(</span><span class="n">attr1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">attr2</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.Primitive.add_prim_attr">
<span class="sig-name descname"><span class="pre">add_prim_attr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#Primitive.add_prim_attr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Primitive.add_prim_attr" title="Permalink to this definition"></a></dt>
<dd><p>Adds primitive attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Attribute Name.</p></li>
<li><p><strong>value</strong> (<em>Any</em>) – Attribute value.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.Primitive.check_elim">
<span class="sig-name descname"><span class="pre">check_elim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#Primitive.check_elim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Primitive.check_elim" title="Permalink to this definition"></a></dt>
<dd><p>Check if certain inputs should go to the backend. Subclass in need should override this method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Primitive args</em>) – Same as arguments of current Primitive.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple consisting of two elements. The first element indicates whether we should filter out current
arguments; the seconde element is the output if we need to filter out the arguments.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.Primitive.init_prim_io_names">
<span class="sig-name descname"><span class="pre">init_prim_io_names</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#Primitive.init_prim_io_names"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Primitive.init_prim_io_names" title="Permalink to this definition"></a></dt>
<dd><p>Initializes the name of inputs and outpus of Tensor or attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>]</em>) – list of inputs names.</p></li>
<li><p><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>]</em>) – list of outputs names.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.Primitive.set_prim_instance_name">
<span class="sig-name descname"><span class="pre">set_prim_instance_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">instance_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#Primitive.set_prim_instance_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Primitive.set_prim_instance_name" title="Permalink to this definition"></a></dt>
<dd><p>Set instance name to primitive operator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It will be called by default when user defines primitive operator.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>instance_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Instance name of primitive operator set by user.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.Primitive.shard">
<span class="sig-name descname"><span class="pre">shard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">strategy</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#Primitive.shard"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Primitive.shard" title="Permalink to this definition"></a></dt>
<dd><p>Add strategies to primitive attribute.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is valid only in semi auto parallel or auto parallel mode.
In other parallel modes, strategies set here will be ignored.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>strategy</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – Strategy describes the distributed parallel mode of the current primitive.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.ops.Primitive.update_parameter">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">update_parameter</span></span><a class="headerlink" href="#mindspore.ops.Primitive.update_parameter" title="Permalink to this definition"></a></dt>
<dd><p>Whether the primitive will update the value of parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.PrimitiveWithInfer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">PrimitiveWithInfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#PrimitiveWithInfer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.PrimitiveWithInfer" title="Permalink to this definition"></a></dt>
<dd><p>PrimitiveWithInfer is the base class of primitives in python defines functions for tracking inference in python.</p>
<p>There are four method can be overide to define the infer logic of the primitive: __infer__(), infer_shape(),
infer_dtype(), and infer_value(). If __infer__() is defined in primitive, the __infer__() has highest priority
to be called. If __infer__() is not defined, infer_shape() and infer_dtype() can be defined to describe the infer
logic of the shape and type. The infer_value() is used for constant propagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the current Primitive.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># init a Primitive class with infer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@prim_attr_register</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">pass</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="c1"># output shape same as first input &#39;x&#39;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="c1"># output type same as first input &#39;x&#39;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># init a Primitive obj</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">Add</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.PrimitiveWithInfer.infer_dtype">
<span class="sig-name descname"><span class="pre">infer_dtype</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#PrimitiveWithInfer.infer_dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.PrimitiveWithInfer.infer_dtype" title="Permalink to this definition"></a></dt>
<dd><p>Infer output dtype based on input dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – data type of inputs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>, data type of outputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.PrimitiveWithInfer.infer_shape">
<span class="sig-name descname"><span class="pre">infer_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#PrimitiveWithInfer.infer_shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.PrimitiveWithInfer.infer_shape" title="Permalink to this definition"></a></dt>
<dd><p>Infer output shape based on input shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The shape of scalar is an empty tuple.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em>) – shapes of input tensors.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>tuple(int)</cite>, shapes of output tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.PrimitiveWithInfer.infer_value">
<span class="sig-name descname"><span class="pre">infer_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#PrimitiveWithInfer.infer_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.PrimitiveWithInfer.infer_value" title="Permalink to this definition"></a></dt>
<dd><p>Infer output value based on input value at compile time.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>args</strong> (<em>Any</em>) – value of inputs.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Value of outputs. Return <cite>None</cite>, the value can not be inferred at compile time in this case.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Print">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#Print"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Print" title="Permalink to this definition"></a></dt>
<dd><p>Outputs tensor or string to stdout.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In pynative mode, please use python print function.</p>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, str]) - The graph node to attach to. The input supports
multiple strings and tensors which are separated by ‘,’.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">PrintDemo</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">PrintDemo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">print</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Print</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Print Tensor x and Tensor y:&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Pull">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Pull</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#Pull"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Pull" title="Permalink to this definition"></a></dt>
<dd><p>Pulls weight from parameter server.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>key</strong> (Tensor) - The key of the weight.</p></li>
<li><p><strong>weight</strong> (Tensor) - The weight to be updated.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Push">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Push</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/other_ops.html#Push"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Push" title="Permalink to this definition"></a></dt>
<dd><p>Pushes the inputs of the corresponding optimizer to parameter server.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optim_type</strong> (<em>string</em>) – The optimizer type. Default: ‘ApplyMomentum’.</p></li>
<li><p><strong>only_shape_indices</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The indices of input of which only shape
will be pushed to parameter server. Default: None.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>optim_inputs</strong> (tuple) - The inputs for this kind of optimizer.</p></li>
<li><p><strong>optim_input_shapes</strong> (tuple) - The shapes of the inputs.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the key of the weight which needs to be updated.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.RNNTLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">RNNTLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#RNNTLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.RNNTLoss" title="Permalink to this definition"></a></dt>
<dd><p>Computes the RNNTLoss and its gradient with respect to the softmax outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>blank_label</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – blank label. Default: 0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>acts</strong> (Tensor) - Tensor of shape <span class="math notranslate nohighlight">\((B, T, U, V)\)</span>. Data type must be float16 or float32.</p></li>
<li><p><strong>labels</strong> (Tensor[int32]) - Tensor of shape <span class="math notranslate nohighlight">\((B, U-1)\)</span>.</p></li>
<li><p><strong>input_lengths</strong> (Tensor[int32]) - Tensor of shape <span class="math notranslate nohighlight">\((B,)\)</span>.</p></li>
<li><p><strong>label_lengths</strong> (Tensor[int32]) - Tensor of shape <span class="math notranslate nohighlight">\((B,)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>costs</strong> (Tensor[int32]) - Tensor of shape <span class="math notranslate nohighlight">\((B,)\)</span>.</p></li>
<li><p><strong>grads</strong> (Tensor[int32]) - Has the same shape as <cite>acts</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">acts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">T</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rnnt_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RNNTLoss</span><span class="p">(</span><span class="n">blank_label</span><span class="o">=</span><span class="n">blank</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">costs</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">rnnt_loss</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">acts</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">input_length</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">label_length</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ROIAlign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ROIAlign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ROIAlign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ROIAlign" title="Permalink to this definition"></a></dt>
<dd><p>Computes Region of Interest (RoI) Align operator.</p>
<p>The operator computes the value of each sampling point by bilinear interpolation from the nearby grid points on the
feature map. No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling
points. The details of (RoI) Align operator are described in <a class="reference external" href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pooled_height</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The output features’ height.</p></li>
<li><p><strong>pooled_width</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The output features’ width.</p></li>
<li><p><strong>spatial_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A scaling factor that maps the raw image coordinates to the input
feature map coordinates. Suppose the height of a RoI is <cite>ori_h</cite> in the raw image and <cite>fea_h</cite> in the
input feature map, the <cite>spatial_scale</cite> must be <cite>fea_h / ori_h</cite>.</p></li>
<li><p><strong>sample_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of sampling points. Default: 2.</p></li>
<li><p><strong>roi_end_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number must be 0 or 1. Default: 1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>features</strong> (Tensor) - The input features, whose shape must be <cite>(N, C, H, W)</cite>.</p></li>
<li><p><strong>rois</strong> (Tensor) - The shape is <cite>(rois_n, 5)</cite>. With data type of float16 or float32.
<cite>rois_n</cite> represents the number of RoI. The size of the second dimension must be <cite>5</cite> and the <cite>5</cite> colunms
are <cite>(image_index, top_left_x, top_left_y, bottom_right_x, bottom_right_y)</cite>. <cite>image_index</cite> represents the
index of image. <cite>top_left_x</cite> and <cite>top_left_y</cite> represent the <cite>x, y</cite> coordinates of the top left corner
of corresponding RoI, respectively. <cite>bottom_right_x</cite> and <cite>bottom_right_y</cite> represent the <cite>x, y</cite>
coordinates of the bottom right corner of corresponding RoI, respectively.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is <cite>(rois_n, C, pooled_height, pooled_width)</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rois</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roi_align</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ROIAlign</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_tensor</span> <span class="o">=</span> <span class="n">roi_align</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">rois</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">output_tensor</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mf">2.15</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.RandomCategorical">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">RandomCategorical</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#RandomCategorical"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.RandomCategorical" title="Permalink to this definition"></a></dt>
<dd><p>Generates random samples from a given categorical distribution tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dtype</strong> (<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><em>mindspore.dtype</em></a>) – The type of output. Its value must be one of mindspore.int16,
mindspore.int32 and mindspore.int64. Default: mindspore.int64.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - The input tensor. 2-D Tensor with shape [batch_size, num_classes].</p></li>
<li><p><strong>num_sample</strong> (int) - Number of sample to be drawn. Only constant values is allowed.</p></li>
<li><p><strong>seed</strong> (int) - Random seed. Default: 0. Only constant values is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>output</strong> (Tensor) - The output Tensor with shape [batch_size, num_samples].</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="bp">self</span><span class="o">.</span><span class="n">random_categorical</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RandomCategorical</span><span class="p">(</span><span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="bp">self</span><span class="o">.</span><span class="n">num_sample</span> <span class="o">=</span> <span class="n">num_sample</span>
<span class="gp">&gt;&gt;&gt; </span>  <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_categorical</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_sample</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.RandomChoiceWithMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">RandomChoiceWithMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#RandomChoiceWithMask"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.RandomChoiceWithMask" title="Permalink to this definition"></a></dt>
<dd><p>Generates a random sample as index tensor with a mask tensor from a given tensor.</p>
<p>The input must be a tensor of rank not less than 1. If its rank is greater than or equal to 2,
the first dimension specifies the number of samples.
The index tensor and the mask tensor have the fixed shapes. The index tensor denotes the index of the nonzero
sample, while the mask tensor denotes which elements in the index tensor are valid.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>count</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of items expected to get and the number must be greater than 0. Default: 256.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><strong>input_x</strong> (Tensor[bool]) - The input tensor.</dt><dd><p>The input tensor rank must be greater than or equal to 1 and less than or equal to 5.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Two tensors, the first one is the index tensor and the other one is the mask tensor.</p>
<ul class="simple">
<li><p><strong>index</strong> (Tensor) - The output shape is 2-D.</p></li>
<li><p><strong>mask</strong> (Tensor) - The output shape is 1-D.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rnd_choice_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RandomChoiceWithMask</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">240000</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output_y</span><span class="p">,</span> <span class="n">output_mask</span> <span class="o">=</span> <span class="n">rnd_choice_mask</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Rank">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Rank"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Rank" title="Permalink to this definition"></a></dt>
<dd><p>Returns the rank of a tensor.</p>
<p>Returns a 0-D int32 Tensor representing the rank of input; the rank of a tensor
is the number of indices required to uniquely select each element of the tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. 0-D int32 Tensor representing the rank of input, i.e., <span class="math notranslate nohighlight">\(R\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rank</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rank</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReLU" title="Permalink to this definition"></a></dt>
<dd><p>Computes ReLU(Rectified Linear Unit) of input tensor element-wise.</p>
<p>It returns <span class="math notranslate nohighlight">\(\max(x,\  0)\)</span> element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[0, 4.0, 0.0], [2.0, 0.0, 9.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReLU6">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReLU6</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ReLU6"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReLU6" title="Permalink to this definition"></a></dt>
<dd><p>Computes ReLU(Rectified Linear Unit) upper bounded by 6 of input tensor element-wise.</p>
<p>It returns <span class="math notranslate nohighlight">\(\min(\max(0,x), 6)\)</span> element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor, with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu6</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">relu6</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReLUV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReLUV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ReLUV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReLUV2" title="Permalink to this definition"></a></dt>
<dd><p>Computes ReLU(Rectified Linear Unit) of input tensor element-wise.</p>
<p>It returns <span class="math notranslate nohighlight">\(\max(x,\  0)\)</span> element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor must be a 4-D tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>output</strong> (Tensor) - Has the same type and shape as the <cite>input_x</cite>.</p></li>
<li><p><strong>mask</strong> (Tensor) - A tensor whose data type must be uint8.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">relu_v2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReLUV2</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">relu_v2</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">([[[[1., 0.], [0., 4.]], [[0., 6.], [7., 0.]]]],</span>
<span class="go"> [[[[1, 0], [2, 0]], [[2, 0], [1, 0]]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.RealDiv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">RealDiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#RealDiv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.RealDiv" title="Permalink to this definition"></a></dt>
<dd><p>Divide the first input tensor by the second input tensor in floating-point type element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">realdiv</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">realdiv</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[0.25, 0.4, 0.5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Reciprocal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Reciprocal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Reciprocal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Reciprocal" title="Permalink to this definition"></a></dt>
<dd><p>Returns reciprocal of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reciprocal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reciprocal</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reciprocal</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0, 0.5, 0.25]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceAll">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceAll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceAll"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceAll" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by the “logical and” of all elements in the dimension.</p>
<p>The dtype of the tensor to be reduced is bool.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions.
Default : False, don’t keep these reduced dimensions.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[bool]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the dtype is bool.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the “logical and” of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceAll</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceAny">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceAny</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceAny"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceAny" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by the “logical OR” of all elements in the dimension.</p>
<p>The dtype of the tensor to be reduced is bool.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions.
Default : False, don’t keep these reduced dimensions.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[bool]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the dtype is bool.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the “logical or” of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceAny</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="go">[[True],</span>
<span class="go"> [True]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceMax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceMax" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by the maximum value in this dimension.</p>
<p>The dtype of the tensor to be reduced is number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions.
Default : False, don’t keep these reduced dimensions.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the <cite>input_x</cite>.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the maximum of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceMean">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceMean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceMean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceMean" title="Permalink to this definition"></a></dt>
<dd><blockquote>
<div><p>Reduce a dimension of a tensor by averaging all elements in the dimension.</p>
<p>The dtype of the tensor to be reduced is number.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the <cite>input_x</cite>.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the mean of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceMin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceMin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceMin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceMin" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by the minimum value in the dimension.</p>
<p>The dtype of the tensor to be reduced is number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions.
Default : False, don’t keep these reduced dimensions.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the <cite>input_x</cite>.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the minimum of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceOp</span></span><a class="reference internal" href="../_modules/mindspore/ops/operations/comm_ops.html#ReduceOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceOp" title="Permalink to this definition"></a></dt>
<dd><p>Operation options for reduce tensors.</p>
<p>There are four kinds of operation options, “SUM”, “MAX”, “MIN”, and “PROD”.</p>
<blockquote>
<div><ul class="simple">
<li><p>SUM: Take the sum.</p></li>
<li><p>MAX: Take the maximum.</p></li>
<li><p>MIN: Take the minimum.</p></li>
<li><p>PROD: Take the product.</p></li>
</ul>
</div></blockquote>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceProd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceProd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceProd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceProd" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by multiplying all elements in the dimension.</p>
<p>The dtype of the tensor to be reduced is number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions.
Default : False, don’t keep these reduced dimensions.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the <cite>input_x</cite>.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the product of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceProd</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceScatter">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceScatter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/comm_ops.html#ReduceScatter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceScatter" title="Permalink to this definition"></a></dt>
<dd><blockquote>
<div><p>Reduces and scatters tensors from the specified communication group.</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The back propagation of the op is not supported yet. Stay tuned for more.
The tensors must have the same shape and format in all processes of the collection.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>op</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Specifies an operation used for element-wise reductions,
like SUM, MAX, AVG. Default: ReduceOp.SUM.</p></li>
<li><p><strong>group</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The communication group to work on. Default: “hccl_world_group”.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If any of operation and group is not a string.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the first dimension of the input cannot be divided by the rank size.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops.operations.comm_ops</span> <span class="kn">import</span> <span class="n">ReduceOp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops.operations</span> <span class="k">as</span> <span class="nn">P</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">reducescatter</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceScatter</span><span class="p">(</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="s2">&quot;nccl_world_group&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducescatter</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReduceSum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReduceSum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#ReduceSum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReduceSum" title="Permalink to this definition"></a></dt>
<dd><p>Reduce a dimension of a tensor by summing all elements in the dimension.</p>
<p>The dtype of the tensor to be reduced is number.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>keep_dims</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, keep these reduced dimensions and the length is 1.
If false, don’t keep these dimensions. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[Number]) - The input tensor.</p></li>
<li><p><strong>axis</strong> (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: (), reduce all dimensions.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same dtype as the <cite>input_x</cite>.</p>
<ul class="simple">
<li><p>If axis is (), and keep_dims is False,
the output is a 0-D tensor representing the sum of all elements in the input tensor.</p></li>
<li><p>If axis is int, set as 2, and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_3, ..., x_R)\)</span>.</p></li>
<li><p>If axis is tuple(int), set as (2, 3), and keep_dims is False,
the shape of output is <span class="math notranslate nohighlight">\((x_1, x_4, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Reshape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Reshape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Reshape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Reshape" title="Permalink to this definition"></a></dt>
<dd><p>Reshapes input tensor with the same values based on a given shape tuple.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – Given a shape tuple, if it has several -1; or if the product
    of its elements is less than or equal to 0 or cannot be divided by the product
    of the input tensor shape; or if it does not match the input’s array size.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>input_shape</strong> (tuple[int]) - The input tuple is constructed by multiple
integers, i.e., <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ResizeBilinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ResizeBilinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#ResizeBilinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ResizeBilinear" title="Permalink to this definition"></a></dt>
<dd><p>Resizes the image to certain size using bilinear interpolation.</p>
<p>The resizing only affects the lower two dimensions which represent the height and width. The input images
can be represented by different data types, but the data types of output images are always float32.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – A tuple of 2 int elements <cite>(new_height, new_width)</cite>, the new size of the images.</p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, rescale input by <cite>(new_height - 1) / (height - 1)</cite>,
which exactly aligns the 4 corners of images and resized images. If false,
rescale by <cite>new_height / height</cite>. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input</strong> (Tensor) - Image to be resized. Input images must be a 4-D tensor with shape
<span class="math notranslate nohighlight">\((batch, channels, height, width)\)</span>, with data type of float32 or float16.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, resized image. 4-D with shape [batch, channels, new_height, new_width] in <cite>float32</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">resize_bilinear</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ResizeBilinear</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">resize_bilinear</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ResizeNearestNeighbor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ResizeNearestNeighbor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ResizeNearestNeighbor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ResizeNearestNeighbor" title="Permalink to this definition"></a></dt>
<dd><p>Resizes the input tensor by using nearest neighbor algorithm.</p>
<p>Resizes the input tensor to a given size by using the nearest neighbor algorithm. The nearest
neighbor algorithm selects the value of the nearest point and does not consider the
values of neighboring points at all, yielding a piecewise-constant interpolant.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em>) – The target size. The dimension of size must be 2.</p></li>
<li><p><strong>align_corners</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the centers of the 4 corner pixels of the input
and output tensors are aligned. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. The shape of the tensor is <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of the output tensor is <span class="math notranslate nohighlight">\((N, C, NEW\_H, NEW\_W)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">resize</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ResizeNearestNeighbor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReverseSequence">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReverseSequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ReverseSequence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReverseSequence" title="Permalink to this definition"></a></dt>
<dd><p>Reverses variable length slices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq_dim</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension where reversal is performed. Required.</p></li>
<li><p><strong>batch_dim</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The input is sliced in this dimension. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input to reverse, supporting all number types including bool.</p></li>
<li><p><strong>seq_lengths</strong> (Tensor) - Must be a 1-D vector with int32 or int64 types.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Reversed tensor with the same shape and data type as input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">seq_lengths</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reverse_sequence</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseSequence</span><span class="p">(</span><span class="n">seq_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">reverse_sequence</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_lengths</span><span class="p">)</span>
<span class="go">[[1 2 3]</span>
<span class="go"> [5 4 6]</span>
<span class="go"> [9 8 7]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ReverseV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ReverseV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ReverseV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ReverseV2" title="Permalink to this definition"></a></dt>
<dd><p>Reverses specific dimensions of a tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em>) – The indices of the dimensions to reverse.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The target tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[4, 3, 2, 1], [8, 7, 6, 5]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Rint">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Rint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Rint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Rint" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise integer closest to x.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The target tensor, which must be one of the following types:
float16, float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.6</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rint</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[-2., 0., 2., 2.]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Round">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Round</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Round"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Round" title="Permalink to this definition"></a></dt>
<dd><p>Returns half to even of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">round</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Round</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">round</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0, 2.0, 2.0, 2.0, -4.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Rsqrt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Rsqrt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Rsqrt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Rsqrt" title="Permalink to this definition"></a></dt>
<dd><p>Computes reciprocal of square root of input tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input of Rsqrt. Each element must be a non-negative number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same type and shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rsqrt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rsqrt</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rsqrt</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
<span class="go">[[0.5, 0.5], [0.333333, 0.333333]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SGD">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SGD" title="Permalink to this definition"></a></dt>
<dd><p>Computes stochastic gradient descent (optionally with momentum).</p>
<p>Nesterov momentum is based on the formula from On the importance of
initialization and momentum in deep learning.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For details, please refer to <cite>nn.SGD</cite> source code.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dampening</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dampening for momentum. Default: 0.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). Default: 0.0.</p></li>
<li><p><strong>nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enable Nesterov momentum. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>parameters</strong> (Tensor) - Parameters to be updated. With float16 or float32 data type.</p></li>
<li><p><strong>gradient</strong> (Tensor) - Gradient, with float16 or float32 data type.</p></li>
<li><p><strong>learning_rate</strong> (Tensor) - Learning rate, a scalar tensor with float16 or float32 data type.
e.g. Tensor(0.1, mindspore.float32)</p></li>
<li><p><strong>accum</strong> (Tensor) - Accum(velocity) to be updated. With float16 or float32 data type.</p></li>
<li><p><strong>momentum</strong> (Tensor) - Momentum, a scalar tensor with float16 or float32 data type.
e.g. Tensor(0.1, mindspore.float32).</p></li>
<li><p><strong>stat</strong> (Tensor) - States to be updated with the same shape as gradient, with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, parameters to be updated.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sgd</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SGD</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameters</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.7</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accum</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">momentum</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stat</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">sgd</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">accum</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">stat</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SameTypeShape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SameTypeShape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#SameTypeShape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SameTypeShape" title="Permalink to this definition"></a></dt>
<dd><p>Checks whether data type and shape of two tensors are the same.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If the data types of two tensors are not the same.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the shapes of two tensors are not the same.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_S)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>,
if data type and shape of <cite>input_x</cite> and <cite>input_y</cite> are the same.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SameTypeShape</span><span class="p">()(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScalarCast">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScalarCast</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/inner_ops.html#ScalarCast"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScalarCast" title="Permalink to this definition"></a></dt>
<dd><p>Cast the input scalar to another type.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (scalar) - The input scalar. Only constant value is allowed.</p></li>
<li><p><strong>input_y</strong> (mindspore.dtype) - The type to be cast. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Scalar. The type is the same as the python type corresponding to <cite>input_y</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scalar_cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="mf">255.0</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScalarSummary">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScalarSummary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#ScalarSummary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScalarSummary" title="Permalink to this definition"></a></dt>
<dd><p>Outputs a scalar to a protocol buffer through a scalar summary operator.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>name</strong> (str) - The name of the input variable, it must not be an empty string.</p></li>
<li><p><strong>value</strong> (Tensor) - The value of scalar, and the shape of value must be [] or [1].</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SummaryDemo</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SummaryDemo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarSummary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScalarToArray">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScalarToArray</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScalarToArray"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScalarToArray" title="Permalink to this definition"></a></dt>
<dd><p>Converts a scalar to a <cite>Tensor</cite>.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[int, float]) - The input is a scalar. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. 0-D Tensor and the content is the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToArray</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScalarToTensor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScalarToTensor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScalarToTensor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScalarToTensor" title="Permalink to this definition"></a></dt>
<dd><p>Converts a scalar to a <cite>Tensor</cite>, and convert data type to specified type.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[int, float]) - The input is a scalar. Only constant value is allowed.</p></li>
<li><p><strong>dtype</strong> (mindspore.dtype) - The target data type. Default: mindspore.float32. Only
constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. 0-D Tensor and the content is the input.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterAdd" title="Permalink to this definition"></a></dt>
<dd><p>Updates the value of the input tensor through the add operation.</p>
<p>Using given values to update tensor value through the add operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do add operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor that performs the add operation with <cite>input_x</cite>,
the data type is the same as <cite>input_x</cite>, the shape is <cite>indices_shape + x_shape[1:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterDiv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterDiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterDiv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterDiv" title="Permalink to this definition"></a></dt>
<dd><p>Updates the value of the input tensor through the div operation.</p>
<p>Using given values to update tensor value through the div operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do div operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor that performs the div operation with <cite>input_x</cite>,
the data type is the same as <cite>input_x</cite>, the shape is <cite>indices_shape + x_shape[1:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterDiv</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[[3.0, 3.0, 3.0], [1.0, 1.0, 1.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterMax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterMax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterMax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterMax" title="Permalink to this definition"></a></dt>
<dd><p>Updates the value of the input tensor through the max operation.</p>
<p>Using given values to update tensor value through the max operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: True.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do max operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor that performs the maximum operation with <cite>input_x</cite>,
the data type is the same as <cite>input_x</cite>, the shape is <cite>indices_shape + x_shape[1:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input_x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="o">*</span> <span class="mi">88</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_max</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>
<span class="go">[[88.0, 88.0, 88.0], [88.0, 88.0, 88.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterMin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterMin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterMin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterMin" title="Permalink to this definition"></a></dt>
<dd><p>Updates the value of the input tensor through the min operation.</p>
<p>Using given values to update tensor value through the min operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do min operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor doing the min operation with <cite>input_x</cite>,
the data type is same as <cite>input_x</cite>, the shape is <cite>indices_shape + x_shape[1:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input_x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_min</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>
<span class="go">[[0.0, 1.0, 1.0], [0.0, 0.0, 0.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterMul">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterMul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterMul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterMul" title="Permalink to this definition"></a></dt>
<dd><p>Updates the value of the input tensor through the mul operation.</p>
<p>Using given values to update tensor value through the mul operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do mul operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor doing the mul operation with <cite>input_x</cite>,
the data type is same as <cite>input_x</cite>, the shape is <cite>indices_shape + x_shape[1:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMul</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[[2.0, 2.0, 2.0], [4.0, 4.0, 4.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterNd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterNd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterNd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterNd" title="Permalink to this definition"></a></dt>
<dd><p>Scatters a tensor into a new tensor depending on the specified indices.</p>
<p>Creates an empty tensor, and set values by scattering the update tensor depending on indices.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>indices</strong> (Tensor) - The index of scattering in the new tensor with int32 data type.</p></li>
<li><p><strong>update</strong> (Tensor) - The source Tensor to be scattered.</p></li>
<li><p><strong>shape</strong> (tuple[int]) - Define the shape of the output tensor, has the same type as indices.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the new tensor, has the same type as <cite>update</cite> and the same shape as <cite>shape</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterNdAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterNdAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterNdAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterNdAdd" title="Permalink to this definition"></a></dt>
<dd><p>Applies sparse addition to individual values or slices in a Tensor.</p>
<p>Using given values to update tensor value through the add operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do add operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor doing the add operation with <cite>input_x</cite>,
the data type is same as <cite>input_x</cite>, the shape is <cite>indices_shape[:-1] + x_shape[indices_shape[-1]:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_nd_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNdAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_nd_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[1, 10, 9, 4, 12, 6, 7, 17]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterNdSub">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterNdSub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterNdSub"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterNdSub" title="Permalink to this definition"></a></dt>
<dd><p>Applies sparse subtraction to individual values or slices in a Tensor.</p>
<p>Using given values to update tensor value through the subtraction operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to do add operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor that performs the subtraction operation with <cite>input_x</cite>,
the data type is the same as <cite>input_x</cite>, the shape is <cite>indices_shape[:-1] + x_shape[indices_shape[-1]:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_nd_sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNdSub</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_nd_sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[1, -6, -3, 4, -2, 6, 7, -1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterNdUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterNdUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterNdUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterNdUpdate" title="Permalink to this definition"></a></dt>
<dd><p>Updates tensor value by using input indices and value.</p>
<p>Using given values to update tensor value, along with the input indices.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: True.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target tensor, with data type of Parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index of input tensor, with int32 data type.</p></li>
<li><p><strong>update</strong> (Tensor) - The tensor to be updated to the input tensor, has the same type as input.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_x</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNdUpdate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterNonAliasingAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterNonAliasingAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterNonAliasingAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterNonAliasingAdd" title="Permalink to this definition"></a></dt>
<dd><p>Applies sparse addition to input using individual values or slices.</p>
<p>Using given values to update tensor value through the add operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter. The data type must be float16, float32 or int32.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to perform the addition operation whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor that performs the addition operation with <cite>input_x</cite>,
the data type is the same as <cite>input_x</cite>, the shape is <cite>indices_shape[:-1] + x_shape[indices_shape[-1]:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_non_aliasing_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNonAliasingAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_non_aliasing_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[1, 10, 9, 4, 12, 6, 7, 17]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterSub">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterSub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterSub"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterSub" title="Permalink to this definition"></a></dt>
<dd><p>Updates the value of the input tensor through the subtraction operation.</p>
<p>Using given values to update tensor value through the subtraction operation, along with the input indices.
This operation outputs the <cite>input_x</cite> after the update is done, which makes it convenient to use the updated value.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index to perform the subtraction operation
whose data type must be mindspore.int32.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor that performs the subtraction operation with <cite>input_x</cite>,
the data type is the same as <cite>input_x</cite>, the shape is <cite>indices_shape + x_shape[1:]</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Parameter, the updated <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scatter_sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterSub</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">scatter_sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[[-1.0, -1.0, -1.0], [-1.0, -1.0, -1.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ScatterUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ScatterUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ScatterUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ScatterUpdate" title="Permalink to this definition"></a></dt>
<dd><p>Updates tensor value by using input indices and value.</p>
<p>Using given values to update tensor value, along with the input indices.</p>
<p>Inputs of <cite>input_x</cite> and <cite>updates</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether protect the assignment by a lock. Default: True.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Parameter) - The target tensor, with data type of Parameter.</p></li>
<li><p><strong>indices</strong> (Tensor) - The index of input tensor. With int32 data type.</p></li>
<li><p><strong>updates</strong> (Tensor) - The tensor to update the input tensor, has the same type as input,
and updates.shape = indices.shape + input_x.shape[1:].</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">np_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_x</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np_updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]],</span> <span class="p">[[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updates</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np_updates</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterUpdate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
<span class="go">[[2.0, 1.2, 1.0],</span>
<span class="go"> [3.0, 1.2, 1.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Select">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Select</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Select"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Select" title="Permalink to this definition"></a></dt>
<dd><p>Returns the selected elements, either from input <span class="math notranslate nohighlight">\(x\)</span> or input <span class="math notranslate nohighlight">\(y\)</span>, depending on the <cite>condition</cite>.</p>
<p>Given a tensor as input, this operation inserts a dimension of 1 at the dimension,
if both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are none, the operation returns the coordinates of the true
element in the <cite>condition</cite>, the coordinates are returned as a two-dimensional
tensor, where the first dimension (row) represents the number of true elements
and the second dimension (columns) represents the coordinates of the true
elements. Keep in mind that the shape of the output tensor can vary depending
on how many true values are in the input. Indexes are output in row-first
order.</p>
<p>If neither is None, <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> must have the same shape. If <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are
scalars, the conditional tensor must be a scalar. If <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are
higher-demensional vectors, the <cite>condition</cite> must be a vector whose size matches the
first dimension of <span class="math notranslate nohighlight">\(x\)</span>, or must have the same shape as <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The conditional tensor acts as an optional compensation (mask), which
determines whether the corresponding element / row in the output must be
selected from <span class="math notranslate nohighlight">\(x\)</span> (if true) or <span class="math notranslate nohighlight">\(y\)</span> (if false) based on the value of each
element.</p>
<p>If condition is a vector, then <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are higher-demensional matrices, then it
chooses to copy that row (external dimensions) from <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. If condition has
the same shape as <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, you can choose to copy these elements from <span class="math notranslate nohighlight">\(x\)</span>
and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor[bool]) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N, ..., x_R)\)</span>.
The condition tensor, decides which element is chosen.</p></li>
<li><p><strong>input_y</strong> (Tensor) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N, ..., x_R)\)</span>.
The first input tensor.</p></li>
<li><p><strong>input_z</strong> (Tensor) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N, ..., x_R)\)</span>.
The second input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_y</cite>. The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">select</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_z</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">select</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">,</span> <span class="n">input_z</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Shape">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Shape"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Shape" title="Permalink to this definition"></a></dt>
<dd><p>Returns the shape of input tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[int], the output tuple is constructed by multiple integers,
<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">shape</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sigmoid" title="Permalink to this definition"></a></dt>
<dd><p>Sigmoid activation function.</p>
<p>Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}(x_i) = \frac{1}{1 + exp(-x_i)},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is the element of the input.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input of Sigmoid, data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the input_x.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.73105866, 0.880797, 0.9525742, 0.98201376, 0.9933071]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SigmoidCrossEntropyWithLogits">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SigmoidCrossEntropyWithLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SigmoidCrossEntropyWithLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SigmoidCrossEntropyWithLogits" title="Permalink to this definition"></a></dt>
<dd><p>Uses the given logits to compute sigmoid cross entropy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sets input logits as <cite>X</cite>, input label as <cite>Y</cite>, output as <cite>loss</cite>. Then,</p>
<div class="math notranslate nohighlight">
\[p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}}\]</div>
<div class="math notranslate nohighlight">
\[loss_{ij} = -[Y_{ij} * ln(p_{ij}) + (1 - Y_{ij})ln(1 - p_{ij})]\]</div>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - Input logits.</p></li>
<li><p><strong>label</strong> (Tensor) - Ground truth label.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same shape and type as input <cite>logits</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SigmoidCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Sign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sign" title="Permalink to this definition"></a></dt>
<dd><p>Perform <span class="math notranslate nohighlight">\(sign\)</span> on tensor element-wise.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math notranslate nohighlight">
\[sign(x) = \begin{cases} -1, &amp;if\ x &lt; 0 \cr
0, &amp;if\ x = 0 \cr
1, &amp;if\ x &gt; 0\end{cases}\]</div>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">sign</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[1.0, 0.0, -1.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Sin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sin" title="Permalink to this definition"></a></dt>
<dd><p>Computes sine of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sin</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.62</span><span class="p">,</span> <span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.62</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sinh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sinh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Sinh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sinh" title="Permalink to this definition"></a></dt>
<dd><p>Computes hyperbolic sine of input element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sinh</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sinh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.62</span><span class="p">,</span> <span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.62</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">sinh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.6604918 0.28367308 0.44337422 0.6604918]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Size">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Size" title="Permalink to this definition"></a></dt>
<dd><p>Returns the elements count size of a tensor.</p>
<p>Returns an int scalar representing the elements size of input, the total number of elements in the tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>int, a scalar representing the elements size of <cite>input_x</cite>, tensor is the number of elements
in a tensor, <span class="math notranslate nohighlight">\(size=x_1*x_2*...x_R\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Size</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">size</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Slice">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Slice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Slice"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Slice" title="Permalink to this definition"></a></dt>
<dd><p>Slices a tensor in the specified shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The target tensor.</p></li>
<li><p><strong>begin</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The beginning of the slice. Only constant value is allowed.</p></li>
<li><p><strong>size</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The size of the slice. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span>                        <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span>                        <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SmoothL1Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SmoothL1Loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SmoothL1Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SmoothL1Loss" title="Permalink to this definition"></a></dt>
<dd><p>Computes smooth L1 loss, a robust L1 loss.</p>
<p>SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the
<a class="reference external" href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a> by Ross Girshick.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sets input prediction as <cite>X</cite>, input target as <cite>Y</cite>, output as <cite>loss</cite>. Then,</p>
<div class="math notranslate nohighlight">
\[\text{SmoothL1Loss} = \begin{cases} \frac{0.5 x^{2}}{\text{beta}}, &amp;if \left |x \right | &lt; \text{beta} \cr
\left |x \right|-0.5 \text{beta}, &amp;\text{otherwise}\end{cases}\]</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>beta</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A parameter used to control the point where the function will change from
quadratic to linear. Default: 1.0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>prediction</strong> (Tensor) - Predict data. Data type must be float16 or float32.</p></li>
<li><p><strong>target</strong> (Tensor) - Ground truth data, with the same type and shape as <cite>prediction</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as <cite>prediction</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">)</span>
<span class="go">[0, 0, 0.5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Softmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Softmax" title="Permalink to this definition"></a></dt>
<dd><p>Softmax operation.</p>
<p>Applies the Softmax operation to the input tensor on the specified axis.
Suppose a slice in the given aixs <span class="math notranslate nohighlight">\(x\)</span>, then for each element <span class="math notranslate nohighlight">\(x_i\)</span>,
the Softmax function is shown as follows:</p>
<div class="math notranslate nohighlight">
\[\text{output}(x_i) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)},\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the length of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>]</em>) – The axis to perform the Softmax operation. Default: -1.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - The input of Softmax, with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the logits.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.01165623, 0.03168492, 0.08612854, 0.23412167, 0.6364086]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SoftmaxCrossEntropyWithLogits">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SoftmaxCrossEntropyWithLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SoftmaxCrossEntropyWithLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SoftmaxCrossEntropyWithLogits" title="Permalink to this definition"></a></dt>
<dd><p>Gets the softmax cross-entropy value between logits and labels with one-hot encoding.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sets input logits as <cite>X</cite>, input label as <cite>Y</cite>, output as <cite>loss</cite>. Then,</p>
<div class="math notranslate nohighlight">
\[p_{ij} = softmax(X_{ij}) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}\]</div>
<div class="math notranslate nohighlight">
\[loss_{ij} = -\sum_j{Y_{ij} * ln(p_{ij})}\]</div>
</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - Input logits, with shape <span class="math notranslate nohighlight">\((N, C)\)</span>. Data type must be float16 or float32.</p></li>
<li><p><strong>labels</strong> (Tensor) - Ground truth labels, with shape <span class="math notranslate nohighlight">\((N, C)\)</span>, has the same data type with <cite>logits</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 tensors, the <cite>loss</cite> shape is <cite>(N,)</cite>, and the <cite>dlogits</cite> with the same shape as <cite>logits</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softmax_cross</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="p">,</span> <span class="n">backprop</span> <span class="o">=</span> <span class="n">softmax_cross</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="go">([0.5899297, 0.52374405], [[0.02760027, 0.20393994, 0.01015357, 0.20393994, -0.44563377],</span>
<span class="go">[0.08015892, 0.02948882, 0.08015892, -0.4077012, 0.21789455]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Softplus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Softplus</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Softplus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Softplus" title="Permalink to this definition"></a></dt>
<dd><p>Softplus activation function.</p>
<p>Softplus is a smooth approximation to the ReLU function.
The function is shown as follows:</p>
<div class="math notranslate nohighlight">
\[\text{output} = \log(1 + \exp(\text{input_x})),\]</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose data type must be float.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softplus</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softplus</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softplus</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.3132615, 2.126928, 3.0485873, 4.01815, 5.0067153]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Softsign">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Softsign</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Softsign"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Softsign" title="Permalink to this definition"></a></dt>
<dd><p>Softsign activation function.</p>
<p>The function is shown as follows:</p>
<div class="math notranslate nohighlight">
\[\text{output} = \frac{\text{input_x}}{1 + \left| \text{input_x} \right|},\]</div>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose data type must be float16 or float32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softsign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Softsign</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">softsign</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0. -0.5 0.6666667 0.9677419 -0.9677419]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sort">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Sort"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sort" title="Permalink to this definition"></a></dt>
<dd><p>Sorts the elements of the input tensor along a given dimension in ascending order by value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension to sort along. Default: -1.</p></li>
<li><p><strong>descending</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Controls the sorting order. If descending is True then the elements
are sorted in descending order by value. Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input to sort, with float16 or float32 data type.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>y1</strong> (Tensor) - A tensor whose values are the sorted values, with the same shape and data type as input.</p></li>
<li><p><strong>y2</strong> (Tensor) - The indices of the elements in the original input tensor. Data type is int32.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sort</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">6.0</span> <span class="p">,</span><span class="mf">7.0</span><span class="p">]],</span>
<span class="go">     [[2, 1, 0], [2, 0, 1], [0, 1, 2]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SpaceToBatch">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SpaceToBatch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#SpaceToBatch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SpaceToBatch" title="Permalink to this definition"></a></dt>
<dd><p>Divides spatial dimensions into blocks and combine the block size with the original batch.</p>
<p>This operation will divide spatial dimensions (H, W) into blocks with <cite>block_size</cite>, the output tensor’s H and W
dimension is the corresponding number of blocks after division. The output tensor’s batch dimension is the
product of the original batch and the square of block_size. Before division, the spatial dimensions
of the input are zero padded according to paddings if necessary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>block_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The block size of dividing blocks with value greater than 2.</p></li>
<li><p><strong>paddings</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The padding values for H and W dimension, containing 2 subtraction lists.
Each subtraction list contains 2 integer value. All values must be greater than 0.
paddings[i] specifies the paddings for the spatial dimension i, which corresponds to the
input dimension i+2. It is required that input_shape[i+2]+paddings[i][0]+paddings[i][1]
is divisible by block_size.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. It must be a 4-D tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the output tensor with the same data type as input. Assume input shape is <span class="math notranslate nohighlight">\((n, c, h, w)\)</span> with
<span class="math notranslate nohighlight">\(block\_size\)</span> and <span class="math notranslate nohighlight">\(paddings\)</span>. The shape of the output tensor will be <span class="math notranslate nohighlight">\((n', c', h', w')\)</span>,
where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(n' = n*(block\_size*block\_size)\)</span></p>
<p><span class="math notranslate nohighlight">\(c' = c\)</span></p>
<p><span class="math notranslate nohighlight">\(h' = (h+paddings[0][0]+paddings[0][1])//block\_size\)</span></p>
<p><span class="math notranslate nohighlight">\(w' = (w+paddings[1][0]+paddings[1][1])//block\_size\)</span></p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paddings</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">space_to_batch</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SpaceToBatch</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">space_to_batch</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[[[1.]]], [[[2.]]], [[[3.]]], [[[4.]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SpaceToBatchND">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SpaceToBatchND</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#SpaceToBatchND"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SpaceToBatchND" title="Permalink to this definition"></a></dt>
<dd><p>Divides spatial dimensions into blocks and combine the block size with the original batch.</p>
<p>This operation will divide spatial dimensions (H, W) into blocks with block_shape, the output tensor’s H and W
dimension is the corresponding number of blocks after division. The output tensor’s batch dimension is the
product of the original batch and the product of <cite>block_shape</cite>. Before division,
the spatial dimensions of the input are zero padded according to paddings if necessary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>block_shape</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>]</em>) – The block shape of dividing block with all value greater than 1.
The length of <cite>block_shape</cite> is M correspoding to the number of spatial dimensions. M must be 2.</p></li>
<li><p><strong>paddings</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The padding values for H and W dimension, containing 2 subtraction list.
Each contains 2 integer value. All values must be greater than 0.
<cite>paddings[i]</cite> specifies the paddings for the spatial dimension i,
which corresponds to the input dimension i+2.
It is required that input_shape[i+2]+paddings[i][0]+paddings[i][1] is divisible by block_shape[i].</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor. It must be a 4-D tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the output tensor with the same data type as input. Assume input shape is <span class="math notranslate nohighlight">\((n, c, h, w)\)</span> with
<span class="math notranslate nohighlight">\(block\_shape\)</span> and <span class="math notranslate nohighlight">\(padddings\)</span>. The shape of the output tensor will be <span class="math notranslate nohighlight">\((n', c', h', w')\)</span>,
where</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(n' = n*(block\_shape[0]*block\_shape[1])\)</span></p>
<p><span class="math notranslate nohighlight">\(c' = c\)</span></p>
<p><span class="math notranslate nohighlight">\(h' = (h+paddings[0][0]+paddings[0][1])//block\_shape[0]\)</span></p>
<p><span class="math notranslate nohighlight">\(w' = (w+paddings[1][0]+paddings[1][1])//block\_shape[1]\)</span></p>
</div></blockquote>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">block_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paddings</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">space_to_batch_nd</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SpaceToBatchND</span><span class="p">(</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">space_to_batch_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[[[[1.]]], [[[2.]]], [[[3.]]], [[[4.]]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SpaceToDepth">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SpaceToDepth</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#SpaceToDepth"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SpaceToDepth" title="Permalink to this definition"></a></dt>
<dd><p>Rearranges blocks of spatial data into depth.</p>
<p>The output tensor’s <cite>height</cite> dimension is <span class="math notranslate nohighlight">\(height / block\_size\)</span>.</p>
<p>The output tensor’s <cite>weight</cite> dimension is <span class="math notranslate nohighlight">\(weight / block\_size\)</span>.</p>
<p>The depth of output tensor is <span class="math notranslate nohighlight">\(block\_size * block\_size * input\_depth\)</span>.</p>
<p>The input tensor’s height and width must be divisible by <cite>block_size</cite>.
The data format is “NCHW”.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>block_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The block size used to divide spatial data. It must be &gt;= 2.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The target tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the same data type as <cite>x</cite>. It must be a 4-D tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SpaceToDepth</span><span class="p">(</span><span class="n">block_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseApplyAdagrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseApplyAdagrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SparseApplyAdagrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseApplyAdagrad" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the adagrad scheme.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[var -= lr * grad * (1 / sqrt(accum))\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Learning rate.</p></li>
<li><p><strong>update_slots</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>True</cite>, <cite>accum</cite> will be updated. Default: True.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the <cite>var</cite> and <cite>accumulation</cite> tensors will be protected from being updated.
Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - Accumulation to be updated. The shape and data type must be the same as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradient. The shape must be the same as <cite>var</cite>’s shape except the first dimension.
Gradients has the same data type as <cite>var</cite>.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices into the first dimension of <cite>var</cite> and <cite>accum</cite>.
The shape of <cite>indices</cite> must be the same as <cite>grad</cite> in first dimension, the type must be int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>accum</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adagrad</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseApplyAdagrad</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adagrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseApplyAdagradV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseApplyAdagradV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SparseApplyAdagradV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseApplyAdagradV2" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the adagrad scheme.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Learning rate.</p></li>
<li><p><strong>epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A small value added for numerical stability.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>True</cite>, the <cite>var</cite> and <cite>accum</cite> tensors will be protected from being updated.
Default: False.</p></li>
<li><p><strong>update_slots</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>True</cite>, the computation logic will be different to <cite>False</cite>. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - Accumulation to be updated. The shape and data type must be the same as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - Gradient. The shape must be the same as <cite>var</cite>’s shape except the first dimension.
Gradients has the same data type as <cite>var</cite>.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices into the first dimension of <cite>var</cite> and <cite>accum</cite>.
The shape of <cite>indices</cite> must be the same as <cite>grad</cite> in first dimension, the type must be int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>accum</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adagrad_v2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseApplyAdagradV2</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_adagrad_v2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseApplyFtrl">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseApplyFtrl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SparseApplyFtrl"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseApplyFtrl" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the FTRL-proximal scheme.</p>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The learning rate value, must be positive.</p></li>
<li><p><strong>l1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l1 regularization strength, must be greater than or equal to zero.</p></li>
<li><p><strong>l2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l2 regularization strength, must be greater than or equal to zero.</p></li>
<li><p><strong>lr_power</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Learning rate power controls how the learning rate decreases during training,
must be less than or equal to zero. Use fixed learning rate if <cite>lr_power</cite> is zero.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use locks for updating operation if true . Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - The variable to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - The accumulation to be updated, must be same data type and shape as <cite>var</cite>.</p></li>
<li><p><strong>linear</strong> (Parameter) - the linear coefficient to be updated, must be the same data type and shape as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices in the first dimension of <cite>var</cite> and <cite>accum</cite>.
The shape of <cite>indices</cite> must be the same as <cite>grad</cite> in the first dimension. The type must be int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Tensor) - Tensor, has the same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - Tensor, has the same shape and data type as <cite>accum</cite>.</p></li>
<li><p><strong>linear</strong> (Tensor) - Tensor, has the same shape and data type as <cite>linear</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SparseApplyFtrlNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SparseApplyFtrlNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_ftrl</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseApplyFtrl</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">l1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">lr_power</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_ftrl</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">SparseApplyFtrlNet</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseApplyFtrlV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseApplyFtrlV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SparseApplyFtrlV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseApplyFtrlV2" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the FTRL-proximal scheme.</p>
<p>All of inputs except <cite>indices</cite> comply with the implicit type conversion rules to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>lr</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The learning rate value, must be positive.</p></li>
<li><p><strong>l1</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l1 regularization strength, must be greater than or equal to zero.</p></li>
<li><p><strong>l2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – l2 regularization strength, must be greater than or equal to zero.</p></li>
<li><p><strong>l2_shrinkage</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – L2 shrinkage regularization.</p></li>
<li><p><strong>lr_power</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Learning rate power controls how the learning rate decreases during training,
must be less than or equal to zero. Use fixed learning rate if <cite>lr_power</cite> is zero.</p></li>
<li><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If <cite>True</cite>, the var and accumulation tensors will be protected from being updated.
Default: False.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - The variable to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - The accumulation to be updated, must be same data type and shape as <cite>var</cite>.</p></li>
<li><p><strong>linear</strong> (Parameter) - the linear coefficient to be updated, must be same data type and shape as <cite>var</cite>.</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices in the first dimension of <cite>var</cite> and <cite>accum</cite>.
The shape of <cite>indices</cite> must be the same as <cite>grad</cite> in the first dimension. The type must be int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - Tensor, has the same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - Tensor, has the same shape and data type as <cite>accum</cite>.</p></li>
<li><p><strong>linear</strong> (Tensor) - Tensor, has the same shape and data type as <cite>linear</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SparseApplyFtrlV2Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SparseApplyFtrlV2Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_ftrl_v2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseApplyFtrlV2</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">l1</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="go">                                                            l2_shrinkage=0.0, lr_power=-0.5)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_ftrl_v2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">SparseApplyFtrlV2Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseApplyProximalAdagrad">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseApplyProximalAdagrad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SparseApplyProximalAdagrad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseApplyProximalAdagrad" title="Permalink to this definition"></a></dt>
<dd><p>Updates relevant entries according to the proximal adagrad algorithm. Compared with ApplyProximalAdagrad,
an additional index tensor is input.</p>
<div class="math notranslate nohighlight">
\[accum += grad * grad\]</div>
<div class="math notranslate nohighlight">
\[\text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}\]</div>
<div class="math notranslate nohighlight">
\[var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)\]</div>
<p>Inputs of <cite>var</cite>, <cite>accum</cite> and <cite>grad</cite> comply with the implicit type conversion rules
to make the data types consistent.
If they have different data types, lower priority data type will be converted to
relatively highest priority data type.
RuntimeError exception will be thrown when the data type conversion of Parameter is required.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>use_locking</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the <cite>var</cite> and <cite>accum</cite> tensors will be protected from being updated.
Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - Variable tensor to be updated. The data type must be float16 or float32.</p></li>
<li><p><strong>accum</strong> (Parameter) - Variable tensor to be updated, has the same dtype as <cite>var</cite>.</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - The learning rate value, must be a float number or
a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>l1</strong> (Union[Number, Tensor]) - l1 regularization strength, must be a float number or
a scalar tensor with float16 or float32 data type.</p></li>
<li><p><strong>l2</strong> (Union[Number, Tensor]) - l2 regularization strength, must be a float number or
a scalar tensor with float16 or float32 data type..</p></li>
<li><p><strong>grad</strong> (Tensor) - A tensor of the same type as <cite>var</cite>, for the gradient.</p></li>
<li><p><strong>indices</strong> (Tensor) - A vector of indices in the first dimension of <cite>var</cite> and <cite>accum</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 tensors, the updated parameters.</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - The same shape and data type as <cite>var</cite>.</p></li>
<li><p><strong>accum</strong> (Tensor) - The same shape and data type as <cite>accum</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_proximal_adagrad</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseApplyProximalAdagrad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_apply_proximal_adagrad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">,</span>
<span class="go">                                                     self.l2, grad, indices)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,),</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseGatherV2">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseGatherV2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#SparseGatherV2"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseGatherV2" title="Permalink to this definition"></a></dt>
<dd><p>Returns a slice of input tensor based on the specified indices and axis.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_params</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
The original Tensor.</p></li>
<li><p><strong>input_indices</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.
Specifies the indices of elements of the original Tensor, must be in the range
<cite>[0, input_param.shape[axis])</cite>.</p></li>
<li><p><strong>axis</strong> (int) - Specifies the dimension index to gather indices.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((z_1, z_2, ..., z_N)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_params</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">42</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">22</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">55</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseGatherV2</span><span class="p">()(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseSoftmaxCrossEntropyWithLogits">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseSoftmaxCrossEntropyWithLogits</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#SparseSoftmaxCrossEntropyWithLogits"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseSoftmaxCrossEntropyWithLogits" title="Permalink to this definition"></a></dt>
<dd><p>Computes the softmax cross-entropy value between logits and sparse encoding labels.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sets input logits as <cite>X</cite>, input label as <cite>Y</cite>, output as <cite>loss</cite>. Then,</p>
<div class="math notranslate nohighlight">
\[p_{ij} = softmax(X_{ij}) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}\]</div>
<div class="math notranslate nohighlight">
\[loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr -ln(1 - p_{ij}), &amp; j \neq y_i \end{cases}\]</div>
<div class="math notranslate nohighlight">
\[loss = \sum_{ij} loss_{ij}\]</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>is_grad</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, this operation returns the computed gradient. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - Input logits, with shape <span class="math notranslate nohighlight">\((N, C)\)</span>. Data type must be float16 or float32.</p></li>
<li><p><strong>labels</strong> (Tensor) - Ground truth labels, with shape <span class="math notranslate nohighlight">\((N)\)</span>.
Data type must be int32 or int64.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, if <cite>is_grad</cite> is False, the output tensor is the value of loss which is a scalar tensor;
if <cite>is_grad</cite> is True, the output tensor is the gradient of input with the same shape as <cite>logits</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Please refer to the usage in nn.SoftmaxCrossEntropyWithLogits source code.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SparseToDense">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SparseToDense</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/sparse_ops.html#SparseToDense"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SparseToDense" title="Permalink to this definition"></a></dt>
<dd><p>Converts a sparse representation into a dense tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>indices</strong> (Tensor) - The indices of sparse representation.</p></li>
<li><p><strong>values</strong> (Tensor) - Values corresponding to each row of indices.</p></li>
<li><p><strong>dense_shape</strong> (tuple) - An int tuple which specifies the shape of dense tensor.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Tensor, the shape of tensor is <cite>dense_shape</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dense_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseToDense</span><span class="p">()(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">dense_shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Split">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Split"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Split" title="Permalink to this definition"></a></dt>
<dd><p>Splits input tensor into output_num of tensors along the given axis and output numbers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Index of the split position. Default: 0.</p></li>
<li><p><strong>output_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of output tensors. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>axis</cite> is out of the range [-len(<cite>input_x.shape</cite>), len(<cite>input_x.shape</cite>)),
    or if the <cite>output_num</cite> is less than or equal to 0, or if the
    dimension which to split cannot be evenly divided by <cite>output_num</cite>.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[Tensor], the shape of each output tensor is the same, which is
<span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">split</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sqrt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sqrt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Sqrt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sqrt" title="Permalink to this definition"></a></dt>
<dd><p>Returns square root of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose dtype is number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqrt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sqrt</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0, 2.0, 3.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Square">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Square</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Square"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Square" title="Permalink to this definition"></a></dt>
<dd><p>Returns square of a tensor element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input tensor whose dtype is number.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and dtype as the <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[1.0, 4.0, 9.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SquareSumAll">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SquareSumAll</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#SquareSumAll"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SquareSumAll" title="Permalink to this definition"></a></dt>
<dd><p>Returns square sum all of a tensor element-wise</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x1</strong> (Tensor) - The input tensor. The data type must be float16 or float32.</p></li>
<li><p><strong>input_x2</strong> (Tensor) - The input tensor has the same type and shape as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SquareSumAll only supports float16 and float32 data type.</p>
</div>
<dl class="simple">
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>output_y1</strong> (Tensor) - The same type as the <cite>input_x1</cite>.</p></li>
<li><p><strong>output_y2</strong> (Tensor) - The same type as the <cite>input_x1</cite>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square_sum_all</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SquareSumAll</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">square_sum_all</span><span class="p">(</span><span class="n">input_x1</span><span class="p">,</span> <span class="n">input_x2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.SquaredDifference">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">SquaredDifference</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#SquaredDifference"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.SquaredDifference" title="Permalink to this definition"></a></dt>
<dd><p>Subtracts the second input tensor from the first input tensor element-wise and returns square of it.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number, or a bool,
or a tensor whose data type is float16, float32, int32 or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input
is a tensor or a tensor whose data type isfloat16, float32, int32 or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">squared_difference</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SquaredDifference</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">squared_difference</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[1.0, 4.0, 9.0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Squeeze">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Squeeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Squeeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Squeeze" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor with the same type but dimensions of 1 are removed based on <cite>axis</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The dimension index starts at 0 and must be in the range <cite>[-input.dim(), input.dim())</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the corresponding dimension of the specified axis does not equal to 1.</p>
</dd>
<dt class="field-even">Parameters</dt>
<dd class="field-even"><p><strong>axis</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>]</em>) – Specifies the dimension indexes of shape to be removed, which will remove
all the dimensions that are equal to 1. If specified, it must be int32 or int64.
Default: (), an empty tuple.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_S)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">squeeze</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.StandardLaplace">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">StandardLaplace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#StandardLaplace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.StandardLaplace" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the Laplace random number distribution (mean=0, lambda=1).
It is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{f}(x;0,1) = \frac{1}{2}\exp(-|x|),\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple) - The shape of random tensor to be generated. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. The shape that the input ‘shape’ denotes. The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stdlaplace</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StandardLaplace</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">stdlaplace</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.StandardNormal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">StandardNormal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#StandardNormal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.StandardNormal" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the standard Normal (or Gaussian) random number distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed, must be non-negative. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2, must be non-negative. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple) - The shape of random tensor to be generated. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. The shape is the same as the input <cite>shape</cite>. The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stdnormal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StandardNormal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">stdnormal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.StridedSlice">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">StridedSlice</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#StridedSlice"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.StridedSlice" title="Permalink to this definition"></a></dt>
<dd><p>Extracts a strided slice of a tensor.</p>
<p>Given an input tensor, this operation inserts a dimension of length 1 at the dimension.
This operation extracts a fragment of size (end-begin)/stride from the given ‘input_tensor’.
Starting from the begining position, the fragment continues adding stride to the index until
all dimensions are not less than the ending position.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The stride may be negative value, which causes reverse slicing.
The shape of <cite>begin</cite>, <cite>end</cite> and <cite>strides</cite> must be the same.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>begin_mask</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Starting index of the slice. Default: 0.</p></li>
<li><p><strong>end_mask</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Ending index of the slice. Default: 0.</p></li>
<li><p><strong>ellipsis_mask</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An int mask. Default: 0.</p></li>
<li><p><strong>new_axis_mask</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An int mask. Default: 0.</p></li>
<li><p><strong>shrink_axis_mask</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An int mask. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input Tensor.</p></li>
<li><p><strong>begin</strong> (tuple[int]) - A tuple which represents the location where to start. Only
constant value is allowed.</p></li>
<li><p><strong>end</strong> (tuple[int]) - A tuple or which represents the maximum location where to end.
Only constant value is allowed.</p></li>
<li><p><strong>strides</strong> (tuple[int]) - A tuple which represents the stride is continuously added
before reaching the maximum location. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor.
The output is explained by following example.</p>
<blockquote>
<div><ul class="simple">
<li><p>In the 0th dimension, begin is 1, end is 2, and strides is 1,
because <span class="math notranslate nohighlight">\(1+1=2\geq2\)</span>, the interval is <span class="math notranslate nohighlight">\([1,2)\)</span>.
Thus, return the element with <span class="math notranslate nohighlight">\(index = 1\)</span> in 0th dimension, i.e., [[3, 3, 3], [4, 4, 4]].</p></li>
<li><p>In the 1st dimension, similarly, the interval is <span class="math notranslate nohighlight">\([0,1)\)</span>.
Based on the return value of the 0th dimension, return the element with <span class="math notranslate nohighlight">\(index = 0\)</span>,
i.e., [3, 3, 3].</p></li>
<li><p>In the 2nd dimension, similarly, the interval is <span class="math notranslate nohighlight">\([0,3)\)</span>.
Based on the return value of the 1st dimension, return the element with <span class="math notranslate nohighlight">\(index = 0,1,2\)</span>,
i.e., [3, 3, 3].</p></li>
<li><p>Finally, the output is [3, 3, 3].</p></li>
</ul>
</div></blockquote>
</dd>
<dt>Examples</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(1, 1, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span>
<span class="go">[[[3, 3, 3]]]</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Sub">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Sub</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Sub"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Sub" title="Permalink to this definition"></a></dt>
<dd><p>Subtracts the second input tensor from the first input tensor element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number, or a bool,
or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input
is a tensor, or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[-3, -3, -3]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TBERegOp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp" title="Permalink to this definition"></a></dt>
<dd><p>Class for TBE op info register.</p>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.async_flag">
<span class="sig-name descname"><span class="pre">async_flag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">async_flag</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.async_flag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.async_flag" title="Permalink to this definition"></a></dt>
<dd><p>Define the calculation efficiency of the operator, whether the asynchronous calculation is supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>async_flag</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Value of async flag. Default: false.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.attr">
<span class="sig-name descname"><span class="pre">attr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.attr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.attr" title="Permalink to this definition"></a></dt>
<dd><p>Register TBE op attribute information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the attribute. Default: None.</p></li>
<li><p><strong>param_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Param type of the attribute. Default: None.</p></li>
<li><p><strong>value_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Type of the attribute. Default: None.</p></li>
<li><p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Value of the attribute. Default: None.</p></li>
<li><p><strong>default_value</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Default value of attribute. Default: None.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Other information of the attribute.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.binfile_name">
<span class="sig-name descname"><span class="pre">binfile_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">binfile_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.binfile_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.binfile_name" title="Permalink to this definition"></a></dt>
<dd><p>Set the binary file name of the operator, it is optional.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>binfile_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The binary file name of the operator.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.compute_cost">
<span class="sig-name descname"><span class="pre">compute_cost</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">compute_cost</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.compute_cost"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.compute_cost" title="Permalink to this definition"></a></dt>
<dd><p>Define the calculation efficiency of operator, which refers to the value of the cost model in the tiling module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>compute_cost</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Value of compute cost. Default: 10.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.dynamic_format">
<span class="sig-name descname"><span class="pre">dynamic_format</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dynamic_format</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.dynamic_format"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.dynamic_format" title="Permalink to this definition"></a></dt>
<dd><p>Whether the operator supports dynamic selection of format and dtype or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dynamic_format</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Value of dynamic format. Default: false.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.input">
<span class="sig-name descname"><span class="pre">input</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_compile</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.input"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.input" title="Permalink to this definition"></a></dt>
<dd><p>Register TBE op input information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Order of the input. Default: None.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the input. Default: None.</p></li>
<li><p><strong>need_compile</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the input needs to be compiled or not. Default: None.</p></li>
<li><p><strong>param_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Type of the input. Default: None.</p></li>
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Shape of the input. Default: None.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Other information of the input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.kernel_name">
<span class="sig-name descname"><span class="pre">kernel_name</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_name</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.kernel_name"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.kernel_name" title="Permalink to this definition"></a></dt>
<dd><p>The name of operator kernel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>kernel_name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of operator kernel.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.op_pattern">
<span class="sig-name descname"><span class="pre">op_pattern</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pattern</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.op_pattern"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.op_pattern" title="Permalink to this definition"></a></dt>
<dd><p>The behavior type of opeator, such as broadcast, reduce and so on.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>pattern</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Value of op pattern.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.output">
<span class="sig-name descname"><span class="pre">output</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">need_compile</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.output"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.output" title="Permalink to this definition"></a></dt>
<dd><p>Register TBE op output information.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Order of the output. Default: None.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Name of the output. Default: None.</p></li>
<li><p><strong>need_compile</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the output needs to be compiled or not. Default: None.</p></li>
<li><p><strong>param_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Type of the output. Default: None.</p></li>
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Shape of the output. Default: None.</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Other information of the output.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.partial_flag">
<span class="sig-name descname"><span class="pre">partial_flag</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">partial_flag</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.partial_flag"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.partial_flag" title="Permalink to this definition"></a></dt>
<dd><p>Define the calculation efficiency of operator, whether the partial calculation is supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>partial_flag</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Value of partial flag. Default: true.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.ops.TBERegOp.reshape_type">
<span class="sig-name descname"><span class="pre">reshape_type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reshape_type</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#TBERegOp.reshape_type"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TBERegOp.reshape_type" title="Permalink to this definition"></a></dt>
<dd><p>Reshape type of operator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>reshape_type</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Value of reshape type.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Tan">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Tan</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Tan"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Tan" title="Permalink to this definition"></a></dt>
<dd><p>Computes tangent of <cite>input_x</cite> element-wise.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>. Data type must be
float16, float32 or int32.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tan</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tan</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">tan</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Tanh">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Tanh</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#Tanh"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Tanh" title="Permalink to this definition"></a></dt>
<dd><p>Tanh activation function.</p>
<p>Computes hyperbolic tangent of input element-wise. The Tanh function is defined as:</p>
<div class="math notranslate nohighlight">
\[tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},\]</div>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is an element of the input Tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The input of Tanh.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, with the same type and shape as the input_x.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tanh</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">[0.7615941, 0.9640276, 0.9950548, 0.9993293, 0.99990916]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TensorAdd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TensorAdd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#TensorAdd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TensorAdd" title="Permalink to this definition"></a></dt>
<dd><p>Adds two input tensors element-wise.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number, or a bool,
or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number,  or a bool when the first input
is a tensor, or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[5,7,9]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TensorScatterUpdate">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TensorScatterUpdate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#TensorScatterUpdate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TensorScatterUpdate" title="Permalink to this definition"></a></dt>
<dd><p>Updates tensor value using given values, along with the input indices.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The target tensor. The dimension of input_x must be equal to indices.shape[-1].</p></li>
<li><p><strong>indices</strong> (Tensor) - The index of input tensor whose data type is int32.</p></li>
<li><p><strong>update</strong> (Tensor) - The tensor to update the input tensor, has the same type as input,
and update.shape = indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and type as <cite>input_x</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">update</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterUpdate</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">update</span><span class="p">)</span>
<span class="go">[[1.0, 0.3, 3.6],</span>
<span class="go"> [0.4, 2.2, -3.2]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TensorSummary">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TensorSummary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/debug_ops.html#TensorSummary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TensorSummary" title="Permalink to this definition"></a></dt>
<dd><p>Outputs a tensor to a protocol buffer through a tensor summary operator.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>name</strong> (str) - The name of the input variable.</p></li>
<li><p><strong>value</strong> (Tensor) - The value of tensor, and the rank of tensor must be greater than 0.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SummaryDemo</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">SummaryDemo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorSummary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Tile">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Tile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Tile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Tile" title="Permalink to this definition"></a></dt>
<dd><p>Replicates a tensor with given multiples times.</p>
<p>Creates a new tensor by replicating input multiples times. The dimension of
output tensor is the larger of the input tensor dimension and the length of <cite>multiples</cite>.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - 1-D or higher Tensor. Set the shape of input tensor as
<span class="math notranslate nohighlight">\((x_1, x_2, ..., x_S)\)</span>.</p></li>
<li><p><strong>multiples</strong> (tuple[int]) - The input tuple is constructed by multiple
integers, i.e., <span class="math notranslate nohighlight">\((y_1, y_2, ..., y_S)\)</span>. The length of <cite>multiples</cite>
cannot be smaller than the length of the shape of <cite>input_x</cite>.
Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same data type as the <cite>input_x</cite>.</p>
<ul class="simple">
<li><p>If the length of <cite>multiples</cite> is the same as the length of shape of <cite>input_x</cite>,
then the shape of their corresponding positions can be multiplied, and
the shape of Outputs is <span class="math notranslate nohighlight">\((x_1*y_1, x_2*y_2, ..., x_S*y_R)\)</span>.</p></li>
<li><p>If the length of <cite>multiples</cite> is larger than the length of shape of <cite>input_x</cite>,
fill in multiple 1 in the length of the shape of <cite>input_x</cite> until their lengths are consistent.
Such as set the shape of <cite>input_x</cite> as <span class="math notranslate nohighlight">\((1, ..., x_1, x_2, ..., x_S)\)</span>,
then the shape of their corresponding positions can be multiplied, and
the shape of Outputs is <span class="math notranslate nohighlight">\((1*y_1, ..., x_S*y_R)\)</span>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multiples</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">tile</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">)</span>
<span class="go">[[1.  2.  1.  2.  1.  2.]</span>
<span class="go"> [3.  4.  3.  4.  3.  4.]</span>
<span class="go"> [1.  2.  1.  2.  1.  2.]</span>
<span class="go"> [3.  4.  3.  4.  3.  4.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TopK">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TopK</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/nn_ops.html#TopK"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TopK" title="Permalink to this definition"></a></dt>
<dd><p>Finds values and indices of the <cite>k</cite> largest entries along the last dimension.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sorted</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, the obtained elements will
be sorted by the values in descending order. Default: False.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input to be computed, data type must be float16, float32 or int32.</p></li>
<li><p><strong>k</strong> (int) - The number of top elements to be computed along the last dimension, constant input is needed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 2 tensors, the values and the indices.</p>
<ul class="simple">
<li><p><strong>values</strong> (Tensor) - The <cite>k</cite> largest elements in each slice of the last dimensional.</p></li>
<li><p><strong>indices</strong> (Tensor) - The indices of values within the last dimension of input.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">topk</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TopK</span><span class="p">(</span><span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">topk</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">values</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">indices</span> <span class="o">==</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Transpose">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Transpose</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Transpose"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Transpose" title="Permalink to this definition"></a></dt>
<dd><p>Permutes the dimensions of input tensor according to input permutation.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape of tensor is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>input_perm</strong> (tuple[int]) - The permutation to be converted. The input tuple is constructed by multiple
indexes. The length of <cite>input_perm</cite> and the shape of <cite>input_x</cite> must be the same. Only constant value is
allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the type of output tensor is the same as <cite>input_x</cite> and the shape of output tensor is decided by the
shape of <cite>input_x</cite> and the value of <cite>input_perm</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">perm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TruncateDiv">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TruncateDiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#TruncateDiv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TruncateDiv" title="Permalink to this definition"></a></dt>
<dd><p>Divide the first input tensor by the second input tensor element-wise for integer types, negative numbers will
round fractional quantities towards zero.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number, or a bool,
or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input
is a tensor, or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truncate_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncateDiv</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truncate_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[0, 1, 0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TruncateMod">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TruncateMod</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#TruncateMod"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TruncateMod" title="Permalink to this definition"></a></dt>
<dd><p>Returns element-wise remainder of division.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number, or a bool,
or a tensor whose data type is number or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number, or a bool when the first input
is a tensor, or a tensor whose data type is number or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truncate_mod</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncateMod</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truncate_mod</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[2, 1, -1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TruncatedNormal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TruncatedNormal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#TruncatedNormal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TruncatedNormal" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor of the specified shape filled with truncated normal values.</p>
<p>The generated values follow a normal distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – A integer number used to create random seed. Default: 0.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.dtype</span></code></a>) – Data type. Default: mindspore.float32.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple[int]) - The shape of the output tensor, is a tuple of positive integer.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the dat type of output tensor is the same as attribute <cite>dtype</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">truncated_normal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncatedNormal</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">truncated_normal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.TupleToArray">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">TupleToArray</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#TupleToArray"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.TupleToArray" title="Permalink to this definition"></a></dt>
<dd><p>Converts a tuple to a tensor.</p>
<p>If the type of the first number in the tuple is integer, the data type of the output tensor is int.
Otherwise, the data type of the output tensor is float.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (tuple) - A tuple of numbers. These numbers have the same type. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, if the input tuple contains <cite>N</cite> numbers, then the shape of the output tensor is (N,).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">type</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TupleToArray</span><span class="p">()((</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.UniformInt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">UniformInt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#UniformInt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.UniformInt" title="Permalink to this definition"></a></dt>
<dd><p>Produces random integer values i, uniformly distributed on the closed interval [minval, maxval), that is,
distributed according to the discrete probability function:</p>
<div class="math notranslate nohighlight">
\[\text{P}(i|a,b) = \frac{1}{b-a+1},\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The number in tensor minval must be strictly less than maxval at any position after broadcasting.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed, must be non-negative. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2, must be non-negative. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple) - The shape of random tensor to be generated. Only constant value is allowed.</p></li>
<li><p><strong>minval</strong> (Tensor) - The distribution parameter, a.
It defines the minimum possibly generated value, with int32 data type. Only one number is supported.</p></li>
<li><p><strong>maxval</strong> (Tensor) - The distribution parameter, b.
It defines the maximum possibly generated value, with int32 data type. Only one number is supported.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. The shape is the same as the input ‘shape’, and the data type is int32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">minval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maxval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uniform_int</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UniformInt</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">uniform_int</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.UniformReal">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">UniformReal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/random_ops.html#UniformReal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.UniformReal" title="Permalink to this definition"></a></dt>
<dd><p>Produces random floating-point values i, uniformly distributed to the interval [0, 1).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed, must be non-negative. Default: 0.</p></li>
<li><p><strong>seed2</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Random seed2, must be non-negative. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>shape</strong> (tuple) - The shape of random tensor to be generated. Only constant value is allowed.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. The shape that the input ‘shape’ denotes. The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">uniformreal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UniformReal</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">uniformreal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Unique">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Unique</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Unique"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Unique" title="Permalink to this definition"></a></dt>
<dd><p>Returns the unique elements of input tensor and also return a tensor containing the index of each value of input
tensor corresponding to the output unique tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, containing Tensor objects <cite>(y, idx)</cite>, <cite>y</cite> is a tensor has the same type as <cite>x</cite>, <cite>idx</cite> is a tensor
containing indices of elements in the input coressponding to the output tensor.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Unique</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="go">(Tensor([1, 2, 5], mindspore.int32), Tensor([0, 1, 2, 1], mindspore.int32))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Unpack">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Unpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#Unpack"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Unpack" title="Permalink to this definition"></a></dt>
<dd><p>Unpacks tensor in specified axis.</p>
<p>Unpacks a tensor of rank <cite>R</cite> along axis dimension, output tensors will have rank <cite>(R-1)</cite>.</p>
<p>Given a tensor of shape <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>. If <span class="math notranslate nohighlight">\(0 \le axis\)</span>,
the shape of tensor in output is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_{axis}, x_{axis+2}, ..., x_R)\)</span>.</p>
<p>This is the opposite of pack.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Dimension along which to pack. Default: 0.
Negative values wrap around. The range is [-R, R).</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
A tensor to be unpacked and the rank of the tensor must be greater than 0.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>A tuple of tensors, the shape of each objects is the same.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If axis is out of the range [-len(input_x.shape), len(input_x.shape)).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">unpack</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Unpack</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">unpack</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
<span class="go">([1, 1, 1, 1], [2, 2, 2, 2])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.UnsortedSegmentMin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">UnsortedSegmentMin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#UnsortedSegmentMin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.UnsortedSegmentMin" title="Permalink to this definition"></a></dt>
<dd><p>Computes the minimum along segments of a tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
The data type must be float16, float32 or int32.</p></li>
<li><p><strong>segment_ids</strong> (Tensor) - A <cite>1-D</cite> tensor whose shape is <span class="math notranslate nohighlight">\((x_1)\)</span>, the value must be &gt;= 0.
The data type must be int32.</p></li>
<li><p><strong>num_segments</strong> (int) - The value spcifies the number of distinct <cite>segment_ids</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, set the number of <cite>num_segments</cite> as <cite>N</cite>, the shape is <span class="math notranslate nohighlight">\((N, x_2, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">segment_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_segments</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unsorted_segment_min</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentMin</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unsorted_segment_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
<span class="go">[[1., 2., 3.], [4., 2., 1.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.UnsortedSegmentProd">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">UnsortedSegmentProd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#UnsortedSegmentProd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.UnsortedSegmentProd" title="Permalink to this definition"></a></dt>
<dd><p>Computes the product along segments of a tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.
With float16, float32 or int32 data type.</p></li>
<li><p><strong>segment_ids</strong> (Tensor) - A <cite>1-D</cite> tensor whose shape is <span class="math notranslate nohighlight">\((x_1)\)</span>, the value must be &gt;= 0.
Data type must be int32.</p></li>
<li><p><strong>num_segments</strong> (int) - The value spcifies the number of distinct <cite>segment_ids</cite>,
must be greater than 0.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, set the number of <cite>num_segments</cite> as <cite>N</cite>, the shape is <span class="math notranslate nohighlight">\((N, x_2, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">segment_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_segments</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unsorted_segment_prod</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentProd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">unsorted_segment_prod</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
<span class="go">[[4., 4., 3.], [4., 5., 6.]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.UnsortedSegmentSum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">UnsortedSegmentSum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#UnsortedSegmentSum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.UnsortedSegmentSum" title="Permalink to this definition"></a></dt>
<dd><p>Computes the sum along segments of a tensor.</p>
<p>Calculates a tensor such that <span class="math notranslate nohighlight">\(\text{output}[i] = \sum_{segment\_ids[j] == i} \text{data}[j, \ldots]\)</span>, where
<span class="math notranslate nohighlight">\(j\)</span> is a tuple describing the index of element in data.  <cite>segment_ids</cite> selects which elements in data to sum
up. Segment_ids does not need to be sorted, and it does not need to cover all values in the entire valid value
range.</p>
<p>If the sum of the given segment_ids <span class="math notranslate nohighlight">\(i\)</span> is empty, then <span class="math notranslate nohighlight">\(\text{output}[i] = 0\)</span>. If the given segment_ids
is negative, the value will be ignored. ‘num_segments’ must be equal to the number of different segment_ids.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - The shape is <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_R)\)</span>.</p></li>
<li><p><strong>segment_ids</strong> (Tensor) - Set the shape as <span class="math notranslate nohighlight">\((x_1, x_2, ..., x_N)\)</span>, where 0 &lt; N &lt;= R. Type must be int.</p></li>
<li><p><strong>num_segments</strong> (int) - Set <span class="math notranslate nohighlight">\(z\)</span> as num_segments.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is <span class="math notranslate nohighlight">\((z, x_{N+1}, ..., x_R)\)</span>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">segment_ids</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_segments</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentSum</span><span class="p">()(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span>
<span class="go">[3, 3, 4, 0]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Xdivy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Xdivy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Xdivy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Xdivy" title="Permalink to this definition"></a></dt>
<dd><p>Divide the first input tensor by the second input tensor element-wise. Returns zero when <cite>x</cite> is zero.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number, or a bool,
or a tensor whose data type is float16, float32 or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number,
or a bool when the first input is a tensor, or a tensor whose data type is float16, float32 or bool.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xdivy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Xdivy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xdivy</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[1.0, 2.0, -0.5]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.Xlogy">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">Xlogy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/math_ops.html#Xlogy"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.Xlogy" title="Permalink to this definition"></a></dt>
<dd><p>Computes first input tensor multiplied by the logarithm of second input tensor element-wise.
Returns zero when <cite>x</cite> is zero.</p>
<p>Inputs of <cite>input_x</cite> and <cite>input_y</cite> comply with the implicit type conversion rules to make the data types consistent.
The inputs must be two tensors or one tensor and one scalar.
When the inputs are two tensors,
dtypes of them cannot be both bool, and the shapes of them could be broadcast.
When the inputs are one tensor and one scalar,
the scalar could only be a constant.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Union[Tensor, Number, bool]) - The first input is a number or
a bool or a tensor whose data type is float16, float32 or bool.</p></li>
<li><p><strong>input_y</strong> (Union[Tensor, Number, bool]) - The second input is a number or
a bool when the first input is a tensor or a tensor whose data type is float16, float32 or bool.
The value must be positive.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the shape is the same as the one after broadcasting,
and the data type is the one with higher precision or higher digits among the two inputs.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">input_x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xlogy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Xlogy</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xlogy</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>
<span class="go">[-3.465736, 0.0, 2.7725887]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.ZerosLike">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">ZerosLike</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/operations/array_ops.html#ZerosLike"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.ZerosLike" title="Permalink to this definition"></a></dt>
<dd><p>Creates a new tensor. All elements value are 0.</p>
<p>Returns a tensor of zeros with the same shape and data type as the input tensor.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_x</strong> (Tensor) - Input tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, has the same shape and data type as <cite>input_x</cite> but filled with zeros.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">zeroslike</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">zeroslike</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">[[0.0, 0.0],</span>
<span class="go"> [0.0, 0.0]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.add_flags">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">add_flags</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/base.html#add_flags"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.add_flags" title="Permalink to this definition"></a></dt>
<dd><p>A decorator that adds a flag to the function.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only supports bool value.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>Function</em>) – Function or cell to add flag. Default: None.</p></li>
<li><p><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – Flags use kwargs. Default: None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Function, the function with added flags.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">add_flags</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">predit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.clip_by_value">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">clip_by_value</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value_min</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value_max</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/clip_ops.html#clip_by_value"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.clip_by_value" title="Permalink to this definition"></a></dt>
<dd><p>Clips tensor values to a specified min and max.</p>
<p>Limits the value of <span class="math notranslate nohighlight">\(x\)</span> to a range, whose lower limit is ‘clip_value_min’
and upper limit is ‘clip_value_max’.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‘clip_value_min’ needs to be less than or equal to ‘clip_value_max’.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Input data.</p></li>
<li><p><strong>clip_value_min</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The minimum value.</p></li>
<li><p><strong>clip_value_max</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The maximum value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor, a clipped Tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.constexpr">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">constexpr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_instance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#constexpr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.constexpr" title="Permalink to this definition"></a></dt>
<dd><p>Make a PrimitiveWithInfer operator that can infer the value at compile time. We can use it to define a function to
compute constant value using the constants in the constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>function</em>) – A <cite>fn</cite> use as the infer_value of the output operator.</p></li>
<li><p><strong>get_instance</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If true, return the instance of operator, otherwise return the operator class.</p></li>
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Defines the operator name. If <cite>name</cite> is None, use the function name as op name.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># make an operator to calculate tuple len</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@constexpr</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">tuple_len</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tuple_len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># make a operator class to calculate tuple len</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nd">@constexpr</span><span class="p">(</span><span class="n">get_instance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;TupleLen&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">tuple_len_class</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">assert</span> <span class="n">tuple_len_class</span><span class="p">()(</span><span class="n">a</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.core">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">core</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">flags</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/base.html#core"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.core" title="Permalink to this definition"></a></dt>
<dd><p>A decorator that adds a flag to the function.</p>
<p>By default, the function is marked as True, enabling to use this decorator to
set flag to a graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> (<em>Function</em>) – Function to add flag. Default: None.</p></li>
<li><p><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – The following flags can be set core, which indicates that this is a core function or
other flag. Default: None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.gamma">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">gamma</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/random_ops.html#gamma"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.gamma" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the Gamma random number distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of random tensor to be generated.</p></li>
<li><p><strong>alpha</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The alpha α distribution parameter. It should be greater than 0 with float32 data type.</p></li>
<li><p><strong>beta</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The beta β distribution parameter. It should be greater than 0 with float32 data type.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed is used as entropy source for the random number engines to generate
pseudo-random numbers, must be non-negative. Default: None, which will be treated as 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor. The shape should be equal to the broadcasted shape between the input “shape” and shapes
of <cite>alpha</cite> and <cite>beta</cite>.
The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alpha</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">beta</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.get_vm_impl_fn">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">get_vm_impl_fn</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prim</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/vm_impl_registry.html#get_vm_impl_fn"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.get_vm_impl_fn" title="Permalink to this definition"></a></dt>
<dd><p>Get the virtual implementation function by a primitive object or primitive name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>prim</strong> (<em>Union</em><em>[</em><a class="reference internal" href="#mindspore.ops.Primitive" title="mindspore.ops.Primitive"><em>Primitive</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>]</em>) – primitive object or name for operator register.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>function, vm function</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.laplace">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">laplace</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_param</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/random_ops.html#laplace"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.laplace" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the Laplace random number distribution.
It is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{f}(x;μ,λ) = \frac{1}{2λ}\exp(-\frac{|x-μ|}{λ}),\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of random tensor to be generated.</p></li>
<li><p><strong>mean</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The mean μ distribution parameter, which specifies the location of the peak.
With float32 data type.</p></li>
<li><p><strong>lambda_param</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The parameter used for controling the variance of this random distribution. The
variance of Laplace distribution is equal to twice the square of lambda_param. With float32 data type.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed is used as entropy source for Random number engines generating pseudo-random numbers.
Default: None, which will be treated as 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor. The shape should be the broadcasted shape of Input “shape” and shapes of mean and lambda_param.
The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lambda_param</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">laplace</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">lambda_param</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.multinomial">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">multinomial</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_sample</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">replacement</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/random_ops.html#multinomial"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.multinomial" title="Permalink to this definition"></a></dt>
<dd><p>Returns a tensor sampled from the multinomial probability distribution located in the corresponding
row of the input tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The rows of input do not need to sum to one (in which case we use the values as weights),
but must be non-negative, finite and have a non-zero sum.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The input tensor containing probabilities, must be 1 or 2 dimensions, with
float32 data type.</p></li>
<li><p><strong>num_sample</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of samples to draw.</p></li>
<li><p><strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to draw with replacement or not, default True.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – Seed is used as entropy source for the random number engines to generate
pseudo-random numbers, must be non-negative. Default: 0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Outputs:</dt><dd><p>Tensor, has the same rows with input. The number of sampled indices of each row is <cite>num_samples</cite>.
The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.normal">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">normal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stddev</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/random_ops.html#normal"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.normal" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the Normal (or Gaussian) random number distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of random tensor to be generated.</p></li>
<li><p><strong>mean</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The mean μ distribution parameter, which specifies the location of the peak.
with float32 data type.</p></li>
<li><p><strong>stddev</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The deviation σ distribution parameter. It should be greater than 0.
with float32 data type.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed is used as entropy source for the Random number engines to generate pseudo-random numbers.
must be non-negative. Default: None, which will be treated as 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor. The shape should be equal to the broadcasted shape between the input <cite>shape</cite> and shapes
of <cite>mean</cite> and <cite>stddev</cite>.
The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stddev</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">stddev</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.op_info_register">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">op_info_register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">op_info</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/op_info_register.html#op_info_register"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.op_info_register" title="Permalink to this definition"></a></dt>
<dd><p>A decorator which is used to register an operator.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>‘op_info’ should represent the operator information by string with json format.
The ‘op_info’ will be added into oplib.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>op_info</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – operator information in json format.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Function, returns a decorator for op info register.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.poisson">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">poisson</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/random_ops.html#poisson"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.poisson" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the Poisson random number distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of random tensor to be generated.</p></li>
<li><p><strong>mean</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The mean μ distribution parameter. It should be greater than 0 with float32 data type.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed is used as entropy source for the random number engines to generate pseudo-random numbers
and must be non-negative. Default: None, which will be treated as 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor. The shape should be equal to the broadcasted shape between the input “shape” and shapes of <cite>mean</cite>.
The dtype is float32.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.prim_attr_register">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">prim_attr_register</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/primitive.html#prim_attr_register"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.prim_attr_register" title="Permalink to this definition"></a></dt>
<dd><p>Primitive attributes register.</p>
<p>Register the decorator of the built-in operator primitive ‘__init__’.
The function will add all the parameters of ‘__init__’ as operator attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>fn</strong> (<em>function</em>) – __init__ function of primitive.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>function, original function.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.ops.uniform">
<span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">uniform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minval</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxval</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mindspore.float32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/ops/composite/random_ops.html#uniform"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.uniform" title="Permalink to this definition"></a></dt>
<dd><p>Generates random numbers according to the Uniform random number distribution.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The number in tensor minval should be strictly less than maxval at any position after broadcasting.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a>) – The shape of random tensor to be generated.</p></li>
<li><p><strong>minval</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The distribution parameter <cite>a</cite>.
It defines the minimum possible generated value, with int32 or float32 data type.
If dtype is int32, only one number is allowed.</p></li>
<li><p><strong>maxval</strong> (<a class="reference internal" href="mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – The distribution parameter <cite>b</cite>.
It defines the maximum possible generated value, with int32 or float32 data type.
If dtype is int32, only one number is allowed.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Seed is used as entropy source for the random number engines to generate pseudo-random numbers,
must be non-negative. Default: None, which will be treated as 0.</p></li>
<li><p><strong>dtype</strong> (<a class="reference internal" href="mindspore.html#mindspore.dtype" title="mindspore.dtype"><em>mindspore.dtype</em></a>) – type of the Uniform distribution. If it is int32, it generates numbers from discrete
uniform distribution; if it is float32, it generates numbers from continuous uniform distribution. It only
supports these two data types. Default: mstype.float32.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor. The shape should be equal to the broadcasted shape between the input <cite>shape</cite> and shapes
of <cite>minval</cite> and <cite>maxval</cite>.
The dtype is designated as the input <cite>dtype</cite>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">For</span> <span class="n">discrete</span> <span class="n">uniform</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">only</span> <span class="n">one</span> <span class="n">number</span> <span class="ow">is</span> <span class="n">allowed</span> <span class="k">for</span> <span class="n">both</span> <span class="n">minval</span> <span class="ow">and</span> <span class="n">maxval</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">minval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maxval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">For</span> <span class="n">continuous</span> <span class="n">uniform</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">minval</span> <span class="ow">and</span> <span class="n">maxval</span> <span class="n">can</span> <span class="n">be</span> <span class="n">multi</span><span class="o">-</span><span class="n">dimentional</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">minval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">maxval</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.nn.probability.html" class="btn btn-neutral float-left" title="mindspore.nn.probability" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.profiler.html" class="btn btn-neutral float-right" title="mindspore.profiler" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>