<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.numpy.array_ops &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore/mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.privacy.sup_privacy.html">mindarmour.privacy.sup_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Serving Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mindspore_serving/mindspore_serving.html">mindspore_serving</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindQuantum Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../mindquantum/mindquantum.html">mindquantum</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.numpy.array_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.numpy.array_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;array operations, the function docs are adapted from Numpy API.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">operator</span>

<span class="kn">from</span> <span class="nn">..common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">..common</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">..ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">..ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">..ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">..nn</span> <span class="kn">import</span> <span class="n">Cell</span>

<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">_convert_list_tensor_to_tuple_tensor</span><span class="p">,</span> <span class="n">_expand</span><span class="p">,</span> <span class="n">_broadcast_to_shape</span><span class="p">,</span> \
    <span class="n">_check_input_tensor</span><span class="p">,</span> <span class="n">_broadcast_to</span><span class="p">,</span> <span class="n">_to_tensor</span>
<span class="kn">from</span> <span class="nn">.utils_const</span> <span class="kn">import</span> <span class="n">_check_axes_range</span><span class="p">,</span> <span class="n">_check_start_normalize</span><span class="p">,</span> \
    <span class="n">_raise_type_error</span><span class="p">,</span> <span class="n">_raise_value_error</span><span class="p">,</span> <span class="n">_infer_out_shape</span><span class="p">,</span> <span class="n">_empty</span><span class="p">,</span> <span class="n">_promote</span><span class="p">,</span> \
    <span class="n">_check_same_type</span><span class="p">,</span> <span class="n">_check_axis_valid</span><span class="p">,</span> <span class="n">_add_unit_axes</span><span class="p">,</span> <span class="n">_broadcast_tuples</span><span class="p">,</span> \
    <span class="n">_check_is_float</span><span class="p">,</span> <span class="n">_check_axis_in_range</span><span class="p">,</span> <span class="n">_check_axis_type</span><span class="p">,</span> <span class="n">_canonicalize_axis</span><span class="p">,</span> \
    <span class="n">_list_comprehensions</span><span class="p">,</span> <span class="n">_check_element_int</span><span class="p">,</span> <span class="n">_is_shape_empty</span><span class="p">,</span> <span class="n">_type_convert</span><span class="p">,</span> \
    <span class="n">_tuple_getitem</span><span class="p">,</span> <span class="n">_expanded_shape</span><span class="p">,</span> <span class="n">_seq_prod</span><span class="p">,</span> <span class="n">_get_device</span><span class="p">,</span> <span class="n">_tuple_setitem</span><span class="p">,</span> \
    <span class="n">_raise_unimplemented_error</span>

<span class="c1"># According to official numpy reference, the dimension of a numpy array must be less</span>
<span class="c1"># than 32</span>
<span class="n">MAX_NUMPY_DIMS</span> <span class="o">=</span> <span class="mi">32</span>


<div class="viewcode-block" id="expand_dims"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.expand_dims.html#mindspore.numpy.expand_dims">[docs]</a><span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expands the shape of a tensor.</span>

<span class="sd">    Inserts a new axis that will appear at the axis position in the expanded tensor shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor array.</span>
<span class="sd">        axis (Union[int, list(int), tuple(int)]): Position in the expanded axes where</span>
<span class="sd">            the new axis is placed,</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the number of dimensions increased at specified axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If axis exceeds a.ndim.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,2))</span>
<span class="sd">        &gt;&gt;&gt; x = np.expand_dims(x,0)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (1, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;axis must be tuple, list or int, but got &quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">ax</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span></div>


<div class="viewcode-block" id="squeeze"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.squeeze.html#mindspore.numpy.squeeze">[docs]</a><span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Removes single-dimensional entries from the shape of an tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor array.</span>
<span class="sd">        axis (Union[None, int, list(int), tuple(list)]): Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with all or a subset of the dimensions of length :math:`1` removed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If specified axis has shape entry :math:`&gt; 1`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((1,2,2,1))</span>
<span class="sd">        &gt;&gt;&gt; x = np.squeeze(x)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.transpose.html#mindspore.numpy.transpose">[docs]</a><span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses or permutes the axes of a tensor; returns the modified tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): a tensor to be transposed</span>
<span class="sd">        axes (Union[None, tuple, list]): the axes order, if `axes` is `None`, transpose</span>
<span class="sd">            the entire tensor. Default is `None`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the transposed tensor array.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If the number of `axes` is not euqal to a.ndim.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((1,2,3))</span>
<span class="sd">        &gt;&gt;&gt; x = np.transpose(x)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (3, 2, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span></div>


<div class="viewcode-block" id="rollaxis"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.rollaxis.html#mindspore.numpy.rollaxis">[docs]</a><span class="k">def</span> <span class="nf">rollaxis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the specified axis backwards, until it lies in the given position.</span>
<span class="sd">    The positions of the other axes do not change relative to one another.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be transposed.</span>
<span class="sd">        axis (int): The axis to be rolled.</span>
<span class="sd">        start (int): Default: 0.</span>
<span class="sd">            If :math:`start &lt;= axis`, the axis is rolled back until it lies in this position (`start`).</span>
<span class="sd">            If :math:`start &gt; axis`: the axis is rolled until it lies before this position (`start`).</span>

<span class="sd">            If :math:`start &lt; 0`, the start will be normalized as shown in the table.</span>
<span class="sd">            (Please refer to the source code.)</span>

<span class="sd">            .. table</span>
<span class="sd">                +===========+=================+</span>
<span class="sd">                |start      |Normalized start |</span>
<span class="sd">                +===========+=================+</span>
<span class="sd">                |-(x.ndim+1)| raise ValueError|</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |-x.ndim    |0                |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |...        |...              |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |-1         |x.ndim-1         |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |...        |...              |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |x.ndim     |x.ndim           |</span>
<span class="sd">                +-----------+-----------------+</span>
<span class="sd">                |x.ndim+1   |raise ValueError |</span>
<span class="sd">                +===========+=================+</span>
<span class="sd">            ..</span>

<span class="sd">    Returns:</span>
<span class="sd">        Transposed Tensor. Has the same data type as the original tensor `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `start` is not integer, or `x` is not tensor.</span>
<span class="sd">        ValueError: If `axis` is not in the range of :math:`[-ndim, ndim-1]` or</span>
<span class="sd">            `start` is not in the range of :math:`[-ndim, ndim]`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,3,4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.rollaxis(x, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 2, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, but got &quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, but got &quot;</span><span class="p">,</span> <span class="n">start</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axes_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">_check_start_normalize</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">start</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">new_perm</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">axis</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">perm</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">start</span> <span class="o">&gt;</span> <span class="n">axis</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">start</span><span class="p">:]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">start</span><span class="p">]</span> <span class="o">+</span> \
                <span class="n">perm</span><span class="p">[</span><span class="n">axis</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="swapaxes"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.swapaxes.html#mindspore.numpy.swapaxes">[docs]</a><span class="k">def</span> <span class="nf">swapaxes</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Interchanges two axes of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor to be transposed.</span>
<span class="sd">        axis1 (int): First axis.</span>
<span class="sd">        axis2 (int): Second axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Transposed tensor, has the same data type as the original tensor `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis1` or `axis2` is not integer, or `x` is not tensor.</span>
<span class="sd">        ValueError: If `axis1` or `axis2` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,3,4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.swapaxes(x, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4,3,2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span><span class="p">)</span></div>


<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.reshape.html#mindspore.numpy.reshape">[docs]</a><span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes a tensor without changing its data.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor to be reshaped.</span>
<span class="sd">        new_shape (Union[int, list(int), tuple(int)]): The new shape should be</span>
<span class="sd">            compatible with the original shape. If the tuple has only one element,</span>
<span class="sd">            the result will be a 1-D tensor of that length. One shape dimension</span>
<span class="sd">            can be :math:`-1`. In this case, the value is inferred from the length of</span>
<span class="sd">            the tensor and remaining dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Reshaped Tensor. Has the same data type as the original tensor `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If new_shape is not integer, list or tuple, or `x` is not tensor.</span>
<span class="sd">        ValueError: If new_shape is not compatible with the original shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.asarray([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.reshape(x, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.reshape(x, (3, -1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.reshape(x, (6, ))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.3  3.6  0.4  0.5 -3.2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="ravel"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.ravel.html#mindspore.numpy.ravel">[docs]</a><span class="k">def</span> <span class="nf">ravel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a contiguous flattened tensor.</span>

<span class="sd">    A 1-D tensor, containing the elements of the input, is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor to be flattened.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Flattened tensor, has the same data type as the original tensor `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones((2,3,4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.ravel(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (24,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_move_axes_for_concatenate</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Moves axis 0 to the desiganated position, while keeps other axes&#39; relative</span>
<span class="sd">    positions unchanged, only used if a single tensor is concatenated.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">original_axes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">)))</span>
    <span class="n">new_axes</span> <span class="o">=</span> <span class="n">original_axes</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">original_axes</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">arr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">],)</span> <span class="o">+</span> \
        <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">2</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">new_axes</span><span class="p">,</span> <span class="n">new_shape</span>


<span class="k">def</span> <span class="nf">_promote_type_for_concatenate</span><span class="p">(</span><span class="n">tuple_of_tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks dtype for all tensors in the tuple. If dtypes are not the same, promote</span>
<span class="sd">    them to the `highest` dtype in the tuple, so that they are ready for the concat</span>
<span class="sd">    operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        tuple_of_tensors(tuple(tensor)): A tuple of tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple of tensors, with each tensor promoted to ths same dtype.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">need_cast</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">final_type</span> <span class="o">=</span> <span class="n">tuple_of_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tuple_of_tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">final_type</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="n">need_cast</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">final_type</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">final_type</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">need_cast</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tuple_of_tensors</span>
    <span class="n">tuple_of_casted_tensors</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tuple_of_tensors</span><span class="p">:</span>
        <span class="n">tuple_of_casted_tensors</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">final_type</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">),)</span>
    <span class="k">return</span> <span class="n">tuple_of_casted_tensors</span>


<div class="viewcode-block" id="concatenate"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.concatenate.html#mindspore.numpy.concatenate">[docs]</a><span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Joins a sequence of tensors along an existing axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        To match Numpy behaviour, :math:`axis &gt;= 32` will not cause value error, the</span>
<span class="sd">        `axis` will be treated as :class:`None` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        arrays (Union[Tensor, tuple(Tensor), list(Tensor)]): a tensor or a list</span>
<span class="sd">            of tensors to be concatenated.</span>
<span class="sd">        axis (Union[None, int], optional): The axis along which the tensors will be joined,</span>
<span class="sd">            if `axis` is :class:`None`, tensors are flattened before use. Default is 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor concatenated from a tensor or a list of tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If `axis` is not in the range of :math:`[-ndim, ndim-1]`, and less than 32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.ones((1,2,3))</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.ones((1,2,1))</span>
<span class="sd">        &gt;&gt;&gt; x = np.concatenate((x1, x2), axis=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(x.shape)</span>
<span class="sd">        (1, 2, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="c1"># if only one tensor is provided, it is treated as a tuple along the</span>
        <span class="c1"># first dimension. For example, a tensor of shape (3,4,5) will be treated</span>
        <span class="c1"># as: tuple(tensor_1(4,5), tensor_2(4,5), tensor_3(4,5))</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">MAX_NUMPY_DIMS</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ravel</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">_check_axes_range</span><span class="p">((</span><span class="n">axis</span><span class="p">,),</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">))</span>
        <span class="c1"># move axis 0 to the disiganated position, while keep other axes&#39; relative</span>
        <span class="c1"># positions unchanged</span>
        <span class="n">new_axes</span><span class="p">,</span> <span class="n">new_shape</span> <span class="o">=</span> <span class="n">_move_axes_for_concatenate</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">new_axes</span><span class="p">)</span>
        <span class="n">arrays</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arrays</span>

    <span class="n">flattened_arrays</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">MAX_NUMPY_DIMS</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrays</span><span class="p">:</span>
            <span class="n">flattened_arrays</span> <span class="o">+=</span> <span class="p">(</span><span class="n">ravel</span><span class="p">(</span><span class="n">arr</span><span class="p">),)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">flattened_arrays</span> <span class="o">=</span> <span class="n">_promote_type_for_concatenate</span><span class="p">(</span><span class="n">flattened_arrays</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)(</span><span class="n">flattened_arrays</span><span class="p">)</span>

    <span class="c1"># convert a list of tensor to a tuple of tensor</span>
    <span class="n">arrays</span> <span class="o">=</span> <span class="n">_convert_list_tensor_to_tuple_tensor</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>

    <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">_check_axes_range</span><span class="p">((</span><span class="n">axis</span><span class="p">,),</span> <span class="nb">len</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">))</span>

    <span class="c1"># if only one tensor in the tuple/list, return the tensor itself</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">arrays</span> <span class="o">=</span> <span class="n">_promote_type_for_concatenate</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)(</span><span class="n">arrays</span><span class="p">)</span></div>


<div class="viewcode-block" id="append"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.append.html#mindspore.numpy.append">[docs]</a><span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Appends values to the end of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        arr (Tensor): Values are appended to a copy of this tensor.</span>
<span class="sd">        values (Tensor): These values are appended to a copy of `arr`. It must be of</span>
<span class="sd">            the correct shape (the same shape as `arr`, excluding `axis`). If `axis` is</span>
<span class="sd">            not specified, `values` can be any shape and will be flattened before use.</span>
<span class="sd">        axis (None, int, optional): The `axis` along which values are appended. If `axis` is not</span>
<span class="sd">            given, both `arr` and `values` are flattened before use, default is :class:`None`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a copy of tensor with values appended to axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If specified axis exceeds `arr.ndim`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones((2, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(np.append(a, b, axis=1).shape)</span>
<span class="sd">        (2, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">arr</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">!=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">values</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;all tensors must have same number of dimensions&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">concatenate</span><span class="p">((</span><span class="n">arr</span><span class="p">,</span> <span class="n">values</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="column_stack"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.column_stack.html#mindspore.numpy.column_stack">[docs]</a><span class="k">def</span> <span class="nf">column_stack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks 1-D tensors as columns into a 2-D tensor. 2-D tensors are stacked as-is,</span>
<span class="sd">    like np.hstack.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of 1-D or 2-D tensors. All</span>
<span class="sd">            of them must have the same shape except the axis to be concatenated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        2-D Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.column_stack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 4]</span>
<span class="sd">         [2 5]</span>
<span class="sd">         [3 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or, list or tuple of tensors are required, but got &quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span></div>


<div class="viewcode-block" id="vstack"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.vstack.html#mindspore.numpy.vstack">[docs]</a><span class="k">def</span> <span class="nf">vstack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence vertically.</span>
<span class="sd">    This is equivalent to concatenation along the first axis. 1-D tensors should firstly be reshaped to `(1, N)`,</span>
<span class="sd">    and then be concatenated along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of 1-D or 2-D tensors. The tensors must have the same shape</span>
<span class="sd">            along all but the first axis. 1-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.vstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2 3]</span>
<span class="sd">         [4 5 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or, list or tuple of tensors are required, but got&quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span></div>


<div class="viewcode-block" id="hstack"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.hstack.html#mindspore.numpy.hstack">[docs]</a><span class="k">def</span> <span class="nf">hstack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence horizontally.</span>
<span class="sd">    This is equivalent to concatenation along the second axis, except for 1-D tensors</span>
<span class="sd">    where it concatenates along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of 1-D or 2-D tensors. The</span>
<span class="sd">            tensors must have the same shape along all but the second axis, except</span>
<span class="sd">            1-D tensors which can be any length.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.hstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3. 4. 5. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or, list or tuple of tensors are required, but got&quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">tuple_of_tensor</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">tuple_of_tensor</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tuple_of_tensor</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tuple_of_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">tuple_of_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">tuple_of_tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="dstack"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.dstack.html#mindspore.numpy.dstack">[docs]</a><span class="k">def</span> <span class="nf">dstack</span><span class="p">(</span><span class="n">tup</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence depth wise (along the third axis).</span>
<span class="sd">    This is equivalent to concatenation along the third axis. 1-D tensors :math:`(N,)` should be</span>
<span class="sd">    reshaped to :math:`(1,N,1)`.</span>
<span class="sd">    2-D tensors :math:`(M,N)` should be reshaped to :math:`(M,N,1)` before concatenation.</span>

<span class="sd">    Args:</span>
<span class="sd">        tup (Union[Tensor, tuple, list]): A sequence of tensors. The tensors must have the same shape along all but</span>
<span class="sd">            the third axis. 1-D or 2-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tup` is not Tensor, list or tuple.</span>
<span class="sd">        ValueError: If `tup` is empty.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([1, 2, 3]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([4, 5, 6]).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.dstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 4.]</span>
<span class="sd">          [2. 5.]</span>
<span class="sd">          [3. 6.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tup</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Tensor or list or tuple of tensors are required, but got&quot;</span><span class="p">,</span> <span class="n">tup</span><span class="p">)</span>

    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tup</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">2</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span></div>


<div class="viewcode-block" id="where"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.where.html#mindspore.numpy.where">[docs]</a><span class="k">def</span> <span class="nf">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns elements chosen from `x` or `y` depending on `condition`.</span>

<span class="sd">    Note:</span>
<span class="sd">        As nonzero is not supported, neither `x` or `y` can be None.</span>

<span class="sd">    Args:</span>
<span class="sd">        condition (Tensor): where True, yield `x`, otherwise yield `y`.</span>
<span class="sd">        x (Tensor)</span>
<span class="sd">        y (Tensor): Values from which to choose. `x`, `y` and `condition` need</span>
<span class="sd">            to be broadcastable to some shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, with elements from `x` where `condition` is True, and</span>
<span class="sd">        elements from `y` elsewhere.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if operands cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; condition = np.full((1, 1, 2), [False, True])</span>
<span class="sd">        &gt;&gt;&gt; x = np.full((1, 3, 2), 5)</span>
<span class="sd">        &gt;&gt;&gt; y = np.full((2, 1, 1), 7)</span>
<span class="sd">        &gt;&gt;&gt; output = np.where(condition, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7 5]</span>
<span class="sd">        [7 5]</span>
<span class="sd">        [7 5]]</span>
<span class="sd">        [[7 5]</span>
<span class="sd">        [7 5]</span>
<span class="sd">        [7 5]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># type promotes input tensors</span>
    <span class="n">dtype1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dtype2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">_promote</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype2</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">is_bool</span> <span class="o">=</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span> <span class="ow">and</span> <span class="n">_check_same_type</span><span class="p">(</span>
        <span class="n">dtype2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_bool</span><span class="p">:</span>
        <span class="c1"># select does not support bool type for x or y</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="c1"># broadcasts input tensors</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">condition</span><span class="p">),</span>
                                 <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">condition</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="c1"># tiling with bool is not supported on GPU</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">condition</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">condition</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="n">condition</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_bool</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">_atleast_xd</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns arys with at least ndim.&quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arys</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span>


<div class="viewcode-block" id="atleast_1d"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.atleast_1d.html#mindspore.numpy.atleast_1d">[docs]</a><span class="k">def</span> <span class="nf">atleast_1d</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts inputs to arrays with at least one dimension.</span>

<span class="sd">    Scalar inputs are converted to 1-dimensional arrays, whilst</span>
<span class="sd">    higher-dimensional inputs are preserved.</span>

<span class="sd">    Note:</span>
<span class="sd">        In graph mode, returns a tuple of tensor instead of a list of</span>
<span class="sd">        tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        *arys (Tensor): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, each with ``a.ndim &gt;= 1``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; c = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; output = np.atleast_1d(a, b, c)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">            [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000],</span>
<span class="sd">            [1.00000000e+000, 1.00000000e+000, 1.00000000e+000]]),</span>
<span class="sd">            Tensor(shape=[1], dtype=Float32, value= [1.00000000e+000]),</span>
<span class="sd">            Tensor(shape=[5], dtype=Float32,</span>
<span class="sd">            value= [1.00000000e+000, 1.00000000e+000, 1.00000000e+000,</span>
<span class="sd">            1.00000000e+000, 1.00000000e+000])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_atleast_xd</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arys</span><span class="p">)</span></div>


<div class="viewcode-block" id="atleast_2d"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.atleast_2d.html#mindspore.numpy.atleast_2d">[docs]</a><span class="k">def</span> <span class="nf">atleast_2d</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes inputs as arrays with at least two dimensions.</span>

<span class="sd">    Note:</span>
<span class="sd">        In graph mode, returns a tuple of tensor instead of a list of</span>
<span class="sd">        tensors.</span>
<span class="sd">    Args:</span>
<span class="sd">        *arys (Tensor): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, each with ``a.ndim &gt;= 2``.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; c = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; output = np.atleast_2d(a, b, c)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">            [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000],</span>
<span class="sd">            [1.00000000e+000, 1.00000000e+000, 1.00000000e+000]]),</span>
<span class="sd">            Tensor(shape=[1, 1], dtype=Float32, value= [[1.00000000e+000]]),</span>
<span class="sd">            Tensor(shape=[1, 5], dtype=Float32,</span>
<span class="sd">            value= [[1.00000000e+000, 1.00000000e+000, 1.00000000e+000,</span>
<span class="sd">            1.00000000e+000, 1.00000000e+000]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_atleast_xd</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">arys</span><span class="p">)</span></div>


<div class="viewcode-block" id="atleast_3d"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.atleast_3d.html#mindspore.numpy.atleast_3d">[docs]</a><span class="k">def</span> <span class="nf">atleast_3d</span><span class="p">(</span><span class="o">*</span><span class="n">arys</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes inputs as arrays with at least three dimensions.</span>

<span class="sd">    Note:</span>
<span class="sd">        In graph mode, returns a tuple of tensor instead of a list of</span>
<span class="sd">        tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        *arys (Tensor): one or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, or list of tensors, each with ``a.ndim &gt;= 3``. For example,</span>
<span class="sd">        a 1-D array of shape `(N,)` becomes a tensor of shape `(1, N, 1)`, and</span>
<span class="sd">        a 2-D array of shape `(M, N)` becomes a tensor of shape `(M, N, 1)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; c = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; output = np.atleast_3d(a, b, c)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [Tensor(shape=[2, 3, 1], dtype=Float32, value=</span>
<span class="sd">            [[[1.00000000e+000], [1.00000000e+000], [1.00000000e+000]],</span>
<span class="sd">            [[1.00000000e+000], [1.00000000e+000], [1.00000000e+000]]]),</span>
<span class="sd">            Tensor(shape=[1, 1, 1], dtype=Float32, value= [[[1.00000000e+000]]]),</span>
<span class="sd">            Tensor(shape=[1, 5, 1], dtype=Float32,</span>
<span class="sd">            value= [[[1.00000000e+000], [1.00000000e+000], [1.00000000e+000],</span>
<span class="sd">            [1.00000000e+000], [1.00000000e+000]]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arys</span><span class="p">:</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">arr</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.stack.html#mindspore.numpy.stack">[docs]</a><span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Joins a sequence of arrays along a new axis.</span>

<span class="sd">    The `axis` parameter specifies the index of the new axis in the</span>
<span class="sd">    dimensions of the result. For example, if ``axis=0`` it will be the</span>
<span class="sd">    first dimension and if ``axis=-1`` it will be the last dimension.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument out is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        arrays (sequence of Tensor): Each array must have the same shape.</span>
<span class="sd">        axis (int, optional): The axis in the result array along which the</span>
<span class="sd">            input arrays are stacked.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The stacked array has one more dimension than the input</span>
<span class="sd">        arrays.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if input is not Tensor, tuple, or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arrays = [np.ones((3, 4)) for _ in range(10)]</span>
<span class="sd">        &gt;&gt;&gt; output = np.stack(arrays, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = np.stack(arrays, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 10, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = np.stack(arrays, axis=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 4, 10)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arrays</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>
        <span class="n">axes</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">axes</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">transpose</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arrays</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arrays</span><span class="p">),)</span> <span class="o">+</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arrays</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">%</span> <span class="n">ndim</span>
        <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrays</span><span class="p">:</span>
            <span class="n">seq</span> <span class="o">+=</span> <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="p">),)</span>
        <span class="k">return</span> <span class="n">concatenate</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;input arrays must be Tensor, tuple, or list&#39;</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">UniqueNet</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The operation is wrapped inside a model. &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UniqueNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unique</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Unique</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="unique"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.unique.html#mindspore.numpy.unique">[docs]</a><span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds the unique elements of a tensor. The input tensor will be flattened first</span>
<span class="sd">    when it has more than one dimension.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `axis`, `return_index` and `return_counts` are not supported.</span>
<span class="sd">        On CPU, this operator must be executed in graph mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor to be processed.</span>
<span class="sd">        return_inverse (bool): If `True`, also return the indices of the unique tensor.</span>
<span class="sd">            Default: `False`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or tuple of Tensors.</span>
<span class="sd">        - If `return_inverse` is `False`, just return the unique tensor.</span>
<span class="sd">        - If `return_inverse` is `True`, return tuple of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(mode=context.GRAPH_MODE)</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.asarray([1, 2, 2, 2, 3, 4, 5]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output_x = np.unique(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_x)</span>
<span class="sd">        [1 2 3 4 5]</span>
<span class="sd">        &gt;&gt;&gt; output_x = np.unique(input_x, return_inverse=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_x)</span>
<span class="sd">        (Tensor(shape=[5], dtype=Int32, value= [ 1, 2, 3, 4, 5]), Tensor(shape=[7], dtype=Int32,</span>
<span class="sd">            value= [0, 1, 1, 1, 2, 3, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">uniq</span> <span class="o">=</span> <span class="n">UniqueNet</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">uniq</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_inverse</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">roll_along_axis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls a tensor along a given axis. This is a helper function of np.roll.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        shift (int): The number of places the tensor is shifted.</span>
<span class="sd">        axis (int): The designated axis for shifting.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Shifted tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">_check_element_int</span><span class="p">((</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">+=</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">shift</span> <span class="o">%</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>
    <span class="c1"># if shift is 0, we do not need to roll at all</span>
    <span class="k">if</span> <span class="n">shift</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="n">begin1</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">begin2</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">end1</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">end2</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">axis</span><span class="p">:</span>
            <span class="n">begin1</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
            <span class="n">end1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
            <span class="n">begin2</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
            <span class="n">end2</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">begin1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shift</span><span class="p">,)</span>
            <span class="n">end1</span> <span class="o">+=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">],)</span>
            <span class="n">begin2</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
            <span class="n">end2</span> <span class="o">+=</span> <span class="p">(</span><span class="n">shift</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">append</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">begin1</span><span class="p">,</span> <span class="n">end1</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                  <span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">begin2</span><span class="p">,</span> <span class="n">end2</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="roll"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.roll.html#mindspore.numpy.roll">[docs]</a><span class="k">def</span> <span class="nf">roll</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls a tensor along given axes.</span>

<span class="sd">    Elements that rolls beyond the last position are re-introduced at the first.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor.</span>
<span class="sd">        shift (Union[int, tuple(int)]: The number of places by which elements are</span>
<span class="sd">            shifted. If a tuple, then axis must be a tuple of the same size, and</span>
<span class="sd">            each of the given axes is shifted by the corresponding number. If shift</span>
<span class="sd">            is an int while axis is a tuple of ints, then the same value is used</span>
<span class="sd">            for all given axes.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): Axis or axes along which elements</span>
<span class="sd">            are shifted. By default, the array is flattened before shifting, after</span>
<span class="sd">            which the original shape is restored.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape as a.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input arguments have types not specified above.</span>
<span class="sd">        ValueError: If axis exceeds `a.ndim`, or `shift` and `axis` cannot broadcast.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.reshape(np.arange(12), (3, 4))</span>
<span class="sd">        &gt;&gt;&gt; print(np.roll(a, [2,-3], [0,-1]))</span>
<span class="sd">            [[ 7  4  5  6]</span>
<span class="sd">             [11  8  9 10]</span>
<span class="sd">             [ 3  0  1  2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">original_shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">restore_shape</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># F.strided_slice only supports float on cpu, this will change once more supports</span>
    <span class="c1"># are added.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">restore_shape</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="c1"># Broadcast shift and axis to the same length</span>
    <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">_broadcast_tuples</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">shift_each</span><span class="p">,</span> <span class="n">axis_each</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">roll_along_axis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shift_each</span><span class="p">,</span> <span class="n">axis_each</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">restore_shape</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">original_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_moved_perm</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for moveaxis, returns permutation after moving axes</span>
<span class="sd">    from source to destination.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dest_sorted_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">destination</span><span class="p">),</span>
                                            <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))]</span>
    <span class="n">axes_orig</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">source</span><span class="p">]</span>

    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dest_sorted_idx</span><span class="p">:</span>
        <span class="c1"># inserts an axis that has been moved, denoted by n, and axes that remain</span>
        <span class="c1"># in their original position, indexed from k to k + n - m, into index m in</span>
        <span class="c1"># the list of permuted axes</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">destination</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
        <span class="n">perm</span> <span class="o">+=</span> <span class="n">axes_orig</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">j</span><span class="p">]</span>
        <span class="n">perm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">perm</span> <span class="o">+=</span> <span class="n">axes_orig</span><span class="p">[</span><span class="n">k</span><span class="p">:]</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_moved_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">perm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for moveaxis, returns the permuated shape after</span>
<span class="sd">    applying perm.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">perm</span><span class="p">])</span>


<div class="viewcode-block" id="moveaxis"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.moveaxis.html#mindspore.numpy.moveaxis">[docs]</a><span class="k">def</span> <span class="nf">moveaxis</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Moves axes of an array to new positions.</span>

<span class="sd">    Other axes remain in their original order.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): The array whose axes should be reordered.</span>
<span class="sd">        source (int or sequence of ints): Original positions of the</span>
<span class="sd">            axes to move. These must be unique.</span>
<span class="sd">        destination (int or sequence of ints): Destination positions</span>
<span class="sd">            for each of the original axes. These must also be unique.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, array with moved axes.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if axes are out of the range of ``[-a.ndim, a.ndim)``, or</span>
<span class="sd">            if the axes contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.zeros((3, 4, 5))</span>
<span class="sd">        &gt;&gt;&gt; output = np.moveaxis(x, 0, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = np.moveaxis(x, -1, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (5, 3, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = np.moveaxis(x, [0, 1, 2], [-1, -2, -3])</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (5, 4, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">destination</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">source</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">destination</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`source` and `destination` arguments must have the same number of elements&#39;</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">_get_moved_perm</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">_get_moved_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">perm</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="tile"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.tile.html#mindspore.numpy.tile">[docs]</a><span class="k">def</span> <span class="nf">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs an array by repeating `a` the number of times given by `reps`.</span>

<span class="sd">    If `reps` has length `d`, the result will have dimension of ``max(d, a.ndim)``.</span>
<span class="sd">    If ``a.ndim &lt; d``, `a` is promoted to be d-dimensional by prepending new axes.</span>
<span class="sd">    So a shape (3,) array is promoted to (1, 3) for 2-D replication, or</span>
<span class="sd">    shape (1, 1, 3) for 3-D replication. If this is not the desired behavior,</span>
<span class="sd">    promote `a` to d-dimensions manually before calling this function.</span>
<span class="sd">    If ``a.ndim &gt; d``, `reps` is promoted to ``a.ndim`` by pre-pending 1s to it. Thus</span>
<span class="sd">    for an `a` of shape (2, 3, 4, 5), a `reps` of (2, 2) is treated as (1, 1, 2, 2).</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): The input array.</span>
<span class="sd">        reps (int or sequence of ints): The number of repetitions of `a` along</span>
<span class="sd">            each axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the tiled output array.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([0, 1, 2])</span>
<span class="sd">        &gt;&gt;&gt; output = np.tile(a, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 2 0 1 2]</span>
<span class="sd">        &gt;&gt;&gt; output = np.tile(a, (2, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1 2 0 1 2]</span>
<span class="sd">        [0 1 2 0 1 2]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.tile(a, (2, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[0 1 2 0 1 2]]</span>
<span class="sd">        [[0 1 2 0 1 2]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">reps</span> <span class="o">=</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">reps</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">reps</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">_add_unit_axes</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">reps</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">_seq_prod</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">reps</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">reps</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_can_broadcast_to</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">target_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Determines if shape can be broadcast to target_shape.&quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ndim_target</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">&gt;</span> <span class="n">ndim_target</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">target_shape</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.broadcast_to.html#mindspore.numpy.broadcast_to">[docs]</a><span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts an array to a new shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        array (Tensor): The array to broadcast.</span>
<span class="sd">        shape (tuple): The shape of the desired array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, original array broadcast to the given shape.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if array cannot be broadcast to shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1, 2, 3])</span>
<span class="sd">        &gt;&gt;&gt; output = np.broadcast_to(x, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2 3]</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">        [1 2 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape_a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_can_broadcast_to</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;cannot broadcast with &#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_arrays"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.broadcast_arrays.html#mindspore.numpy.broadcast_arrays">[docs]</a><span class="k">def</span> <span class="nf">broadcast_arrays</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts any number of arrays against each other.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument `subok` is not supported.</span>
<span class="sd">        In graph mode, returns a tuple of Tensor instead of a list</span>
<span class="sd">        of Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        *args (Tensor): The arrays to broadcast.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List of Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if arrays cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[1,2,3]])</span>
<span class="sd">        &gt;&gt;&gt; y = np.array([[4],[5]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.broadcast_arrays(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [Tensor(shape=[2, 3], dtype=Int32, value=</span>
<span class="sd">        [[1, 2, 3],</span>
<span class="sd">        [1, 2, 3]]), Tensor(shape=[2, 3], dtype=Int32, value=</span>
<span class="sd">        [[4, 4, 4],</span>
<span class="sd">        [5, 5, 5]])]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="o">*</span><span class="n">shapes</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">array_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, array_split only supports :class:`mindspore.float32` on ``CPU``.</span>

<span class="sd">    The only difference between ``np.split`` and ``np.array_split`` is that</span>
<span class="sd">    ``np.array_split`` allows indices_or_sections to be an integer that does not</span>
<span class="sd">    equally divide the axis. For a tensor of length l that should be split into</span>
<span class="sd">    n sections, it returns :math:`l % n` sub-arrays of size :math:`l//n + 1` and</span>
<span class="sd">    the rest of size :math:`l//n`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>
<span class="sd">        axis (int): The axis along which to split. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer,</span>
<span class="sd">            tuple(int) or list(int) or argument `axis` is not integer.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-x.ndim, x.ndim)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.array_split(input_x, 4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">            value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">            value= [ 3.00000000e+00,  4.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">            value= [ 5.00000000e+00,  6.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">            value= [ 7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">opname</span><span class="o">=</span><span class="s2">&quot;array_split&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.split.html#mindspore.numpy.split">[docs]</a><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors along the given axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>
<span class="sd">        axis (int): The axis along which to split. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer,</span>
<span class="sd">            tuple(int) or list(int) or argument `axis` is not integer.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-x.ndim, x.ndim)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.split(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">          value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">          value= [ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32,</span>
<span class="sd">          value= [ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">opname</span><span class="o">=</span><span class="s2">&quot;split&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">opname</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Splits a tensor based on ``np.split`` or ``np.array_split``.&quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">_check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">opname</span> <span class="o">==</span> <span class="s2">&quot;split&quot;</span> <span class="ow">or</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">indices_or_sections</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">num_long_tensor</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">indices_or_sections</span>
            <span class="n">num_short_tensor</span> <span class="o">=</span> <span class="n">indices_or_sections</span> <span class="o">-</span> <span class="n">num_long_tensor</span>
            <span class="n">length1</span> <span class="o">=</span> <span class="n">num_long_tensor</span> <span class="o">*</span> <span class="p">(</span><span class="n">length_along_dim</span> <span class="o">//</span> <span class="n">indices_or_sections</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">length2</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">-</span> <span class="n">length1</span>
            <span class="n">start1</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">size1</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
            <span class="n">start2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">start1</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
            <span class="n">size2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length2</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_long_tensor</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start1</span><span class="p">,</span> <span class="n">size1</span><span class="p">))</span> <span class="o">+</span> \
                <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_short_tensor</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start2</span><span class="p">,</span> <span class="n">size2</span><span class="p">))</span>

    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="n">_check_element_int</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;Argument `indices_or_sections` in `mindspore.numpy.split`</span><span class="se">\</span>
<span class="s2">            should be integer, tuple(int) or list(int), but got&quot;</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor `x` into multiple sub-tensors</span>
<span class="sd">    along the axis according to the given indices.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">indices</span> <span class="o">+=</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">],)</span>

    <span class="n">sub_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
        <span class="n">begin</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">end</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">sliced_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_type_convert</span><span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">begin</span><span class="p">),</span> <span class="n">_type_convert</span><span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="n">end</span><span class="p">),</span> <span class="n">strides</span><span class="p">)</span>
        <span class="n">sub_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliced_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sub_tensors</span>


<div class="viewcode-block" id="vsplit"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.vsplit.html#mindspore.numpy.vsplit">[docs]</a><span class="k">def</span> <span class="nf">vsplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors vertically (row-wise).</span>
<span class="sd">    It is equivalent to split with :math:`axis=0` (default), the array is always</span>
<span class="sd">    split along the first axis regardless of the array dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).reshape((3, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.vsplit(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 3], dtype=Float32,</span>
<span class="sd">          value=[[ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[1, 3], dtype=Float32,</span>
<span class="sd">          value=[[ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[1, 3], dtype=Float32,</span>
<span class="sd">          value=[[ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="hsplit"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.hsplit.html#mindspore.numpy.hsplit">[docs]</a><span class="k">def</span> <span class="nf">hsplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors horizontally (column-wise).</span>
<span class="sd">    It is equivalent to split with :math:`axis=1` (default), the array is always</span>
<span class="sd">    split along the second axis regardless of the array dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(6).reshape((2, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.hsplit(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Float32,</span>
<span class="sd">        value=[[ 0.00000000e+00],</span>
<span class="sd">               [ 3.00000000e+00]]),</span>
<span class="sd">        Tensor(shape=[2, 1], dtype=Float32,</span>
<span class="sd">        value=[[ 1.00000000e+00],</span>
<span class="sd">               [ 4.00000000e+00]]),</span>
<span class="sd">        Tensor(shape=[2, 1], dtype=Float32,</span>
<span class="sd">        value=[[ 2.00000000e+00],</span>
<span class="sd">               [ 5.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="dsplit"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.dsplit.html#mindspore.numpy.dsplit">[docs]</a><span class="k">def</span> <span class="nf">dsplit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors along the 3rd axis (depth).</span>
<span class="sd">    It is equivalent to split with :math:`axis=2` (default), the array is always</span>
<span class="sd">    split along the third axis regardless of the array dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If integer, :math:`N`, the tensor will be divided into</span>
<span class="sd">            :math:`N` equal tensors along axis.</span>
<span class="sd">            If tuple(int), list(int) or of sorted integers,</span>
<span class="sd">            the entries indicate where along axis the array is split.</span>
<span class="sd">            For example, :math:`[2, 3]` would, for :math:`axis=0`, result in</span>
<span class="sd">            three sub-tensors :math:`x[:2]`, :math:`x[2:3]`and :math:`x[3:]`.</span>
<span class="sd">            If an index exceeds the dimension of the array along axis,</span>
<span class="sd">            an empty sub-array is returned correspondingly.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(6).reshape((1, 2, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = np.dsplit(input_x, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 1], dtype=Float32,</span>
<span class="sd">        value=[[[ 0.00000000e+00],</span>
<span class="sd">                [ 3.00000000e+00]]]),</span>
<span class="sd">        Tensor(shape=[1, 2, 1], dtype=Float32,</span>
<span class="sd">        value=[[[ 1.00000000e+00],</span>
<span class="sd">                [ 4.00000000e+00]]]),</span>
<span class="sd">        Tensor(shape=[1, 2, 1], dtype=Float32,</span>
<span class="sd">        value=[[[ 2.00000000e+00],</span>
<span class="sd">                [ 5.00000000e+00]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="n">shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">axes</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)])</span>


<div class="viewcode-block" id="flip"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.flip.html#mindspore.numpy.flip">[docs]</a><span class="k">def</span> <span class="nf">flip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses the order of elements in an array along the given axis.</span>

<span class="sd">    The shape of the array is preserved, but the elements are reordered.</span>

<span class="sd">    Note:</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Tensor): Input array.</span>
<span class="sd">        axis (None or int or tuple of ints, optional): Axis or axes along which</span>
<span class="sd">            to flip over. The default, ``axis=None``, will flip over all of the axes</span>
<span class="sd">            of the input array. If `axis` is negative it counts from the last to</span>
<span class="sd">            the first axis. If `axis` is a tuple of ints, flipping is performed on</span>
<span class="sd">            all of the axes specified in the tuple.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the entries of `axis` reversed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.arange(8.0).reshape((2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.flip(A)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7, 6],</span>
<span class="sd">        [5, 4]],</span>
<span class="sd">        [[3, 2],</span>
<span class="sd">        [1, 0]]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.flip(A, (0, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5, 4],</span>
<span class="sd">        [7, 6]],</span>
<span class="sd">        [[1, 0],</span>
<span class="sd">        [3, 2]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_is_shape_empty</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">_get_flip_start</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_get_flip_end</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_get_flip_strides</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">strided_slice</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">res</span><span class="p">),</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="flipud"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.flipud.html#mindspore.numpy.flipud">[docs]</a><span class="k">def</span> <span class="nf">flipud</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the entries in each column in the up/down direction.</span>
<span class="sd">    Rows are preserved, but appear in a different order than before.</span>

<span class="sd">    Note:</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Tensor): Input array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.arange(8.0).reshape((2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.flipud(A)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4., 5.],</span>
<span class="sd">        [6., 7.]],</span>
<span class="sd">        [[0., 1.],</span>
<span class="sd">        [2., 3.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="fliplr"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.fliplr.html#mindspore.numpy.fliplr">[docs]</a><span class="k">def</span> <span class="nf">fliplr</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips the entries in each row in the left/right direction.</span>
<span class="sd">    Columns are preserved, but appear in a different order than before.</span>

<span class="sd">    Note:</span>
<span class="sd">        On CPU, the supported dtypes are np.float16, np.float32, and np.float64.</span>

<span class="sd">    Args:</span>
<span class="sd">        m (Tensor): Input array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; A = np.arange(8.0).reshape((2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; output = np.fliplr(A)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2., 3.],</span>
<span class="sd">        [0., 1.]],</span>
<span class="sd">        [[6., 7.],</span>
<span class="sd">        [4., 5.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="take_along_axis"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.take_along_axis.html#mindspore.numpy.take_along_axis">[docs]</a><span class="k">def</span> <span class="nf">take_along_axis</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes values from the input array by matching 1d index and data slices.</span>

<span class="sd">    This iterates over matching 1d slices oriented along the specified axis in the</span>
<span class="sd">    index and data arrays, and uses the former to look up values in the latter.</span>
<span class="sd">    These slices can be different lengths.</span>

<span class="sd">    Args:</span>
<span class="sd">        arr (Tensor): Source array with shape `(Ni, M, Nk)`.</span>
<span class="sd">        indices (Tensor): Indices with shape `(Ni, J, Nk)` to take along each 1d</span>
<span class="sd">            slice of `arr`. This must match the dimension of `arr`, but dimensions `Ni`</span>
<span class="sd">            and `Nj` only need to broadcast against `arr`.</span>
<span class="sd">        axis (int): The axis to take 1d slices along. If `axis` is None, the input</span>
<span class="sd">            array is treated as if it had first been flattened to 1d.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the indexed result, with shape `(Ni, J, Nk)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if input array and indices have different number of dimensions.</span>
<span class="sd">        TypeError: if the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(12).reshape(3, 4)</span>
<span class="sd">        &gt;&gt;&gt; indices = np.arange(3).reshape(1, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = np.take_along_axis(x, indices, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0  1  2]</span>
<span class="sd">        [ 4  5  6]</span>
<span class="sd">        [ 8  9 10]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">!=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">indices</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;`indices` and `arr` must have the same number of dimensions&#39;</span><span class="p">)</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span>

    <span class="n">shape_arr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
    <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="c1"># broadcasts indices against the shape of arr except at axis</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">_tuple_getitem</span><span class="p">(</span><span class="n">shape_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
                            <span class="n">_tuple_getitem</span><span class="p">(</span><span class="n">shape_arr</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">_tuple_getitem</span><span class="p">(</span><span class="n">shape_arr</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="o">+</span>
                            <span class="n">_tuple_getitem</span><span class="p">(</span><span class="n">shape_indices</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">shape_arr</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_d</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes x mod y.&quot;&quot;&quot;</span>
    <span class="n">quotient</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">prod</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_mul</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">quotient</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prod</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_indices</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks whether indices are out of bounds.&quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">lowerbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="o">-</span><span class="n">size</span><span class="p">)</span>
    <span class="n">upperbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">out_of_lowerbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_lt</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">lowerbounds</span><span class="p">)</span>
    <span class="n">out_of_upperbounds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_gt</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">upperbounds</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;raise&#39;</span><span class="p">:</span>
        <span class="n">_raise_unimplemented_error</span><span class="p">(</span><span class="s1">&#39;&quot;raise&quot; mode is not implemented&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;wrap&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_mod</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">clipped</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">out_of_lowerbounds</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="n">clipped</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">out_of_upperbounds</span><span class="p">,</span> <span class="n">upperbounds</span><span class="p">,</span> <span class="n">clipped</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">clipped</span>


<div class="viewcode-block" id="take"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.take.html#mindspore.numpy.take">[docs]</a><span class="k">def</span> <span class="nf">take</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;clip&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes elements from an array along an axis.</span>

<span class="sd">    When axis is not None, this function does the same thing as fancy indexing</span>
<span class="sd">    (indexing arrays using arrays); however, it can be easier to use if you need</span>
<span class="sd">    elements along a given axis. A call such as ``np.take(arr, indices, axis=3)`` is</span>
<span class="sd">    equivalent to ``arr[:,:,:,indices,...]``.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy argument out is not supported.</span>
<span class="sd">        ``mode = &#39;raise&#39;`` is not supported, and the default mode is &#39;clip&#39; instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Source array with shape `(Ni, M, Nk)`.</span>
<span class="sd">        indices (Tensor): The indices with shape `(Nj...)` of the values to extract.</span>
<span class="sd">        axis (int, optional): The axis over which to select values. By default,</span>
<span class="sd">            the flattened input array is used.</span>
<span class="sd">        mode (raise, wrap, clip, optional): Specifies how out-of-bounds</span>
<span class="sd">            indices will behave.</span>

<span class="sd">            raise  raise an error (default);</span>

<span class="sd">            wrap  wrap around;</span>

<span class="sd">            clip  clip to the range. clip mode means that all indices that are</span>
<span class="sd">            too large are replaced by the index that addresses the last element</span>
<span class="sd">            along that axis. Note that this disables indexing with negative numbers.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the indexed result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if axis is out of range.</span>
<span class="sd">        TypeError: if the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.array([4, 3, 5, 7, 6, 8])</span>
<span class="sd">        &gt;&gt;&gt; indices = np.array([0, 1, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = np.take(a, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4 3 6]</span>
<span class="sd">        &gt;&gt;&gt; indices = np.array([[0, 1], [2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.take(a, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 3]</span>
<span class="sd">        [5 7]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;raise&#39;</span><span class="p">,</span> <span class="s1">&#39;wrap&#39;</span><span class="p">,</span> <span class="s1">&#39;clip&#39;</span><span class="p">):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;raise should be one of &quot;raise&quot;, &quot;wrap&quot;, or &quot;clip&quot;&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span>

    <span class="n">shape_a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">size_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">size</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_check_indices</span><span class="p">(</span><span class="n">shape_a</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span> <span class="n">indices</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="c1"># reshapes indices to shape (Ni..., Nj..., Nk)</span>
    <span class="n">shape_ni</span> <span class="o">=</span> <span class="n">_tuple_getitem</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">shape_nk</span> <span class="o">=</span> <span class="n">_tuple_getitem</span><span class="p">(</span><span class="n">shape_a</span><span class="p">,</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">shape_ni</span> <span class="o">+</span> <span class="n">shape_indices</span> <span class="o">+</span> <span class="n">shape_nk</span>
    <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">_expanded_shape</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">size_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape_indices</span><span class="p">)</span>
    <span class="n">shape_indices</span> <span class="o">=</span> <span class="n">shape_ni</span> <span class="o">+</span> <span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">size</span><span class="p">,)</span> <span class="o">+</span> <span class="n">shape_nk</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">shape_indices</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_d</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span></div>


<div class="viewcode-block" id="repeat"><a class="viewcode-back" href="../../../mindspore/numpy/mindspore.numpy.repeat.html#mindspore.numpy.repeat">[docs]</a><span class="k">def</span> <span class="nf">repeat</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Repeats elements of an array.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input array.</span>
<span class="sd">        repeats (int or sequence of ints): The number of repetitions for each element.</span>
<span class="sd">            `repeats` is broadcasted to fit the shape of the given axis.</span>
<span class="sd">        axis (int, optional): The axis along which to repeat values. By default,</span>
<span class="sd">            use the flattened input array, and return a flat output array.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output array which has the same shape as `a`, except along the given</span>
<span class="sd">        axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if axis is out of range.</span>
<span class="sd">        TypeError: if input `a` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(np.array(3), 4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 3 3 3]</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[1,2],[3,4]])</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 1 2 2 3 3 4 4]</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(x, 3, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1 1 2 2 2]</span>
<span class="sd">        [3 3 3 4 4 4]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.repeat(x, [1, 2], axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2]</span>
<span class="sd">        [3 4]</span>
<span class="sd">        [3 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">repeats</span> <span class="o">=</span> <span class="p">(</span><span class="n">repeats</span><span class="p">,)</span>
    <span class="n">_check_element_int</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">ravel</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">repeats</span> <span class="o">=</span> <span class="n">repeats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">repeats</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_empty</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
        <span class="k">return</span> <span class="n">C</span><span class="o">.</span><span class="n">repeat_elements</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">repeats</span><span class="p">)</span> <span class="o">!=</span> <span class="n">size</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;operands could not be broadcast together&#39;</span><span class="p">)</span>
    <span class="n">subs</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">repeated_subs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sub</span><span class="p">,</span> <span class="n">rep</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">subs</span><span class="p">,</span> <span class="n">repeats</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">rep</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">repeated_subs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">repeat_elements</span><span class="p">(</span><span class="n">sub</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">axis</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">concatenate</span><span class="p">(</span><span class="n">repeated_subs</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">rot90</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rotates a tensor by 90 degrees in the plane specified by axes.</span>
<span class="sd">    Rotation direction is from the first towards the second axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Input tensor of two or more dimensions.</span>
<span class="sd">        k (int): Number of times the tensor is rotated by 90 degrees. Default: 1.</span>
<span class="sd">        axes (Union[tuple(int), list(int)]): The tensor is rotated in the plane</span>
<span class="sd">            defined by the axes. Default: `(0, 1)`.</span>
<span class="sd">            Axes must be different and with the shape of `(2,)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if input `a` is not a Tensor or</span>
<span class="sd">            the argument `k` is not integer or</span>
<span class="sd">            the argument `axes` is not tuple of ints or list of ints.</span>
<span class="sd">        ValueError: if any axis is out of range or</span>
<span class="sd">            the length of `axes` is not `2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = np.arange(24).reshape((2, 3, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = np.rot90(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 8  9 10 11]</span>
<span class="sd">          [20 21 22 23]]</span>
<span class="sd">         [[ 4  5  6  7]</span>
<span class="sd">          [16 17 18 19]]</span>
<span class="sd">         [[ 0  1  2  3]</span>
<span class="sd">          [12 13 14 15]]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.rot90(a, 3, (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 8  4  0]</span>
<span class="sd">          [ 9  5  1]</span>
<span class="sd">          [10  6  2]</span>
<span class="sd">          [11  7  3]]</span>
<span class="sd">         [[20 16 12]</span>
<span class="sd">          [21 17 13]</span>
<span class="sd">          [22 18 14]</span>
<span class="sd">          [23 19 15]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_input_tensor</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;integer argument expected, but got &quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">%</span> <span class="mi">4</span> <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">4</span> <span class="o">-</span> <span class="p">(</span><span class="o">-</span><span class="n">k</span> <span class="o">%</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">_raise_type_error</span><span class="p">(</span><span class="s2">&quot;tuple(ints) or list(ints) expected, but got &quot;</span><span class="p">,</span> <span class="n">axes</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axes</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;len(axes) must be 2.&quot;</span><span class="p">)</span>
    <span class="n">axis1</span><span class="p">,</span> <span class="n">axis2</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">axis1</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">axis2</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis2</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis1</span> <span class="o">==</span> <span class="n">axis2</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;Axes must be different.&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">flip</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">axis1</span><span class="p">),</span> <span class="n">axis2</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span><span class="p">]</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis2</span><span class="p">],</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">axis1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">flip</span><span class="p">(</span><span class="n">transpose</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">perm</span><span class="p">),</span> <span class="n">axis2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">select</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an array drawn from elements in `choicelist`, depending on conditions.</span>

<span class="sd">    Args:</span>
<span class="sd">        condlist (array_like): The list of conditions which determine from which array</span>
<span class="sd">            in `choicelist` the output elements are taken. When multiple conditions are</span>
<span class="sd">            satisfied, the first one encountered in `condlist` is used.</span>
<span class="sd">        choicelist (array_like): The list of arrays from which the output elements are</span>
<span class="sd">            taken. It has to be of the same length as `condlist`.</span>
<span class="sd">        default (scalar, optional): The element inserted in output when all conditions</span>
<span class="sd">            evaluate to `False`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output at position `m` is the `m-th` element of the array in</span>
<span class="sd">        `choicelist` where the `m-th` element of the corresponding array in `condlist`</span>
<span class="sd">        is `True`.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if ``len(condlist) != len(choicelist)``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; condlist = [[True, True, True, False, False],</span>
<span class="sd">                       [False, False, True, False, True]]</span>
<span class="sd">        &gt;&gt;&gt; choicelist = [[0, 1, 2, 3, 4], [0, 1, 4, 9, 16]]</span>
<span class="sd">        &gt;&gt;&gt; output = np.select(condlist, choicelist)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  2  0 16]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span> <span class="o">=</span> <span class="n">_to_tensor</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">choicelist</span><span class="p">)</span>
    <span class="n">shape_cond</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span>
    <span class="n">shape_choice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">choicelist</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">condlist</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;input cannot be scalars&#39;</span><span class="p">)</span>
    <span class="n">case_num</span> <span class="o">=</span> <span class="n">shape_cond</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">shape_choice</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">case_num</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s1">&#39;list of cases must be same length as list of conditions&#39;</span><span class="p">)</span>

    <span class="c1"># performs broadcast over the cases in condlist and choicelist</span>
    <span class="n">case_size</span> <span class="o">=</span> <span class="n">_infer_out_shape</span><span class="p">(</span><span class="n">shape_cond</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">shape_choice</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">shape_broadcasted</span> <span class="o">=</span> <span class="p">(</span><span class="n">case_num</span><span class="p">,)</span> <span class="o">+</span> <span class="n">case_size</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape_broadcasted</span><span class="p">)</span>
    <span class="n">shape_cond_expanded</span> <span class="o">=</span> <span class="p">((</span><span class="n">case_num</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">condlist</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="o">+</span>
                           <span class="n">shape_cond</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">condlist</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">condlist</span><span class="p">,</span> <span class="n">shape_cond_expanded</span><span class="p">),</span> <span class="n">shape_broadcasted</span><span class="p">)</span>
    <span class="n">shape_choice_expanded</span> <span class="o">=</span> <span class="p">((</span><span class="n">case_num</span><span class="p">,)</span> <span class="o">+</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">choicelist</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="o">+</span>
                             <span class="n">shape_choice</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">choicelist</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">choicelist</span><span class="p">,</span> <span class="n">shape_choice_expanded</span><span class="p">),</span> <span class="n">shape_broadcasted</span><span class="p">)</span>

    <span class="n">slice_start</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">slice_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">case_size</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">choicelist</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;CPU&#39;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
        <span class="c1"># F.tensor_slice only supports float on CPU</span>
        <span class="n">choicelist</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">choicelist</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">default_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">choicelist</span><span class="p">),</span> <span class="n">slice_size</span><span class="p">,</span> <span class="n">default</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">case_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">cond_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">condlist</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span> <span class="o">+</span> <span class="n">slice_start</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>
        <span class="n">choice_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">choicelist</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span> <span class="o">+</span> <span class="n">slice_start</span><span class="p">,</span> <span class="n">slice_size</span><span class="p">)</span>
        <span class="n">default_slice</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">cond_slice</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">),</span> <span class="n">choice_slice</span><span class="p">,</span> <span class="n">default_slice</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">default_slice</span><span class="p">,</span> <span class="p">(</span><span class="n">case_size</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>