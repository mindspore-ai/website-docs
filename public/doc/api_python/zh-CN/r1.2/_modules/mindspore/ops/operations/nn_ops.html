<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations.nn_ops &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore/mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.privacy.sup_privacy.html">mindarmour.privacy.sup_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Serving Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindspore_serving/mindspore_serving.html">mindspore_serving</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindQuantum Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../mindquantum/mindquantum.html">mindquantum</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.operations.nn_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.operations.nn_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for nn.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">partial</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">_check_3d_int_or_tuple</span>
<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">..._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">...common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">...common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">..primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">prim_attr_register</span>


<span class="k">def</span> <span class="nf">_check_positive_int_or_tuple</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an argument is a positive int or tuple with 2 or 4(when allow_four is True) positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be an positive int number or a tuple of two &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;or four &#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">allow_four</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_return_value</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="n">arg_value</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_four</span><span class="p">:</span>
                <span class="n">_raise_message</span><span class="p">()</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_raise_message</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">ret_value</span> <span class="o">=</span> <span class="n">_get_return_value</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ret_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret_value</span>


<span class="k">def</span> <span class="nf">_check_shape</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an shape dims is a positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; dims elements should be positive int numbers, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arg_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">arg_value</span>


<span class="k">def</span> <span class="nf">_update_attr_by_format</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_format</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If the format is NHWC, should modify the strides or dilation shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">arg_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Flatten.html#mindspore.ops.Flatten">[docs]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)` to be flattened.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, X)`, where :math:`X` is</span>
<span class="sd">        the product of the remaining dimension.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; flatten = ops.Flatten()</span>
<span class="sd">        &gt;&gt;&gt; output = flatten(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;input_x rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">prod</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">reduce</span><span class="p">(</span><span class="n">operator</span><span class="o">.</span><span class="n">mul</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="k">return</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prod</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Softmax.html#mindspore.ops.Softmax">[docs]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softmax operation.</span>

<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given aixs :math:`x`, then for each element :math:`x_i`,</span>
<span class="sd">    the Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \frac{exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)},</span>

<span class="sd">    where :math:`N` is the length of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple]): The axis to perform the Softmax operation. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input of Softmax, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose length is less than 1.</span>
<span class="sd">        ValueError: If `axis` is a tuple whose elements are not all in range [-len(logits), len(logits)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax = ops.Softmax()</span>
<span class="sd">        &gt;&gt;&gt; output = softmax(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of axis&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;length of axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">axis_v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">axis_v</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>


<div class="viewcode-block" id="LogSoftmax"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.LogSoftmax.html#mindspore.ops.LogSoftmax">[docs]</a><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log Softmax activation function.</span>

<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Suppose a slice in the given aixs, :math:`x` for each element :math:`x_i`,</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The axis to perform the Log softmax operation. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input of Log Softmax, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is not in range [-len(logits), len(logits)].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log_softmax = ops.LogSoftmax()</span>
<span class="sd">        &gt;&gt;&gt; output = log_softmax(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span></div>


<div class="viewcode-block" id="Softplus"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Softplus.html#mindspore.ops.Softplus">[docs]</a><span class="k">class</span> <span class="nc">Softplus</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus activation function.</span>

<span class="sd">    Softplus is a smooth approximation to the ReLU function.</span>
<span class="sd">    It can be used to constrain the output of a machine to always be positive.</span>
<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \log(1 + \exp(\text{input_x})),</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose data type must be float.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softplus = ops.Softplus()</span>
<span class="sd">        &gt;&gt;&gt; output = softplus(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.3132615 2.126928  3.0485873 4.01815   5.0067153]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softplus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Softsign"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Softsign.html#mindspore.ops.Softsign">[docs]</a><span class="k">class</span> <span class="nc">Softsign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softsign activation function.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{SoftSign}(x) = \frac{x}{ 1 + |x|}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softsign = ops.Softsign()</span>
<span class="sd">        &gt;&gt;&gt; output = softsign(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        -0.5         0.6666667  0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softsign&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ReLU.html#mindspore.ops.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) of input tensors element-wise.</span>

<span class="sd">    It returns :math:`\max(x,\  0)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not number.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu = ops.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; output = relu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Mish"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Mish.html#mindspore.ops.Mish">[docs]</a><span class="k">class</span> <span class="nc">Mish</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = x * \tan(\log(1 + \exp(\text{x})))</span>

<span class="sd">    See more details in `A Self Regularized Non-Monotonic Neural Activation Function</span>
<span class="sd">    &lt;https://arxiv.org/abs/1908.08681&gt;`_.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor. Only support float16 and float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Raise:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mish = ops.Mish()</span>
<span class="sd">        &gt;&gt;&gt; output = mish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.30273438  3.9974136 -0.015625]</span>
<span class="sd">         [ 1.9439697  -0.02929688 8.999999]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Mish&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="SeLU"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SeLU.html#mindspore.ops.SeLU">[docs]</a><span class="k">class</span> <span class="nc">SeLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes SeLU (scaled exponential Linear Unit) of input tensors element-wise.</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        E_{i} =</span>
<span class="sd">        scale *</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x, &amp;\text{if } x \geq 0; \cr</span>
<span class="sd">        \text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`</span>
<span class="sd">    and :math:`scale=1.05070098`).</span>

<span class="sd">    See more details in `Self-Normalizing Neural Networks &lt;https://arxiv.org/abs/1706.02515&gt;`_.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Raise:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; selu = ops.SeLU()</span>
<span class="sd">        &gt;&gt;&gt; output = selu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.1113307 4.202804 -1.7575096]</span>
<span class="sd">        [ 2.101402 -1.7462534 9.456309 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ReLU6.html#mindspore.ops.ReLU6">[docs]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ReLU6}(x) = \min(\max(0,x), 6)</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu6 = ops.ReLU6()</span>
<span class="sd">        &gt;&gt;&gt; result = relu6(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLU6&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReLUV2"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ReLUV2.html#mindspore.ops.ReLUV2">[docs]</a><span class="k">class</span> <span class="nc">ReLUV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) of input tensors element-wise.</span>

<span class="sd">    It returns :math:`\max(x,\  0)` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor must be a 4-D tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - Has the same type and shape as the `input_x`.</span>
<span class="sd">        - **mask** (Tensor) - A tensor whose data type must be uint8.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x`, `output` or `mask` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `output` is not same as `input_x` .</span>
<span class="sd">        TypeError: If dtype of `mask` is not unit8.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, -2], [-3, 4]], [[-5, 6], [7, -8]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu_v2 = ops.ReLUV2()</span>
<span class="sd">        &gt;&gt;&gt; output, mask= relu_v2(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 0.]</span>
<span class="sd">           [0. 4.]]</span>
<span class="sd">          [[0. 6.]</span>
<span class="sd">           [7. 0.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(mask)</span>
<span class="sd">        [[[[[1 0]</span>
<span class="sd">            [2 0]]</span>
<span class="sd">           [[2 0]</span>
<span class="sd">            [1 0]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLUV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">mask_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The `input_x` should be a 4-D tensor, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got a </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">-D tensor whose shape is </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_shape</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">):</span>
                    <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">31</span><span class="p">)</span> <span class="o">//</span> <span class="mi">32</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span> <span class="o">//</span> <span class="mi">16</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">):</span>
            <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mask_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">mask_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">mask_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span>
        <span class="n">output_dtype</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_dtype</span><span class="p">,</span> <span class="n">mask_dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">output_shape</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">output_dtype</span><span class="p">,</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="Elu"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Elu.html#mindspore.ops.Elu">[docs]</a><span class="k">class</span> <span class="nc">Elu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes exponential linear:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ELU}(x)= \left\{</span>
<span class="sd">        \begin{array}{align}</span>
<span class="sd">            \alpha(e^{x}  - 1) &amp; \text{if } x \le 0\\</span>
<span class="sd">            x &amp; \text{if } x \gt 0\\</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    The data type of input tensor must be float.</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (float): The coefficient of negative factor whose type is float,</span>
<span class="sd">            only support &#39;1.0&#39; currently. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Elu with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; elu = ops.Elu()</span>
<span class="sd">        &gt;&gt;&gt; output = elu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.63212055  4.         -0.99966455]</span>
<span class="sd">         [ 2.         -0.99326205  9.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Elu&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="HSwish"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.HSwish.html#mindspore.ops.HSwish">[docs]</a><span class="k">class</span> <span class="nc">HSwish</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard swish activation function.</span>

<span class="sd">    Applies hswish-type activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard swish is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{hswish}(x_{i}) = x_{i} * \frac{ReLU6(x_{i} + 3)}{6},</span>

<span class="sd">    where :math:`x_{i}` is the :math:`i`-th slice in the given dimension of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The input of HSwish, data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_data`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_data` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_data` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; hswish = ops.HSwish()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hswish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xshape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">xshape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Sigmoid.html#mindspore.ops.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sigmoid activation function.</span>

<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)},</span>

<span class="sd">    where :math:`x_i` is the element of the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Sigmoid, data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = ops.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; output = sigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="HSigmoid"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.HSigmoid.html#mindspore.ops.HSigmoid">[docs]</a><span class="k">class</span> <span class="nc">HSigmoid</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard sigmoid is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),</span>

<span class="sd">    where :math:`x_{i}` is the :math:`i`-th slice in the given dimension of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The input of HSigmoid, data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_data`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_data` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_data` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; hsigmoid = ops.HSigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hsigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.3333  0.1666  0.5  0.833  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Tanh.html#mindspore.ops.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tanh activation function.</span>

<span class="sd">    Computes hyperbolic tangent of input element-wise. The Tanh function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Tanh with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input_x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tanh = ops.Tanh()</span>
<span class="sd">        &gt;&gt;&gt; output = tanh(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7615941 0.9640276 0.9950547 0.9993293 0.9999092]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<span class="k">class</span> <span class="nc">FusedBatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The FusedBatchNorm interface is deprecated, please use the BatchNorm interface.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The FusedBatchNorm interface is deprecated, please use the BatchNorm interface.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedBatchNormEx</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The FusedBatchNormEx interface is deprecated, please use the BatchNorm interface.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;FusedBatchnormEx interface is deprecated, please use BatchNorm interface.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">InstanceNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instance Normalization over a 4D input.</span>

<span class="sd">    This operator applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with</span>
<span class="sd">    additional channel dimension) as described in the paper `Instance Normalization: The Missing Ingredient for</span>
<span class="sd">    Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;`_. It rescales and recenters the feature using a mini-batch</span>
<span class="sd">    of data and the learned parameters which can be described in the following formula.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_training (bool): Is training or inference. Default: True.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.1.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NCHW&#39;. Default: &quot;NCHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of InstanceNorm, Tensor of shape :math:`(N, C)`,</span>
<span class="sd">          data type: float16 or float32.</span>
<span class="sd">        - **gamma** (Parameter) - scale, Tensor of shape :math:`(C,)`,</span>
<span class="sd">          data type: float32.</span>
<span class="sd">        - **beta** (Parameter) - bias, Tensor of shape :math:`(C,)`,</span>
<span class="sd">          data type: float32.</span>
<span class="sd">        - **mean** (Parameter) - mean value, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **variance** (Parameter) - variance value, Tensor of shape :math:`(C,)`, data type: float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the normalized input, the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The output of InstanceNorm, same type and shape as the `input_x`.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Updated mean value, Tensor of shape :math:`(NC,)`, data type: float32.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Updated variance value, Tensor of shape :math:`(NC,)`,</span>
<span class="sd">          data type: float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Raise:</span>
<span class="sd">        TypeError: If any validator check fails.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class InstanceNormNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(InstanceNormNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.instance_norm = ops.InstanceNorm()</span>
<span class="sd">        &gt;&gt;&gt;         self.gamma = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;gamma&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.beta = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;beta&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.mean = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.variance = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;variance&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, input_x):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.instance_norm(input_x, self.gamma, self.beta, self.mean, self.variance)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = InstanceNormNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; result = output[0].shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (128, 64, 32, 64)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_variance&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">is_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_parameter</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">input_shape_norm</span> <span class="o">=</span> <span class="n">input_x</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gamma</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;gamma rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;gamma shape&quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="s2">&quot;beta shape&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;gamma shape[0]&quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input channel&quot;</span><span class="p">,</span> <span class="n">input_shape_norm</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;mean rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;gamma shape&quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">save_mean_shape</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="n">save_mean_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">save_mean_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_shape_norm</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">save_mean_shape</span><span class="p">,</span> <span class="n">save_mean_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="n">gamma</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_types_same_and_valid</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>


<div class="viewcode-block" id="BNTrainingReduce"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BNTrainingReduce.html#mindspore.ops.BNTrainingReduce">[docs]</a><span class="k">class</span> <span class="nc">BNTrainingReduce</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For the BatchNorm operation this operator update the moving averages for training and is used in conjunction with</span>
<span class="sd">    BNTrainingUpdate.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D Tensor with float16 or float32 data type. Tensor of shape :math:`(N, C, A, B)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **sum** (Tensor) - A 1-D Tensor with float32 data type. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **square_sum** (Tensor) - A 1-D Tensor with float32 data type. Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `sum` or `square_sum` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `square_sum` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 3, 32, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bn_training_reduce = ops.BNTrainingReduce()</span>
<span class="sd">        &gt;&gt;&gt; output = bn_training_reduce(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value=</span>
<span class="sd">        [ 1.22880000e+04, 1.22880000e+04, 1.22880000e+04]), Tensor(shape=[3], dtype=Float32, value=</span>
<span class="sd">        [ 1.22880000e+04, 1.22880000e+04, 1.22880000e+04]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;square_sum&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">([</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x_type</span><span class="p">,</span> <span class="n">x_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="BNTrainingUpdate"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BNTrainingUpdate.html#mindspore.ops.BNTrainingUpdate">[docs]</a><span class="k">class</span> <span class="nc">BNTrainingUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For the BatchNorm operation, this operator update the moving averages for training and is used in conjunction with</span>
<span class="sd">    BNTrainingReduce.</span>

<span class="sd">    Args:</span>
<span class="sd">        isRef (bool): If a ref. Default: True.</span>
<span class="sd">        epsilon (float): A small value added to variance avoid dividing by zero. Default: 1e-5.</span>
<span class="sd">        factor (float): A weight for updating the mean and variance. Default: 0.1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D Tensor with float16 or float32 data type. Tensor of shape :math:`(N, C, A, B)`.</span>
<span class="sd">        - **sum** (Tensor) - A 1-D Tensor with float16 or float32 data type for the output of operator</span>
<span class="sd">          BNTrainingReduce. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **square_sum** (Tensor) - A 1-D Tensor with float16 or float32 data type for the output of operator</span>
<span class="sd">          BNTrainingReduce. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **scale** (Tensor) - A 1-D Tensor with float16 or float32, for the scaling factor.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **offset** (Tensor) - A 1-D Tensor with float16 or float32, for the scaling offset.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **mean** (Tensor) - A 1-D Tensor with float16 or float32, for the scaling mean. Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - A 1-D Tensor with float16 or float32, for the update variance.</span>
<span class="sd">          Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Tensor, has the same shape data type as `x`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor for the updated mean, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor for the updated variance, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>
<span class="sd">        - **batch_mean** (Tensor) - Tensor for the mean of `x`, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>
<span class="sd">        - **batch_variance** (Tensor) - Tensor for the mean of `variance`, with float32 data type.</span>
<span class="sd">          Has the same shape as `variance`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `isRef` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `epsilon` or `factor` is not float.</span>
<span class="sd">        TypeError: If `x`, `sum`, `square_sum`, `scale`, `offset`, `mean` or `variance` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `sum`, `square_sum`, `scale`, `offset`, `mean` or `variance` is neither float16 nor</span>
<span class="sd">                   float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([1, 2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; square_sum = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; offset = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bn_training_update = ops.BNTrainingUpdate()</span>
<span class="sd">        &gt;&gt;&gt; output = bn_training_update(input_x, sum, square_sum, scale, offset, mean, variance)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 2, 2], dtype=Float32, value=</span>
<span class="sd">        [[[[ 2.73200464e+00,  2.73200464e+00],</span>
<span class="sd">           [ 2.73200464e+00,  2.73200464e+00]],</span>
<span class="sd">          [[ 2.73200464e+00,  2.73200464e+00],</span>
<span class="sd">           [ 2.73200464e+00,  2.73200464e+00]]]]), Tensor(shape=[2], dtype=Float32, value= [9.24999952e-0.1,</span>
<span class="sd">        9.24999952e-0.1]), Tensor(shape=[2], dtype=Float32, value= [ 9.24999952e-0.1, 9.24999952e-0.1]),</span>
<span class="sd">        Tensor(shape=[2], dtype=Float32, value= [ 2.50000000e-0.1, 2.50000000e-0.1]), Tensor(shape=[2], dtype=Float32,</span>
<span class="sd">        value= [ 1.87500000e-0.1, 1.87500000e-0.1]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">isRef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;square_sum&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;running_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_inv_variance&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;isRef&quot;</span><span class="p">,</span> <span class="n">isRef</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;factor&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;BNTrainingUpdate&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">factor</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;factor&#39;</span><span class="p">,</span> <span class="s1">&#39;BNTrainingUpdate&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">sum</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;sum rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">square_sum</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;square_sum rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;scale rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;mean rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">variance</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;variance rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;sum shape&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;square_sum shape&quot;</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;offset shape&quot;</span><span class="p">,</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;square_sum&quot;</span><span class="p">,</span> <span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">square_sum</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span></div>


<div class="viewcode-block" id="BatchNorm"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BatchNorm.html#mindspore.ops.BatchNorm">[docs]</a><span class="k">class</span> <span class="nc">BatchNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over input to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters which can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_training (bool): If `is_training` is True, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `is_training` is False, they&#39;re loaded from checkpoint during inference. Default: False.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = (1 - momentum) * running\_mean + momentum * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.1.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &quot;NCHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        If `is_training` is False, inputs are Tensors.</span>

<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `mean`.</span>

<span class="sd">        If `is_training` is True, `scale`, `bias`, `mean` and `variance` are Parameters.</span>

<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        - **scale** (Parameter) - Parameter of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **bias** (Parameter) - Parameter of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **mean** (Parameter) - Parameter of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **variance** (Parameter) - Parameter of shape :math:`(C,)`, has the same data type with `mean`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensor, the normalized inputs and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the input_x. The shape is :math:`(N, C)`.</span>
<span class="sd">        - **updated_scale** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **updated_bias** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_1** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_2** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `is_training` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `epsilon` or `momentum` is not float.</span>
<span class="sd">        TypeError: If `data_format` is not a str.</span>
<span class="sd">        TypeError: If `input_x`, `scale`, `bias`, `mean` or `variance` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x`, `scale` or `mean` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batch_norm = ops.BatchNorm()</span>
<span class="sd">        &gt;&gt;&gt; output = batch_norm(input_x, scale, bias, mean, variance)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00, 1.00000000e+00],</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]), Tensor(shape=[2], dtype=Float32, value=</span>
<span class="sd">         [ 1.00000000e+00, 1.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_training</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_signatures</span><span class="p">(</span><span class="nb">tuple</span><span class="p">())</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_training&#39;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;NHWC format only support in GPU target.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_1&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_2&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">input_x_channel</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> <span class="n">input_x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;scale rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;bias shape&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;scale shape[0]&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_x channel&quot;</span><span class="p">,</span> <span class="n">input_x_channel</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;mean rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance shape&quot;</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;mean shape&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;scale shape&quot;</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">scale</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">bias</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">:</span>
            <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_types_same_and_valid</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args_moving</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="n">variance</span><span class="p">}</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args_moving</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Conv2D.html#mindspore.ops.Conv2D">[docs]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Applies a 2D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size and :math:`C_{in}` is channel number. For each batch of shape</span>
<span class="sd">    :math:`(C_{in}, H_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is the cross correlation operator, :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to the :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{ks_h}, \text{ks_w})`, where :math:`\text{ks_h}` and</span>
<span class="sd">    :math:`\text{ks_w}` are the height and width of the convolution kernel. The full kernel has shape</span>
<span class="sd">    :math:`(C_{out}, C_{in} // \text{group}, \text{ks_h}, \text{ks_w})`, where group is the group number</span>
<span class="sd">    to split the input in the channel dimension.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -</span>
<span class="sd">    (\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -</span>
<span class="sd">    (\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` respectively.</span>


<span class="sd">    The first introduction can be found in paper `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_. More detailed introduction can be found here:</span>
<span class="sd">    http://cs231n.github.io/convolutional-networks/.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimension of the output.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 2D convolution.</span>
<span class="sd">        mode (int): Modes for different convolutions. 0 Math convolutiuon, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 1.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the</span>
<span class="sd">                    padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;. Default: &quot;NCHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 2D convolution. The shape is :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; not &#39;NHWC&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(input_tensor, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, padding must be zero when pad_mode is &#39;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;NHWC format only support in GPU target.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">x_shape_norm</span> <span class="o">=</span> <span class="n">x_shape</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">w_shape_norm</span> <span class="o">=</span> <span class="n">w_shape</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape_norm</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape_norm</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="DepthwiseConv2dNative"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.DepthwiseConv2dNative.html#mindspore.ops.DepthwiseConv2dNative">[docs]</a><span class="k">class</span> <span class="nc">DepthwiseConv2dNative</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the depth-wise convolution value for the input.</span>

<span class="sd">    Applies depthwise conv2d for the input, which will generate more channels with channel_multiplier.</span>
<span class="sd">    Given an input tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})` where :math:`N` is the batch size and a</span>
<span class="sd">    filter tensor with kernel size :math:`(ks_{h}, ks_{w})`, containing :math:`C_{in} * \text{channel_multiplier}`</span>
<span class="sd">    convolutional filters of depth 1; it applies different filters to each input channel (channel_multiplier channels</span>
<span class="sd">    for each input channel has the default value 1), then concatenates the results together. The output has</span>
<span class="sd">    :math:`\text{in_channels} * \text{channel_multiplier}` channels.</span>

<span class="sd">    Args:</span>
<span class="sd">        channel_multiplier (int): The multiplier for the original output convolution. Its value must be greater than 0.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution kernel.</span>
<span class="sd">        mode (int): Modes for different convolutions. 0 Math convolution, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 3.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union[int, tuple[int]]): The pad value to be filled. If `pad` is an integer, the paddings of</span>
<span class="sd">            top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the padding</span>
<span class="sd">            of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly. Default: 0.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Specifies the dilation rate to be used for the dilated convolution.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **weight** (Tensor) - Set the size of kernel as :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(K, C_{in}, K_1, K_2)`, `K` must be 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in} * \text{channel_multiplier}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `channel_multiplier` or `group` is not an int.</span>
<span class="sd">        ValueError: If `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of the following:&#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([1, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; depthwise_conv2d = ops.DepthwiseConv2dNative(channel_multiplier=3, kernel_size=(3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = depthwise_conv2d(input_tensor, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 96, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">channel_multiplier</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DepthwiseConv2dNative&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN_DEPRECATED: The usage of DepthwiseConv2dNative is deprecated.&quot;</span>
                       <span class="s2">&quot; Please use nn.Conv2D.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of stride should be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of dilation should be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, padding must be zero when pad_mode is &#39;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">channel_multiplier</span><span class="p">,</span> <span class="s2">&quot;channel_multiplier&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_size_n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_size_h</span><span class="p">,</span> <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dilation_h</span><span class="p">,</span> <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>
        <span class="k">if</span> <span class="n">kernel_size_n</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The batch of input weight should be 1, but got </span><span class="si">{</span><span class="n">kernel_size_n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="n">stride_w</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<span class="k">class</span> <span class="nc">_Pool</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max/avg pooling operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the kernel, that must be a tuple</span>
<span class="sd">           of two `int` for height and width. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The stride of the window, that must be</span>
<span class="sd">            a tuple of two `int` for height and width. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &quot;NCHW&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;MaxPoolWithArgmax&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;NHWC format only support in GPU target.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">x_shape_norm</span> <span class="o">=</span> <span class="n">x_shape</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape_norm</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">input_h</span><span class="p">,</span> <span class="n">input_w</span> <span class="o">=</span> <span class="n">x_shape_norm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_h</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_w</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">shape_value</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape_value</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; The kernel size is not valid, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;please check it if is larger than data&#39;s shape size.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<div class="viewcode-block" id="MaxPool"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.MaxPool.html#mindspore.ops.MaxPool">[docs]</a><span class="k">class</span> <span class="nc">MaxPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling operation.</span>

<span class="sd">    Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width are both kernel_size, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>
<span class="sd">        data_format (str) : The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_op = ops.MaxPool(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output = maxpool_op(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxPoolWithArgmax"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.MaxPoolWithArgmax.html#mindspore.ops.MaxPoolWithArgmax">[docs]</a><span class="k">class</span> <span class="nc">MaxPoolWithArgmax</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max pooling on the input Tensor and returns both max values and indices.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents height and width are both kernel_size, or a tuple of</span>
<span class="sd">            two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>


<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</span>

<span class="sd">        - **output** (Tensor) -  Maxpooling result, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `input`.</span>
<span class="sd">        - **mask** (Tensor) -  Max values&#39; index represented by the mask. Data type is int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input data type is not float16 or float32.</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_arg_op = ops.MaxPoolWithArgmax(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = maxpool_arg_op(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPoolWithArgmax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">_Pool</span><span class="o">.</span><span class="n">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">argmax_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
        <span class="k">return</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">argmax_dtype</span></div>


<div class="viewcode-block" id="MaxPool3D"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.MaxPool3D.html#mindspore.ops.MaxPool3D">[docs]</a><span class="k">class</span> <span class="nc">MaxPool3D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D max pooling operation.</span>

<span class="sd">    Applies a 3D max pooling over an input Tensor which can be regarded as a composition of 3D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width are both kernel_size, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support &#39;NCDHW&#39;. Default: &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`. Data type must be float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the data type with `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither an int not a tuple.</span>
<span class="sd">        TypeError: If `pad_mode` or `data_format` is not a string.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `strides` are not positive.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; max_pool3d = ops.MaxPool3D(kernel_size=2, strides=1, pad_mode=&quot;valid&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = max_pool3d(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[10. 11.]]]</span>
<span class="sd">          [[[22. 23.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                  <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">input_d</span><span class="p">,</span> <span class="n">input_h</span><span class="p">,</span> <span class="n">input_w</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_d</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="n">out_d</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_d</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">out_d</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_d</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_h</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_w</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">out_d</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span>

        <span class="n">_check_shape</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="AvgPool"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.AvgPool.html#mindspore.ops.AvgPool">[docs]</a><span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, AvgPool outputs</span>
<span class="sd">    regional average in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents height and width are both kernel_size, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;, not case sensitive.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>
<span class="sd">        data_format (str) - The format of input and output data. It should be &#39;NHWC&#39; or &#39;NCHW&#39;，\</span>
<span class="sd">            default is &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.avgpool_op = ops.AvgPool(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         result = self.avgpool_op(x)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[14.5  15.5  16.5]</span>
<span class="sd">           [18.5  19.5  20.5]]</span>
<span class="sd">          [[26.5  27.5  28.5]</span>
<span class="sd">           [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DBackpropInput"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Conv2DBackpropInput.html#mindspore.ops.Conv2DBackpropInput">[docs]</a><span class="k">class</span> <span class="nc">Conv2DBackpropInput</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of convolution with respect to the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimensionality of the output space.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution window.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the</span>
<span class="sd">                    padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</span>
<span class="sd">        mode (int): Modes for different convolutions. 0 Math convolutiuon, 1 cross-correlation convolution ,</span>
<span class="sd">                       2 deconvolution, 3 depthwise convolution. Default: 1.</span>
<span class="sd">        stride (Union[int. tuple[int]]): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union[int. tuple[int]]): Specifies the dilation rate to be used for the dilated convolution.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>
<span class="sd">        data_format (str) - The format of input and output data. It should be &#39;NHWC&#39; or &#39;NCHW&#39;，\</span>
<span class="sd">            default is &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dout** (Tensor) - the gradients w.r.t the output of the convolution. The shape conforms to the default</span>
<span class="sd">          data_format :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>
<span class="sd">        - **input_size** (Tensor) - A tuple describes the shape of the input which conforms to the format</span>
<span class="sd">          :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients w.r.t the input of convolution. It has the same shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; not &#39;NHWC&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([10, 32, 30, 30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]))</span>
<span class="sd">        &gt;&gt;&gt; conv2d_backprop_input = ops.Conv2DBackpropInput(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d_backprop_input(dout, weight, F.shape(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 32, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_sizes&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;input_sizes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;NHWC format only support in GPU target.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_update_attr_by_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_update_attr_by_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, padding must be zero when pad_mode is &#39;</span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;element of pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad_list</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doutput</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x_size</span><span class="p">):</span>
        <span class="n">x_size_v</span> <span class="o">=</span> <span class="n">x_size</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;x_size&#39;</span><span class="p">,</span> <span class="n">x_size_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_size_v</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;x_size[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;doutput&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># infer shape</span>
        <span class="n">dout_shape</span> <span class="o">=</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">dout_shape_norm</span> <span class="o">=</span> <span class="n">dout_shape</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> \
            <span class="p">[</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dout_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">dout_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">dout_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">kernel_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">stride_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">dilation_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="c1"># default pad mode is valid</span>
        <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape_norm</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape_norm</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">:</span>
            <span class="n">pad_list</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="n">pad_list</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_size_v</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="BiasAdd"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BiasAdd.html#mindspore.ops.BiasAdd">[docs]</a><span class="k">class</span> <span class="nc">BiasAdd</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sum of input and bias tensor.</span>

<span class="sd">    Adds the 1-D bias tensor to the input tensor, and broadcasts the shape on all axis</span>
<span class="sd">    except for the channel axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        data_format (str): The format of input and output data. It should be &#39;NHWC&#39;, &#39;NCHW&#39; or &#39;NCDHW&#39;，</span>
<span class="sd">            default is &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape can be 2-5 dimensions.</span>
<span class="sd">        - **bias** (Tensor) - The bias tensor, with shape :math:`(C)`. The shape of</span>
<span class="sd">          `bias` must be the same as `input_x`&#39;s channel dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `data_format`, `input_x` or `bias` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3,)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias_add = ops.BiasAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;NHWC format only support in GPU target.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCDHW&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span> <span class="ow">or</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;NCDHW format only support 5-dims input in Ascend target.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;bias rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_channel</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_channel&quot;</span><span class="p">,</span> <span class="n">x_channel</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">b_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">:</span> <span class="n">b_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="TopK"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.TopK.html#mindspore.ops.TopK">[docs]</a><span class="k">class</span> <span class="nc">TopK</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values and indices of the `k` largest entries along the last dimension.</span>

<span class="sd">    If the `input_x` is a one-dimensional Tensor, finds the `k` largest entries in the Tensor,</span>
<span class="sd">    and outputs its value and index as a Tensor. Therefore, values[`k`] is the `k` largest item in `input_x`,</span>
<span class="sd">    and its index is indices [`k`].</span>

<span class="sd">    For a multi-dimensional matrix,</span>
<span class="sd">    calculates the first `k` entries in each row (corresponding vector along the last dimension), therefore:</span>

<span class="sd">    .. math::</span>

<span class="sd">    values.shape = indices.shape = input.shape[:-1] + [k].</span>

<span class="sd">    If the two compared elements are the same, the one with the smaller index value is returned first.</span>

<span class="sd">    Args:</span>
<span class="sd">        sorted (bool): If true, the obtained elements will</span>
<span class="sd">            be sorted by the values in descending order. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to be computed, data type must be float16, float32 or int32.</span>
<span class="sd">        - **k** (int) - The number of top elements to be computed along the last dimension, constant input is needed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the values and the indices.</span>

<span class="sd">        - **values** (Tensor) - The `k` largest elements in each slice of the last dimensional.</span>
<span class="sd">        - **indices** (Tensor) - The indices of values within the last dimension of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sorted` is not a bool.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of the following: float16, float32 or int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; topk = ops.TopK(sorted=True)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 5], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; k = 3</span>
<span class="sd">        &gt;&gt;&gt; values, indices = topk(input_x, k)</span>
<span class="sd">        &gt;&gt;&gt; print((values, indices))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float16, value= [ 5.0000e+00,  4.0000e+00,  3.0000e+00]), Tensor(shape=[3],</span>
<span class="sd">          dtype=Int32, value= [4, 3, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sorted&quot;</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">k_v</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">k_v</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x_shape</span><span class="p">[</span><span class="n">ndim</span><span class="p">]</span> <span class="o">=</span> <span class="n">k_v</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">),</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="NLLLoss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.NLLLoss.html#mindspore.ops.NLLLoss">[docs]</a><span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between logits and labels.</span>

<span class="sd">    The nll loss with reduction=none can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot 1</span>

<span class="sd">    where x is the input, t is the target. w is the weight. and N is the batch size. c belonging [0, C-1] is</span>
<span class="sd">    class index, where C is the number of classes.</span>

<span class="sd">    If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;; } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (string): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type only support float32 or float16.</span>
<span class="sd">        - **target** (Tensor) - Ground truth labels, with shape :math:`(N)`. Data type only support int32.</span>
<span class="sd">        - **weight** (Tensor) - The rescaling weight to each class, with shape :math:`(C)` and data type only</span>
<span class="sd">          support float32 or float16`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors composed with `loss` and `total_weight`.</span>

<span class="sd">        - **loss** (Tensor) - when `reduction` is `none` and `input` is 2D tensor, the `loss` shape is `(N,)`.</span>
<span class="sd">          Otherwise, the `loss` is a scalar. The data type is same with `input&#39;s`.</span>
<span class="sd">        - **total_weight** (Tensor) - the `total_weight` is a scalar. The data type is same with `weight&#39;s`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If x and weight data type are not float16 or float32 tensor, target data type is not int32 tensor.</span>
<span class="sd">        ValueError: If x is not a one or two dimension tensor, target and weight not a one dimension tensor.</span>
<span class="sd">                    When x is a two dimension tensor, the first dimension of x is not equal to target, and second</span>
<span class="sd">                    dimension of x is not equal to weight.</span>
<span class="sd">                    When x is a one dimension tensor, the dimensions of x, target and weight should be equal to</span>
<span class="sd">                    each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[0.5488135, 0.71518934],</span>
<span class="sd">        ...                                 [0.60276335, 0.5448832],</span>
<span class="sd">        ...                                 [0.4236548, 0.6458941]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([0, 0, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.3834415, 0.79172504]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; nll_loss = ops.NLLLoss(reduction=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; loss, weight = nll_loss(input_tensor, target, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -0.52507716</span>
<span class="sd">        &gt;&gt;&gt; print(weight)</span>
<span class="sd">        1.1503246</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NLLLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">t_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;target rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;target_shape&quot;</span><span class="p">,</span> <span class="n">t_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;weight_shape&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;weight_shape&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">t_shape</span><span class="p">,</span> <span class="p">()</span>
        <span class="k">return</span> <span class="p">(),</span> <span class="p">()</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">t_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;t_dtype&quot;</span><span class="p">,</span> <span class="n">t_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;w_dtype&quot;</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SoftmaxCrossEntropyWithLogits.html#mindspore.ops.SoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the softmax cross-entropy value between logits and labels with one-hot encoding.</span>

<span class="sd">    The updating formulas of SoftmaxCrossEntropyWithLogits algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = -\sum_j{Y_{ij} * ln(p_{ij})}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`X` represents `logits`.</span>
<span class="sd">    :math:`Y` represents `label`.</span>
<span class="sd">    :math:`loss` represents `output`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N, C)`, has the same data type with `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the `loss` shape is `(N,)`, and the `dlogits` with the same shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `logits` or `labels` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 4, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax_cross = ops.SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss, dlogits = softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [0.5899297  0.52374405]</span>
<span class="sd">        &gt;&gt;&gt; print(dlogits)</span>
<span class="sd">        [[ 0.02760027  0.20393994  0.01015357  0.20393994 -0.44563377]</span>
<span class="sd">         [ 0.08015892  0.02948882  0.08015892 -0.4077012   0.21789455]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;logits_shape&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="s2">&quot;labels_shape&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">dlogits_shape</span> <span class="o">=</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">loss_shape</span><span class="p">,</span> <span class="n">dlogits_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;logits&quot;</span><span class="p">:</span> <span class="n">logits_type</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">:</span> <span class="n">labels_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">logits_type</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseSoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SparseSoftmaxCrossEntropyWithLogits.html#mindspore.ops.SparseSoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Note:</span>
<span class="sd">        Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">        .. math::</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr -ln(1 - p_{ij}), &amp; j \neq y_i \end{cases}</span>

<span class="sd">        .. math::</span>
<span class="sd">            loss = \sum_{ij} loss_{ij}</span>

<span class="sd">    Args:</span>
<span class="sd">        is_grad (bool): If true, this operation returns the computed gradient. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if `is_grad` is False, the output tensor is the value of loss which is a scalar tensor;</span>
<span class="sd">        if `is_grad` is True, the output tensor is the gradient of input with the same shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `is_grad` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If logits_shape[0] != labels_shape[0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 3, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross = ops.SparseSoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss = sparse_softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        3.4878292</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross_grad = ops.SparseSoftmaxCrossEntropyWithLogits(is_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; loss_grad = sparse_softmax_cross_grad(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss_grad)</span>
<span class="sd">        [[-0.48415753  0.04306427  0.00582811  0.11706084  0.3182043 ]</span>
<span class="sd">         [ 0.04007946 -0.4852556   0.04007946  0.2961494   0.10894729]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_grad&#39;</span><span class="p">,</span> <span class="n">is_grad</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span> <span class="o">=</span> <span class="n">is_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;sens&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;logits_shape[0]&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;labels_shape[0]&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="n">loss_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits_type</span></div>


<div class="viewcode-block" id="ApplyMomentum"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyMomentum.html#mindspore.ops.ApplyMomentum">[docs]</a><span class="k">class</span> <span class="nc">ApplyMomentum</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Momentum algorithm.</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Refer to :class:`mindspore.nn.Momentum` for more details about the formula and usage.</span>

<span class="sd">    Inputs of `variable`, `accumulation` and `gradient` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    Data type conversion of Parameter is not supported. RuntimeError exception will be thrown.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>
<span class="sd">        use_nesterov (bool): Enable Nesterov momentum. Default: False.</span>
<span class="sd">        gradient_scale (float): The scale of the gradient. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - Weights to be updated. data type must be float.</span>
<span class="sd">        - **accumulation** (Parameter) - Accumulated gradient value by moment weight.</span>
<span class="sd">          Has the same data type with `variable`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same data type as `variable`.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum, must be a float number or</span>
<span class="sd">          a scalar tensor with float data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the use_locking or use_nesterov is not a bool or gradient_scale is not a float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage in :class:`mindspore.nn.Momentum`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_nesterov</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">use_nesterov</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">use_locking</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;gradient_scale&#39;</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">a_shape</span><span class="p">,</span> <span class="n">l_shape</span><span class="p">,</span> <span class="n">g_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">a_dtype</span><span class="p">,</span> <span class="n">l_dtype</span><span class="p">,</span> <span class="n">g_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">v_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_refkey</span> <span class="ow">and</span> <span class="n">a_dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">type_refkey</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">a_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l_dtype&quot;</span><span class="p">:</span> <span class="n">l_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;g_dtype&quot;</span><span class="p">:</span> <span class="n">g_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;m_dtype&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SmoothL1Loss.html#mindspore.ops.SmoothL1Loss">[docs]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes smooth L1 loss, a robust L1 loss.</span>

<span class="sd">    SmoothL1Loss is a Loss similar to MSELoss but less sensitive to outliers as described in the</span>
<span class="sd">    `Fast R-CNN &lt;https://arxiv.org/abs/1504.08083&gt;`_ by Ross Girshick.</span>

<span class="sd">    The updating formulas of SmoothL1Loss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{SmoothL1Loss} = \begin{cases} \frac{0.5 x^{2}}{\text{beta}}, &amp;if \left |x \right | &lt; \text{beta} \cr</span>
<span class="sd">        \left |x \right|-0.5 \text{beta}, &amp;\text{otherwise}\end{cases}</span>

<span class="sd">    where :math:`X` represents `prediction`.</span>
<span class="sd">    :math:`Y` represents `target`.</span>
<span class="sd">    :math:`loss` represents `output`.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta (float): A parameter used to control the point where the function will change from</span>
<span class="sd">            quadratic to linear. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **prediction** (Tensor) - Predict data. Data type must be float16 or float32.</span>
<span class="sd">        - **target** (Tensor) - Ground truth data, with the same type and shape as `prediction`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `prediction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `beta` is not a float.</span>
<span class="sd">        TypeError: If `prediction` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `prediction` or `target` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `beta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If shape of `prediction` is not the same as `target`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.SmoothL1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target_data = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input_data, target_data)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;prediction shape&#39;</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="s1">&#39;target shape&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prediction&quot;</span><span class="p">:</span> <span class="n">prediction</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">target</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prediction</span></div>


<div class="viewcode-block" id="L2Loss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.L2Loss.html#mindspore.ops.L2Loss">[docs]</a><span class="k">class</span> <span class="nc">L2Loss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates half of the L2 norm of a tensor without using the `sqrt`.</span>

<span class="sd">    Set `input_x` as x and output as loss.</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss = sum(x ** 2) / 2</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A input Tensor. Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `input_x`. The output tensor is the value of loss which is a scalar tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; l2_loss = ops.L2Loss()</span>
<span class="sd">        &gt;&gt;&gt; output = l2_loss(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        7.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L2Loss&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">return</span> <span class="n">loss_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x_type&#39;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="DataFormatDimMap"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.DataFormatDimMap.html#mindspore.ops.DataFormatDimMap">[docs]</a><span class="k">class</span> <span class="nc">DataFormatDimMap</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the dimension index in the destination data format given in the source data format.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_format (string): An optional value for source data format. Default: &#39;NHWC&#39;.</span>
<span class="sd">        dst_format (string): An optional value for destination data format. Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor with each element as a dimension index in source data format.</span>
<span class="sd">          The suggested values is in the range [-4, 4). It&#39;s type is int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `src_format` or `dst_format` is not a str.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor whose dtype is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([0, 1, 2, 3], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dfdm = ops.DataFormatDimMap()</span>
<span class="sd">        &gt;&gt;&gt; output = dfdm(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 3 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="n">dst_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">src_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;src_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">dst_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;dst_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="RNNTLoss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.RNNTLoss.html#mindspore.ops.RNNTLoss">[docs]</a><span class="k">class</span> <span class="nc">RNNTLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the RNNTLoss and its gradient with respect to the softmax outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank_label (int): blank label. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **acts** (Tensor) - Tensor of shape :math:`(B, T, U, V)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor[int32]) - Tensor of shape :math:`(B, U-1)`.</span>
<span class="sd">        - **input_lengths** (Tensor[int32]) - Tensor of shape :math:`(B,)`.</span>
<span class="sd">        - **label_lengths** (Tensor[int32]) - Tensor of shape :math:`(B,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **costs** (Tensor[int32]) - Tensor of shape :math:`(B,)`.</span>
<span class="sd">        - **grads** (Tensor[int32]) - Has the same shape as `acts`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `acts`, `labels`, `input_lengths` or `label_lengths` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `acts` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels`, `input_lengths` or `label_lengths` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; B, T, U, V = 1, 2, 3, 5</span>
<span class="sd">        &gt;&gt;&gt; blank = 0</span>
<span class="sd">        &gt;&gt;&gt; acts = np.random.random((B, T, U, V)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = np.array([[1, 2]]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_length = np.array([T] * B).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; label_length = np.array([len(l) for l in labels]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; rnnt_loss = ops.RNNTLoss(blank_label=0)</span>
<span class="sd">        &gt;&gt;&gt; costs, grads = rnnt_loss(Tensor(acts), Tensor(labels), Tensor(input_length), Tensor(label_length))</span>
<span class="sd">        &gt;&gt;&gt; print(costs.shape)</span>
<span class="sd">        (1,)</span>
<span class="sd">        &gt;&gt;&gt; print(grads.shape)</span>
<span class="sd">        (1, 2, 3, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank_label</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;blank_label&#39;</span><span class="p">,</span> <span class="n">blank_label</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acts&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;input_length&#39;</span><span class="p">,</span> <span class="s1">&#39;label_length&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;costs&#39;</span><span class="p">,</span> <span class="s1">&#39;grads&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">acts_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;acts_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;labels_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;input_length_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;label_length_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[0]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[1]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;acts shape[2]-1&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;input_length size&#39;</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;label_length size&#39;</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">costs_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">costs_shape</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;acts_type&quot;</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="s2">&quot;input_length&quot;</span><span class="p">,</span> <span class="s2">&quot;label_length&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">)))</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">acts_type</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SGD.html#mindspore.ops.SGD">[docs]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the stochastic gradient descent. Momentum is optional.</span>

<span class="sd">    Nesterov momentum is based on the formula from paper `On the importance of</span>
<span class="sd">    initialization and momentum in deep learning &lt;http://proceedings.mlr.press/v28/sutskever13.html&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        For details, please refer to `nn.SGD` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        dampening (float): The dampening for momentum. Default: 0.0.</span>
<span class="sd">        weight_decay (float): Weight decay (L2 penalty). Default: 0.0.</span>
<span class="sd">        nesterov (bool): Enable Nesterov momentum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **parameters** (Tensor) - Parameters to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, with float16 or float32 data type.</span>
<span class="sd">        - **learning_rate** (Tensor) - Learning rate, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32)</span>
<span class="sd">        - **accum** (Tensor) - Accum(velocity) to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **momentum** (Tensor) - Momentum, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32).</span>
<span class="sd">        - **stat** (Tensor) - States to be updated with the same shape as gradient, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dampening` or `weight_decay` is not a float.</span>
<span class="sd">        TypeError: If `nesterov` is not a bool.</span>
<span class="sd">        TypeError: If `parameters`, `gradient`, `learning_rate`, `accum`, `momentum` or `stat` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `parameters`, `gradient`, `learning_rate`, `accum`, `momentum` or `stat` is neither</span>
<span class="sd">                   float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; sgd = ops.SGD()</span>
<span class="sd">        &gt;&gt;&gt; parameters = Tensor(np.array([2, -0.5, 1.7, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([1, -1, 0.5, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.01, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; accum = Tensor(np.array([0.1, 0.3, -0.2, -0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stat = Tensor(np.array([1.5, -0.3, 0.2, -0.7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sgd(parameters, gradient, learning_rate, accum, momentum, stat)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[4], dtype=Float32,</span>
<span class="sd">         value= [ 1.98989999e+00, -4.90300000e-01,  1.69520009e+00,  3.98009992e+00]),)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nesterov&quot;</span><span class="p">,</span> <span class="n">nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nesterov</span> <span class="ow">and</span> <span class="n">dampening</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Nesterov need zero dampening!&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;stat&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span>
                    <span class="n">accum_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">parameters_shape</span><span class="p">),</span> <span class="s2">&quot;parameters rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gradient_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;gradient rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;learning rate rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">accum_shape</span><span class="p">),</span> <span class="s2">&quot;accumulation rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">momentum_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;momentum rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stat_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;stat rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="s2">&quot;stat shape&quot;</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span>
                    <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;parameters&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;accum&quot;</span><span class="p">,</span> <span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="s2">&quot;stat&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">)))</span></div>


<div class="viewcode-block" id="ApplyRMSProp"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyRMSProp.html#mindspore.ops.ApplyRMSProp">[docs]</a><span class="k">class</span> <span class="nc">ApplyRMSProp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Root Mean Square prop(RMSProp) algorithm.</span>
<span class="sd">    Please refer to the usage in source code of `nn.RMSProp`.</span>

<span class="sd">    The updating formulas of ApplyRMSProp algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            s_{t} = \rho s_{t-1} + (1 - \rho)(\nabla Q_{i}(w))^2 \\</span>
<span class="sd">            m_{t} = \beta m_{t-1} + \frac{\eta} {\sqrt{s_{t} + \epsilon}} \nabla Q_{i}(w) \\</span>
<span class="sd">            w = w - m_{t}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">    :math:`s_{t}` represents `mean_square`, :math:`s_{t-1}` is the last momentent of :math:`s_{t}`,</span>
<span class="sd">    :math:`m_{t}` represents `moment`, :math:`m_{t-1}` is the last momentent of :math:`m_{t}`.</span>
<span class="sd">    :math:`\\rho` represents `decay`. :math:`\\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">    :math:`\\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">    :math:`\\eta` represents `learning_rate`. :math:`\\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumlation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be update.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must have the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must have the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must have the same type as `var`.</span>
<span class="sd">        - **decay** (float) - Decay rate. Only constant value is allowed.</span>
<span class="sd">        - **momentum** (float) - Momentum. Only constant value is allowed.</span>
<span class="sd">        - **epsilon** (float) - Ridge term. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be update.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var, `mean_square`, `moment` or `decay` is not a Tensor.</span>
<span class="sd">        TypeError: If `learning_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `decay`, `momentum` or `epsilon` is not float.</span>
<span class="sd">        TypeError: If dtype of `learning_rate` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `decay`, `momentum` or `epsilon` is not a constant value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; apply_rms = ops.ApplyRMSProp()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(1., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(2., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(1., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(2., mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; decay = 0.0</span>
<span class="sd">        &gt;&gt;&gt; momentum = 1e-10</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 0.001</span>
<span class="sd">        &gt;&gt;&gt; output = apply_rms(input_x, mean_square, moment, learning_rate, grad, decay, momentum, epsilon)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        Tensor(shape=[], dtype=Float32, value= 0.100112)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_square&#39;</span><span class="p">,</span> <span class="s1">&#39;moment&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">decay_shape</span><span class="p">,</span>
                    <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;mean_square_shape&quot;</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;moment_shape&quot;</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">decay_dtype</span><span class="p">,</span>
                    <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;mean_square&quot;</span><span class="p">:</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="s2">&quot;moment&quot;</span><span class="p">:</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args_decay</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;decay&quot;</span><span class="p">:</span> <span class="n">decay_dtype</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_types_same_and_valid</span><span class="p">(</span><span class="n">args_decay</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_lr</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="s2">&quot;decay&quot;</span><span class="p">:</span> <span class="n">decay_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args_lr</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_mix</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">mean_square</span><span class="p">,</span> <span class="n">moment</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">decay</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">momentum</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">epsilon</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, decay, momentum, epsilon must be const.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyCenteredRMSProp"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyCenteredRMSProp.html#mindspore.ops.ApplyCenteredRMSProp">[docs]</a><span class="k">class</span> <span class="nc">ApplyCenteredRMSProp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the centered RMSProp algorithm.</span>
<span class="sd">    Please refer to the usage in source code of `nn.RMSProp`.</span>

<span class="sd">    The updating formulas of ApplyCenteredRMSProp algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            g_{t} = \rho g_{t-1} + (1 - \rho)\nabla Q_{i}(w) \\</span>
<span class="sd">            s_{t} = \rho s_{t-1} + (1 - \rho)(\nabla Q_{i}(w))^2 \\</span>
<span class="sd">            m_{t} = \beta m_{t-1} + \frac{\eta} {\sqrt{s_{t} - g_{t}^2 + \epsilon}} \nabla Q_{i}(w) \\</span>
<span class="sd">            w = w - m_{t}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">    :math:`g_{t}` represents `mean_gradient`, :math:`g_{t-1}` is the last momentent of :math:`g_{t}`.</span>
<span class="sd">    :math:`s_{t}` represents `mean_square`, :math:`s_{t-1}` is the last momentent of :math:`s_{t}`,</span>
<span class="sd">    :math:`m_{t}` represents `moment`, :math:`m_{t-1}` is the last momentent of :math:`m_{t}`.</span>
<span class="sd">    :math:`\\rho` represents `decay`. :math:`\\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">    :math:`\\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">    :math:`\\eta` represents `learning_rate`. :math:`\\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumlation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be update.</span>
<span class="sd">        - **mean_gradient** (Tensor) - Mean gradients, must have the same type as `var`.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must have the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must have the same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must have the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **decay** (float) - Decay rate.</span>
<span class="sd">        - **momentum** (float) - Momentum.</span>
<span class="sd">        - **epsilon** (float) - Ridge term.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be update.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mean_gradient`, `mean_square`, `moment` or `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `learing_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `learing_rate` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `decay`, `momentum` or `epsilon` is not float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; centered_rms_prop = ops.ApplyCenteredRMSProp()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(-2, 2).astype(np.float32).reshape(2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean_grad = Tensor(np.arange(4).astype(np.float32).reshape(2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(np.arange(-3, 1).astype(np.float32).reshape(2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(np.arange(4).astype(np.float32).reshape(2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.arange(4).astype(np.float32).reshape(2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; decay = 0.0</span>
<span class="sd">        &gt;&gt;&gt; momentum = 1e-10</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 0.05</span>
<span class="sd">        &gt;&gt;&gt; output = centered_rms_prop(input_x, mean_grad, mean_square, moment, grad,</span>
<span class="sd">        ...                            learning_rate, decay, momentum, epsilon)</span>
<span class="sd">        &gt;&gt;&gt; output</span>
<span class="sd">        Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[-2.00000000e+00, -5.02492237e+00],</span>
<span class="sd">         [-8.04984474e+00, -1.10747662e+01]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">mean_gradient_shape</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span>
                    <span class="n">learning_rate_shape</span><span class="p">,</span> <span class="n">decay_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;mean_gradient_shape&quot;</span><span class="p">,</span> <span class="n">mean_gradient_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;mean_square_shape&quot;</span><span class="p">,</span> <span class="n">mean_square_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;moment_shape&quot;</span><span class="p">,</span> <span class="n">moment_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">mean_gradient_dtype</span><span class="p">,</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span>
                    <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">rho_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;mean_gradient&quot;</span><span class="p">:</span> <span class="n">mean_gradient_dtype</span><span class="p">,</span>
                <span class="s2">&quot;mean_square&quot;</span><span class="p">:</span> <span class="n">mean_square_dtype</span><span class="p">,</span> <span class="s2">&quot;moment&quot;</span><span class="p">:</span> <span class="n">moment_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args_rho</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="n">rho_dtype</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">:</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_types_same_and_valid</span><span class="p">(</span><span class="n">args_rho</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args_lr</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="n">rho_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args_lr</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_mix</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.LayerNorm.html#mindspore.ops.LayerNorm">[docs]</a><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_norm_axis (int): The begin axis of the `input_x` to apply LayerNorm,</span>
<span class="sd">            the value must be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        begin_params_axis (int): The begin axis of the parameter input (`gamma`, `beta`) to</span>
<span class="sd">            apply LayerNorm, the value must be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        epsilon (float): A value added to the denominator for numerical stability. Default: 1e-7.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          The input of LayerNorm.</span>
<span class="sd">        - **gamma** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `gamma` as the scale on norm.</span>
<span class="sd">        - **beta** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `beta` as the scale on norm.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.</span>
<span class="sd">          The shape is :math:`(N, C)`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin_norm_axis` or `begin_params_axis` is not an int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `input_x`, `gamma` or `beta` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; layer_norm = ops.LayerNorm()</span>
<span class="sd">        &gt;&gt;&gt; output, mean, variance = layer_norm(input_x, gamma, beta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.2247448  1.         2.2247448]</span>
<span class="sd">         [-0.2247448  1.         2.2247448]]</span>
<span class="sd">        &gt;&gt;&gt; print(mean)</span>
<span class="sd">        [[2.]</span>
<span class="sd">         [2.]]</span>
<span class="sd">        &gt;&gt;&gt; print(variance)</span>
<span class="sd">        [[0.6666667]</span>
<span class="sd">         [0.6666667]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;begin_norm_axis&#39;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;begin_params_axis&#39;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Normalize"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.L2Normalize.html#mindspore.ops.L2Normalize">[docs]</a><span class="k">class</span> <span class="nc">L2Normalize</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L2 Normalization Operator.</span>

<span class="sd">    This operator will normalize the input using the given axis. The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac{x}{\sqrt{\text{max}(\text{sum} (\text{input_x}^2), \epsilon)}},</span>

<span class="sd">    where :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[list(int), tuple(int), int]): The starting axis for the input to apply the L2 Normalization.</span>
<span class="sd">                                                  Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-4.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the normalization. Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not one of the following: list, tuple or int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; l2_normalize = ops.L2Normalize()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(-256, 256, (2, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = l2_normalize(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_attrs</span><span class="p">[</span><span class="s1">&#39;axis&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The length of axis must be 1, later will support multiple axis!&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis value&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="DropoutGenMask"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.DropoutGenMask.html#mindspore.ops.DropoutGenMask">[docs]</a><span class="k">class</span> <span class="nc">DropoutGenMask</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates the mask value for the input shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        Seed0 (int): Seed0 value for random generating. Default: 0.</span>
<span class="sd">        Seed1 (int): Seed1 value for random generating. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (tuple[int]) - The shape of target mask.</span>
<span class="sd">        - **keep_prob** (Tensor) - The keep rate, greater than 0 and less equal than 1, e.g. keep_prob = 0.9,</span>
<span class="sd">          means dropping out 10% of input units.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value of generated mask for input shape.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `seed0` nor `seed1` is an int.</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        TypeError: If `keep_prob` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout_gen_mask = ops.DropoutGenMask()</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 4, 5)</span>
<span class="sd">        &gt;&gt;&gt; keep_prob = Tensor(0.5, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = dropout_gen_mask(shape, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16,)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;keep_prob&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;_random_effect&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="DropoutDoMask"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.DropoutDoMask.html#mindspore.ops.DropoutDoMask">[docs]</a><span class="k">class</span> <span class="nc">DropoutDoMask</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies dropout mask on the input tensor.</span>

<span class="sd">    Take the mask output of DropoutGenMask as input, and apply dropout on the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">        - **mask** (Tensor) - The mask to be applied on `input_x`, which is the output of `DropoutGenMask`. And the</span>
<span class="sd">          shape of `input_x` must be the same as the value of `DropoutGenMask`&#39;s input `shape`. If input wrong `mask`,</span>
<span class="sd">          the output of `DropoutDoMask` are unpredictable.</span>
<span class="sd">        - **keep_prob** (Union[Tensor, float]) - The keep rate, greater than 0 and less equal than 1, e.g. keep_prob =</span>
<span class="sd">          0.9, means dropping out 10% of input units. The value of `keep_prob` is the same as the input `keep_prob` of</span>
<span class="sd">          `DropoutGenMask`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied dropout on.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x`, `mask` or `keep_prob` is not a Tensor.</span>
<span class="sd">        TypeError: If `keep_prob` is not a float.</span>
<span class="sd">        ValueError: If value of `keep_prob` is not same as `DropoutGenMaks`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; keep_prob = Tensor(0.5, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; dropout_gen_mask = ops.DropoutGenMask()</span>
<span class="sd">        &gt;&gt;&gt; dropout_do_mask = ops.DropoutDoMask()</span>
<span class="sd">        &gt;&gt;&gt; mask = dropout_gen_mask(shape, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; output = dropout_do_mask(x, mask, keep_prob)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
        <span class="n">input_x_shape</span> <span class="o">=</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">mask_shape</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">keep_prob_shape</span> <span class="o">=</span> <span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;keep_prob&#39;s dim&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_prob_shape</span><span class="p">),</span> <span class="s1">&#39;0(scalar)&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">size_x</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;DropoutDoMask mask shape should be 1-dimension.&quot;</span><span class="p">)</span>
        <span class="n">size_y</span> <span class="o">=</span> <span class="n">mask_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span>
        <span class="k">if</span> <span class="n">size_x</span> <span class="o">&gt;</span> <span class="n">size_y</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;DropoutDoMask y mask do not math input input_x shape:&quot;</span>
                             <span class="s2">&quot;</span><span class="si">{input_x_shape}</span><span class="s2">, mask shape: </span><span class="si">{mask_shape}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="n">mask</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">keep_prob_v</span> <span class="o">=</span> <span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">keep_prob_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">)):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob_v</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob_v</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">input_x_shape</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="ResizeBilinear"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ResizeBilinear.html#mindspore.ops.ResizeBilinear">[docs]</a><span class="k">class</span> <span class="nc">ResizeBilinear</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes an image to a certain size using the bilinear interpolation.</span>

<span class="sd">    The resizing only affects the lower two dimensions which represent the height and width. The input images</span>
<span class="sd">    can be represented by different data types, but the data types of output images are always float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union[tuple[int], list[int]]): A tuple or list of 2 int elements `(new_height, new_width)`, the new size</span>
<span class="sd">            of the images.</span>
<span class="sd">        align_corners (bool): If true, rescale input by `(new_height - 1) / (height - 1)`,</span>
<span class="sd">                       which exactly aligns the 4 corners of images and resized images. If false,</span>
<span class="sd">                       rescale by `new_height / height`. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Image to be resized. Input images must be a 4-D tensor with shape</span>
<span class="sd">          :math:`(batch, channels, height, width)`, with data type of float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, resized image. 4-D with shape [batch, channels, new_height, new_width] in `float32`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is neither a tuple nor list.</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; resize_bilinear = ops.ResizeBilinear((5, 5))</span>
<span class="sd">        &gt;&gt;&gt; output = resize_bilinear(tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;input shape rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_dtype&#39;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span></div>


<div class="viewcode-block" id="OneHot"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.OneHot.html#mindspore.ops.OneHot">[docs]</a><span class="k">class</span> <span class="nc">OneHot</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    Makes a new tensor, whose locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Position to insert the value. e.g. If `indices` shape is [n, c], and `axis` is `-1` the output</span>
<span class="sd">            shape will be [n, c, depth], If `axis` is `0` the output shape will be [depth, n, c]. Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>
<span class="sd">        - **depth** (int) - A scalar defining the depth of the one hot dimension.</span>
<span class="sd">        - **on_value** (Tensor) - A value to fill in output when `indices[j] = i`.</span>
<span class="sd">          With data type of float16 or float32.</span>
<span class="sd">        - **off_value** (Tensor) - A value to fill in output when `indices[j] != i`.</span>
<span class="sd">          Has the same data type with as `on_value`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `depth` is not an int.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If `indices`, `on_value` or `off_value` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not in range [-1, len(indices_shape)].</span>
<span class="sd">        ValueError: If `depth` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; onehot = ops.OneHot()</span>
<span class="sd">        &gt;&gt;&gt; output = onehot(indices, depth, on_value, off_value)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 0.]</span>
<span class="sd">         [0. 1. 0.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;depth&#39;</span><span class="p">,</span> <span class="s1">&#39;on_value&#39;</span><span class="p">,</span> <span class="s1">&#39;off_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">):</span>
        <span class="c1"># check type</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;depth&quot;</span><span class="p">,</span> <span class="n">depth</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;on_value&quot;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s2">&quot;off_value&quot;</span><span class="p">:</span> <span class="n">off_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># check shape</span>
        <span class="n">indices_shp</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shp</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">depth_val</span> <span class="o">=</span> <span class="n">depth</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">depth_val</span><span class="p">,</span> <span class="s2">&quot;depth&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="c1"># create new dimension at end if self.axis is -1</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">indices_shp</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="n">depth_val</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">indices_shp</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">depth_val</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">indices_shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">on_value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<span class="k">class</span> <span class="nc">Gelu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator GeLU. Gelu will be deprecated in the future.</span>
<span class="sd">    Please use GeLU instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;GeLU&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Gelu&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>


<div class="viewcode-block" id="GeLU"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.GeLU.html#mindspore.ops.GeLU">[docs]</a><span class="k">class</span> <span class="nc">GeLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    GeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = 0.5 * x * (1 + erf(x / \sqrt{2})),</span>

<span class="sd">    where :math:`erf` is the &quot;Gauss error function&quot; .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the GeLU with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gelu = ops.GeLU()</span>
<span class="sd">        &gt;&gt;&gt; result = gelu(tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.841192  1.9545976  2.9963627]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<span class="k">class</span> <span class="nc">FastGelu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator FastGeLU. FastGelu will be deprecated in the future.</span>
<span class="sd">    Please use FastGeLU instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;FastGeLU&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init FastGelu&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>


<div class="viewcode-block" id="FastGeLU"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.FastGeLU.html#mindspore.ops.FastGeLU">[docs]</a><span class="k">class</span> <span class="nc">FastGeLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast Gaussian Error Linear Units activation function.</span>

<span class="sd">    FastGeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac {x} {1 + \exp(-1.702 * \left| x \right|)} * \exp(0.851 * (x - \left| x \right|)),</span>

<span class="sd">    where :math:`x` is the element of the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to compute the FastGeLU with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; fast_gelu = P.FastGeLU()</span>
<span class="sd">        &gt;&gt;&gt; output = fast_gelu(tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.5420423e-01  3.9955849e+00 -9.7664278e-06]</span>
<span class="sd">         [ 1.9356585e+00 -1.0070159e-03  8.9999981e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init FastGeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span></div>


<div class="viewcode-block" id="GetNext"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.GetNext.html#mindspore.ops.GetNext">[docs]</a><span class="k">class</span> <span class="nc">GetNext</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next element in the dataset queue.</span>

<span class="sd">    Note:</span>
<span class="sd">        The GetNext operation needs to be associated with network and it also depends on the init_dataset interface,</span>
<span class="sd">        it can&#39;t be used directly as a single operation.</span>
<span class="sd">        For details, please refer to `connect_network_with_dataset` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        types (list[:class:`mindspore.dtype`]): The type of the outputs.</span>
<span class="sd">        shapes (list[tuple[int]]): The dimensionality of the outputs.</span>
<span class="sd">        output_num (int): The output number, length of `types` and `shapes`.</span>
<span class="sd">        shared_name (str): The queue name of `init_dataset` interface.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        No inputs.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the output of Dataset. The shape is described in `shapes`</span>
<span class="sd">        and the type is described in `types`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; train_dataset = create_custom_dataset()</span>
<span class="sd">        &gt;&gt;&gt; dataset_helper = mindspore.DatasetHelper(train_dataset, dataset_sink_mode=True)</span>
<span class="sd">        &gt;&gt;&gt; dataset = dataset_helper.iter.dataset</span>
<span class="sd">        &gt;&gt;&gt; dataset_types, dataset_shapes = dataset_helper.types_shapes()</span>
<span class="sd">        &gt;&gt;&gt; queue_name = dataset.__transfer_dataset__.queue_name</span>
<span class="sd">        &gt;&gt;&gt; get_next = P.GetNext(dataset_types, dataset_shapes, len(dataset_types), queue_name)</span>
<span class="sd">        &gt;&gt;&gt; data, label = get_next()</span>
<span class="sd">        &gt;&gt;&gt; relu = P.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; result = relu(data).asnumpy()</span>
<span class="sd">        &gt;&gt;&gt; print(result.shape)</span>
<span class="sd">        (32, 1, 32, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;types&quot;</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shapes&quot;</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;types length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">),</span> <span class="s2">&quot;shapes length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shapes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">types</span><span class="p">)</span></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.PReLU.html#mindspore.ops.PReLU">[docs]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of an channel of the input.</span>

<span class="sd">    Note:</span>
<span class="sd">        1-dimensional input_x is not supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Float tensor, representing the output of the preview layer.</span>
<span class="sd">          With data type of float16 or float32.</span>
<span class="sd">        - **weight** (Tensor) -  Float Tensor, w &gt; 0, there are only two shapes are legitimate,</span>
<span class="sd">          1 or the number of channels of the input. With data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as `input_x`.</span>

<span class="sd">    For detailed information, please refer to `nn.PReLU`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input_x` or `weight` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is equal to 1.</span>
<span class="sd">        ValueError: If length of shape of `weight` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.prelu = ops.PReLU()</span>
<span class="sd">        ...     def construct(self, input_x, weight):</span>
<span class="sd">        ...         result = self.prelu(input_x, weight)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.random.randint(-3, 3, (2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">):</span>
        <span class="n">input_x_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x_shape</span><span class="p">)</span>
        <span class="n">weight_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_x_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> input_x rank 1 is not supported.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_dim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> weight_dim must be 1, while weight_dim is </span><span class="si">{</span><span class="n">weight_dim</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">input_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1"> channel of input_x and weight must be matched,&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39; while channel of input_x is </span><span class="si">{</span><span class="n">input_x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">,&#39;</span>
                             <span class="sa">f</span><span class="s1">&#39; weight_shape[0] is </span><span class="si">{</span><span class="n">weight_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x_dtype</span></div>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.LSTM.html#mindspore.ops.LSTM">[docs]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the Long Short-Term Memory (LSTM) on the input.</span>

<span class="sd">    For detailed information, please refer to `nn.LSTM</span>
<span class="sd">    &lt;https://www.mindspore.cn/doc/api_python/zh-CN/master/mindspore/nn/mindspore.nn.LSTM.html&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Number of features of input.</span>
<span class="sd">        hidden_size (int):  Number of features of hidden layer.</span>
<span class="sd">        num_layers (int): Number of layers of stacked LSTM . Default: 1.</span>
<span class="sd">        has_bias (bool): Whether the cell has bias `b_ih` and `b_hh`. Default: True.</span>
<span class="sd">        bidirectional (bool): Specifies whether it is a bidirectional LSTM. Default: False.</span>
<span class="sd">        dropout (float): If not 0, append `Dropout` layer on the outputs of each</span>
<span class="sd">            LSTM layer except the last layer. Default 0. The range of dropout is [0.0, 1.0].</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape (seq_len, batch_size, `input_size`) or</span>
<span class="sd">          (batch_size, seq_len, `input_size`).</span>
<span class="sd">        - **h** (tuple) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **c** (tuple) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains (`output`, `h_n`, `c_n`, `reserve`, `state`).</span>

<span class="sd">        - **output** (Tensor) - Tensor of shape (seq_len, batch_size, num_directions * `hidden_size`).</span>
<span class="sd">        - **h_n** (Tensor) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **c_n** (Tensor) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **reserve** (Tensor) - Tensor of shape (r, 1).</span>
<span class="sd">        - **state** (Tensor) - Random number generator state and its shape is (s, 1).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_size`, `hidden_size` or `num_layers` is not an int.</span>
<span class="sd">        TypeError: If `has_bias` or `bidirectional` is not a bool.</span>
<span class="sd">        TypeError: If `dropout` is not a float.</span>
<span class="sd">        ValueError: If `dropout` is not in range [0.0, 1.0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_size = 10</span>
<span class="sd">        &gt;&gt;&gt; hidden_size = 2</span>
<span class="sd">        &gt;&gt;&gt; num_layers = 1</span>
<span class="sd">        &gt;&gt;&gt; seq_len = 5</span>
<span class="sd">        &gt;&gt;&gt; batch_size = 2</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = P.LSTM(input_size, hidden_size, num_layers, True, False, 0.0)</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones([seq_len, batch_size, input_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; h0 = Tensor(np.ones([num_layers, batch_size, hidden_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; c0 = Tensor(np.ones([num_layers, batch_size, hidden_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.ones([112, 1, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output, hn, cn, _, _ = net(input_tensor, h0, c0, w)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[0.9640267  0.9640267 ]</span>
<span class="sd">          [0.9640267  0.9640267 ]]</span>
<span class="sd">         [[0.9950539  0.9950539 ]</span>
<span class="sd">          [0.9950539  0.9950539 ]]</span>
<span class="sd">         [[0.99932843 0.99932843]</span>
<span class="sd">          [0.99932843 0.99932843]]</span>
<span class="sd">         [[0.9999084  0.9999084 ]</span>
<span class="sd">          [0.9999084  0.9999084 ]]</span>
<span class="sd">         [[0.9999869  0.9999869 ]</span>
<span class="sd">          [0.9999869  0.9999869 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="s2">&quot;num_layers&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="s2">&quot;x[2]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># h and c should be same shape</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;h[1]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h[2]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">)</span>

        <span class="c1"># set arbitrary shape for reserved space</span>
        <span class="n">reserved_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">y_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">reserved_shape</span><span class="p">,</span> <span class="n">state_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SigmoidCrossEntropyWithLogits.html#mindspore.ops.SigmoidCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SigmoidCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the given logits to compute sigmoid cross entropy between the target and the output.</span>

<span class="sd">    Measures the distribution error in discrete classification tasks where each class is independent</span>
<span class="sd">    and not mutually exclusive using cross entropy loss.</span>

<span class="sd">    Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}}</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss_{ij} = -[Y_{ij} * ln(p_{ij}) + (1 - Y_{ij})ln(1 - p_{ij})]</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label. With the same shape and type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as input `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` or `label` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = ops.SigmoidCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; output = sigmoid(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.6111007   0.5032824   0.26318604]</span>
<span class="sd">         [ 0.58439666  0.5530153  -0.4368139 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SigmoidCrossEntropyWithLogits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s2">&quot;y_shape&quot;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">y_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;y_dtype&quot;</span><span class="p">:</span> <span class="n">y_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="BCEWithLogitsLoss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BCEWithLogitsLoss.html#mindspore.ops.BCEWithLogitsLoss">[docs]</a><span class="k">class</span> <span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input `predict`, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the target and the output.</span>

<span class="sd">    Sets input predict as `X`, input target as `Y`, output as `L`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}}</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{ij} = -[Y_{ij} * log(p_{ij}) + (1 - Y_{ij})log(1 - p_{ij})]</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &#39;mean&#39;, &#39;sum&#39;, and &#39;none&#39;.</span>
<span class="sd">            If &#39;none&#39;, do not perform reduction. Default:&#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **predict** (Tensor) - Input logits. Data type must be float16 or float32.</span>
<span class="sd">        - **target** (Tensor) - Ground truth label. Has the same shape with `predict`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        - **weight** (Tensor) - A rescaling weight applied to the loss of each batch element. It must can be</span>
<span class="sd">          broadcast to a tensor with shape of `predict`. Data type must be float16 or float32.</span>
<span class="sd">        - **pos_weight** (Tensor) - A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">          number of classes. It must can be broadcast to a tensor with shape of `predict`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Scalar. If reduction is &#39;none&#39;, it&#39;s a tensor with the same shape and type as input `predict`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of any input is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `predict`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; predict = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1.0, 1.0, 1.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = Tensor(np.array([1.0, 1.0, 1.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(predict, target, weight, pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BCEWithLogitsLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;predict_shape&#39;</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="s1">&#39;target_shape&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">reversed_weight_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">weight</span><span class="p">))</span>
        <span class="n">reversed_target</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">predict</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reversed_weight_shape</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">reversed_target</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, shapes can not broadcast. &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;predict: </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span><span class="si">}</span><span class="s2">, weight shape </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">reversed_pos_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">pos_weight</span><span class="p">))</span>
        <span class="n">reversed_target</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">predict</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">reversed_pos_shape</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">v</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">reversed_target</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, shapes can not broadcast. &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;predict: </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span><span class="si">}</span><span class="s2">, pos_weight shape </span><span class="si">{</span><span class="nb">tuple</span><span class="p">(</span><span class="n">pos_weight</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">predict</span>
        <span class="k">return</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;predict dtype&#39;</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;target dtype&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;weight dtype&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;pos_weight dtype&#39;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">predict</span></div>


<div class="viewcode-block" id="Pad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Pad.html#mindspore.ops.Pad">[docs]</a><span class="k">class</span> <span class="nc">Pad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        paddings (tuple): The shape of parameter `paddings` is (N, 2). N is the rank of input data. All elements of</span>
<span class="sd">            paddings are int type. For the input in `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">            extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">            be extended behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `paddings` is not a tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `paddings` is not (n, 2).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_op = ops.Pad(((1, 2), (2, 1)))</span>
<span class="sd">        &gt;&gt;&gt; output = pad_op(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.  -0.1  0.3  3.6  0. ]</span>
<span class="sd">         [ 0.   0.   0.4  0.5 -3.2  0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">paddings</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Paddings must be tuple type.&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">paddings</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The shape of paddings must be (n, 2).&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">paddings</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">paddings</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;paddings.shape&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">paddings</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All elements of paddings must be &gt;= 0.&#39;</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">paddings</span><span class="o">.</span><span class="n">size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="n">y_shape</span> <span class="o">+=</span> <span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),)</span>
        <span class="k">return</span> <span class="n">y_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="MirrorPad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.MirrorPad.html#mindspore.ops.MirrorPad">[docs]</a><span class="k">class</span> <span class="nc">MirrorPad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (str): Specifies the padding mode. The optional values are &quot;REFLECT&quot; and &quot;SYMMETRIC&quot;.</span>
<span class="sd">            Default: &quot;REFLECT&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">        - **paddings** (Tensor) - The paddings tensor. The value of `paddings` is a matrix(list),</span>
<span class="sd">          and its shape is (N, 2). N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">          extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">          be extended behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is &quot;REFLECT&quot;, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the</span>
<span class="sd">          Outputs is [[6,5,4,5,6,5,4], [3,2,1,2,3,2,1], [6,5,4,5,6,5,4], [9,8,7,8,9,8,7], [6,5,4,5,6,5,4]].</span>
<span class="sd">        - If `mode` is &quot;SYMMETRIC&quot;, the filling method is similar to the &quot;REFLECT&quot;. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the Outputs is</span>
<span class="sd">          [[2,1,1,2,3,3,2], [2,1,1,2,3,3,2], [5,4,4,5,6,6,5], [8,7,7,8,9,9,8], [8,7,7,8,9,9,8]].</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is not a str.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.pad = ops.MirrorPad(mode=&quot;REFLECT&quot;)</span>
<span class="sd">        ...     def construct(self, x, paddings):</span>
<span class="sd">        ...         return self.pad(x, paddings)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = np.random.random(size=(2, 3)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; paddings = Tensor([[1, 1], [2, 2]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = pad(Tensor(x), paddings)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 7)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">,</span> <span class="s1">&#39;SYMMETRIC&#39;</span><span class="p">],</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_const_input_indexes</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;paddings&quot;</span><span class="p">,</span> <span class="n">paddings</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">paddings_value</span> <span class="o">=</span> <span class="n">paddings</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="n">paddings_size</span> <span class="o">=</span> <span class="n">paddings_value</span><span class="o">.</span><span class="n">size</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">paddings_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;paddings.shape&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">paddings_value</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;All elements of paddings must be &gt;= 0.&#39;</span><span class="p">)</span>
        <span class="n">adjust</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;SYMMETRIC&#39;</span><span class="p">:</span>
            <span class="n">adjust</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">paddings_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">adjust</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">adjust</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;At least one dim has too high a padding value for this input and mode&#39;</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">paddings_size</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)):</span>
            <span class="n">y_shape</span> <span class="o">+=</span> <span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">paddings_value</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">y_shape</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">input_x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span></div>


<div class="viewcode-block" id="ComputeAccidentalHits"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ComputeAccidentalHits.html#mindspore.ops.ComputeAccidentalHits">[docs]</a><span class="k">class</span> <span class="nc">ComputeAccidentalHits</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute accidental hits of sampled classes which happen to match target classes.</span>

<span class="sd">    When a target class matches the sample class, we call it &quot;accidental hit&quot;.</span>
<span class="sd">    The result of calculating accidental hit contains three parts (index, id, weight),</span>
<span class="sd">    where index represents the row number in true_classes, and id represents the position in sampled_candidates,</span>
<span class="sd">    the weight is -FLOAT_MAX.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_true (int): The number of target classes per training example.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **true_classes** (Tensor) - The target classes. With data type of int32 or int64</span>
<span class="sd">          and shape [batch_size, num_true].</span>
<span class="sd">        - **sampled_candidates** (Tensor) - The sampled_candidates output of CandidateSampler,</span>
<span class="sd">          with data type of int32 or int64 and shape [num_sampled].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors.</span>

<span class="sd">        - **indices** (Tensor) - A Tensor with shape (num_accidental_hits,), with the same type as `true_classes`.</span>
<span class="sd">        - **ids** (Tensor) - A Tensor with shape (num_accidental_hits,), with the same type as `true_classes`.</span>
<span class="sd">        - **weights** (Tensor) - A Tensor with shape (num_accidental_hits,), with the type float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `num_true` is not int.</span>
<span class="sd">        TypeError: If `true_classes` or `sampled_candidates` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `true_classes` or `sampled_candidates` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[1, 2], [0, 4], [3, 3]])</span>
<span class="sd">        &gt;&gt;&gt; y = np.array([0, 1, 2, 3, 4])</span>
<span class="sd">        &gt;&gt;&gt; sampler = ops.ComputeAccidentalHits(2)</span>
<span class="sd">        &gt;&gt;&gt; output1, output2, output3 = sampler(Tensor(x), Tensor(y))</span>
<span class="sd">        &gt;&gt;&gt; print(output1, output2, output3)</span>
<span class="sd">        [0 0 1 1 2 2]</span>
<span class="sd">        [1 2 0 4 3 3]</span>
<span class="sd">        [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ComputeAccidentalHits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;true_classes&#39;</span><span class="p">,</span> <span class="s1">&#39;sampled_candidates&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;ids&#39;</span><span class="p">,</span> <span class="s1">&#39;weights&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span> <span class="o">=</span> <span class="n">num_true</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">true_classes_shape</span><span class="p">,</span> <span class="n">sampled_candidates_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_classes_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;dim of true_classes&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sampled_candidates_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;dim of sampled_candidates&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;true_classes shape[1]&quot;</span><span class="p">,</span> <span class="n">true_classes_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">indices_len</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">indices_len</span><span class="p">,),</span> <span class="p">(</span><span class="n">indices_len</span><span class="p">,),</span> <span class="p">(</span><span class="n">indices_len</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">sampled_candidates_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;true_classes_type&quot;</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;sampled_candidates_type&quot;</span><span class="p">,</span> <span class="n">sampled_candidates_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;true_classes_type&quot;</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;sampled_candidates_type&quot;</span><span class="p">,</span> <span class="n">sampled_candidates_type</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">weights_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
        <span class="k">return</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">weights_type</span></div>


<div class="viewcode-block" id="ROIAlign"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ROIAlign.html#mindspore.ops.ROIAlign">[docs]</a><span class="k">class</span> <span class="nc">ROIAlign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Region of Interest (RoI) Align operator.</span>

<span class="sd">    The operator computes the value of each sampling point by bilinear interpolation from the nearby grid points on the</span>
<span class="sd">    feature map. No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling</span>
<span class="sd">    points. The details of (RoI) Align operator are described in `Mask R-CNN &lt;https://arxiv.org/abs/1703.06870&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooled_height (int): The output features height.</span>
<span class="sd">        pooled_width (int): The output features width.</span>
<span class="sd">        spatial_scale (float): A scaling factor that maps the raw image coordinates to the input</span>
<span class="sd">            feature map coordinates. Suppose the height of a RoI is `ori_h` in the raw image and `fea_h` in the</span>
<span class="sd">            input feature map, the `spatial_scale` must be `fea_h / ori_h`.</span>
<span class="sd">        sample_num (int): Number of sampling points. Default: 2.</span>
<span class="sd">        roi_end_mode (int): Number must be 0 or 1. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape must be `(N, C, H, W)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is `(rois_n, 5)`. With data type of float16 or float32.</span>
<span class="sd">          `rois_n` represents the number of RoI. The size of the second dimension must be `5` and the `5` colunms</span>
<span class="sd">          are `(image_index, top_left_x, top_left_y, bottom_right_x, bottom_right_y)`. `image_index` represents the</span>
<span class="sd">          index of image. `top_left_x` and `top_left_y` represent the `x, y` coordinates of the top left corner</span>
<span class="sd">          of corresponding RoI, respectively. `bottom_right_x` and `bottom_right_y` represent the `x, y`</span>
<span class="sd">          coordinates of the bottom right corner of corresponding RoI, respectively.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is `(rois_n, C, pooled_height, pooled_width)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `pooled_height`, `pooled_width`, `sample_num` or `roi_end_mode` is not an int.</span>
<span class="sd">        TypeError: If `spatial_scale` is not a float.</span>
<span class="sd">        TypeError: If `features` or `rois` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[[[1., 2.], [3., 4.]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor(np.array([[0, 0.2, 0.3, 0.2, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; roi_align = ops.ROIAlign(2, 2, 0.5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = roi_align(input_tensor, rois)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.775 2.025]</span>
<span class="sd">           [2.275 2.525]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">sample_num</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ROIAlign&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_height&quot;</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_width&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sample_num&quot;</span><span class="p">,</span> <span class="n">sample_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">roi_end_mode</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span> <span class="o">=</span> <span class="n">pooled_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span> <span class="o">=</span> <span class="n">pooled_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_num</span> <span class="o">=</span> <span class="n">sample_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roi_end_mode</span> <span class="o">=</span> <span class="n">roi_end_mode</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">,</span> <span class="n">rois_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;input shape rank&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">rois_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_type</span><span class="p">,</span> <span class="n">rois_type</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;inputs_type&quot;</span><span class="p">,</span> <span class="n">inputs_type</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;rois_type&quot;</span><span class="p">,</span> <span class="n">rois_type</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs_type</span></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Adam.html#mindspore.ops.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation (Adam) algorithm.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated.</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula, has the same type as `var`.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula.</span>
<span class="sd">          Mean square gradients with the same type as `var`.</span>
<span class="sd">        - **beta1_power** (float) - :math:`beta_1^t` in the updating formula.</span>
<span class="sd">        - **beta2_power** (float) - :math:`beta_2^t` in the updating formula.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations.</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations.</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nesterov` is a bool.</span>
<span class="sd">        TypeError: If `var`, `m` or `v` is not a Tensor.</span>
<span class="sd">        TypeError: If `beta1_power`, `beta2_power1`, `lr`, `beta1`, `beta2`, `epsilon` or `gradient` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adam = ops.Adam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        ...                               epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(0.9, 0.999, 0.001, 0.9, 0.999, 1e-8, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.9996838 0.9996838]</span>
<span class="sd">         [0.9996838 0.9996838]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="AdamNoUpdateParam"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.AdamNoUpdateParam.html#mindspore.ops.AdamNoUpdateParam">[docs]</a><span class="k">class</span> <span class="nc">AdamNoUpdateParam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by Adaptive Moment Estimation (Adam) algorithm. This operator do not update the parameter, but</span>
<span class="sd">    calculate the value that should be added to the parameter instead.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            \Delta{w} = - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents the parameter to be updated,</span>
<span class="sd">    :math:`\epsilon`represents `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula. The data type must be float32.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula. The shape must be the same as `m`.</span>
<span class="sd">          The data type must be float32.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula. The data type must be float32.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula. The data type must be float32.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula. The data type must be float32.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations.</span>
<span class="sd">          The data type must be float32.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations.</span>
<span class="sd">          The data type must be float32.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability. The data type must be</span>
<span class="sd">          float32.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, the shape must be the same as `m`, the data type must be float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose shape and data type are the same with `gradient`, is a value that should be added to the</span>
<span class="sd">        parameter to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nesterov` is a bool.</span>
<span class="sd">        TypeError: If `m`,  `v`, `beta1_power`, `beta2_power1`, `lr`, `beta1`, `beta2`, `epsilon` or `gradient`</span>
<span class="sd">                   is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.adam = ops.AdamNoUpdateParam()</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                            name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                            name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.adam(self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[-0.00010004 -0.00010004 -0.00010004]</span>
<span class="sd">        [-0.00013441 -0.00013441 -0.00013441]]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_dtype</span></div>


<div class="viewcode-block" id="FusedSparseAdam"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.FusedSparseAdam.html#mindspore.ops.FusedSparseAdam">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseAdam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same type as `var` with</span>
<span class="sd">          float32 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula.</span>
<span class="sd">          Mean square gradients, has the same type as `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula. With float32 data type.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient value with float32 data type.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_neserov` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta1_power`, `beta2_power`, `lr`, `beta1`, `beta2`, `epsilon`,</span>
<span class="sd">                   `gradient` or `indices` is not float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adam = ops.FusedSparseAdam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        ...                                      epsilon, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.random.rand(2, 1, 2), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.9996963  0.9996977 ]]</span>
<span class="sd">         [[0.99970144 0.9996992 ]]</span>
<span class="sd">         [[0.99971527 0.99971527]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">grad_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span> <span class="o">+</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of updates should be [] or &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;grad_shape = indices_shape + var_shape[1:], but got var_shape: </span><span class="si">{</span><span class="n">var_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, grad_shape: </span><span class="si">{</span><span class="n">grad_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="FusedSparseLazyAdam"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.FusedSparseLazyAdam.html#mindspore.ops.FusedSparseLazyAdam">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseLazyAdam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (LazyAdam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse. The behavior is not equivalent to the</span>
<span class="sd">    original Adam algorithm, as only the current indices parameters will be updated.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t` and :math:`beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same type as `var` with</span>
<span class="sd">          float32 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula.</span>
<span class="sd">          Mean square gradients, has the same type as `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula with float32 data type.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient value with float32 data type.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nestrov` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta1_power`, `beta2_power`, `lr`, `beta1`, `beta2`, `epsilon` or</span>
<span class="sd">                   gradient is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_lazyadam = ops.FusedSparseLazyAdam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_lazyadam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1,</span>
<span class="sd">        ...                                          beta2, epsilon, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.random.rand(2, 1, 2), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.9996866 0.9997078]]</span>
<span class="sd">         [[0.9997037 0.9996869]]</span>
<span class="sd">         [[1.        1.       ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">grad_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span> <span class="o">+</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of updates should be [] or &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;grad_shape = indices_shape + var_shape[1:], but got var_shape: </span><span class="si">{</span><span class="n">var_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, grad_shape: </span><span class="si">{</span><span class="n">grad_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="FusedSparseFtrl"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.FusedSparseFtrl.html#mindspore.ops.FusedSparseFtrl">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseFtrl</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`. The shape</span>
<span class="sd">          of `indices` must be the same as `grad` in first dimension. The type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **linear** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is not a float.</span>
<span class="sd">        ValueError: If shape of `lr_power` less than or equal to zero.</span>
<span class="sd">        TypeError: If dtype of `var` is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        TypeError: If shape of `accum`, `linear` or `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl = ops.FusedSparseFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(2, 1, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1], dtype=Float32, value= [0.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[1], dtype=Float32, value= [0.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[1], dtype=Float32, value= [0.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span></div>


<div class="viewcode-block" id="FusedSparseProximalAdagrad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.FusedSparseProximalAdagrad.html#mindspore.ops.FusedSparseProximalAdagrad">[docs]</a><span class="k">class</span> <span class="nc">FusedSparseProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the proximal adagrad</span>
<span class="sd">    algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the variable and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float32.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Tensor) - The learning rate value. The data type must be float32.</span>
<span class="sd">        - **l1** (Tensor) - l1 regularization strength. The data type must be float32.</span>
<span class="sd">        - **l2** (Tensor) - l2 regularization strength. The data type must be float32.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient. The data type must be float32.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`. The data type</span>
<span class="sd">          must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape (1,).</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape (1,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `l1`, `l2` or `grad` is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_proximal_adagrad = ops.FusedSparseProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.random.rand(3, 1, 2).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = Tensor(0.01, mstype.float32)</span>
<span class="sd">        ...         self.l1 = Tensor(0.0, mstype.float32)</span>
<span class="sd">        ...         self.l2 = Tensor(0.0, mstype.float32)</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        ...                                                  self.l2, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.random.rand(2, 1, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1], dtype=Float32, value= [0.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[1], dtype=Float32, value= [0.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span>
                    <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span>
                    <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                        <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="KLDivLoss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.KLDivLoss.html#mindspore.ops.KLDivLoss">[docs]</a><span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the target and the output.</span>

<span class="sd">    The updating formulas of KLDivLoss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = y_n \cdot (\log y_n - x_n)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `input`.</span>
<span class="sd">    :math:`y` represents `label`.</span>
<span class="sd">    :math:`\ell(x, y)` represents `output`.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor. The data type must be float32.</span>
<span class="sd">        - **input_y** (Tensor) - The label Tensor which has the same shape as `input_x`. The data type must be float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `input_x`.</span>
<span class="sd">        Otherwise it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `input_x` nor `input_y` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `input_y` is not float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.kldiv_loss = ops.KLDivLoss()</span>
<span class="sd">        ...     def construct(self, x, y):</span>
<span class="sd">        ...         result = self.kldiv_loss(x, y)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.23333333</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x_shape&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_type</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="BinaryCrossEntropy"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BinaryCrossEntropy.html#mindspore.ops.BinaryCrossEntropy">[docs]</a><span class="k">class</span> <span class="nc">BinaryCrossEntropy</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the binary cross entropy between the target and the output.</span>

<span class="sd">    Sets input as :math:`x`, input label as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor. The data type must be float16 or float32.</span>
<span class="sd">        - **input_y** (Tensor) - The label Tensor which has same shape and data type as `input_x`.</span>
<span class="sd">        - **weight** (Tensor, optional) - A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">          And it must have same shape and data type as `input_x`. Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `input_x`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x`, `input_y` or `weight` (if given) is neither float16 not float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        ValueError: If shape of `input_y` is not the same as `input_x` or `weight` (if given).</span>
<span class="sd">        TypeError: If `input_x`, `input_y` or `weight` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.binary_cross_entropy = ops.BinaryCrossEntropy()</span>
<span class="sd">        ...     def construct(self, x, y, weight):</span>
<span class="sd">        ...         result = self.binary_cross_entropy(x, y, weight)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x, input_y, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x_shape&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;y_shape&#39;</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="s1">&#39;weight_shape&#39;</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">):</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="k">return</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">y_type</span><span class="p">,</span> <span class="n">weight_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_type</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_type</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">:</span> <span class="n">weight_type</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span>
                                                          <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="ApplyAdaMax"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyAdaMax.html#mindspore.ops.ApplyAdaMax">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdaMax</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adamax scheme.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t} = \beta_1 * m_{t-1} + (1 - \beta_1) * g \\</span>
<span class="sd">            v_{t} = \max(\beta_2 * v_{t-1}, \left| g \right|) \\</span>
<span class="sd">            var = var - \frac{l}{1 - \beta_1^t} * \frac{m_{t}}{v_{t} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`m_{t}`, :math:`v` represents the 2nd moment vector, :math:`v_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`v_{t}`, :math:`l` represents scaling factor `lr`,</span>
<span class="sd">    :math:`g` represents `grad`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`beta_1^t` represents `beta1_power`, :math:`var` represents the variable to be updated,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `m`, `v` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients</span>
<span class="sd">          with the same shape and type as `var`. With float32 or float16 data type.</span>
<span class="sd">        - **beta1_power** (Union[Number, Tensor]) - :math:`beta_1^t` in the updating formula, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, :math:`l` in the updating formula, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **beta1** (Union[Number, Tensor]) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta2** (Union[Number, Tensor]) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta_power`, `lr`, `beta1`, `beta2`, `epsilon` or `grad` is neither</span>
<span class="sd">                   float16 nor float32.</span>
<span class="sd">        TypeError: If `beta_power`, `lr`, `beta1`, `beta2` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_ada_max = ops.ApplyAdaMax()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                             [0.7, 0.8]]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_ada_max(self.var, self.m, self.v, beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power =Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.99, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-10, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.93602717e-01,  3.92571449e-01],</span>
<span class="sd">         [ 9.72582996e-01,  4.92249995e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.69999993e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000005e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 8.90999973e-01,  6.99999988e-01],</span>
<span class="sd">         [ 6.93000019e-01,  8.00000012e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T5</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdaMax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta1_power_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta1_power_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta1_power_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;beta1 power&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta1_power_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta1_power_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;beta1_power_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta1_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta1_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta1_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;beta1&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta1_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;beta1_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta2_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta2_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta2_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;beta2&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta2_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;beta2_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">epsilon_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">epsilon_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;epsilon&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epsilon_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;epsilon_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<div class="viewcode-block" id="ApplyAdadelta"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyAdadelta.html#mindspore.ops.ApplyAdadelta">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdadelta</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adadelta scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum = \rho * accum + (1 - \rho) * grad^2</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{update} = \sqrt{\text{accum_update} + \epsilon} * \frac{grad}{\sqrt{accum + \epsilon}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * update^2</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * update</span>

<span class="sd">    Inputs of `var`, `accum`, `accum_update` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **accum_update** (Parameter) - Accum_update to be updated, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **rho** (Union[Number, Tensor]) - Decay rate, must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradients, has the same shape and type as `var`. With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>
<span class="sd">        - **accum_update** (Tensor) - The same shape and data type as `accum_update`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `accum_update`, `lr`, `rho`, `epsilon` or `grad` is neither float16 nor</span>
<span class="sd">                   float32.</span>
<span class="sd">        TypeError: If `accum_update`, `lr`, `rho` or `epsilon` is neither a Number nor a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adadelta = ops.ApplyAdadelta()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.accum_update = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                                        [0.7, 0.8]]).astype(np.float32)),</span>
<span class="sd">        ...                                                             name=&quot;accum_update&quot;)</span>
<span class="sd">        ...     def construct(self, lr, rho, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_adadelta(self.var, self.accum, self.accum_update, lr, rho, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; rho = Tensor(0.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-6, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, rho, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99051356e-01,  3.99683774e-01],</span>
<span class="sd">         [ 9.91633832e-02,  4.99105573e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 9.00000036e-02,  4.89999980e-01],</span>
<span class="sd">         [ 1.00000007e-02,  6.40000045e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 8.99990976e-01,  1.00000791e-01],</span>
<span class="sd">         [ 6.99930906e-01,  7.99999654e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum_update&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdadelta&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">accum_update_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">rho_shape</span><span class="p">,</span>
                    <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;accum_shape&quot;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;accum_update_shape&quot;</span><span class="p">,</span> <span class="n">accum_update_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">rho_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rho_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">rho_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;rho&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">rho_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">rho_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;rho_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">epsilon_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">epsilon_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lepsilon&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epsilon_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;epsilon_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">accum_update_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">accum_update_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">rho_dtype</span><span class="p">,</span>
                    <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_update&quot;</span><span class="p">:</span> <span class="n">accum_update_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="n">rho_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">accum_update_dtype</span></div>


<div class="viewcode-block" id="ApplyAdagrad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyAdagrad.html#mindspore.ops.ApplyAdagrad">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum}}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent..</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be scalar. With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad = ops.ApplyAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adagrad(self.var, self.accum, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99638879e-01,  3.99296492e-01],</span>
<span class="sd">         [ 9.97817814e-02,  4.99281585e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 1.06441569e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="ApplyAdagradV2"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyAdagradV2.html#mindspore.ops.ApplyAdagradV2">[docs]</a><span class="k">class</span> <span class="nc">ApplyAdagradV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagradv2 scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float16 or float32 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape and dtype must be the same as `var`.</span>
<span class="sd">          With float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad_v2 = ops.ApplyAdagradV2(epsilon=1e-6)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adagrad_v2(self.var, self.accum, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99638879e-01,  3.99296492e-01],</span>
<span class="sd">         [ 9.97817814e-02,  4.99281585e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;grad shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="SparseApplyAdagrad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SparseApplyAdagrad.html#mindspore.ops.SparseApplyAdagrad">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * (1 / sqrt(accum))</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>
<span class="sd">        use_locking (bool): If true, the `var` and `accumulation` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The shape must be the same as `var`&#39;s shape except the first dimension.</span>
<span class="sd">          Gradients has the same data type as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in first dimension, the type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr` is not a float.</span>
<span class="sd">        TypeError: If neither `update_slots` nor `use_locking` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adagrad = ops.SparseApplyAdagrad(lr=1e-8)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[[0.2]]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[[0.1]]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adagrad(self.var, self.accum, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[[0.7]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1, 1], dtype=Float32, value=</span>
<span class="sd">        [[[1.99999988e-01]]]), Tensor(shape=[1, 1, 1], dtype=Float32, value=</span>
<span class="sd">        [[[1.00000001e-01]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;len of var shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">),</span> <span class="s1">&#39;len of grad shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span></div>


<div class="viewcode-block" id="SparseApplyAdagradV2"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SparseApplyAdagradV2.html#mindspore.ops.SparseApplyAdagradV2">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyAdagradV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme, one more epsilon attribute than SparseApplyAdagrad.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        use_locking (bool): If `True`, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        update_slots (bool): If `True`, the computation logic will be different to `False`. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The shape must be the same as `var`&#39;s shape except the first dimension.</span>
<span class="sd">          Gradients has the same data type as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in first dimension, the type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `lr` nor `epsilon` is a float.</span>
<span class="sd">        TypeError: If neither `update_slots` nor `use_locking` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adagrad_v2 = ops.SparseApplyAdagradV2(lr=1e-8, epsilon=1e-6)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.1]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adagrad_v2(self.var, self.accum, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 2.00000003e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000001e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_slots</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;len of var shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">),</span> <span class="s1">&#39;len of grad shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grad_shape</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span></div>


<div class="viewcode-block" id="ApplyProximalAdagrad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyProximalAdagrad.html#mindspore.ops.ApplyProximalAdagrad">[docs]</a><span class="k">class</span> <span class="nc">ApplyProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. Must has the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **grad** (Tensor) - Gradient with the same shape and dtype as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_blocking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `l1` or `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_proximal_adagrad = ops.ApplyProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = 0.01</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1, self.l2, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.96388459e-01,  3.92964751e-01],</span>
<span class="sd">         [ 9.78178233e-02,  4.952815793e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.900000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.240000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l1_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l1_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;l1&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l1_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;l1_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l2_shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l2_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l2_shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;l2&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l2_shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;l2_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span></div>


<div class="viewcode-block" id="SparseApplyProximalAdagrad"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SparseApplyProximalAdagrad.html#mindspore.ops.SparseApplyProximalAdagrad">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm. Compared with ApplyProximalAdagrad,</span>
<span class="sd">    an additional index tensor is input.</span>

<span class="sd">    .. math::</span>
<span class="sd">            accum += grad * grad</span>
<span class="sd">    .. math::</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">    .. math::</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type..</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `l1`, `l2`, `scalar` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_proximal_adagrad = ops.SparseApplyProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[4.1, 7.2], [1.1, 3.0]], np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0, 0], [0, 0]], np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = 1.0</span>
<span class="sd">        ...         self.l1 = 1.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        ...                                                  self.l2, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[1, 1], [1, 1]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.09999990e+00,  5.19999981e+00],</span>
<span class="sd">         [ 0.00000000e+00,  1.00000000e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00,  1.00000000e+00],</span>
<span class="sd">         [ 1.00000000e+00,  1.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span>
                    <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span>
                    <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAddSign"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyAddSign.html#mindspore.ops.ApplyAddSign">[docs]</a><span class="k">class</span> <span class="nc">ApplyAddSign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t} = \beta * m_{t-1} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = (\alpha + \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`m_{t}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `alpha`, `sign_decay` or `beta` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `alpha` or `sign_decay` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_add_sign = ops.ApplyAddSign()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.alpha = 1.0</span>
<span class="sd">        ...         self.sign_decay = 0.99</span>
<span class="sd">        ...         self.beta = 0.9</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_add_sign(self.var, self.m, self.lr, self.alpha, self.sign_decay, self.beta, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99403024e-01,  3.98607016e-01],</span>
<span class="sd">         [ 9.98010039e-02,  4.98407990e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000064e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyAddSign&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">,</span> <span class="n">sign_decay_shape</span><span class="p">,</span>
                    <span class="n">beta_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;m_shape&#39;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">alpha_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">alpha_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;alpha&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;alpha_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">sign_decay_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sign_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">sign_decay_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;sign_decay&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sign_decay_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">sign_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;sign_decay_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;beta&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;beta_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">alpha_dtype</span><span class="p">,</span> <span class="n">sign_decay_dtype</span><span class="p">,</span>
                    <span class="n">beta_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;sign_decay&quot;</span><span class="p">:</span> <span class="n">sign_decay_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span></div>


<div class="viewcode-block" id="ApplyPowerSign"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyPowerSign.html#mindspore.ops.ApplyPowerSign">[docs]</a><span class="k">class</span> <span class="nc">ApplyPowerSign</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t} = \beta * m_{t-1} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t-1}`</span>
<span class="sd">    is the last momentent of :math:`m_{t}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`.</span>

<span class="sd">    All of inputs comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If `lr`, `logbase`, `sign_decay` or `beta` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>
<span class="sd">    If inputs are tensors and have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">          If data type of `var` is float16, all inputs must have the same data type as `var`.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **logbase** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `logbase`, `sign_decay`, `beta` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `logbase`, `sign_decay` or `beta` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_power_sign = ops.ApplyPowerSign()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.logbase = np.e</span>
<span class="sd">        ...         self.sign_decay = 0.99</span>
<span class="sd">        ...         self.beta = 0.9</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_power_sign(self.var, self.m, self.lr, self.logbase,</span>
<span class="sd">        ...                                        self.sign_decay, self.beta, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.95575690e-01,  3.89676481e-01],</span>
<span class="sd">         [ 9.85252112e-02,  4.88201708e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000064e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;logbase&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyPowerSign&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">logbase_shape</span><span class="p">,</span> <span class="n">sign_decay_shape</span><span class="p">,</span>
                    <span class="n">beta_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;m_shape&#39;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s1">&#39;var_shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">lr_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;lr&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">lr_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;lr_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">logbase_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">logbase_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">logbase_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;logbase&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logbase_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">logbase_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;logbase_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">sign_decay_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sign_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">sign_decay_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;sign_decay&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sign_decay_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">sign_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;sign_decay_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">beta_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">beta_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;beta&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">beta_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">beta_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;beta_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">logbase_dtype</span><span class="p">,</span> <span class="n">sign_decay_dtype</span><span class="p">,</span>
                    <span class="n">beta_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;logbase&quot;</span><span class="p">:</span> <span class="n">logbase_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;sign_decay&quot;</span><span class="p">:</span> <span class="n">sign_decay_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;beta&quot;</span><span class="p">:</span> <span class="n">beta_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span></div>


<div class="viewcode-block" id="ApplyGradientDescent"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyGradientDescent.html#mindspore.ops.ApplyGradientDescent">[docs]</a><span class="k">class</span> <span class="nc">ApplyGradientDescent</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the following.</span>

<span class="sd">    .. math::</span>
<span class="sd">        var = var - \alpha * \delta</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change, has the same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var` or `alpha` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `delta` is not a Tensor.</span>
<span class="sd">        TypeError: If `alpha` is neither a Number nor a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_gradient_descent = ops.ApplyGradientDescent()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.random.rand(2, 2).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.alpha = 0.001</span>
<span class="sd">        ...     def construct(self, delta):</span>
<span class="sd">        ...         out = self.apply_gradient_descent(self.var, self.alpha, delta)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.random.rand(2, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyGradientDescent&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;delta shape&#39;</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">alpha_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">alpha_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;alpha&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;alpha_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">alpha_dtype</span><span class="p">,</span> <span class="n">delta_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="n">delta_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="ApplyProximalGradientDescent"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyProximalGradientDescent.html#mindspore.ops.ApplyProximalGradientDescent">[docs]</a><span class="k">class</span> <span class="nc">ApplyProximalGradientDescent</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FOBOS(Forward Backward Splitting) algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{prox_v} = var - \alpha * \delta</span>
<span class="sd">    .. math::</span>
<span class="sd">        var = \frac{sign(\text{prox_v})}{1 + \alpha * l2} * \max(\left| \text{prox_v} \right| - alpha * l1, 0)</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change, has the same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `alpha`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `alpha`, `l1` or `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `delta` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_proximal_gradient_descent = ops.ApplyProximalGradientDescent()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.random.rand(2, 2).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.alpha = 0.001</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, delta):</span>
<span class="sd">        ...         out = self.apply_proximal_gradient_descent(self.var, self.alpha, self.l1, self.l2, delta)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.random.rand(2, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Initialize ApplyGradientDescent&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">alpha_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;delta shape&#39;</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">,</span> <span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">alpha_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">alpha_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;alpha&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">alpha_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">alpha_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;alpha_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l1_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l1_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l1_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;l1&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l1_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;l1_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">l2_shape_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">l2_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l2_shape_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;l2&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l2_shape_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">l2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;l2_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">alpha_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span> <span class="n">delta_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="n">delta_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alpha_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span></div>


<div class="viewcode-block" id="LARSUpdate"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.LARSUpdate.html#mindspore.ops.LARSUpdate">[docs]</a><span class="k">class</span> <span class="nc">LARSUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Conducts LARS (layer-wise adaptive rate scaling) update on the sum of squares of gradient.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): Term added to the denominator to improve numerical stability. Default: 1e-05.</span>
<span class="sd">        hyperpara (float): Trust coefficient for calculating the local learning rate. Default: 0.001.</span>
<span class="sd">        use_clip (bool): Whether to use clip operation for calculating the local learning rate. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - A tensor, representing the weight.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of weight, which has the same shape and dtype with weight.</span>
<span class="sd">        - **norm_weight** (Tensor) - A scalar tensor, representing the sum of squares of weight.</span>
<span class="sd">        - **norm_gradient** (Tensor) - A scalar tensor, representing the sum of squares of gradient.</span>
<span class="sd">        - **weight_decay** (Union[Number, Tensor]) - Weight decay. It must be a scalar tensor or number.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. It must be a scalar tensor or number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the new gradient.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `epsilon` nor `hyperpara` is a float.</span>
<span class="sd">        TypeError: If `use_clip` is a bool.</span>
<span class="sd">        TypeError: If `weight`, `gradient`, `norm_weight` or `norm_gradient` is not a Tensor.</span>
<span class="sd">        TypeError: If `weight_decay` or `learning_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If shape of `gradient` is not same as `weight`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.lars = ops.LARSUpdate()</span>
<span class="sd">        ...         self.reduce = ops.ReduceSum()</span>
<span class="sd">        ...         self.square = ops.Square()</span>
<span class="sd">        ...     def construct(self, weight, gradient):</span>
<span class="sd">        ...         w_square_sum = self.reduce(self.square(weight))</span>
<span class="sd">        ...         grad_square_sum = self.reduce(self.square(gradient))</span>
<span class="sd">        ...         grad_t = self.lars(weight, gradient, w_square_sum, grad_square_sum, 0.0, 1.0)</span>
<span class="sd">        ...         return grad_t</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([[0.5, 0.8, 0.2], [0.6, 0.4, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[0.4, 0.4, 0.5], [0.2, 0.4, 0.3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(weight), Tensor(gradient))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.0005265  0.0005265 0.00065813]</span>
<span class="sd">         [0.00026325 0.0005265 0.00039488]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">hyperpara</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">use_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;hyperpara&quot;</span><span class="p">,</span> <span class="n">hyperpara</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_clip&quot;</span><span class="p">,</span> <span class="n">use_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">norm_weight_shape</span><span class="p">,</span> <span class="n">norm_gradient_shape</span><span class="p">,</span> <span class="n">weight_decay_shape</span><span class="p">,</span>
                    <span class="n">learning_rate_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight shape&quot;</span><span class="p">,</span> <span class="n">weight_shape</span><span class="p">,</span> <span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;norm weight shape&quot;</span><span class="p">,</span> <span class="n">norm_weight_shape</span><span class="p">,</span> <span class="s2">&quot;norm gradient shape&quot;</span><span class="p">,</span> <span class="n">norm_gradient_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">weight_decay_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;weight decay&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">weight_decay_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;weight_decay_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">shp_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">shp_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;learning rate&#39;s rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">shp_len</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;learning_rate_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">norm_weight_dtype</span><span class="p">,</span> <span class="n">norm_gradient_dtype</span><span class="p">,</span>
                    <span class="n">weight_decay_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Weight dtype&quot;</span><span class="p">:</span> <span class="n">weight_dtype</span><span class="p">,</span> <span class="s2">&quot;gradient dtype&quot;</span><span class="p">:</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="s2">&quot;norm weight dtype&quot;</span><span class="p">:</span> <span class="n">norm_weight_dtype</span><span class="p">,</span>
                <span class="s2">&quot;norm gradient dtype&quot;</span><span class="p">:</span> <span class="n">norm_gradient_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span>
                                                      <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span> <span class="n">weight_decay_dtype</span><span class="p">},</span>
                                                    <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate_dtype</span><span class="p">},</span>
                                                    <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">weight_dtype</span></div>


<div class="viewcode-block" id="ApplyFtrl"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.ApplyFtrl.html#mindspore.ops.ApplyFtrl">[docs]</a><span class="k">class</span> <span class="nc">ApplyFtrl</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL scheme.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The data type must be float16 or float32.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be positive. Default: 0.001.</span>
<span class="sd">          It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **lr_power** (Union[Number, Tensor]) - Learning rate power controls how the learning rate decreases</span>
<span class="sd">          during training, must be less than or equal to zero. Use fixed learning rate if lr_power is zero.</span>
<span class="sd">          Default: -0.5. It must be a float number or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - represents the updated `var`. As the input parameters has been updated in-place, this</span>
<span class="sd">          value is always zero when the platforms is GPU.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `grad`, `lr`, `l1`, `l2` or `lr_power` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class ApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(ApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.apply_ftrl = ops.ApplyFtrl()</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...         self.lr_power = -0.5</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                                  [0.7, 0.8]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_ftrl(self.var, self.accum, self.linear, grad, self.lr, self.l1, self.l2,</span>
<span class="sd">        ...                               self.lr_power)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.0390525,  0.11492836],</span>
<span class="sd">         [ 0.00066425, 0.15075898]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_power&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span>
                    <span class="n">lr_power_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_type</span><span class="p">,</span> <span class="n">accum_type</span><span class="p">,</span> <span class="n">linear_type</span><span class="p">,</span> <span class="n">grad_type</span><span class="p">,</span> <span class="n">lr_type</span><span class="p">,</span> <span class="n">l1_type</span><span class="p">,</span> <span class="n">l2_type</span><span class="p">,</span>
                    <span class="n">lr_power_type</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_type</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_type</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span> <span class="n">linear_type</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_type</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_type</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_type</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_type</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr_power&quot;</span><span class="p">:</span> <span class="n">lr_power_type</span><span class="p">},</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_type</span></div>


<div class="viewcode-block" id="SparseApplyFtrl"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SparseApplyFtrl.html#mindspore.ops.SparseApplyFtrl">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyFtrl</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be the same data type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in the first dimension. If there are</span>
<span class="sd">          duplicates in `indices`, the behavior is undefined. The type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `linear` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl = ops.SparseApplyFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.1]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.6]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[2.00000003e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[1.00000001e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[6.00000024e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices_dtype&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyFtrlV2"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.SparseApplyFtrlV2.html#mindspore.ops.SparseApplyFtrlV2">[docs]</a><span class="k">class</span> <span class="nc">SparseApplyFtrlV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme. This class has one more attribute, named</span>
<span class="sd">    l2_shrinkage, than class SparseApplyFtrl.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2_shrinkage (float): L2 shrinkage regularization.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): If `True`, the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var`, for the gradient.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          The shape of `indices` must be the same as `grad` in the first dimension. The type must be int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2`, `lr_power` or `use_locking` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `linear` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlV2Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlV2Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl_v2 = ops.SparseApplyFtrlV2(lr=0.01, l1=0.0, l2=0.0,</span>
<span class="sd">        ...                                                         l2_shrinkage=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.3]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.5, 0.9]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.7, 0.5]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl_v2(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlV2Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.8, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.00000003e-01,  3.00000012e-01]]), Tensor(shape=[1, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.00000000e-01,  8.99999976e-01]]), Tensor(shape=[1, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.99999988e-01,  5.00000000e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2_shrinkage</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2_shrinkage&quot;</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indicese&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span></div>


<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Dropout.html#mindspore.ops.Dropout">[docs]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor with probability.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): The keep rate, between 0 and 1, e.g. keep_prob = 0.9,</span>
<span class="sd">            means dropping out 10% of input units.</span>
<span class="sd">        Seed0 (int): Seed0 value for random generating. Default: 0.</span>
<span class="sd">        Seed1 (int): Seed1 value for random generating. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input of Dropout with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - with the same shape as the input tensor.</span>
<span class="sd">        - **mask** (Tensor) - with the same shape as the input tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_prob` is not a float.</span>
<span class="sd">        TypeError: If `Seed0` or `Seed1` is not an int.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor((20, 16, 50, 50), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 32. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; print(mask)</span>
<span class="sd">        [0. 1. 0. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed0</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dropout2D"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Dropout2D.html#mindspore.ops.Dropout2D">[docs]</a><span class="k">class</span> <span class="nc">Dropout2D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the channels of the input tensor</span>
<span class="sd">    with probability 1-`keep_prob` from a Bernoulli distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): The keep probability of a channel, between 0 and 1, e.g. `keep_prob` = 0.8,</span>
<span class="sd">            means dropping out 20% of channels. Default: 0.5.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - A 4-D tensor with shape :math:`(N, C, H, W)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - with the same shape and data type as the input tensor.</span>
<span class="sd">        - **mask** (Tensor[bool]) - with the same shape as the input tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `keep_prob` is not float.</span>
<span class="sd">        ValueError: If `keep_prob` is out of the range [0.0, 1.0];</span>
<span class="sd">                    or if the dim of input is not 4-D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout2D(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(2, 1, 2, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0. 0. 0.]</span>
<span class="sd">           [0. 0. 0.]]]</span>
<span class="sd">         [[[0.88 -2.98 -0.01]</span>
<span class="sd">           [2.16 -0.34 1.57]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(mask)</span>
<span class="sd">        [[[[False False False]</span>
<span class="sd">           [False False False]]]</span>
<span class="sd">         [[[True True True]</span>
<span class="sd">           [True True True]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;dim of input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">mask_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mask_dtype</span></div>


<div class="viewcode-block" id="Dropout3D"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Dropout3D.html#mindspore.ops.Dropout3D">[docs]</a><span class="k">class</span> <span class="nc">Dropout3D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the channels of the input tensor</span>
<span class="sd">    with probability 1-`keep_prob` from a Bernoulli distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): The keep probability of a channel, between 0 and 1, e.g. `keep_prob` = 0.8,</span>
<span class="sd">            means dropping out 20% of channels. Default: 0.5.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - A 5-D tensor with shape :math:`(N, C, D, H, W)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - with the same shape and data type as the input tensor.</span>
<span class="sd">        - **mask** (Tensor[bool]) - with the same shape as the input tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `keep_prob` is not float.</span>
<span class="sd">        ValueError: If `keep_prob` is out of the range [0.0, 1.0];</span>
<span class="sd">                    or if the dim of input is not 5-D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout3D(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(2, 1, 2, 1, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[0. 0.]]</span>
<span class="sd">           [[0. 0.]]]]</span>
<span class="sd">         [[[[-2.98 -0.01]]</span>
<span class="sd">           [[-0.34 1.57]]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(mask)</span>
<span class="sd">        [[[[[False False]]</span>
<span class="sd">           [[False False]]]]</span>
<span class="sd">         [[[[True True]]</span>
<span class="sd">           [[True True]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;dim of input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">mask_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mask_dtype</span></div>


<div class="viewcode-block" id="CTCLoss"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.CTCLoss.html#mindspore.ops.CTCLoss">[docs]</a><span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        preprocess_collapse_repeated (bool): If true, repeated labels will be collapsed prior to the CTC calculation.</span>
<span class="sd">                                             Default: False.</span>
<span class="sd">        ctc_merge_repeated (bool): If false, during CTC calculation, repeated non-blank labels will not be merged</span>
<span class="sd">                                   and these labels will be interpreted as individual ones. This is a simplfied</span>
<span class="sd">                                   version of CTC. Default: True.</span>
<span class="sd">        ignore_longer_outputs_than_inputs (bool): If true, sequences with longer outputs than inputs will be ignored.</span>
<span class="sd">                                                  Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - The input Tensor must be a `3-D` tensor whose shape is</span>
<span class="sd">          (`max_time`, `batch_size`, `num_classes`). `num_classes` must be `num_labels + 1` classes, `num_labels`</span>
<span class="sd">          indicates the number of actual labels. Blank labels are reserved. Default blank label is `num_classes - 1`.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels_indices** (Tensor) - The indices of labels. `labels_indices[i, :] == [b, t]` means</span>
<span class="sd">          `labels_values[i]` stores the id for `(batch b, time t)`. The type must be int64 and rank must be 2.</span>
<span class="sd">        - **labels_values** (Tensor) - A `1-D` input tensor. The values are associated with the given batch and time.</span>
<span class="sd">          The type must be int32. `labels_values[i]` must in the range of `[0, num_classes)`.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of (`batch_size`).</span>
<span class="sd">          The type must be int32. Each value in the tensor must not be greater than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - A tensor containing log-probabilities, the shape is (`batch_size`). The tensor has</span>
<span class="sd">          the same type with `inputs`.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of `loss`, has the same type and shape with `inputs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `preprocess_collapse_repeated`, `ctc_merge_repeated` or `ignore_longer_outputs_than_inputs`</span>
<span class="sd">                   is not a bool.</span>
<span class="sd">        TypeError: If `inputs`, `labels_indices`, `labels_values` or `sequence_length` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `inputs` is not one of the following: float16, float32 or float64.</span>
<span class="sd">        TypeError: If dtype of `labels_indices` is not int64.</span>
<span class="sd">        TypeError: If dtype of `labels_values` or `sequence_length` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import operations as ops</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.array([[[0.3, 0.6, 0.6],</span>
<span class="sd">        ...                            [0.4, 0.3, 0.9],</span>
<span class="sd">        ...</span>
<span class="sd">        ...                           [[0.9, 0.4, 0.2],</span>
<span class="sd">        ...                            [0.9, 0.9, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels_indices = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; labels_values = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = ops.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss, gradient = ctc_loss(inputs, labels_indices, labels_values, sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [ 0.75466496  0.6259288 ]</span>
<span class="sd">        &gt;&gt;&gt; print(gradient)</span>
<span class="sd">        [[[ 0.26944962  0.34967452  -0.6191241  ]</span>
<span class="sd">          [ 0.28437287  0.25814265  -0.5425156 ]]</span>
<span class="sd">         [[ 0.46983105  0.29572973  0.04452367 ]</span>
<span class="sd">          [ 0.41254193  0.4185341  0.02441157 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">ignore_longer_outputs_than_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_values&quot;</span><span class="p">,</span> <span class="s2">&quot;sequence_length&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;preprocess_collapse_repeated&quot;</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_collapse_repeated_</span> <span class="o">=</span> <span class="n">preprocess_collapse_repeated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ctc_merge_repeated_</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ctc_merge_repeated&quot;</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="p">,</span>
                                                              <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_longer_outputs_than_inputs&quot;</span><span class="p">,</span>
                                   <span class="n">ignore_longer_outputs_than_inputs</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_longer_outputs_than_inputs_</span> <span class="o">=</span> <span class="n">ignore_longer_outputs_than_inputs</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">,</span> <span class="n">labels_values</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;inputs rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels_indices</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;labels_indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">labels_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;labels_indices dim one&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels_values</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;labels_values rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;sequence_length rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels_indices size&#39;</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;labels_values size&#39;</span><span class="p">,</span>
                        <span class="n">labels_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;inputs batch_size&#39;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;sequence_length batch_size&#39;</span><span class="p">,</span>
                        <span class="n">sequence_length</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_size</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">,</span> <span class="n">labels_values</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
        <span class="n">valid_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">double</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">valid_dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;labels_indices&quot;</span><span class="p">,</span> <span class="n">labels_indices</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;labels_values&quot;</span><span class="p">,</span> <span class="n">labels_values</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;sequence_length&quot;</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span></div>


<div class="viewcode-block" id="CTCGreedyDecoder"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.CTCGreedyDecoder.html#mindspore.ops.CTCGreedyDecoder">[docs]</a><span class="k">class</span> <span class="nc">CTCGreedyDecoder</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        merge_repeated (bool): If true, merge repeated classes in output. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - The input Tensor must be a `3-D` tensor whose shape is</span>
<span class="sd">          (`max_time`, `batch_size`, `num_classes`). `num_classes` must be `num_labels + 1` classes,</span>
<span class="sd">          `num_labels` indicates the number of actual labels. Blank labels are reserved.</span>
<span class="sd">          Default blank label is `num_classes - 1`. Data type must be float32 or float64.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of (`batch_size`).</span>
<span class="sd">          The type must be int32. Each value in the tensor must be equal to or less than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **decoded_indices** (Tensor) - A tensor with shape of (`total_decoded_outputs`, 2).</span>
<span class="sd">          Data type is int64.</span>
<span class="sd">        - **decoded_values** (Tensor) - A tensor with shape of (`total_decoded_outputs`),</span>
<span class="sd">          it stores the decoded classes. Data type is int64.</span>
<span class="sd">        - **decoded_shape** (Tensor) - The value of tensor is [`batch_size`, `max_decoded_legth`].</span>
<span class="sd">          Data type is int64.</span>
<span class="sd">        - **log_probability** (Tensor) - A tensor with shape of (`batch_size`, 1),</span>
<span class="sd">          containing sequence log-probability, has the same type as `inputs`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `merge_repeated` is not a bool.</span>
<span class="sd">        ValueError: If length of shape of `inputs` is not equal to 3.</span>
<span class="sd">        ValueError: If length of shape of `sequence_length` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.random.random((2, 2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_greedy_decoder = ops.CTCGreedyDecoder()</span>
<span class="sd">        &gt;&gt;&gt; out1, out2, out3, out4 = ctc_greedy_decoder(inputs, sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(out1, out2, out3, out4)</span>
<span class="sd">        [[0 0] [0 1] [1 0]]</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">        [2 2]</span>
<span class="sd">        [[-0.7443749] [0.18251707]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merge_repeated</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;merge_repeated&quot;</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">,</span> <span class="n">sequence_length_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;inputs rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequence_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;sequence_length rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;inputs batch_size&#39;</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;sequence_length batch_size&#39;</span><span class="p">,</span>
                        <span class="n">sequence_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">total_decoded_outputs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">decoded_indices_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">total_decoded_outputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">decoded_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">total_decoded_outputs</span><span class="p">]</span>
        <span class="n">decoded_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">log_probability_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">decoded_indices_shape</span><span class="p">,</span> <span class="n">decoded_values</span><span class="p">,</span> <span class="n">decoded_shape</span><span class="p">,</span> <span class="n">log_probability_shape</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_dtype</span><span class="p">,</span> <span class="n">sequence_length_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;inputs_dtype&quot;</span><span class="p">,</span> <span class="n">inputs_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">double</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;sequence_length_dtype&quot;</span><span class="p">,</span> <span class="n">sequence_length_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">decoded_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">decoded_type</span><span class="p">,</span> <span class="n">decoded_type</span><span class="p">,</span> <span class="n">decoded_type</span><span class="p">,</span> <span class="n">inputs_dtype</span></div>


<div class="viewcode-block" id="BasicLSTMCell"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.BasicLSTMCell.html#mindspore.ops.BasicLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">BasicLSTMCell</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It&#39;s similar to operator DynamicRNN. BasicLSTMCell will be deprecated in the future.</span>
<span class="sd">    Please use DynamicRNN instead.</span>

<span class="sd">    Applies the long short-term memory (LSTM) to the input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            i_t = \sigma(W_{ix} x_t + b_{ix} + W_{ih} h_{(t-1)} + b_{ih}) \\</span>
<span class="sd">            f_t = \sigma(W_{fx} x_t + b_{fx} + W_{fh} h_{(t-1)} + b_{fh}) \\</span>
<span class="sd">            \tilde{c}_t = \tanh(W_{cx} x_t + b_{cx} + W_{ch} h_{(t-1)} + b_{ch}) \\</span>
<span class="sd">            o_t = \sigma(W_{ox} x_t + b_{ox} + W_{oh} h_{(t-1)} + b_{oh}) \\</span>
<span class="sd">            c_t = f_t * c_{(t-1)} + i_t * \tilde{c}_t \\</span>
<span class="sd">            h_t = o_t * \tanh(c_t) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Here :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product. :math:`W, b`</span>
<span class="sd">    are learnable weights between the output and the input in the formula. For instance,</span>
<span class="sd">    :math:`W_{ix}, b_{ix}` are the weight and bias used to transform from input :math:`x` to :math:`i`.</span>
<span class="sd">    Details can be found in paper `LONG SHORT-TERM MEMORY</span>
<span class="sd">    &lt;https://www.bioinf.jku.at/publications/older/2604.pdf&gt;`_ and</span>
<span class="sd">    `Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</span>
<span class="sd">    &lt;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43905.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): If not 1.0, append `Dropout` layer on the outputs of each</span>
<span class="sd">            LSTM layer except the last layer. Default 1.0. The range of dropout is [0.0, 1.0].</span>
<span class="sd">        forget_bias (float): Add forget bias to forget gate biases in order to decrease former scale. Default: 1.0.</span>
<span class="sd">        state_is_tuple (bool): If true, the state is a tuple of 2 tensors, containing h and c; If false, the state is</span>
<span class="sd">            a tensor and it needs to be split first. Default: True.</span>
<span class="sd">        activation (str): Activation. Default: &quot;tanh&quot;. Only &quot;tanh&quot; is currently supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words. Tensor of shape (`batch_size`, `input_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **h** (Tensor) - Hidden state last moment. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **c** (Tensor) - Cell state last moment. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **w** (Tensor) - Weight. Tensor of shape (`input_size + hidden_size`, `4 x hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **b** (Tensor) - Bias. Tensor of shape (`4 x hidden_size`).</span>
<span class="sd">          The data type must be the same as `c`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **ct** (Tensor) - Forward :math:`c_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **ht** (Tensor) - Cell output. Tensor of shape (`batch_size`, `hidden_size`). With data type of float16.</span>
<span class="sd">        - **it** (Tensor) - Forward :math:`i_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **jt** (Tensor) - Forward :math:`j_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **ft** (Tensor) - Forward :math:`f_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **ot** (Tensor) - Forward :math:`o_t` cache at moment `t`. Tensor of shape (`batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `c`.</span>
<span class="sd">        - **tanhct** (Tensor) - Forward :math:`tanh c_t` cache at moment `t`.</span>
<span class="sd">          Tensor of shape (`batch_size`, `hidden_size`), has the same type with input `c`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `keep_prob` or `forget_bias` is not float.</span>
<span class="sd">        TypeError: If `state_is_tuple` is not a bool.</span>
<span class="sd">        TypeError: If `activation` is not a str.</span>
<span class="sd">        TypeError: If `x`, `h`, `c`, `w` or `b` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `h`, `c` or `w` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; np.random.seed(0)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(1, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; h = Tensor(np.random.rand(1, 2).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; c = Tensor(np.random.rand(1, 2).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.rand(34, 8).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.random.rand(8, ).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; lstm = ops.BasicLSTMCell(keep_prob=1.0, forget_bias=1.0, state_is_tuple=True, activation=&#39;tanh&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = lstm(x, h, c, w, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">        ([[7.6953e-01, 9.2432e-01]]), Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">         [[1.0000e+00, 1.0000e+00]]), Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">         [[1.0000e+00, 1.0000e+00]]), Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">         [[1.0000e+00, 1.0000e+00]]), Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">         [[1.0000e+00, 1.0000e+00]]), Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">         [[7.6953e-01, 9.2432e-01]]), Tensor(shape=[1, 2], dtype=Float16, value=</span>
<span class="sd">         [[0.0000e+00, 0.0000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_is_tuple</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;state_is_tuple&quot;</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;c rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[0]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[1]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]+h_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">ct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ht_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">it_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">jt_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ft_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ot_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">tanhct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">ct_shape</span><span class="p">,</span> <span class="n">ht_shape</span><span class="p">,</span> <span class="n">it_shape</span><span class="p">,</span> <span class="n">jt_shape</span><span class="p">,</span> <span class="n">ft_shape</span><span class="p">,</span> <span class="n">ot_shape</span><span class="p">,</span> <span class="n">tanhct_shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;h_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;w_dtype&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">)))</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;c_dtype&quot;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s2">&quot;b_dtype&quot;</span><span class="p">:</span> <span class="n">b_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">c_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DynamicRNN"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.DynamicRNN.html#mindspore.ops.DynamicRNN">[docs]</a><span class="k">class</span> <span class="nc">DynamicRNN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a recurrent neural network to the input.</span>
<span class="sd">    Only long short-term memory (LSTM) currently supported.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            i_t = \sigma(W_{ix} x_t + b_{ix} + W_{ih} h_{(t-1)} + b_{ih}) \\</span>
<span class="sd">            f_t = \sigma(W_{fx} x_t + b_{fx} + W_{fh} h_{(t-1)} + b_{fh}) \\</span>
<span class="sd">            \tilde{c}_t = \tanh(W_{cx} x_t + b_{cx} + W_{ch} h_{(t-1)} + b_{ch}) \\</span>
<span class="sd">            o_t = \sigma(W_{ox} x_t + b_{ox} + W_{oh} h_{(t-1)} + b_{oh}) \\</span>
<span class="sd">            c_t = f_t * c_{(t-1)} + i_t * \tilde{c}_t \\</span>
<span class="sd">            h_t = o_t * \tanh(c_t) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Here :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product. :math:`W, b`</span>
<span class="sd">    are learnable weights between the output and the input in the formula. For instance,</span>
<span class="sd">    :math:`W_{ix}, b_{ix}` are the weight and bias used to transform from input :math:`x` to :math:`i`.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_type (str): A string identifying the cell type in the op. Default: &#39;LSTM&#39;.</span>
<span class="sd">            Only &#39;LSTM&#39; is currently supported.</span>
<span class="sd">        direction (str): A string identifying the direction in the op. Default: &#39;UNIDIRECTIONAL&#39;.</span>
<span class="sd">            Only &#39;UNIDIRECTIONAL&#39; is currently supported.</span>
<span class="sd">        cell_depth (int): An integer identifying the cell depth in the op. Default: 1.</span>
<span class="sd">        use_peephole (bool): A bool identifying if use peephole in the op. Default: False.</span>
<span class="sd">        keep_prob (float): A float identifying the keep prob in the op. Default: 1.0.</span>
<span class="sd">        cell_clip (float): A float identifying the cell clip in the op. Default: -1.0.</span>
<span class="sd">        num_proj (int): An integer identifying the num proj in the op. Default: 0.</span>
<span class="sd">        time_major (bool): A bool identifying the time major in the op. Default: True.</span>
<span class="sd">            Only `True` is currently supported.</span>
<span class="sd">        activation (str): A string identifying the type of activation function in the op. Default: &#39;tanh&#39;.</span>
<span class="sd">            Only &#39;tanh&#39; is currently supported.</span>
<span class="sd">        forget_bias (float): A float identifying the forget bias in the op. Default: 0.0.</span>
<span class="sd">        is_training (bool): A bool identifying is training in the op. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words. Tensor of shape (`num_step`, `batch_size`, `input_size`).</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **w** (Tensor) - Weight. Tensor of shape (`input_size + hidden_size`, `4 x hidden_size`).</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **b** (Tensor) - Bias. Tensor of shape (`4 x hidden_size`).</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape (`batch_size`).</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time. Tensor of shape (1, `batch_size`, `hidden_size`).</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **init_c** (Tensor) - Cell state of initial time. Tensor of shape (1, `batch_size`, `hidden_size`).</span>
<span class="sd">          The data type must be float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          With data type of float16.</span>
<span class="sd">        - **output_c** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **i** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **j** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **f** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **o** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **tanhct** (Tensor) - A Tensor of shape (`num_step`, `batch_size`, `hidden_size`).</span>
<span class="sd">          Has the same type with input `b`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `cell_type`, `direction` or `activation` is not a str.</span>
<span class="sd">        TypeError: If `cell_depth` or `num_proj` is not an int.</span>
<span class="sd">        TypeError: If `keep_prob`, `cell_clip` or `forget_bias` is not a float.</span>
<span class="sd">        TypeError: If `use_peehpole`, `time_major` or `is_training` is not a bool.</span>
<span class="sd">        TypeError: If `x`, `w`, `b`, `seq_length`, `init_h` or `init_c` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `w`, `init_h` or `nit_c` is not float16.</span>
<span class="sd">        TypeError: If dtype of `b` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 16, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.rand(96, 128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.random.rand(128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_c = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_rnn = ops.DynamicRNN()</span>
<span class="sd">        &gt;&gt;&gt; output = dynamic_rnn(x, w, b, None, init_h, init_c)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (2, 16, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cell_type</span><span class="o">=</span><span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">use_peephole</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">num_proj</span><span class="p">,</span> <span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_peephole</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_peephole&quot;</span><span class="p">,</span> <span class="n">use_peephole</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">cell_type</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">],</span> <span class="s2">&quot;cell_type&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">,</span> <span class="n">seq_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seq_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, seq_shape should be None.&quot;</span><span class="p">)</span>

        <span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[-1]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[-1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, w_shape[-1] should multiple of 4.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_size + hidden_size&quot;</span><span class="p">,</span>
                        <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape[2]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">seq_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">)))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span></div>


<div class="viewcode-block" id="DynamicGRUV2"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.DynamicGRUV2.html#mindspore.ops.DynamicGRUV2">[docs]</a><span class="k">class</span> <span class="nc">DynamicGRUV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a single-layer gated recurrent unit (GRU) to an input sequence.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll}</span>
<span class="sd">            r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\</span>
<span class="sd">            z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\</span>
<span class="sd">            n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\</span>
<span class="sd">            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the input</span>
<span class="sd">    at time `t`, :math:`h_{(t-1)}` is the hidden state of the layer</span>
<span class="sd">    at time `t-1` or the initial hidden state at time `0`, and :math:`r_t`,</span>
<span class="sd">    :math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.</span>
<span class="sd">    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.</span>

<span class="sd">    Args:</span>
<span class="sd">        direction (str): A string identifying the direction in the op. Default: &#39;UNIDIRECTIONAL&#39;.</span>
<span class="sd">            Only &#39;UNIDIRECTIONAL&#39; is currently supported.</span>
<span class="sd">        cell_depth (int): An integer identifying the cell depth in the op. Default: 1.</span>
<span class="sd">        keep_prob (float): A float identifying the keep prob in the op. Default: 1.0.</span>
<span class="sd">        cell_clip (float): A float identifying the cell clip in the op. Default: -1.0.</span>
<span class="sd">        num_proj (int): An integer identifying the num proj in the op. Default: 0.</span>
<span class="sd">        time_major (bool): A bool identifying the time major in the op. Default: True.</span>
<span class="sd">        activation (str) : A string identifying the type of activation function in the op. Default: &#39;tanh&#39;.</span>
<span class="sd">            Only &#39;tanh&#39; is currently supported.</span>
<span class="sd">        gate_order (str): A string identifying the gate order in weight and bias. Default: &#39;rzh.</span>
<span class="sd">            &#39;zrh&#39; is another option.</span>
<span class="sd">        reset_after (bool): A bool identifying whether to apply reset gate after matrix multiplication. Default: True.</span>
<span class="sd">        is_training (bool): A bool identifying is training in the op. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words.</span>
<span class="sd">          Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{input_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **weight_input** (Tensor) - Input-hidden weight.</span>
<span class="sd">          Tensor of shape :math:`(\text{input_size}, 3 \times \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **weight_hidden** (Tensor) - Hidden-hidden weight.</span>
<span class="sd">          Tensor of shape :math:`(\text{hidden_size}, 3 \times \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **bias_input** (Tensor) - Input-hidden bias. Tensor of shape :math:`(3 \times \text{hidden_size})`, or None.</span>
<span class="sd">          Has the same data type with input `init_h`.</span>
<span class="sd">        - **bias_hidden** (Tensor) - Hidden-hidden bias. Tensor of shape :math:`(3 \times \text{hidden_size})`,</span>
<span class="sd">          or None. Has the same data type with input `init_h`.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape :math:`(\text{batch_size})`.</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time.</span>
<span class="sd">          Tensor of shape :math:`(\text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape :math:</span>
<span class="sd">          if num_proj &gt; 0 `(num_step, batch_size, min(hidden_size, num_proj)`,</span>
<span class="sd">          if num_proj == 0 `(num_step, batch_size, hidden_size)`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **update** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **reset** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **new** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **hidden_new** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>

<span class="sd">        A note about the bias_type:</span>

<span class="sd">        - If `bias_input` and `bias_hidden` both are `None`, `bias_type` is date type of `init_h`.</span>
<span class="sd">        - If `bias_input` is not `None`, `bias_type` is the date type of `bias_input`.</span>
<span class="sd">        - If `bias_input` is `None` and `bias_hidden` is not `None, `bias_type` is the date type of `bias_hidden`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `direction`, `activation` or `gate_order` is not a str.</span>
<span class="sd">        TypeError: If `cell_depth` or `num_proj` is not an int.</span>
<span class="sd">        TypeError: If `keep_prob` or `cell_clip` is not a float.</span>
<span class="sd">        TypeError: If `time_major`, `reset_after` or `is_training` is not a bool.</span>
<span class="sd">        TypeError: If `x`, `weight_input`, `weight_hidden`, `bias_input`, `bias_hidden`, `seq_length` or `ini_h` is not</span>
<span class="sd">                   a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `weight_input` or `weight_hidden` is not float16.</span>
<span class="sd">        TypeError: If dtype of `init_h` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 8, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; weight_i = Tensor(np.random.rand(64, 48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; weight_h = Tensor(np.random.rand(16, 48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias_i = Tensor(np.random.rand(48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias_h = Tensor(np.random.rand(48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(8, 16).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_gru_v2 = ops.DynamicGRUV2()</span>
<span class="sd">        &gt;&gt;&gt; output = dynamic_gru_v2(x, weight_i, weight_h, bias_i, bias_h, None, init_h)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (2, 8, 16)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
                 <span class="n">gate_order</span><span class="o">=</span><span class="s2">&quot;rzh&quot;</span><span class="p">,</span>
                 <span class="n">reset_after</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">num_proj</span><span class="p">,</span> <span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_order</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">gate_order</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;zrh&#39;</span><span class="p">,</span> <span class="s1">&#39;rzh&#39;</span><span class="p">],</span> <span class="s2">&quot;gate_order&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_after</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reset_after&quot;</span><span class="p">,</span> <span class="n">reset_after</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">winput_shape</span><span class="p">,</span> <span class="n">whidden_shape</span><span class="p">,</span> <span class="n">binput_shape</span><span class="p">,</span> <span class="n">bhidden_shape</span><span class="p">,</span> <span class="n">seq_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">winput_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;weight input shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">whidden_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;weight hidden shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="k">if</span> <span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, weight_input_shape[-1] should multiple of 3.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">binput_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">binput_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;bias input shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;bias_input_shape&quot;</span><span class="p">,</span> <span class="n">binput_shape</span><span class="p">,</span> <span class="s2">&quot;3 * hidden_shape&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bhidden_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bhidden_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;bias hidden shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;bias_hidden_shape&quot;</span><span class="p">,</span> <span class="n">bhidden_shape</span><span class="p">,</span>
                            <span class="s2">&quot;3 * hidden_shape&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seq_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, seq_shape should be None.&quot;</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;init_h shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;init_h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;init_h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight_input_shape[-1]&quot;</span><span class="p">,</span> <span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;weight_hidden_shape[-1]&quot;</span><span class="p">,</span>
                        <span class="n">whidden_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight_input_shape[0]&quot;</span><span class="p">,</span> <span class="n">winput_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight_hidden_shape[0]&quot;</span><span class="p">,</span> <span class="n">whidden_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;placeholder_index&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">winput_dtype</span><span class="p">,</span> <span class="n">whidden_dtype</span><span class="p">,</span> <span class="n">binput_dtype</span><span class="p">,</span> <span class="n">bhidden_dtype</span><span class="p">,</span> <span class="n">seq_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x dtype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;weight input dtype&quot;</span><span class="p">,</span> <span class="n">winput_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;weight hidden dtype&quot;</span><span class="p">,</span> <span class="n">whidden_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;init_h dtype&quot;</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">b_dtype</span> <span class="o">=</span> <span class="n">h_dtype</span>
        <span class="k">if</span> <span class="n">binput_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;bias_input&#39;</span><span class="p">:</span> <span class="n">binput_dtype</span><span class="p">}</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">b_dtype</span> <span class="o">=</span> <span class="n">binput_dtype</span>
        <span class="k">if</span> <span class="n">bhidden_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;bias_hidden&#39;</span><span class="p">:</span> <span class="n">bhidden_dtype</span><span class="p">}</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">b_dtype</span> <span class="o">=</span> <span class="n">bhidden_dtype</span>

        <span class="k">return</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span></div>


<div class="viewcode-block" id="InTopK"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.InTopK.html#mindspore.ops.InTopK">[docs]</a><span class="k">class</span> <span class="nc">InTopK</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the targets are in the top `k` predictions.</span>

<span class="sd">    Args:</span>
<span class="sd">        k (int): Specifies the number of top elements to be used for computing precision.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - A 2D Tensor defines the predictions of a batch of samples with float16 or float32</span>
<span class="sd">          data type.</span>
<span class="sd">        - **x2** (Tensor) - A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of x2</span>
<span class="sd">          must be equal to x1&#39;s first dimension. The values of `x2` can not be negative and</span>
<span class="sd">          must be equal to or less than index of x1&#39;s second dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor has 1 dimension of type bool and the same shape with `x2`. For labeling sample `i` in `x2`,</span>
<span class="sd">        if the label in the first `k` predictions for sample `i` is in `x1`, then the value is True, otherwise False.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If `x1` or `x2` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x1` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; in_top_k = ops.InTopK(3)</span>
<span class="sd">        &gt;&gt;&gt; output = in_top_k(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InTopK&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_dtype</span><span class="p">,</span> <span class="n">x2_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="n">x1_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">,</span> <span class="n">x2_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x1 shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x2 shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;size of x2&quot;</span><span class="p">,</span> <span class="n">x2_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x1&#39;s first dimension&quot;</span><span class="p">,</span> <span class="n">x1_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x2_shape</span></div>


<div class="viewcode-block" id="LRN"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.LRN.html#mindspore.ops.LRN">[docs]</a><span class="k">class</span> <span class="nc">LRN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    .. math::</span>

<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D.</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0).</span>
<span class="sd">        alpha (float): A scale factor, usually positive.</span>
<span class="sd">        beta (float): An exponent.</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: &quot;ACROSS_CHANNELS&quot;. Default: &quot;ACROSS_CHANNELS&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4D Tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as the input tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `depth_radius` is not an int.</span>
<span class="sd">        TypeError: If `bias`, `alpha` or `beta` is not a float.</span>
<span class="sd">        TypeError: If `norm_region` is not a str.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(1, 2, 2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lrn = ops.LRN()</span>
<span class="sd">        &gt;&gt;&gt; output = lrn(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 2, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LRN&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="n">depth_radius</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;norm_region&quot;</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ACROSS_CHANNELS&#39;</span><span class="p">],</span> <span class="s1">&#39;norm_region&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">depth_radius</span><span class="p">,</span> <span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span></div>


<div class="viewcode-block" id="Conv3D"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Conv3D.html#mindspore.ops.Conv3D">[docs]</a><span class="k">class</span> <span class="nc">Conv3D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D convolution layer.</span>

<span class="sd">    Applies a 3D convolution over an input tensor which is typically of shape</span>
<span class="sd">    :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` and output shape</span>
<span class="sd">    :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. where :math:`N` is batch size. :math:`C` is channel number.</span>
<span class="sd">    the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \operatorname{out}\left(N_{i}, C_{\text {out}_j}\right)=\operatorname{bias}\left(C_{\text {out}_j}\right)+</span>
<span class="sd">        \sum_{k=0}^{C_{in}-1} ccor(\text {weight}\left(C_{\text {out}_j}, k\right),</span>
<span class="sd">        \operatorname{input}\left(N_{i}, k\right))</span>

<span class="sd">    where :math:`ccor` is the cross-correlation operator.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{D_{in} + 2 \times \text{padding} - \text{ks_d} -</span>
<span class="sd">    (\text{ks_d} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -</span>
<span class="sd">    (\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -</span>
<span class="sd">    (\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channels (int): The number of output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers. Specifies the depth,</span>
<span class="sd">            height and width of the 3D convolution window. Single int means the value is for the depth, height</span>
<span class="sd">            and the width of the kernel. A tuple of 3 ints means the first value is for the depth, height and</span>
<span class="sd">            the other is for the width of the kernel.</span>
<span class="sd">        mode (int): Modes for different convolutions. Not currently used.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in depth, horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to head and tail, top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height, width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six</span>
<span class="sd">                    integers, the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2],</span>
<span class="sd">                    pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers</span>
<span class="sd">                                      : math:`(dilation_d, dilation_h, dilation_w)`.</span>
<span class="sd">                                      Currently, dilation on depth only supports the case of 1.</span>
<span class="sd">                                      Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">                                      If set to be :math:`k &gt; 1`, there will be :math:`k - 1` pixels skipped</span>
<span class="sd">                                      for each sampling location. Its value must be greater or equal to 1 and</span>
<span class="sd">                                      bounded by the height and width of the input. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_ channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &quot;NCDHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently input data type only support float16 and float32.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(k_d, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}//groups, k_d, K_h, K_w)`.</span>
<span class="sd">          Currently weight data type only support float16 and float32.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`C_{in}`. Currently, only support none.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 3D convolution. The shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones([16, 3, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 3, 4, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = P.Conv3D(out_channel=32, kernel_size=(4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(input_tensor, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 7, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                               <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `conv3d` attr &#39;pad&#39; should be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got `</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, when pad is not 0, pad_mode should be set as &#39;pad&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">b_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Bias currently only support None.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x_shape[1] // group&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;w_shape[0]&#39;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[1:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_size_d</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">kernel_size_h</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

        <span class="n">stride_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">stride_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

        <span class="n">dilation_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dilation_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_d</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
            <span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>

            <span class="n">pad_needed_d</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">d_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_d</span> <span class="o">+</span> <span class="n">dilation_d</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_head</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_d</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_tail</span> <span class="o">=</span> <span class="n">pad_needed_d</span> <span class="o">-</span> <span class="n">pad_head</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_head</span> <span class="o">+</span> <span class="n">pad_tail</span> <span class="o">-</span> <span class="n">kernel_size_d</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                         <span class="o">*</span> <span class="p">(</span><span class="n">dilation_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_d</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                         <span class="o">*</span> <span class="p">(</span><span class="n">dilation_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                         <span class="o">*</span> <span class="p">(</span><span class="n">dilation_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">d_out</span><span class="p">)</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">]</span>
        <span class="n">filter_d</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dilation_d</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">filter_h</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dilation_h</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">filter_w</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dilation_w</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">filter_d</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;pad_d belonging [0, filter_d)&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">filter_d</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;pad_d belonging [0, filter_d)&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">filter_h</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;pad_h belonging [0, filter_h)&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">filter_h</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;pad_h belonging [0, filter_h)&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;pad_w belonging [0, filter_w)&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">filter_w</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;pad_w belonging [0, filter_w)&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">))</span>
        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="n">_check_shape</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<span class="k">class</span> <span class="nc">Conv3DBackpropInput</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of convolution 3D with respect to the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimension of the output.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 3D convolution.</span>
<span class="sd">        mode (int): Modes for different convolutions. Not currently used.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four</span>
<span class="sd">                    integers, the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2],</span>
<span class="sd">                    pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(D_in, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, D_{in}, K_h, K_w)`. Currently weight data type only support float16 and float32.</span>
<span class="sd">        - **dout** (Tensor) - the gradients w.r.t the output of the convolution. The shape conforms to the default</span>
<span class="sd">          data_format :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. Currently dout data type only support float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **input_size** (tuple(int)) - A tuple describes the shape of the input which conforms to the format</span>
<span class="sd">          :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients w.r.t the input of convolution 3D. It has the same shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([16, 32, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 32, 13, 37, 33]))</span>
<span class="sd">        &gt;&gt;&gt; conv3d_backprop_input = P.Conv3DBackpropInput(out_channel=4, kernel_size=(4, 6, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_backprop_input(dout, weight, F.shape(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;input_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, when pad is not 0, pad_mode should be set as &#39;pad&#39;.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">doutput</span><span class="p">,</span> <span class="n">x_size</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]),</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;The dimension of weight &#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]),</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;The dimension of dout&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_size_v</span> <span class="o">=</span> <span class="n">x_size</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_size_v</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;The dimension of input_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;x_size&#39;</span><span class="p">,</span> <span class="n">x_size_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_size_v</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;x_size[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">dim_len</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;doutput&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;filter&#39;s batch&quot;</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;dout&#39;s channel&quot;</span><span class="p">,</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;filter&#39;s channel&quot;</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;input_size&#39;s channel&quot;</span><span class="p">,</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;input_size&#39;s batch&quot;</span><span class="p">,</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;dout&#39;s batch&quot;</span><span class="p">,</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># infer shape</span>
        <span class="n">dout_shape</span> <span class="o">=</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">kernel_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">kernel_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">stride_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">stride_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
        <span class="n">dilation_d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">dilation_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
        <span class="c1"># The pad_mode is valid by default. If pad_mode is not valid or same, then pad.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">pad_needed_d</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_d</span> <span class="o">+</span> <span class="n">dilation_d</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_head</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_d</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_tail</span> <span class="o">=</span> <span class="n">pad_needed_d</span> <span class="o">-</span> <span class="n">pad_head</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">dout_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_size_v</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_size_v</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">doutput</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">_deconv_output_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride_size</span><span class="p">,</span> <span class="n">dilation_size</span><span class="p">):</span>
    <span class="n">filter_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span> <span class="o">+</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span>
    <span class="k">return</span> <span class="n">length</span>


<div class="viewcode-block" id="Conv3DTranspose"><a class="viewcode-back" href="../../../../mindspore/ops/mindspore.ops.Conv3DTranspose.html#mindspore.ops.Conv3DTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv3DTranspose</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Input is typically of shape :math:`(N, C, D, H, W)`, where :math:`N` is batch size and :math:`C` is channel number.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;pad&quot;, the height and width of output are defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        D_{out} = (D_{in} - 1) \times \text{stride_d} - 2 \times \text{padding_d} + \text{dilation_d} \times</span>
<span class="sd">        (\text{kernel_size_d} - 1) + \text{output_padding_d} + 1</span>

<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride_h} - 2 \times \text{padding_h} + \text{dilation_h} \times</span>
<span class="sd">        (\text{kernel_size_h} - 1) + \text{output_padding_h} + 1</span>

<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride_w} - 2 \times \text{padding_w} + \text{dilation_w} \times</span>
<span class="sd">        (\text{kernel_size_w} - 1) + \text{output_padding_w} + 1</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channel (int): The channel of the input x.</span>
<span class="sd">        out_channel (int): The channel of the weight x.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 3D convolution.</span>
<span class="sd">        mode (int): Modes for different convolutions. Default is 1. Not currently used.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in depth, horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to head and tail, top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` and `output_padding` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              and `output_padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height, width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">             head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six integers,</span>
<span class="sd">             the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2], pad[3], pad[4]</span>
<span class="sd">             and pad[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        output_padding (Union(int, tuple[int])): Add extra size to each dimension of the output. Default: 0.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dout** (Tensor) - the gradients w.r.t the output of the convolution. The shape conforms to the default</span>
<span class="sd">          data_format :math:`(N, C_{in}, D_{out}, H_{out}, W_{out})`. Currently dout data type only support float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(k_d, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{in}, C_{out}//groups, k_d, K_h, K_w)`. Currently weight data type only support float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`C_{out}`. Currently, only support none.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients w.r.t the input of convolution 3D. It has the same shape as the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channel`, `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` , `dilation` or `output_padding` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `in_channel`, `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        TypeError: If dout and weight data type not float16.</span>
<span class="sd">        ValueError: If bias not none. The rank of dout and weight is not 5.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([16, 3, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; conv3d_transpose = P.Conv3DTranspose(in_channel=16, out_channel=3, kernel_size=(4, 6, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(input_x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channel</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3DTranspose&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">in_channel</span><span class="p">,</span> <span class="s1">&#39;in_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;in_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                             <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                               <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `Conv3DTranspose` attr &#39;pad&#39; should be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got `</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, when pad is not 0, pad_mode should be set as &#39;pad&#39;.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">output_padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, when output_padding is not 0, pad_mode should be set as &#39;pad&#39;.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">343</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of kernel_size belonging [1, 343]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">343</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of stride belonging [1, 343]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of stride belonging [1, 256]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_d belonging [0, max(stride_d, dilation_d))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_h belonging [0, max(stride_h,dilation_h))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_w belonging [0, max(stride_w,dilation_w))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]}</span>
        <span class="k">if</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Bias currently only support None.&quot;</span><span class="p">)</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># infer shape</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">w_shape</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;filter&#39;s batch&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input x&#39;s channel&quot;</span><span class="p">,</span>
                        <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_d</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dilation_d</span><span class="p">,</span> <span class="n">dilation_h</span><span class="p">,</span> <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">kernel_d</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span> <span class="n">dilation_d</span><span class="p">)</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">dilation_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">kernel_w</span><span class="p">,</span> <span class="n">stride_w</span><span class="p">,</span> <span class="n">dilation_w</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_d</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">stride_w</span>

            <span class="n">pad_needed_d</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_d</span> <span class="o">+</span> <span class="n">dilation_d</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">d_out</span><span class="p">)</span>
            <span class="n">pad_head</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_d</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_tail</span> <span class="o">=</span> <span class="n">pad_needed_d</span> <span class="o">-</span> <span class="n">pad_head</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">h_out</span><span class="p">)</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">w_out</span><span class="p">)</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_head</span><span class="p">,</span> <span class="n">pad_tail</span><span class="p">,</span> <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span>
            <span class="n">d_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">pad_head</span> <span class="o">+</span> <span class="n">pad_tail</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> \
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;input_size&#39;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">output_shape</span><span class="p">,</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>