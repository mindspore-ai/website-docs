<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.GradOperation &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.ops.HyperMap" href="mindspore.ops.HyperMap.html" />
    <link rel="prev" title="mindspore.ops.gamma" href="mindspore.ops.gamma.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#operations">operations</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.ops.html#composite">composite</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.batch_dot.html">mindspore.ops.batch_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.clip_by_global_norm.html">mindspore.ops.clip_by_global_norm</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.clip_by_value.html">mindspore.ops.clip_by_value</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.core.html">mindspore.ops.core</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.count_nonzero.html">mindspore.ops.count_nonzero</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.dot.html">mindspore.ops.dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.gamma.html">mindspore.ops.gamma</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.ops.GradOperation</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.HyperMap.html">mindspore.ops.HyperMap</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.laplace.html">mindspore.ops.laplace</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.matmul.html">mindspore.ops.matmul</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.multinomial.html">mindspore.ops.multinomial</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.MultitypeFuncGraph.html">mindspore.ops.MultitypeFuncGraph</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.normal.html">mindspore.ops.normal</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.poisson.html">mindspore.ops.poisson</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.repeat_elements.html">mindspore.ops.repeat_elements</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.sequence_mask.html">mindspore.ops.sequence_mask</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.tensor_dot.html">mindspore.ops.tensor_dot</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ops.uniform.html">mindspore.ops.uniform</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#functional">functional</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#primitive">primitive</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#vm-impl-registry">vm_impl_registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#op-info-register">op_info_register</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindArmour Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.html">mindarmour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.adv_robustness.attacks.html">mindarmour.adv_robustness.attacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.adv_robustness.defenses.html">mindarmour.adv_robustness.defenses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.adv_robustness.detectors.html">mindarmour.adv_robustness.detectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.adv_robustness.evaluations.html">mindarmour.adv_robustness.evaluations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.fuzz_testing.html">mindarmour.fuzz_testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.privacy.diff_privacy.html">mindarmour.privacy.diff_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.privacy.evaluation.html">mindarmour.privacy.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.privacy.sup_privacy.html">mindarmour.privacy.sup_privacy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mindarmour/mindarmour.utils.html">mindarmour.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Hub Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mindspore_hub/mindspore_hub.html">mindspore_hub</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore Serving Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mindspore_serving/mindspore_serving.html">mindspore_serving</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindQuantum Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mindquantum/mindquantum.html">mindquantum</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.ops.html">mindspore.ops</a> &raquo;</li>
      <li>mindspore.ops.GradOperation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/mindspore/ops/mindspore.ops.GradOperation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-ops-gradoperation">
<h1>mindspore.ops.GradOperation<a class="headerlink" href="#mindspore-ops-gradoperation" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.ops.GradOperation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.ops.</span></span><span class="sig-name descname"><span class="pre">GradOperation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">get_all</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">get_by_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens_param</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/ops/composite/base.html#GradOperation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.ops.GradOperation" title="Permalink to this definition"></a></dt>
<dd><p>A higher-order function which is used to generate the gradient function for the input function.</p>
<p>The gradient function generated by <cite>GradOperation</cite> higher-order function can be customized by
construction arguments.</p>
<p>Given an input function <cite>net = Net()</cite> that takes <cite>x</cite> and <cite>y</cite> as inputs, and has a parameter <cite>z</cite>,
see <cite>Net</cite> in Examples.</p>
<p>To generate a gradient function that returns gradients with respect to the first input
(see <cite>GradNetWrtX</cite> in Examples).</p>
<ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with default arguments:
<cite>grad_op = GradOperation()</cite>.</p></li>
<li><p>Call it with input function as argument to get the gradient function: <cite>gradient_function = grad_op(net)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs to get the gradients with respect to the first input:
<cite>grad_op(net)(x, y)</cite>.</p></li>
</ol>
<p>To generate a gradient function that returns gradients with respect to all inputs (see <cite>GradNetWrtXY</cite> in Examples).</p>
<ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_all=True</cite> which
indicates getting gradients with respect to all inputs, they are <cite>x</cite> and <cite>y</cite> in example function <cite>Net()</cite>:
<cite>grad_op = GradOperation(get_all=True)</cite>.</p></li>
<li><p>Call it with input function as argument to get the gradient function: <cite>gradient_function = grad_op(net)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs to get the gradients with respect to all inputs:
<cite>gradient_function(x, y)</cite>.</p></li>
</ol>
<p>To generate a gradient function that returns gradients with respect to given parameters
(see <cite>GradNetWithWrtParams</cite> in Examples).</p>
<ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_by_list=True</cite>:
<cite>grad_op = GradOperation(get_by_list=True)</cite>.</p></li>
<li><p>Construct a <cite>ParameterTuple</cite> that will be passed to the input function when constructing
<cite>GradOperation</cite> higher-order function, it will be used as a parameter filter that determine
which gradient to return: <cite>params = ParameterTuple(net.trainable_params())</cite>.</p></li>
<li><p>Call it with input function and <cite>params</cite> as arguments to get the gradient function:
<cite>gradient_function = grad_op(net, params)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs to get the gradients with
respect to given parameters: <cite>gradient_function(x, y)</cite>.</p></li>
</ol>
<p>To generate a gradient function that returns gradients with respect to all inputs and given parameters
in the format of ((dx, dy), (dz))(see <cite>GradNetWrtInputsAndParams</cite> in Examples).</p>
<ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_all=True</cite> and <cite>get_by_list=True</cite>:
<cite>grad_op = GradOperation(get_all=True, get_by_list=True)</cite>.</p></li>
<li><p>Construct a <cite>ParameterTuple</cite> that will be passed along input function when constructing
<cite>GradOperation</cite> higher-order function: <cite>params = ParameterTuple(net.trainable_params())</cite>.</p></li>
<li><p>Call it with input function and <cite>params</cite> as arguments to get the gradient function:
<cite>gradient_function = grad_op(net, params)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs
to get the gradients with respect to all inputs and given parameters: <cite>gradient_function(x, y)</cite>.</p></li>
</ol>
<p>We can configure the sensitivity(gradient with respect to output) by setting <cite>sens_param</cite> as True and
passing an extra sensitivity input to the gradient function, the sensitivity input should has the
same shape and type with input function’s output(see <cite>GradNetWrtXYWithSensParam</cite> in Examples).</p>
<ol class="arabic simple">
<li><p>Construct a <cite>GradOperation</cite> higher-order function with <cite>get_all=True</cite> and <cite>sens_param=True</cite>:
<cite>grad_op = GradOperation(get_all=True, sens_param=True)</cite>.</p></li>
<li><p>Define <cite>grad_wrt_output</cite> as <cite>sens_param</cite> which works as the gradient with respect to output:
<cite>grad_wrt_output = Tensor(np.ones([2, 2]).astype(np.float32))</cite>.</p></li>
<li><p>Call it with input function as argument to get the gradient function:
<cite>gradient_function = grad_op(net)</cite>.</p></li>
<li><p>Call the gradient function with input function’s inputs and <cite>sens_param</cite> to
get the gradients with respect to all inputs:
<cite>gradient_function(x, y, grad_wrt_output)</cite>.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>get_all</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, get all the gradients with respect to inputs. Default: False.</p></li>
<li><p><strong>get_by_list</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – If True, get all the gradients with respect to Parameter variables.
If get_all and get_by_list are both False, get the gradient with respect to first input.
If get_all and get_by_list are both True, get the gradients with respect to inputs and Parameter variables
at the same time in the form of ((gradients with respect to inputs),
(gradients with respect to parameters)). Default: False.</p></li>
<li><p><strong>sens_param</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to append sensitivity (gradient with respect to output) as input.
If sens_param is False, a ‘ones_like(outputs)’ sensitivity will be attached automatically.
Default: False.
If sensor_param is True, a sensitivity (gradient with respect to output) needs to be transferred through
the location parameter or key-value pair parameter. If the value is transferred through the key-value pair
parameter, the key must be sens.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The higher-order function which takes a function as argument and returns gradient function for it.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>get_all</cite>, <cite>get_by_list</cite> or <cite>sens_param</cite> is not a bool.</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">ParameterTuple</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWrtX</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">[[1.4100001 1.5999999 6.6      ]</span>
<span class="go"> [1.4100001 1.5999999 6.6      ]]</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtXY</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtXY</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWrtXY</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="go">[[ 4.50999975e+00,  2.70000005e+00,  3.60000014e+00],</span>
<span class="go"> [ 4.50999975e+00,  2.70000005e+00,  3.60000014e+00]]), Tensor(shape=[3, 3], dtype=Float32, value=</span>
<span class="go">[[ 2.59999990e+00,  2.59999990e+00,  2.59999990e+00],</span>
<span class="go"> [ 1.89999998e+00,  1.89999998e+00,  1.89999998e+00],</span>
<span class="go"> [ 1.30000007e+00,  1.30000007e+00,  1.30000007e+00]]))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtXYWithSensParam</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtXYWithSensParam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_wrt_output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_wrt_output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWrtXYWithSensParam</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="go">[[ 2.21099997e+00,  5.09999990e-01,  1.49000001e+00],</span>
<span class="go"> [ 5.58799982e+00,  2.68000007e+00,  4.07000017e+00]]), Tensor(shape=[3, 3], dtype=Float32, value=</span>
<span class="go">[[ 1.51999998e+00,  2.81999993e+00,  2.14000010e+00],</span>
<span class="go"> [ 1.09999990e+00,  2.04999971e+00,  1.54999995e+00],</span>
<span class="go"> [ 9.00000036e-01,  1.54999995e+00,  1.25000000e+00]]))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWithWrtParams</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWithWrtParams</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWithWrtParams</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">(Tensor(shape=[1], dtype=Float32, value= [ 2.15359993e+01]),)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">GradNetWrtInputsAndParams</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtInputsAndParams</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span> <span class="o">=</span> <span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">gradient_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.12</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWrtInputsAndParams</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">((Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="go">[[ 3.51999998e+00,  3.90000010e+00,  2.59999990e+00],</span>
<span class="go"> [ 3.51999998e+00,  3.90000010e+00,  2.59999990e+00]]), Tensor(shape=[3, 3], dtype=Float32, value=</span>
<span class="go">[[ 6.00000024e-01,  6.00000024e-01,  6.00000024e-01],</span>
<span class="go"> [ 1.89999998e+00,  1.89999998e+00,  1.89999998e+00],</span>
<span class="go"> [ 1.30000007e+00,  1.30000007e+00,  1.30000007e+00]])), (Tensor(shape=[1], dtype=Float32, value=</span>
<span class="go"> [ 1.29020004e+01]),))</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.ops.gamma.html" class="btn btn-neutral float-left" title="mindspore.ops.gamma" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.ops.HyperMap.html" class="btn btn-neutral float-right" title="mindspore.ops.HyperMap" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>