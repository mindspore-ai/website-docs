<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Backend Running &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Migration from a Third-party Framework" href="usage_migrate_3rd.html" />
    <link rel="prev" title="Platform and System" href="platform_and_system.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="supported_operators.html">Supported Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_models.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform_and_system.html">Platform and System</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Backend Running</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_language_extensions.html">Programming Language Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="supported_features.html">Supported Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_use.html">MindInsight use</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_lite.html">MindSpore Lite Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_cpp_library.html">MindSpore C++ Library Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_serving.html">MindSpore Serving Class</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Backend Running</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/backend_running.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="backend-running">
<h1>Backend Running<a class="headerlink" href="#backend-running" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Environmental</span> <span class="pre">Setup</span></code> <code class="docutils literal notranslate"><span class="pre">Operation</span> <span class="pre">Mode</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Training</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.2/docs/faq/source_en/backend_running.md" target="_blank"><img src="./_static/logo_source.png"></a></p>
<p><font size=3><strong>Q: What is the difference between <code class="docutils literal notranslate"><span class="pre">c_transforms</span></code> and <code class="docutils literal notranslate"><span class="pre">py_transforms</span></code>? Which one is recommended?</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">c_transforms</span></code> is recommended. Its performance is better because it is executed only at the C layer.</p>
<p>Principle: The underlying layer of <code class="docutils literal notranslate"><span class="pre">c_transform</span></code> uses <code class="docutils literal notranslate"><span class="pre">opencv/jpeg-turbo</span></code> of the C version for data processing, and <code class="docutils literal notranslate"><span class="pre">py_transform</span></code> uses <code class="docutils literal notranslate"><span class="pre">Pillow</span></code> of the Python version for data processing.</p>
<br/>
<p><font size=3><strong>Q: When MindSpore performs multi-device training on the NPU hardware platform, how does the user-defined dataset transfer data to different NPUs?</strong></font></p>
<p>A: When <code class="docutils literal notranslate"><span class="pre">GeneratorDataset</span></code> is used, the <code class="docutils literal notranslate"><span class="pre">num_shards=num_shards</span></code> and <code class="docutils literal notranslate"><span class="pre">shard_id=device_id</span></code> parameters can be used to control which shard of data is read by different devices. <code class="docutils literal notranslate"><span class="pre">__getitem__</span></code> and <code class="docutils literal notranslate"><span class="pre">__len__</span></code> are processed as full datasets.</p>
<p>An example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Device 0:</span>
<span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="c1"># Device 1:</span>
<span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="c1"># Device 2:</span>
<span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="o">...</span>
<span class="c1"># Device 7:</span>
<span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">num_shards</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shard_id</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I view the number of model parameters?</strong></font></p>
<p>A: You can load the checkpoint to count the parameter number. Variables in the momentum and optimizer may be counted, so you need to filter them out.
You can refer to the following APIs to collect the number of network parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span><span class="n">net</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Count number of parameters in the network</span>
<span class="sd">    Args:</span>
<span class="sd">        net (mindspore.nn.Cell): Mindspore network instance</span>
<span class="sd">    Returns:</span>
<span class="sd">        total_params (int): Total number of trainable params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
        <span class="n">total_params</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_params</span>
</pre></div>
</div>
<p><a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.2/model_zoo/research/cv/tinynet/src/utils.py">Script Link</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I build a multi-label MindRecord dataset for images?</strong></font></p>
<p>A: The data schema can be defined as follows:<code class="docutils literal notranslate"><span class="pre">cv_schema_json</span> <span class="pre">=</span> <span class="pre">{&quot;label&quot;:</span> <span class="pre">{&quot;type&quot;:</span> <span class="pre">&quot;int32&quot;,</span> <span class="pre">&quot;shape&quot;:</span> <span class="pre">[-1]},</span> <span class="pre">&quot;data&quot;:</span> <span class="pre">{&quot;type&quot;:</span> <span class="pre">&quot;bytes&quot;}}</span></code></p>
<p>Note: A label is an array of the numpy type, where label values 1, 1, 0, 1, 0, 1 are stored. These label values correspond to the same data, that is, the binary value of the same image.
For details, see <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.2/advanced_use/convert_dataset.html#id3">Converting Dataset to MindRecord</a>.</p>
<br/>
<p><font size=3><strong>Q: How do I monitor the loss during training and save the training parameters when the <code class="docutils literal notranslate"><span class="pre">loss</span></code> is the lowest?</strong></font></p>
<p>A: You can customize a <code class="docutils literal notranslate"><span class="pre">callback</span></code>.For details, see the writing method of <code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code>. In addition, the logic for determining loss is added.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EarlyStop</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
     <span class="n">loss</span> <span class="o">=</span>  <span class="o">****</span><span class="p">(</span><span class="n">get</span> <span class="n">current</span> <span class="n">loss</span><span class="p">)</span>
     <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">):</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
         <span class="c1"># do save ckpt</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How do I execute a single <code class="docutils literal notranslate"><span class="pre">ut</span></code> case in <code class="docutils literal notranslate"><span class="pre">mindspore/tests</span></code>?</strong></font></p>
<p>A: <code class="docutils literal notranslate"><span class="pre">ut</span></code> cases are usually based on the MindSpore package of the debug version, which is not provided on the official website. You can run <code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span></code> to compile the source code and then run the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> command. The compilation in debug mode does not depend on the backend. Run the <code class="docutils literal notranslate"><span class="pre">sh</span> <span class="pre">build.sh</span> <span class="pre">-t</span> <span class="pre">on</span></code> command. For details about how to execute cases, see the <code class="docutils literal notranslate"><span class="pre">tests/runtest.sh</span></code> script.</p>
<br/>
<p><font size=3><strong>Q: How do I obtain the expected <code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">map</span></code> when <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> is used?</strong></font></p>
<p>A: For details about how to derive the <code class="docutils literal notranslate"><span class="pre">Conv2d</span> <span class="pre">shape</span></code>, click <a class="reference external" href="https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d.">here</a> Change <code class="docutils literal notranslate"><span class="pre">pad_mode</span></code> of <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> to <code class="docutils literal notranslate"><span class="pre">same</span></code>. Alternatively, you can calculate the <code class="docutils literal notranslate"><span class="pre">pad</span></code> based on the Conv2d shape derivation formula to keep the <code class="docutils literal notranslate"><span class="pre">shape</span></code> unchanged. Generally, the pad is <code class="docutils literal notranslate"><span class="pre">(kernel_size-1)//2</span></code>.</p>
<br/>
<p><font size=3><strong>Q: What can I do if the network performance is abnormal and weight initialization takes a long time during training after MindSpore is installed?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">SciPy</span> <span class="pre">1.4</span></code> series versions may be used in the environment. Run the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">list</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">scipy</span></code> command to view the <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> version and change the <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> version to that required by MindSpore. You can view the third-party library dependency in the <code class="docutils literal notranslate"><span class="pre">requirement.txt</span></code> file.
<a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/%7Bversion%7D/requirements.txt">https://gitee.com/mindspore/mindspore/blob/{version}/requirements.txt</a></p>
<blockquote>
<div><p>Replace version with the specific version branch of MindSpore.</p>
</div></blockquote>
<br/>
<p><font size=3><strong>Q: Can MindSpore be used to customize a loss function that can return multiple values?</strong></font></p>
<p>A: After customizing the <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">function</span></code>, you need to customize <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>. The number of <code class="docutils literal notranslate"><span class="pre">sens</span></code> for implementing gradient calculation is the same as the number of <code class="docutils literal notranslate"><span class="pre">network</span></code> outputs. For details, see the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>

<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">MyLoss</span><span class="p">()</span>

<span class="n">loss_with_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>

<span class="n">train_net</span> <span class="o">=</span> <span class="n">MyTrainOneStepCell</span><span class="p">(</span><span class="n">loss_with_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="o">=</span><span class="n">train_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: How does MindSpore implement the early stopping function?</strong></font></p>
<p>A: You can customize the <code class="docutils literal notranslate"><span class="pre">callback</span></code> method to implement the early stopping function.
Example: When the loss value decreases to a certain value, the training stops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EarlyStop</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EarlyStep</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span> <span class="o">=</span> <span class="n">control_loss</span>

    <span class="k">def</span> <span class="nf">step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">net_outputs</span>
        <span class="k">if</span> <span class="n">loss</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_control_loss</span><span class="p">:</span>
            <span class="c1"># Stop training.</span>
            <span class="n">run_context</span><span class="o">.</span><span class="n">_stop_requested</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">stop_cb</span> <span class="o">=</span> <span class="n">EarlyStop</span><span class="p">(</span><span class="n">control_loss</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">ds_train</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">stop_cb</span><span class="p">])</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: What can I do if an error message <code class="docutils literal notranslate"><span class="pre">wrong</span> <span class="pre">shape</span> <span class="pre">of</span> <span class="pre">image</span></code> is displayed when I use a model trained by MindSpore to perform prediction on a <code class="docutils literal notranslate"><span class="pre">28</span> <span class="pre">x</span> <span class="pre">28</span></code> digital image with white text on a black background?</strong></font></p>
<p>A: The MNIST gray scale image dataset is used for MindSpore training. Therefore, when the model is used, the data must be set to a <code class="docutils literal notranslate"><span class="pre">28</span> <span class="pre">x</span> <span class="pre">28</span></code> gray scale image, that is, a single channel.</p>
<br/>
<p><font size=3><strong>Q: What can I do if the error message <code class="docutils literal notranslate"><span class="pre">device</span> <span class="pre">target</span> <span class="pre">[CPU]</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">supported</span> <span class="pre">in</span> <span class="pre">pynative</span> <span class="pre">mode</span></code> is displayed for the operation operator of MindSpore?</strong></font></p>
<p>A: Currently, the PyNative mode supports only Ascend and GPU and does not support the CPU.</p>
<br/>
<p><font size=3><strong>Q: For Ascend users, how to get more detailed logs when the <code class="docutils literal notranslate"><span class="pre">run</span> <span class="pre">task</span> <span class="pre">error</span></code> is reported?</strong></font></p>
<p>A: Use the msnpureport tool to set the on-device log level. The tool is stored in <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend/driver/tools/msnpureport</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Global:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-g<span class="w"> </span>info
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Module-level:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-m<span class="w"> </span>SLOG:error
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Event-level:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-e<span class="w"> </span>disable/enable
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>-<span class="w"> </span>Multi-device<span class="w"> </span>ID-level:<span class="w"> </span>/usr/local/Ascend/driver/tools/msnpureport<span class="w"> </span>-d<span class="w"> </span><span class="m">1</span><span class="w"> </span>-g<span class="w"> </span>warning
</pre></div>
</div>
<p>Assume that the value range of deviceID is [0, 7], and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">0‚Äì3</span></code> and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4‚Äì7</span></code> are on the same OS. <code class="docutils literal notranslate"><span class="pre">Devices</span> <span class="pre">0‚Äì3</span></code> share the same log configuration file and <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4‚Äì7</span></code> share the same configuration file. In this way, changing the log level of any device (for example device 0) will change that of other devices (for example <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">1‚Äì3</span></code>). This rule also applies to <code class="docutils literal notranslate"><span class="pre">devices</span> <span class="pre">4‚Äì7</span></code>.</p>
<p>After the driver package is installed (assuming that the installation path is /usr/local/HiAI and the execution file <code class="docutils literal notranslate"><span class="pre">msnpureport.exe</span></code> is in the C:\ProgramFiles\Huawei\Ascend\Driver\tools\ directory on Windows), run the command in the /home/shihangbo/ directory to export logs on the device to the current directory and store logs in a folder named after the timestamp.</p>
<br/>
<p><font size=3><strong>Q: What can I do if the error message <code class="docutils literal notranslate"><span class="pre">Pynative</span> <span class="pre">run</span> <span class="pre">op</span> <span class="pre">ExpandDims</span> <span class="pre">failed</span></code> is displayed when the ExpandDims operator is used? The code is as follows:</strong></font></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span>
<span class="n">mode</span><span class="o">=</span><span class="n">cintext</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span>
<span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;ascend&#39;</span><span class="p">)</span>
<span class="n">input_tensor</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]]),</span><span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">expand_dims</span><span class="o">=</span><span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">output</span><span class="o">=</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>A: The problem is that the Graph mode is selected but the PyNative mode is used. As a result, an error is reported. MindSpore supports the following running modes which are optimized in terms of debugging or running:</p>
<ul class="simple">
<li><p>PyNative mode: dynamic graph mode. In this mode, operators in the neural network are delivered and executed one by one, facilitating the compilation and debugging of the neural network model.</p></li>
<li><p>Graph mode: static graph mode. In this mode, the neural network model is compiled into an entire graph and then delivered for execution. This mode uses technologies such as graph optimization to improve the running performance and facilitates large-scale deployment and cross-platform running.</p></li>
</ul>
<p>You can select a proper mode and writing method to complete the training by referring to the official website <a class="reference external" href="https://www.mindspore.cn/tutorial/training/en/r1.2/advanced_use/debug_in_pynative_mode.html">tutorial</a>.</p>
<br/>
<p><font size=3><strong>Q: How to fix the error below when running MindSpore distributed training with GPU:</strong></font></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Loading libgpu_collective.so failed. Many reasons could cause this:
1.libgpu_collective.so is not installed.
2.nccl is not installed or found.
3.mpi is not installed or found
</pre></div>
</div>
<p>A: This message means that MindSpore failed to load library <code class="docutils literal notranslate"><span class="pre">libgpu_collective.so</span></code>. The Possible causes are:</p>
<ul class="simple">
<li><p>OpenMPI or NCCL is not installed in this environment.</p></li>
<li><p>NCCL version is not updated to <code class="docutils literal notranslate"><span class="pre">v2.7.6</span></code>: MindSpore <code class="docutils literal notranslate"><span class="pre">v1.1.0</span></code> supports GPU P2P communication operator which relies on NCCL <code class="docutils literal notranslate"><span class="pre">v2.7.6</span></code>. <code class="docutils literal notranslate"><span class="pre">libgpu_collective.so</span></code> can‚Äôt be loaded successfully if NCCL is not updated to this version.</p></li>
</ul>
<br/>
<p><font size=3><strong>QÔºöHow to set environment variable <code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code> when using GPU version of MindSpore</strong></font></p>
<p>AÔºöNormally, GPU version of MindSpore doesn‚Äôt need to set <code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code>. MindSpore automatically chooses visible GPU devices according to the cuda environment variable <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code>. After setting <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code>, <code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code> refers to the ordinal of the GPU device:</p>
<ul class="simple">
<li><p>After <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">CUDA_VISIBLE_DEVICES=1,3,5</span></code>, <code class="docutils literal notranslate"><span class="pre">DEVICE_ID</span></code> should be exported as <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">2</span></code>. If <code class="docutils literal notranslate"><span class="pre">3</span></code> is exported, MindSpore will fail to execute because of the invalid device ordinal.</p></li>
</ul>
<br/>
<p><font size=3><strong>QÔºöWhy does context.set_ps_context(enable_ps=True) in model_zoo/official/cv/resnet/train.py in the MindSpore code have to be set before init?</strong></font></p>
<p>AÔºöIn MindSpore Ascend mode, if init is called first, then all processes will be allocated cards, but in parameter server training mode, the server does not need to allocate cards, then the worker and server will use the same card, resulting in an error: Hccl dependent tsd is not open.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="platform_and_system.html" class="btn btn-neutral float-left" title="Platform and System" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="usage_migrate_3rd.html" class="btn btn-neutral float-right" title="Migration from a Third-party Framework" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>