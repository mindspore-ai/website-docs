<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Supported Operators &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Network Models" href="network_models.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Supported Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_models.html">Network Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform_and_system.html">Platform and System</a></li>
<li class="toctree-l1"><a class="reference internal" href="backend_running.html">Backend Running</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_language_extensions.html">Programming Language Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="supported_features.html">Supported Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindinsight_use.html">MindInsight use</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_lite.html">MindSpore Lite Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_cpp_library.html">MindSpore C++ Library Use</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_serving.html">MindSpore Serving Class</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Supported Operators</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/supported_operators.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="supported-operators">
<h1>Supported Operators<a class="headerlink" href="#supported-operators" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code> <code class="docutils literal notranslate"><span class="pre">Environmental</span> <span class="pre">Setup</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.2/docs/faq/source_en/supported_operators.md" target="_blank"><img src="./_static/logo_source.png"></a></p>
<p><font size=3><strong>Q: What is the function of the <code class="docutils literal notranslate"><span class="pre">TransData</span></code> operator? Can the performance be optimized?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">TransData</span></code> operator is used in the scenario where the data formats (such as NC1HWC0) used by interconnected operators on the network are inconsistent. In this case, the framework automatically inserts the <code class="docutils literal notranslate"><span class="pre">TransData</span></code> operator to convert the data formats into the same format and then performs computation. You can consider using the <code class="docutils literal notranslate"><span class="pre">amp</span></code> for mixed-precision training. In this way, some <code class="docutils literal notranslate"><span class="pre">FP32</span></code> operations and the invocation of some <code class="docutils literal notranslate"><span class="pre">TransData</span></code> operators can be reduced.</p>
<br/>
<p><font size=3><strong>Q: An error occurs when the <code class="docutils literal notranslate"><span class="pre">Concat</span></code> operator concatenates tuples containing multiple tensors. An error occurs when the number of <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">list</span></code> elements entered is greater than or equal to 192. What is a better solution (running in dynamic mode) for <code class="docutils literal notranslate"><span class="pre">Concat</span></code> to concatenate tuples containing multiple Tensors?</strong></font></p>
<p>A: The number of tensors to be concatenated at a time cannot exceed 192 according to the bottom-layer specifications of the Ascend operator. You can try to concatenate them twice.</p>
<br/>
<p><font size=3><strong>Q: When <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> is used to define convolution, the <code class="docutils literal notranslate"><span class="pre">group</span></code> parameter is used. Is it necessary to ensure that the value of <code class="docutils literal notranslate"><span class="pre">group</span></code> can be exactly divided by the input and output dimensions? How is the group parameter transferred?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> operator has the following constraint: When the value of <code class="docutils literal notranslate"><span class="pre">group</span></code> is greater than 1, the value must be the same as the number of input and output channels. Do not use <code class="docutils literal notranslate"><span class="pre">ops.Conv2D</span></code>. Currently, this operator does not support a value of <code class="docutils literal notranslate"><span class="pre">group</span></code> that is greater than 1. Currently, only the <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> API of MindSpore supports <code class="docutils literal notranslate"><span class="pre">group</span></code> convolution. However, the number of groups must be the same as the number of input and output channels.
The <code class="docutils literal notranslate"><span class="pre">Conv2D</span></code> operator function is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
</pre></div>
</div>
<p>If the function contains a <code class="docutils literal notranslate"><span class="pre">group</span></code> parameter, the parameter will be transferred to the C++ layer by default.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore provide 3D convolutional layers?</strong></font></p>
<p>A: 3D convolutional layers on Ascend are coming soon. Go to the <a class="reference external" href="https://www.mindspore.cn/doc/programming_guide/en/r1.2/operator_list.html">Operator List</a> on the official website to view the operators that are supported.</p>
<br/>
<p><font size=3><strong>Q: Does MindSpore support matrix transposition?</strong></font></p>
<p>A: Yes. For details, see <a class="reference external" href="https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/ops/mindspore.ops.Transpose.html#mindspore.ops.Transpose">mindspore.ops.Transpose</a>.</p>
<br/>
<p><font size=3><strong>Q: Can MindSpore calculate the variance of any tensor?</strong></font></p>
<p>A: Currently, MindSpore does not have APIs or operators similar to variance which can directly calculate the variance of a <code class="docutils literal notranslate"><span class="pre">tensor</span></code>. However, MindSpore has sufficient small operators to support such operations. For details, see <a class="reference external" href="https://www.mindspore.cn/doc/api_python/en/r1.2/_modules/mindspore/nn/layer/math.html#Moments">class Moments(Cell)</a>.</p>
<br/>
<p><font size=3><strong>Q: Why is data loading abnormal when MindSpore1.0.1 is used in graph data offload mode?</strong></font></p>
<p>A: An operator with the <code class="docutils literal notranslate"><span class="pre">axis</span></code> attribute, for example, <code class="docutils literal notranslate"><span class="pre">ops.Concat(axis=1)((x1,</span> <span class="pre">x2))</span></code>, is directly used in <code class="docutils literal notranslate"><span class="pre">construct</span></code>. You are advised to initialize the operator in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<br/>
<p><font size=3><strong>Q: When the <code class="docutils literal notranslate"><span class="pre">Tile</span></code> module in operations executes <code class="docutils literal notranslate"><span class="pre">__infer__</span></code>, the <code class="docutils literal notranslate"><span class="pre">value</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>. Why is the value lost?</strong></font></p>
<p>A: The <code class="docutils literal notranslate"><span class="pre">multiples</span> <span class="pre">input</span></code> of the <code class="docutils literal notranslate"><span class="pre">Tile</span></code> operator must be a constant. (The value cannot directly or indirectly come from the input of the graph.) Otherwise, the <code class="docutils literal notranslate"><span class="pre">None</span></code> data will be obtained during graph composition because the graph input is transferred only during graph execution and the input data cannot be obtained during graph composition.</p>
<br/>
<p><font size=3><strong>Q: Compared with PyTorch, the <code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer lacks the padding operation. Can other operators implement this operation?</strong></font></p>
<p>A: In PyTorch, <code class="docutils literal notranslate"><span class="pre">padding_idx</span></code> is used to set the word vector in the <code class="docutils literal notranslate"><span class="pre">padding_idx</span></code> position in the embedding matrix to 0, and the word vector in the <code class="docutils literal notranslate"><span class="pre">padding_idx</span></code> position is not updated during backward propagation.
In MindSpore, you can manually initialize the weight corresponding to the <code class="docutils literal notranslate"><span class="pre">padding_idx</span></code> position of embedding to 0. In addition, the loss corresponding to <code class="docutils literal notranslate"><span class="pre">padding_idx</span></code> is filtered out through the mask operation during training.</p>
<br/>
<p><font size=3><strong>Q: What can I do if the LSTM example on the official website cannot run on Ascend?</strong></font></p>
<p>A: Currently, the LSTM runs only on a GPU or CPU and does not support the hardware environment. You can click <a class="reference external" href="https://www.mindspore.cn/doc/note/en/r1.2/operator_list_ms.html">MindSpore Operator List</a> to view the supported operators.</p>
<br/>
<p><font size=3><strong>Q: When conv2d is set to (3,10), Tensor[2,2,10,10] and it runs on Ascend on ModelArts, the error message <code class="docutils literal notranslate"><span class="pre">FM_W+pad_left+pad_right-KW&gt;=strideW</span></code> is displayed. However, no error message is displayed when it runs on a CPU. What should I do?</strong></font></p>
<p>A: This is a TBE operator restriction that the width of x must be greater than that of the kernel. The CPU does not have this operator restriction. Therefore, no error is reported.</p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="network_models.html" class="btn btn-neutral float-right" title="Network Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>