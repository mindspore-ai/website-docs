<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimization Algorithms &mdash; MindSpore r1.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">Data Type</a></li>
<li class="toctree-l1"><a class="reference internal" href="compute_component.html">Compute Component</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_pipeline.html">Data Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution_management.html">Execution Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list.html">Operator List</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Optimization Algorithms</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/optim.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="optimization-algorithms">
<h1>Optimization Algorithms<a class="headerlink" href="#optimization-algorithms" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/docs/programming_guide/source_en/optim.md" target="_blank"><img src="./_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code> is a module in the MindSpore framework for implementing various optimization algorithms, including common optimizers and learning rates. In addition, the universal APIs can integrate updated and complex methods into the module.</p>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code> provides common optimizers for models, such as <code class="docutils literal notranslate"><span class="pre">SGD</span></code>, <code class="docutils literal notranslate"><span class="pre">ADAM</span></code>, and <code class="docutils literal notranslate"><span class="pre">Momentum</span></code>. The optimizer is used to compute and update the gradient. The selection of the model optimization algorithm directly affects the performance of the final model. If the effect is poor, the problem may be caused by the optimization algorithm instead of the feature or model design. In addition, <code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code> provides the learning rate module. Learning rates are classified into <code class="docutils literal notranslate"><span class="pre">dynamic_lr</span></code> and <code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>, which are both dynamic learning rates. However, the implementation methods are different. The learning rate is the most important parameter in supervised learning and deep learning. It determines whether the objective function can converge to a local minimum and when it can converge to a minimum. An appropriate learning rate can make the objective function converge to a local minimum in an appropriate time.</p>
<blockquote>
<div><p>All the following examples support the CPU, GPU, and Ascend environments.</p>
</div></blockquote>
</section>
<section id="learning-rates">
<h2>Learning Rates<a class="headerlink" href="#learning-rates" title="Permalink to this headline"></a></h2>
<section id="dynamic-lr">
<h3>dynamic_lr<a class="headerlink" href="#dynamic-lr" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">mindspore.nn.dynamic_lr</span></code> module contains the following classes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">piecewise_constant_lr</span></code> class: computes the learning rate based on the unchanged segment.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exponential_decay_lr</span></code> class: computes the learning rate based on the exponential decay function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">natural_exp_decay_lr</span></code> class: computes the learning rate based on the natural exponential decay function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inverse_decay_lr</span></code> class: computes the learning rate based on the inverse time attenuation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cosine_decay_lr</span></code> class: computes the learning rate based on the cosine attenuation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">polynomial_decay_lr</span></code> class: computes the learning rate based on the polynomial attenuation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warmup_lr</span></code> class: improves the learning rate.</p></li>
</ul>
<p>They are different implementations of <code class="docutils literal notranslate"><span class="pre">dynamic_lr</span></code>.</p>
<p>For example, the code example of the <code class="docutils literal notranslate"><span class="pre">piecewise_constant_lr</span></code> class is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.nn.dynamic_lr</span> <span class="kn">import</span> <span class="n">piecewise_constant_lr</span>

<span class="k">def</span> <span class="nf">test_dynamic_lr</span><span class="p">():</span>
    <span class="n">milestone</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
    <span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">piecewise_constant_lr</span><span class="p">(</span><span class="n">milestone</span><span class="p">,</span> <span class="n">learning_rates</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">test_dynamic_lr</span><span class="p">()</span>
</pre></div>
</div>
<p>The following information is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[0.1, 0.1, 0.05, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.01]
</pre></div>
</div>
</section>
<section id="learning-rate-schedule">
<h3>learning_rate_schedule<a class="headerlink" href="#learning-rate-schedule" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">mindspore.nn.learning_rate_schedule</span></code> module has the following classes: <code class="docutils literal notranslate"><span class="pre">ExponentialDecayLR</span></code>, <code class="docutils literal notranslate"><span class="pre">NaturalExpDecayLR</span></code>, <code class="docutils literal notranslate"><span class="pre">InverseDecayLR</span></code>, and <code class="docutils literal notranslate"><span class="pre">CosineDecayLR</span></code>. <code class="docutils literal notranslate"><span class="pre">PolynomialDecayLR</span></code> class and <code class="docutils literal notranslate"><span class="pre">WarmUpLR</span></code> class. They belong to <code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code> but are implemented in different ways. Their meanings are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ExponentialDecayLR</span></code> class: computes the learning rate based on the exponential decay function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NaturalExpDecayLR</span></code> class: computes the learning rate based on the natural exponential decay function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">InverseDecayLR</span></code> class: computes the learning rate based on the inverse time attenuation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CosineDecayLR</span></code> class: computes the learning rate based on the cosine attenuation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PolynomialDecayLR</span></code> class: computes the learning rate based on the polynomial attenuation function.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WarmUpLR</span></code> class: improves the learning rate.</p></li>
</ul>
<p>They are different implementations of <code class="docutils literal notranslate"><span class="pre">learning_rate_schedule</span></code>.</p>
<p>For example, the code example of the ExponentialDecayLR class is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">ExponentialDecayLR</span>

<span class="k">def</span> <span class="nf">test_learning_rate_schedule</span><span class="p">():</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>    <span class="c1"># learning_rate(float) - The initial value of learning rate.</span>
    <span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.9</span>    <span class="c1"># decay_rate(float) - The decay rate.</span>
    <span class="n">decay_steps</span> <span class="o">=</span> <span class="mi">4</span>    <span class="c1"># decay_steps(int) - A value used to calculate decayed learning rate.</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">exponential_decay_lr</span> <span class="o">=</span> <span class="n">ExponentialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">exponential_decay_lr</span><span class="p">(</span><span class="n">global_step</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">test_learning_rate_schedule</span><span class="p">()</span>
</pre></div>
</div>
<p>The following information is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0.094868325
</pre></div>
</div>
</section>
</section>
<section id="optimzers">
<h2>Optimzers<a class="headerlink" href="#optimzers" title="Permalink to this headline"></a></h2>
<section id="usage">
<h3>Usage<a class="headerlink" href="#usage" title="Permalink to this headline"></a></h3>
<p>To use <code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code>, you need to build an <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> object. This object can maintain the current parameter status and update parameters based on the computed gradient.</p>
<ul class="simple">
<li><p>Building</p></li>
</ul>
<p>To build an <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code>, you need to give it an iterable that contains the parameters (must be Variable objects) that need to be optimized. Then, you can set the <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> parameter options, such as the learning rate and weight attenuation.</p>
<p>A code example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

</pre></div>
</div>
<ul class="simple">
<li><p>Setting options for each parameter separately</p></li>
</ul>
<p>The optimizer also allows you to set options for each parameter separately. Do not pass in the variable directly but pass in the iterable of a dictionary. Each dictionary defines a group of parameters and contains a key, which corresponds to a parameter value. Other keys should be other parameters accepted by the optimizer and will be used to optimize this group of parameters.</p>
<p>You can pass options as keyword parameters, which are used as default values in groups where these options are not overridden. This is useful when you want to change the options of only one parameter group without changing the options of other parameter groups.
Take <code class="docutils literal notranslate"><span class="pre">SGD</span></code> as an example. When you want to determine the learning rate of each layer, run the following command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">([{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}],</span>
               <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

</pre></div>
</div>
<p>This example indicates that when the parameter is conv_params, the weight attenuation is 0.01 and the learning rate is 0.1. When the parameter is no_conv_params, the weight attenuation is 0.0 and the learning rate is 0.01. The learning_rate=0.1 is used for all groups where the learning rate is not set. The same rule applies to weight_deca.</p>
</section>
<section id="built-in-optimizers">
<h3>Built-in Optimizers<a class="headerlink" href="#built-in-optimizers" title="Permalink to this headline"></a></h3>
<p>Common deep learning optimization algorithms include <code class="docutils literal notranslate"><span class="pre">SGD</span></code>, <code class="docutils literal notranslate"><span class="pre">Adam</span></code>, <code class="docutils literal notranslate"><span class="pre">Ftrl</span></code>, <code class="docutils literal notranslate"><span class="pre">lazyadam</span></code>, <code class="docutils literal notranslate"><span class="pre">Momentum</span></code>, <code class="docutils literal notranslate"><span class="pre">RMSprop</span></code>, <code class="docutils literal notranslate"><span class="pre">Lars</span></code>, <code class="docutils literal notranslate"><span class="pre">Proximal_ada_grad</span></code>, and <code class="docutils literal notranslate"><span class="pre">lamb</span></code>.
In the <code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code> module, they have corresponding class implementations. For example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SGD</span></code>: The default parameter is pure SGD. When the <code class="docutils literal notranslate"><span class="pre">momentum</span></code> parameter is set to a value other than 0, the first-order momentum is considered. After <code class="docutils literal notranslate"><span class="pre">nesterov</span></code> is set to True, the value changes to <code class="docutils literal notranslate"><span class="pre">NAG</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">Nesterov</span> <span class="pre">Accelerated</span> <span class="pre">Gradient</span></code>. When the gradient is computed, the gradient of the step forward is computed.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RMSprop</span></code> considers the second-order momentum. Different parameters have different learning rates, that is, adaptive learning rates. <code class="docutils literal notranslate"><span class="pre">Adagrad</span></code> is optimized. Only the second-order momentum within a certain window is considered through exponential smoothing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Adam</span></code> considers both first-order momentum and second-order momentum. It can be seen as a further consideration of the first-order momentum based on <code class="docutils literal notranslate"><span class="pre">RMSprop</span></code>.</p></li>
</ul>
<p>For example, the code example of <code class="docutils literal notranslate"><span class="pre">SGD</span></code> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Model</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)))</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>

<span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>

</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>