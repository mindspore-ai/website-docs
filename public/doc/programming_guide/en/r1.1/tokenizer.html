<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tokenizer &mdash; MindSpore r1.1 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MindSpore Data Format Conversion" href="dataset_conversion.html" />
    <link rel="prev" title="Data Augmentation" href="augmentation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">Data Type</a></li>
<li class="toctree-l1"><a class="reference internal" href="compute_component.html">Compute Component</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="data_pipeline.html">Data Pipeline</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dataset_loading.html">Loading Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="sampler.html">Sampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="augmentation.html">Data Augmentation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tokenizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-tokenizers">MindSpore Tokenizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#berttokenizer">BertTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jiebatokenizer">JiebaTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sentencepiecetokenizer">SentencePieceTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unicodechartokenizer">UnicodeCharTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#whitespacetokenizer">WhitespaceTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#wordpiecetokenizer">WordpieceTokenizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dataset_conversion.html">MindSpore Data Format Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="cache.html">Single Node Data Cache</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="execution_management.html">Execution Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list.html">Operator List</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="data_pipeline.html">Data Pipeline</a> &raquo;</li>
      <li>Tokenizer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tokenizer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tokenizer">
<h1>Tokenizer<a class="headerlink" href="#tokenizer" title="Permalink to this headline">ïƒ</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/docs/programming_guide/source_en/tokenizer.md" target="_blank"><img src="./_static/logo_source.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">ïƒ</a></h2>
<p>Tokenization is a process of re-combining continuous character sequences into word sequences according to certain specifications. Reasonable tokenization is helpful for semantic comprehension.</p>
<p>MindSpore provides a tokenizer for multiple purposes to help you process text with high performance. You can build your own dictionaries, use appropriate tokenizers to split sentences into different tokens, and search for indexes of the tokens in the dictionaries.</p>
<p>MindSpore provides the following tokenizers. In addition, you can customize tokenizers as required.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Tokenizer</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BasicTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on specified rules.</p></td>
</tr>
<tr class="row-odd"><td><p>BertTokenizer</p></td>
<td><p>Processes BERT text data.</p></td>
</tr>
<tr class="row-even"><td><p>JiebaTokenizer</p></td>
<td><p>Dictionary-based Chinese character string tokenizer.</p></td>
</tr>
<tr class="row-odd"><td><p>RegexTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on a specified regular expression.</p></td>
</tr>
<tr class="row-even"><td><p>SentencePieceTokenizer</p></td>
<td><p>Performs tokenization based on the open-source tool package SentencePiece.</p></td>
</tr>
<tr class="row-odd"><td><p>UnicodeCharTokenizer</p></td>
<td><p>Tokenizes scalar text data into Unicode characters.</p></td>
</tr>
<tr class="row-even"><td><p>UnicodeScriptTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on Unicode boundaries.</p></td>
</tr>
<tr class="row-odd"><td><p>WhitespaceTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on spaces.</p></td>
</tr>
<tr class="row-even"><td><p>WordpieceTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on the word set.</p></td>
</tr>
</tbody>
</table>
<p>For details about tokenizers, see <a class="reference external" href="https://www.mindspore.cn/doc/api_python/en/r1.1/mindspore/mindspore.dataset.text.html">MindSpore API</a>.</p>
</section>
<section id="mindspore-tokenizers">
<h2>MindSpore Tokenizers<a class="headerlink" href="#mindspore-tokenizers" title="Permalink to this headline">ïƒ</a></h2>
<p>The following describes how to use common tokenizers.</p>
<section id="berttokenizer">
<h3>BertTokenizer<a class="headerlink" href="#berttokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> performs tokenization by calling <code class="docutils literal notranslate"><span class="pre">BasicTokenizer</span></code> and <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code>.</p>
<p>The following example builds a text dataset and a character string list, uses <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;åºŠå‰æ˜æœˆå…‰&quot;</span><span class="p">,</span> <span class="s2">&quot;ç–‘æ˜¯åœ°ä¸Šéœœ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¸¾å¤´æœ›æ˜æœˆ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä½å¤´æ€æ•…ä¹¡&quot;</span><span class="p">,</span> <span class="s2">&quot;I am making small mistakes during working hours&quot;</span><span class="p">,</span>
                <span class="s2">&quot;ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»&quot;</span><span class="p">,</span> <span class="s2">&quot;ç¹é«”å­—&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">vocab_list</span> <span class="o">=</span> <span class="p">[</span>
  <span class="s2">&quot;åºŠ&quot;</span><span class="p">,</span> <span class="s2">&quot;å‰&quot;</span><span class="p">,</span> <span class="s2">&quot;æ˜&quot;</span><span class="p">,</span> <span class="s2">&quot;æœˆ&quot;</span><span class="p">,</span> <span class="s2">&quot;å…‰&quot;</span><span class="p">,</span> <span class="s2">&quot;ç–‘&quot;</span><span class="p">,</span> <span class="s2">&quot;æ˜¯&quot;</span><span class="p">,</span> <span class="s2">&quot;åœ°&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¸Š&quot;</span><span class="p">,</span> <span class="s2">&quot;éœœ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¸¾&quot;</span><span class="p">,</span> <span class="s2">&quot;å¤´&quot;</span><span class="p">,</span> <span class="s2">&quot;æœ›&quot;</span><span class="p">,</span> <span class="s2">&quot;ä½&quot;</span><span class="p">,</span> <span class="s2">&quot;æ€&quot;</span><span class="p">,</span> <span class="s2">&quot;æ•…&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¹¡&quot;</span><span class="p">,</span>
  <span class="s2">&quot;ç¹&quot;</span><span class="p">,</span> <span class="s2">&quot;é«”&quot;</span><span class="p">,</span> <span class="s2">&quot;å­—&quot;</span><span class="p">,</span> <span class="s2">&quot;å˜¿&quot;</span><span class="p">,</span> <span class="s2">&quot;å“ˆ&quot;</span><span class="p">,</span> <span class="s2">&quot;å¤§&quot;</span><span class="p">,</span> <span class="s2">&quot;ç¬‘&quot;</span><span class="p">,</span> <span class="s2">&quot;å˜»&quot;</span><span class="p">,</span> <span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;am&quot;</span><span class="p">,</span> <span class="s2">&quot;mak&quot;</span><span class="p">,</span> <span class="s2">&quot;make&quot;</span><span class="p">,</span> <span class="s2">&quot;small&quot;</span><span class="p">,</span> <span class="s2">&quot;mistake&quot;</span><span class="p">,</span>
  <span class="s2">&quot;##s&quot;</span><span class="p">,</span> <span class="s2">&quot;during&quot;</span><span class="p">,</span> <span class="s2">&quot;work&quot;</span><span class="p">,</span> <span class="s2">&quot;##ing&quot;</span><span class="p">,</span> <span class="s2">&quot;hour&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜€&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜ƒ&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜„&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜&quot;</span><span class="p">,</span> <span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="s2">&quot;12&quot;</span><span class="p">,</span>
  <span class="s2">&quot;28&quot;</span><span class="p">,</span> <span class="s2">&quot;40&quot;</span><span class="p">,</span> <span class="s2">&quot;16&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[unused1]&quot;</span><span class="p">,</span> <span class="s2">&quot;[unused10]&quot;</span><span class="p">]</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">)</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
åºŠå‰æ˜æœˆå…‰
ç–‘æ˜¯åœ°ä¸Šéœœ
ä¸¾å¤´æœ›æ˜æœˆ
ä½å¤´æ€æ•…ä¹¡
I am making small mistakes during working hours
ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»
ç¹é«”å­—
------------------------after tokenization-----------------------------
[&#39;åºŠ&#39; &#39;å‰&#39; &#39;æ˜&#39; &#39;æœˆ&#39; &#39;å…‰&#39;]
[&#39;ç–‘&#39; &#39;æ˜¯&#39; &#39;åœ°&#39; &#39;ä¸Š&#39; &#39;éœœ&#39;]
[&#39;ä¸¾&#39; &#39;å¤´&#39; &#39;æœ›&#39; &#39;æ˜&#39; &#39;æœˆ&#39;]
[&#39;ä½&#39; &#39;å¤´&#39; &#39;æ€&#39; &#39;æ•…&#39; &#39;ä¹¡&#39;]
[&#39;I&#39; &#39;am&#39; &#39;mak&#39; &#39;##ing&#39; &#39;small&#39; &#39;mistake&#39; &#39;##s&#39; &#39;during&#39; &#39;work&#39; &#39;##ing&#39;
 &#39;hour&#39; &#39;##s&#39;]
[&#39;ğŸ˜€&#39; &#39;å˜¿&#39; &#39;å˜¿&#39; &#39;ğŸ˜ƒ&#39; &#39;å“ˆ&#39; &#39;å“ˆ&#39; &#39;ğŸ˜„&#39; &#39;å¤§&#39; &#39;ç¬‘&#39; &#39;ğŸ˜&#39; &#39;å˜»&#39; &#39;å˜»&#39;]
[&#39;ç¹&#39; &#39;é«”&#39; &#39;å­—&#39;]
</pre></div>
</div>
</section>
<section id="jiebatokenizer">
<h3>JiebaTokenizer<a class="headerlink" href="#jiebatokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code> performs Chinese tokenization based on Jieba.</p>
<p>The following example builds a text dataset, uses the HMM and MP dictionary files to create a <code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code> object, performs tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="c1"># files from open source repository https://github.com/yanyiwu/cppjieba/tree/master/dict</span>
<span class="n">HMM_FILE</span> <span class="o">=</span> <span class="s2">&quot;hmm_model.utf8&quot;</span>
<span class="n">MP_FILE</span> <span class="o">=</span> <span class="s2">&quot;jieba.dict.utf8&quot;</span>
<span class="n">jieba_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">JiebaTokenizer</span><span class="p">(</span><span class="n">HMM_FILE</span><span class="p">,</span> <span class="n">MP_FILE</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">jieba_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§
------------------------after tokenization-----------------------------
[&#39;ä»Šå¤©å¤©æ°”&#39; &#39;å¤ªå¥½äº†&#39; &#39;æˆ‘ä»¬&#39; &#39;ä¸€èµ·&#39; &#39;å»&#39; &#39;å¤–é¢&#39; &#39;ç©å§&#39;]
</pre></div>
</div>
</section>
<section id="sentencepiecetokenizer">
<h3>SentencePieceTokenizer<a class="headerlink" href="#sentencepiecetokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">SentencePieceTokenizer</span></code> performs tokenization based on an open-source natural language processing tool package <a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a>.</p>
<p>The following example builds a text dataset, creates a <code class="docutils literal notranslate"><span class="pre">vocab</span></code> object from the <code class="docutils literal notranslate"><span class="pre">vocab_file</span></code> file, uses <code class="docutils literal notranslate"><span class="pre">SentencePieceTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.text</span> <span class="kn">import</span> <span class="n">SentencePieceModel</span><span class="p">,</span> <span class="n">SPieceTokenizerOutType</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;I saw a girl with a telescope.&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="c1"># file from MindSpore repository https://gitee.com/mindspore/mindspore/blob/r1.1/tests/ut/data/dataset/test_sentencepiece/botchan.txt</span>
<span class="n">vocab_file</span> <span class="o">=</span> <span class="s2">&quot;botchan.txt&quot;</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">SentencePieceVocab</span><span class="o">.</span><span class="n">from_file</span><span class="p">([</span><span class="n">vocab_file</span><span class="p">],</span> <span class="mi">5000</span><span class="p">,</span> <span class="mf">0.9995</span><span class="p">,</span> <span class="n">SentencePieceModel</span><span class="o">.</span><span class="n">UNIGRAM</span><span class="p">,</span> <span class="p">{})</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">SentencePieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">SPieceTokenizerOutType</span><span class="o">.</span><span class="n">STRING</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
I saw a girl with a telescope.
------------------------after tokenization-----------------------------
[&#39;â–I&#39; &#39;â–sa&#39; &#39;w&#39; &#39;â–a&#39; &#39;â–girl&#39; &#39;â–with&#39; &#39;â–a&#39; &#39;â–te&#39; &#39;les&#39; &#39;co&#39; &#39;pe&#39; &#39;.&#39;]
</pre></div>
</div>
</section>
<section id="unicodechartokenizer">
<h3>UnicodeCharTokenizer<a class="headerlink" href="#unicodechartokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">UnicodeCharTokenizer</span></code> performs tokenization based on the Unicode character set.</p>
<p>The following example builds a text dataset, uses <code class="docutils literal notranslate"><span class="pre">UnicodeCharTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Welcome to Beijing!&quot;</span><span class="p">,</span> <span class="s2">&quot;åŒ—äº¬æ¬¢è¿æ‚¨ï¼ &quot;</span><span class="p">,</span> <span class="s2">&quot;æˆ‘å–œæ¬¢English!&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
[&#39;W&#39;, &#39;e&#39;, &#39;l&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;e&#39;, &#39; &#39;, &#39;t&#39;, &#39;o&#39;, &#39; &#39;, &#39;B&#39;, &#39;e&#39;, &#39;i&#39;, &#39;j&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39;!&#39;]
[&#39;åŒ—&#39;, &#39;äº¬&#39;, &#39;æ¬¢&#39;, &#39;è¿&#39;, &#39;æ‚¨&#39;, &#39;ï¼ &#39;]
[&#39;æˆ‘&#39;, &#39;å–œ&#39;, &#39;æ¬¢&#39;, &#39;E&#39;, &#39;n&#39;, &#39;g&#39;, &#39;l&#39;, &#39;i&#39;, &#39;s&#39;, &#39;h&#39;, &#39;!&#39;]
</pre></div>
</div>
</section>
<section id="whitespacetokenizer">
<h3>WhitespaceTokenizer<a class="headerlink" href="#whitespacetokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> performs tokenization based on spaces.</p>
<p>The following example builds a text dataset, uses <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Welcome to Beijing!&quot;</span><span class="p">,</span> <span class="s2">&quot;åŒ—äº¬æ¬¢è¿æ‚¨ï¼ &quot;</span><span class="p">,</span> <span class="s2">&quot;æˆ‘å–œæ¬¢English!&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
[&#39;Welcome&#39;, &#39;to&#39;, &#39;Beijing!&#39;]
[&#39;åŒ—äº¬æ¬¢è¿æ‚¨ï¼ &#39;]
[&#39;æˆ‘å–œæ¬¢English!&#39;]
</pre></div>
</div>
</section>
<section id="wordpiecetokenizer">
<h3>WordpieceTokenizer<a class="headerlink" href="#wordpiecetokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code> performs tokenization based on the word set. A token can be a single word in the word set or a combination of words.</p>
<p>The following example builds a text dataset, creates a <code class="docutils literal notranslate"><span class="pre">vocab</span></code> object from the word list, uses <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;my&quot;</span><span class="p">,</span> <span class="s2">&quot;favorite&quot;</span><span class="p">,</span> <span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;during&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;cholera&quot;</span><span class="p">,</span> <span class="s2">&quot;era&quot;</span><span class="p">,</span> <span class="s2">&quot;what&quot;</span><span class="p">,</span>
    <span class="s2">&quot;æˆ‘&quot;</span><span class="p">,</span> <span class="s2">&quot;æœ€&quot;</span><span class="p">,</span> <span class="s2">&quot;å–œ&quot;</span><span class="p">,</span> <span class="s2">&quot;æ¬¢&quot;</span><span class="p">,</span> <span class="s2">&quot;çš„&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¹¦&quot;</span><span class="p">,</span> <span class="s2">&quot;æ˜¯&quot;</span><span class="p">,</span> <span class="s2">&quot;éœ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¹±&quot;</span><span class="p">,</span> <span class="s2">&quot;æ—¶&quot;</span><span class="p">,</span> <span class="s2">&quot;æœŸ&quot;</span><span class="p">,</span> <span class="s2">&quot;çš„&quot;</span><span class="p">,</span> <span class="s2">&quot;çˆ±&quot;</span><span class="p">,</span> <span class="s2">&quot;æƒ…&quot;</span><span class="p">,</span> <span class="s2">&quot;æ‚¨&quot;</span><span class="p">]</span>
<span class="n">vocab_english</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;cholera&quot;</span><span class="p">,</span> <span class="s2">&quot;era&quot;</span><span class="p">,</span> <span class="s2">&quot;favor&quot;</span><span class="p">,</span> <span class="s2">&quot;##ite&quot;</span><span class="p">,</span> <span class="s2">&quot;my&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;dur&quot;</span><span class="p">,</span> <span class="s2">&quot;##ing&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">]</span>
<span class="n">vocab_chinese</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;æˆ‘&quot;</span><span class="p">,</span> <span class="s1">&#39;æœ€&#39;</span><span class="p">,</span> <span class="s1">&#39;å–œ&#39;</span><span class="p">,</span> <span class="s1">&#39;æ¬¢&#39;</span><span class="p">,</span> <span class="s1">&#39;çš„&#39;</span><span class="p">,</span> <span class="s1">&#39;ä¹¦&#39;</span><span class="p">,</span> <span class="s1">&#39;æ˜¯&#39;</span><span class="p">,</span> <span class="s1">&#39;éœ&#39;</span><span class="p">,</span> <span class="s1">&#39;ä¹±&#39;</span><span class="p">,</span> <span class="s1">&#39;æ—¶&#39;</span><span class="p">,</span> <span class="s1">&#39;æœŸ&#39;</span><span class="p">,</span> <span class="s1">&#39;çˆ±&#39;</span><span class="p">,</span> <span class="s1">&#39;æƒ…&#39;</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">vocab_english</span><span class="o">+</span><span class="n">vocab_chinese</span><span class="p">)</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WordpieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
my
favorite
book
is
love
during
the
cholera
era
what
æˆ‘
æœ€
å–œ
æ¬¢
çš„
ä¹¦
æ˜¯
éœ
ä¹±
æ—¶
æœŸ
çš„
çˆ±
æƒ…
æ‚¨
------------------------after tokenization-----------------------------
[&#39;my&#39;]
[&#39;favor&#39; &#39;##ite&#39;]
[&#39;book&#39;]
[&#39;is&#39;]
[&#39;love&#39;]
[&#39;dur&#39; &#39;##ing&#39;]
[&#39;the&#39;]
[&#39;cholera&#39;]
[&#39;era&#39;]
[&#39;[UNK]&#39;]
[&#39;æˆ‘&#39;]
[&#39;æœ€&#39;]
[&#39;å–œ&#39;]
[&#39;æ¬¢&#39;]
[&#39;çš„&#39;]
[&#39;ä¹¦&#39;]
[&#39;æ˜¯&#39;]
[&#39;éœ&#39;]
[&#39;ä¹±&#39;]
[&#39;æ—¶&#39;]
[&#39;æœŸ&#39;]
[&#39;çš„&#39;]
[&#39;çˆ±&#39;]
[&#39;æƒ…&#39;]
[&#39;[UNK]&#39;]
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="augmentation.html" class="btn btn-neutral float-left" title="Data Augmentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dataset_conversion.html" class="btn btn-neutral float-right" title="MindSpore Data Format Conversion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>