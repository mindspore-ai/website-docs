<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cell Building and Its Subclasses &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Common Network Components" href="network_component.html" />
    <link rel="prev" title="Parameter" href="parameter.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">Data Type</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="compute_component.html">Compute Component</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="operators.html">Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="parameter.html">Parameter</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Cell Building and Its Subclasses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#key-member-functions">Key Member Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#construct">construct</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameters-dict">parameters_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cells-and-names">cells_and_names</a></li>
<li class="toctree-l4"><a class="reference internal" href="#set-grad">set_grad</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#relationship-between-the-nn-module-and-the-ops-module">Relationship Between the nn Module and the ops Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-layers">Model Layers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#built-in-model-layers">Built-in Model Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#application-cases">Application Cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loss-functions">Loss Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#built-in-loss-functions">Built-in Loss Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Application Cases</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#optimization-algorithms">Optimization Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-a-customized-network">Building a Customized Network</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="network_component.html">Common Network Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="initializer.html">Initialization of Network Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="numpy.html">Numpy Interfaces in MindSpore</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_pipeline.html">Data Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution_management.html">Execution Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list.html">Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntax_list.html">Syntax list</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="compute_component.html">Compute Component</a> &raquo;</li>
      <li>Cell Building and Its Subclasses</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/cell.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="cell-building-and-its-subclasses">
<h1>Cell Building and Its Subclasses<a class="headerlink" href="#cell-building-and-its-subclasses" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.2/docs/programming_guide/source_en/cell.md"><img alt="View Source On Gitee" src="_images/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class of MindSpore is the base class for building all networks and the basic unit of a network. When you need to customize a network, you need to inherit the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class and override the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> and <code class="docutils literal notranslate"><span class="pre">construct</span></code> methods.</p>
<p>Loss functions, optimizers, and model layers are parts of the network structure and can be implemented only by inheriting the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class. You can also customize them based on service requirements.</p>
<p>The following describes the key member functions of the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class, the built-in loss functions, optimizers, and model layers of MindSpore implemented based on the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class, and how to use them, as well as describes how to use the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class to build a customized network.</p>
</section>
<section id="key-member-functions">
<h2>Key Member Functions<a class="headerlink" href="#key-member-functions" title="Permalink to this headline"></a></h2>
<section id="construct">
<h3>construct<a class="headerlink" href="#construct" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class overrides the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method. When the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class instance is called, the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method is executed. The network structure is defined in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method.</p>
<p>In the following example, a simple network is built to implement the convolution computing function. The operators in the network are defined in <code class="docutils literal notranslate"><span class="pre">__init__</span></code> and used in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method. The network structure of the case is as follows: <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">BiasAdd</span></code>.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method, <code class="docutils literal notranslate"><span class="pre">x</span></code> is the input data, and <code class="docutils literal notranslate"><span class="pre">output</span></code> is the result obtained after the network structure computation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</section>
<section id="parameters-dict">
<h3>parameters_dict<a class="headerlink" href="#parameters-dict" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">parameters_dict</span></code> method is used to identify all parameters in the network structure and return <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code> with key as the parameter name and value as the parameter value.</p>
<p>There are many other methods for returning parameters in the <code class="docutils literal notranslate"><span class="pre">Cell</span></code> class, such as <code class="docutils literal notranslate"><span class="pre">get_parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">trainable_params</span></code>. For details, see <a class="reference external" href="https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/nn/mindspore.nn.Cell.html">MindSpore API</a>.</p>
<p>A code example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters_dict</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>The following information is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>odict_keys([&#39;weight&#39;])
Parameter (name=weight, shape=(20, 10, 3, 3), dtype=Float32, requires_grad=True)
</pre></div>
</div>
<p>In the example, <code class="docutils literal notranslate"><span class="pre">Net</span></code> uses the preceding network building case to print names of all parameters on the network and the result of the <code class="docutils literal notranslate"><span class="pre">weight</span></code> parameter.</p>
</section>
<section id="cells-and-names">
<h3>cells_and_names<a class="headerlink" href="#cells-and-names" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">cells_and_names</span></code> method is an iterator that returns the name and content of each <code class="docutils literal notranslate"><span class="pre">Cell</span></code> on the network.</p>
<p>The case simply implements the function of obtaining and printing the name of each <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. According to the network structure, there is a <code class="docutils literal notranslate"><span class="pre">Cell</span></code> whose name is <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> is a convolutional layer encapsulated by MindSpore using <code class="docutils literal notranslate"><span class="pre">Cell</span></code> as the base class. For details, see “Model Layers”.</p>
<p>A code example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">Net1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net1</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net1</span><span class="p">()</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
    <span class="n">names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">else</span> <span class="kc">None</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------names-------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(&#39;&#39;, Net1&lt;
  (conv): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(3, 3),stride=(1, 1),  pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False,weight_init=normal, bias_init=zeros, format=NCHW&gt;
  &gt;)
(&#39;conv&#39;, Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(3, 3),stride=(1, 1),  pad_mode=same, padding=0, dilation=(1, 1), group=1, has_bias=False,weight_init=normal, bias_init=zeros, format=NCHW&gt;)
-------names-------
[&#39;conv&#39;]
</pre></div>
</div>
</section>
<section id="set-grad">
<h3>set_grad<a class="headerlink" href="#set-grad" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">set_grad</span></code> API is used to construct a backward network. If no parameter is transferred for calling the API, the default value of <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> is True. This API needs to be used in the scenario where the backward network is computed.</p>
<p>Take <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> as an example. Its API function is to perform single-step training on the network. The backward network needs to be computed. Therefore, <code class="docutils literal notranslate"><span class="pre">set_grad</span></code> needs to be used in the initialization method.</p>
<p>A part of the <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code> code is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TrainOneStepCell</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainOneStepCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">set_grad</span><span class="p">()</span>
        <span class="o">......</span>
</pre></div>
</div>
<p>If using similar APIs such as <code class="docutils literal notranslate"><span class="pre">TrainOneStepCell</span></code>, you do not need to use <code class="docutils literal notranslate"><span class="pre">set_grad</span></code>. The internal encapsulation is implemented.</p>
<p>If you need to customize APIs of this training function, call APIs internally or set <code class="docutils literal notranslate"><span class="pre">network.set_grad</span></code> externally.</p>
</section>
</section>
<section id="relationship-between-the-nn-module-and-the-ops-module">
<h2>Relationship Between the nn Module and the ops Module<a class="headerlink" href="#relationship-between-the-nn-module-and-the-ops-module" title="Permalink to this headline"></a></h2>
<p>The nn module of MindSpore is a model component implemented by Python. It encapsulates low-level APIs, including various model layers, loss functions, and optimizers.</p>
<p>In addition, nn provides some APIs with the same name as the <code class="docutils literal notranslate"><span class="pre">Primitive</span></code> operator to further encapsulate the <code class="docutils literal notranslate"><span class="pre">Primitive</span></code> operator and provide more friendly APIs.</p>
<p>Reanalyze the case of the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method described above. This case is the simplified content of the <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> source code of MindSpore, and <code class="docutils literal notranslate"><span class="pre">ops.Conv2D</span></code> is internally called. The <code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> convolution API adds the input parameter validation function and determines whether <code class="docutils literal notranslate"><span class="pre">bias</span></code> is used. It is an advanced encapsulated model layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</section>
<section id="model-layers">
<h2>Model Layers<a class="headerlink" href="#model-layers" title="Permalink to this headline"></a></h2>
<p>MindSpore can use <code class="docutils literal notranslate"><span class="pre">Cell</span></code> as the base class to build the network structure.</p>
<p>To facilitate user operations, MindSpore provides a large number of built-in model layers, which can be directly called by using APIs.</p>
<p>You can also customize a model. For details, see “Building a Customized Network.”</p>
<section id="built-in-model-layers">
<h3>Built-in Model Layers<a class="headerlink" href="#built-in-model-layers" title="Permalink to this headline"></a></h3>
<p>The MindSpore framework provides abundant APIs at the layer of <code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code>. The APIs are as follows:</p>
<ul>
<li><p>Activation layer</p>
<p>The activation layer has a large number of built-in activation functions, which are often used in defining the network structure. The activation function adds a nonlinear operation to the network, so that the network can have a better fitting effect.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">Softmax</span></code>, <code class="docutils literal notranslate"><span class="pre">Relu</span></code>, <code class="docutils literal notranslate"><span class="pre">Elu</span></code>, <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> and <code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code>.</p>
</li>
<li><p>Basic layer</p>
<p>The basic layer implements some common basic structures on the network, such as the full connection layer, Onehot encoding, Dropout, and flat layer.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">Dense</span></code>, <code class="docutils literal notranslate"><span class="pre">Flatten</span></code>, <code class="docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="docutils literal notranslate"><span class="pre">Norm</span></code> and <code class="docutils literal notranslate"><span class="pre">OneHot</span></code>.</p>
</li>
<li><p>Container layer</p>
<p>The main function of the container layer is to implement the data structures for storing multiple cells.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">SequentialCell</span></code> and <code class="docutils literal notranslate"><span class="pre">CellList</span></code>.</p>
</li>
<li><p>Convolutional layer</p>
<p>Convolutional layer provides some convolution computation functions, such as common convolution, deep convolution, and convolution transposition.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code>, <code class="docutils literal notranslate"><span class="pre">Conv1d</span></code>, <code class="docutils literal notranslate"><span class="pre">Conv2dTranspose</span></code> and <code class="docutils literal notranslate"><span class="pre">Conv1dTranspose</span></code>.</p>
</li>
<li><p>Pooling layer</p>
<p>The pooling layer provides computation functions such as average pooling and maximum pooling.</p>
<p>The main APIs are <code class="docutils literal notranslate"><span class="pre">AvgPool2d</span></code>, <code class="docutils literal notranslate"><span class="pre">MaxPool2d</span></code>, and <code class="docutils literal notranslate"><span class="pre">AvgPool1d</span></code>.</p>
</li>
<li><p>Embedding layer</p>
<p>The embedding layer provides the word embedding computation function to map input words into dense vectors.</p>
<p>The main APIs include <code class="docutils literal notranslate"><span class="pre">Embedding</span></code>, <code class="docutils literal notranslate"><span class="pre">EmbeddingLookup</span></code> and <code class="docutils literal notranslate"><span class="pre">EmbeddingLookUpSplitMode</span></code>.</p>
</li>
<li><p>Long short-term memory recurrent layer</p>
<p>The long short-term memory recurrent layer provides the LSTM computation function. <code class="docutils literal notranslate"><span class="pre">LSTM</span></code> internally calls the <code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code> API. The <code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code> is an LSTM unit that performs operations on an LSTM layer. When operations at multiple LSTM network layers are involved, the <code class="docutils literal notranslate"><span class="pre">LSTM</span></code> API is used.</p>
<p>The main APIs include <code class="docutils literal notranslate"><span class="pre">LSTM</span></code> and <code class="docutils literal notranslate"><span class="pre">LSTMCell</span></code>.</p>
</li>
<li><p>Normalization layer</p>
<p>The normalization layer provides some normalization methods, that is, converting data into a mean value and a standard deviation by means of linear transformation or the like.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">BatchNorm1d</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code>, <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code>, <code class="docutils literal notranslate"><span class="pre">GroupNorm</span></code> and <code class="docutils literal notranslate"><span class="pre">GlobalBatchNorm</span></code>.</p>
</li>
<li><p>Mathematical computation layer</p>
<p>The mathematical computation layer provides some computation functions formed by operators, for example, data generation and some other mathematical computations.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">ReduceLogSumExp</span></code>, <code class="docutils literal notranslate"><span class="pre">Range</span></code>, <code class="docutils literal notranslate"><span class="pre">LinSpace</span></code> and <code class="docutils literal notranslate"><span class="pre">LGamma</span></code>.</p>
</li>
<li><p>Image layer</p>
<p>The image computation layer provides some functions related to matrix computing to transform and compute image data.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">ImageGradients</span></code>, <code class="docutils literal notranslate"><span class="pre">SSIM</span></code>, <code class="docutils literal notranslate"><span class="pre">MSSSIM</span></code>, <code class="docutils literal notranslate"><span class="pre">PSNR</span></code> and <code class="docutils literal notranslate"><span class="pre">CentralCrop</span></code>.</p>
</li>
<li><p>Quantization layer</p>
<p>Quantization is to convert data from the float type to the int type within a data range. Therefore, the quantization layer provides some data quantization methods and model layer structure encapsulation.</p>
<p>Main APIs include <code class="docutils literal notranslate"><span class="pre">Conv2dBnAct</span></code>, <code class="docutils literal notranslate"><span class="pre">DenseBnAct</span></code>, <code class="docutils literal notranslate"><span class="pre">Conv2dBnFoldQuant</span></code> and <code class="docutils literal notranslate"><span class="pre">LeakyReLUQuant</span></code>.</p>
</li>
</ul>
</section>
<section id="application-cases">
<h3>Application Cases<a class="headerlink" href="#application-cases" title="Permalink to this headline"></a></h3>
<p>Model layers of MindSpore are under <code class="docutils literal notranslate"><span class="pre">mindspore.nn</span></code>. The usage method is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span> <span class="o">*</span> <span class="mi">222</span> <span class="o">*</span> <span class="mi">222</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
<p>The preceding network building case shows that the program calls the APIs of the <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code>, <code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code>, <code class="docutils literal notranslate"><span class="pre">ReLU</span></code>, <code class="docutils literal notranslate"><span class="pre">Flatten</span></code>, and <code class="docutils literal notranslate"><span class="pre">Dense</span></code> model layers.</p>
<p>It is defined in the <code class="docutils literal notranslate"><span class="pre">Net</span></code> initialization method and runs in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method. These model layer APIs are connected in sequence to form an executable network.</p>
</section>
</section>
<section id="loss-functions">
<h2>Loss Functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline"></a></h2>
<p>Currently, MindSpore supports the following loss functions: <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>, <code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>, <code class="docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code>, <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyWithLogits</span></code>, <code class="docutils literal notranslate"><span class="pre">SampledSoftmaxLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">BCELoss</span></code>, and <code class="docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code>.</p>
<p>All loss functions of MindSpore are implemented by subclasses of <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. Therefore, customized loss functions are also supported. For details about how to build a loss function, see “Building a Customized Network.”</p>
<section id="built-in-loss-functions">
<h3>Built-in Loss Functions<a class="headerlink" href="#built-in-loss-functions" title="Permalink to this headline"></a></h3>
<ul>
<li><p>L1Loss</p>
<p>Computes the absolute value error of two input data for the regression model. The default value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is mean. If the value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is sum, the loss accumulation result is returned. If the value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is none, the result of each loss is returned.</p>
</li>
<li><p>MSELoss</p>
<p>Computes the square error of two input data for the regression model. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> parameter.</p>
</li>
<li><p>SmoothL1Loss</p>
<p><code class="docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code> is the smooth L1 loss function, which is used for the regression model. The default value of the <code class="docutils literal notranslate"><span class="pre">beta</span></code> threshold is 1.</p>
</li>
<li><p>SoftmaxCrossEntropyWithLogits</p>
<p>Cross entropy loss function, which is used to classify models. If the tag data is not encoded in the one-hot mode, set <code class="docutils literal notranslate"><span class="pre">sparse</span></code> to True. The default value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> is none. The meaning of this parameter is the same as that of <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>.</p>
</li>
<li><p>CosineEmbeddingLoss</p>
<p><code class="docutils literal notranslate"><span class="pre">CosineEmbeddingLoss</span></code> is used to measure the similarity between two inputs and is used for classification models. The default value of <code class="docutils literal notranslate"><span class="pre">margin</span></code> is 0.0. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> parameter.</p>
</li>
<li><p>BCELoss</p>
<p>Binary cross entropy loss is used for binary classification. <code class="docutils literal notranslate"><span class="pre">weight</span></code> is a rescaling weight applied to the loss of each batch element. The default value of <code class="docutils literal notranslate"><span class="pre">weight</span></code> is None, which means the weight values are all 1. The default value of <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is none. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> parameter.</p>
</li>
<li><p>SampledSoftmaxLoss</p>
<p>Sampled softmax loss function, which is used for classification model when the number of class is large. <code class="docutils literal notranslate"><span class="pre">num_sampled</span></code> is the number of classes to randomly sample. <code class="docutils literal notranslate"><span class="pre">num_class</span></code> is the number of possible classes. <code class="docutils literal notranslate"><span class="pre">num_true</span></code> is the number of target classes per training example. <code class="docutils literal notranslate"><span class="pre">sampled_values</span></code> is the sampled candidate. The default value of <code class="docutils literal notranslate"><span class="pre">sampled_values</span></code> is None, which means UniformCandidateSampler is applied. <code class="docutils literal notranslate"><span class="pre">remove_accidental_hits</span></code> is the switch of whether to remove “accidental hits”. The default value of <code class="docutils literal notranslate"><span class="pre">remove_accidental_hits</span></code> is True. <code class="docutils literal notranslate"><span class="pre">seed</span></code> is the random seed for candidate sampling with the default value of 0. The default value of reduction parameter is none. The <code class="docutils literal notranslate"><span class="pre">reduction</span></code> parameter is the same as the L1Loss parameter.</p>
</li>
</ul>
</section>
<section id="id1">
<h3>Application Cases<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>All loss functions of MindSpore are stored in mindspore.nn. The usage method is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span>
<span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">target_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">target_data</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1.5
</pre></div>
</div>
<p>In this case, two pieces of tensor data are built. The <code class="docutils literal notranslate"><span class="pre">nn.L1Loss</span></code> API is used to define the loss, <code class="docutils literal notranslate"><span class="pre">input_data</span></code> and <code class="docutils literal notranslate"><span class="pre">target_data</span></code> are transferred to the loss, and the L1Loss computation is performed. The result is 1.5. If loss is set to nn.L1Loss(reduction=’sum’), the result is 9.0. If loss is set to nn.L1Loss(reduction=’none’), the result is [[1. 0. 2.] [1. 2. 3.]].</p>
</section>
</section>
<section id="optimization-algorithms">
<h2>Optimization Algorithms<a class="headerlink" href="#optimization-algorithms" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.nn.optim</span></code> is a module that implements various optimization algorithms in the MindSpore framework. For details, see <a class="reference external" href="https://www.mindspore.cn/doc/programming_guide/en/r1.2/optim.html">Optimization Algorithms</a></p>
</section>
<section id="building-a-customized-network">
<h2>Building a Customized Network<a class="headerlink" href="#building-a-customized-network" title="Permalink to this headline"></a></h2>
<p>Both the network structure and the model layers (e.g. loss functions and optimizers mentioned above) are essentially a <code class="docutils literal notranslate"><span class="pre">Cell</span></code>. Therefore, they can be customized.</p>
<p>Construct a subclass inherited from <code class="docutils literal notranslate"><span class="pre">Cell</span></code>, define the operator and model layer in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, and build the network structure in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method.</p>
<p>Take the LeNet network as an example. Structure units such as the convolutional layer, pooling layer, and full connection layer are defined in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method, and the defined content is connected together in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method to form a complete LeNet network structure.</p>
<p>The LeNet network is implemented as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">LeNet5</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span> <span class="o">*</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="parameter.html" class="btn btn-neutral float-left" title="Parameter" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="network_component.html" class="btn btn-neutral float-right" title="Common Network Components" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>