<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tokenizer &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MindSpore Data Format Conversion" href="dataset_conversion.html" />
    <link rel="prev" title="Data Augmentation" href="augmentation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">Data Type</a></li>
<li class="toctree-l1"><a class="reference internal" href="compute_component.html">Compute Component</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="data_pipeline.html">Data Pipeline</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dataset_loading.html">Loading Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="sampler.html">Sampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeline.html">Processing Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="augmentation.html">Data Augmentation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tokenizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-tokenizers">MindSpore Tokenizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#berttokenizer">BertTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jiebatokenizer">JiebaTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sentencepiecetokenizer">SentencePieceTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unicodechartokenizer">UnicodeCharTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#whitespacetokenizer">WhitespaceTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#wordpiecetokenizer">WordpieceTokenizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dataset_conversion.html">MindSpore Data Format Conversion</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_augmentation.html">Auto Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="cache.html">Single-Node Tensor Cache</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="execution_management.html">Execution Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_usage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_list.html">Network List</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list.html">Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntax_list.html">Syntax list</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="data_pipeline.html">Data Pipeline</a> &raquo;</li>
      <li>Tokenizer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tokenizer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tokenizer">
<h1>Tokenizer<a class="headerlink" href="#tokenizer" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.2/docs/programming_guide/source_en/tokenizer.md"><img alt="View Source On Gitee" src="_images/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Tokenization is a process of re-combining continuous character sequences into word sequences according to certain specifications. Reasonable tokenization is helpful for semantic comprehension.</p>
<p>MindSpore provides a tokenizer for multiple purposes to help you process text with high performance. You can build your own dictionaries, use appropriate tokenizers to split sentences into different tokens, and search for indexes of the tokens in the dictionaries.</p>
<p>MindSpore provides the following tokenizers. In addition, you can customize tokenizers as required.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Tokenizer</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BasicTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on specified rules.</p></td>
</tr>
<tr class="row-odd"><td><p>BertTokenizer</p></td>
<td><p>Processes BERT text data.</p></td>
</tr>
<tr class="row-even"><td><p>JiebaTokenizer</p></td>
<td><p>Dictionary-based Chinese character string tokenizer.</p></td>
</tr>
<tr class="row-odd"><td><p>RegexTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on a specified regular expression.</p></td>
</tr>
<tr class="row-even"><td><p>SentencePieceTokenizer</p></td>
<td><p>Performs tokenization based on the open-source tool package SentencePiece.</p></td>
</tr>
<tr class="row-odd"><td><p>UnicodeCharTokenizer</p></td>
<td><p>Tokenizes scalar text data into Unicode characters.</p></td>
</tr>
<tr class="row-even"><td><p>UnicodeScriptTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on Unicode boundaries.</p></td>
</tr>
<tr class="row-odd"><td><p>WhitespaceTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on spaces.</p></td>
</tr>
<tr class="row-even"><td><p>WordpieceTokenizer</p></td>
<td><p>Performs tokenization on scalar text data based on the word set.</p></td>
</tr>
</tbody>
</table>
<p>For details about tokenizers, see <a class="reference external" href="https://www.mindspore.cn/doc/api_python/en/r1.2/mindspore/mindspore.dataset.text.html">MindSpore API</a>.</p>
</section>
<section id="mindspore-tokenizers">
<h2>MindSpore Tokenizers<a class="headerlink" href="#mindspore-tokenizers" title="Permalink to this headline"></a></h2>
<p>The following describes how to use common tokenizers.</p>
<section id="berttokenizer">
<h3>BertTokenizer<a class="headerlink" href="#berttokenizer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> performs tokenization by calling <code class="docutils literal notranslate"><span class="pre">BasicTokenizer</span></code> and <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code>.</p>
<p>The following example builds a text dataset and a character string list, uses <code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;床前明月光&quot;</span><span class="p">,</span> <span class="s2">&quot;疑是地上霜&quot;</span><span class="p">,</span> <span class="s2">&quot;举头望明月&quot;</span><span class="p">,</span> <span class="s2">&quot;低头思故乡&quot;</span><span class="p">,</span> <span class="s2">&quot;I am making small mistakes during working hours&quot;</span><span class="p">,</span>
                <span class="s2">&quot;😀嘿嘿😃哈哈😄大笑😁嘻嘻&quot;</span><span class="p">,</span> <span class="s2">&quot;繁體字&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">vocab_list</span> <span class="o">=</span> <span class="p">[</span>
  <span class="s2">&quot;床&quot;</span><span class="p">,</span> <span class="s2">&quot;前&quot;</span><span class="p">,</span> <span class="s2">&quot;明&quot;</span><span class="p">,</span> <span class="s2">&quot;月&quot;</span><span class="p">,</span> <span class="s2">&quot;光&quot;</span><span class="p">,</span> <span class="s2">&quot;疑&quot;</span><span class="p">,</span> <span class="s2">&quot;是&quot;</span><span class="p">,</span> <span class="s2">&quot;地&quot;</span><span class="p">,</span> <span class="s2">&quot;上&quot;</span><span class="p">,</span> <span class="s2">&quot;霜&quot;</span><span class="p">,</span> <span class="s2">&quot;举&quot;</span><span class="p">,</span> <span class="s2">&quot;头&quot;</span><span class="p">,</span> <span class="s2">&quot;望&quot;</span><span class="p">,</span> <span class="s2">&quot;低&quot;</span><span class="p">,</span> <span class="s2">&quot;思&quot;</span><span class="p">,</span> <span class="s2">&quot;故&quot;</span><span class="p">,</span> <span class="s2">&quot;乡&quot;</span><span class="p">,</span>
  <span class="s2">&quot;繁&quot;</span><span class="p">,</span> <span class="s2">&quot;體&quot;</span><span class="p">,</span> <span class="s2">&quot;字&quot;</span><span class="p">,</span> <span class="s2">&quot;嘿&quot;</span><span class="p">,</span> <span class="s2">&quot;哈&quot;</span><span class="p">,</span> <span class="s2">&quot;大&quot;</span><span class="p">,</span> <span class="s2">&quot;笑&quot;</span><span class="p">,</span> <span class="s2">&quot;嘻&quot;</span><span class="p">,</span> <span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;am&quot;</span><span class="p">,</span> <span class="s2">&quot;mak&quot;</span><span class="p">,</span> <span class="s2">&quot;make&quot;</span><span class="p">,</span> <span class="s2">&quot;small&quot;</span><span class="p">,</span> <span class="s2">&quot;mistake&quot;</span><span class="p">,</span>
  <span class="s2">&quot;##s&quot;</span><span class="p">,</span> <span class="s2">&quot;during&quot;</span><span class="p">,</span> <span class="s2">&quot;work&quot;</span><span class="p">,</span> <span class="s2">&quot;##ing&quot;</span><span class="p">,</span> <span class="s2">&quot;hour&quot;</span><span class="p">,</span> <span class="s2">&quot;😀&quot;</span><span class="p">,</span> <span class="s2">&quot;😃&quot;</span><span class="p">,</span> <span class="s2">&quot;😄&quot;</span><span class="p">,</span> <span class="s2">&quot;😁&quot;</span><span class="p">,</span> <span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="s2">&quot;12&quot;</span><span class="p">,</span>
  <span class="s2">&quot;28&quot;</span><span class="p">,</span> <span class="s2">&quot;40&quot;</span><span class="p">,</span> <span class="s2">&quot;16&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[unused1]&quot;</span><span class="p">,</span> <span class="s2">&quot;[unused10]&quot;</span><span class="p">]</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">)</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
床前明月光
疑是地上霜
举头望明月
低头思故乡
I am making small mistakes during working hours
😀嘿嘿😃哈哈😄大笑😁嘻嘻
繁體字
------------------------after tokenization-----------------------------
[&#39;床&#39; &#39;前&#39; &#39;明&#39; &#39;月&#39; &#39;光&#39;]
[&#39;疑&#39; &#39;是&#39; &#39;地&#39; &#39;上&#39; &#39;霜&#39;]
[&#39;举&#39; &#39;头&#39; &#39;望&#39; &#39;明&#39; &#39;月&#39;]
[&#39;低&#39; &#39;头&#39; &#39;思&#39; &#39;故&#39; &#39;乡&#39;]
[&#39;I&#39; &#39;am&#39; &#39;mak&#39; &#39;##ing&#39; &#39;small&#39; &#39;mistake&#39; &#39;##s&#39; &#39;during&#39; &#39;work&#39; &#39;##ing&#39;
 &#39;hour&#39; &#39;##s&#39;]
[&#39;😀&#39; &#39;嘿&#39; &#39;嘿&#39; &#39;😃&#39; &#39;哈&#39; &#39;哈&#39; &#39;😄&#39; &#39;大&#39; &#39;笑&#39; &#39;😁&#39; &#39;嘻&#39; &#39;嘻&#39;]
[&#39;繁&#39; &#39;體&#39; &#39;字&#39;]
</pre></div>
</div>
</section>
<section id="jiebatokenizer">
<h3>JiebaTokenizer<a class="headerlink" href="#jiebatokenizer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code> performs Chinese tokenization based on Jieba.</p>
<p>Download the dictionary files <code class="docutils literal notranslate"><span class="pre">hmm_model.utf8</span></code> and <code class="docutils literal notranslate"><span class="pre">jieba.dict.utf8</span></code> and put them in the specified location.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!wget<span class="w"> </span>-N<span class="w"> </span>https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/hmm_model.utf8
!wget<span class="w"> </span>-N<span class="w"> </span>https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/jieba.dict.utf8
!mkdir<span class="w"> </span>-p<span class="w"> </span>./datasets/tokenizer/
!mv<span class="w"> </span>hmm_model.utf8<span class="w"> </span>jieba.dict.utf8<span class="w"> </span>-t<span class="w"> </span>./datasets/tokenizer/
!tree<span class="w"> </span>./datasets/tokenizer/
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>./datasets/tokenizer/
├── hmm_model.utf8
└── jieba.dict.utf8

0 directories, 2 files
</pre></div>
</div>
<p>The following example builds a text dataset, uses the HMM and MP dictionary files to create a <code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code> object, performs tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;今天天气太好了我们一起去外面玩吧&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="c1"># files from open source repository https://github.com/yanyiwu/cppjieba/tree/master/dict</span>
<span class="n">HMM_FILE</span> <span class="o">=</span> <span class="s2">&quot;./datasets/tokenizer/hmm_model.utf8&quot;</span>
<span class="n">MP_FILE</span> <span class="o">=</span> <span class="s2">&quot;./datasets/tokenizer/jieba.dict.utf8&quot;</span>
<span class="n">jieba_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">JiebaTokenizer</span><span class="p">(</span><span class="n">HMM_FILE</span><span class="p">,</span> <span class="n">MP_FILE</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">jieba_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
今天天气太好了我们一起去外面玩吧
------------------------after tokenization-----------------------------
[&#39;今天天气&#39; &#39;太好了&#39; &#39;我们&#39; &#39;一起&#39; &#39;去&#39; &#39;外面&#39; &#39;玩吧&#39;]
</pre></div>
</div>
</section>
<section id="sentencepiecetokenizer">
<h3>SentencePieceTokenizer<a class="headerlink" href="#sentencepiecetokenizer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">SentencePieceTokenizer</span></code> performs tokenization based on an open-source natural language processing tool package <a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a>.</p>
<p>Download the text dataset file <code class="docutils literal notranslate"><span class="pre">botchan.txt</span></code> and place it in the specified location.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>!wget<span class="w"> </span>-N<span class="w"> </span>https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/botchan.txt
!mkdir<span class="w"> </span>-p<span class="w"> </span>./datasets/tokenizer/
!mv<span class="w"> </span>botchan.txt<span class="w"> </span>./datasets/tokenizer/
!tree<span class="w"> </span>./datasets/tokenizer/
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>./datasets/tokenizer/
└── botchan.txt

0 directories, 1 files
</pre></div>
</div>
<p>The following example builds a text dataset, creates a <code class="docutils literal notranslate"><span class="pre">vocab</span></code> object from the <code class="docutils literal notranslate"><span class="pre">vocab_file</span></code> file, uses <code class="docutils literal notranslate"><span class="pre">SentencePieceTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.text</span> <span class="kn">import</span> <span class="n">SentencePieceModel</span><span class="p">,</span> <span class="n">SPieceTokenizerOutType</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;I saw a girl with a telescope.&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="c1"># file from MindSpore repository https://gitee.com/mindspore/mindspore/blob/r1.2/tests/ut/data/dataset/test_sentencepiece/botchan.txt</span>
<span class="n">vocab_file</span> <span class="o">=</span> <span class="s2">&quot;./datasets/tokenizer/botchan.txt&quot;</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">SentencePieceVocab</span><span class="o">.</span><span class="n">from_file</span><span class="p">([</span><span class="n">vocab_file</span><span class="p">],</span> <span class="mi">5000</span><span class="p">,</span> <span class="mf">0.9995</span><span class="p">,</span> <span class="n">SentencePieceModel</span><span class="o">.</span><span class="n">UNIGRAM</span><span class="p">,</span> <span class="p">{})</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">SentencePieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">SPieceTokenizerOutType</span><span class="o">.</span><span class="n">STRING</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
I saw a girl with a telescope.
------------------------after tokenization-----------------------------
[&#39;▁I&#39; &#39;▁sa&#39; &#39;w&#39; &#39;▁a&#39; &#39;▁girl&#39; &#39;▁with&#39; &#39;▁a&#39; &#39;▁te&#39; &#39;les&#39; &#39;co&#39; &#39;pe&#39; &#39;.&#39;]
</pre></div>
</div>
</section>
<section id="unicodechartokenizer">
<h3>UnicodeCharTokenizer<a class="headerlink" href="#unicodechartokenizer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">UnicodeCharTokenizer</span></code> performs tokenization based on the Unicode character set.</p>
<p>The following example builds a text dataset, uses <code class="docutils literal notranslate"><span class="pre">UnicodeCharTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Welcome to Beijing!&quot;</span><span class="p">,</span> <span class="s2">&quot;北京欢迎您！&quot;</span><span class="p">,</span> <span class="s2">&quot;我喜欢English!&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
Welcome to Beijing!
北京欢迎您！
我喜欢English!
------------------------after tokenization-----------------------------
[&#39;W&#39;, &#39;e&#39;, &#39;l&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;e&#39;, &#39; &#39;, &#39;t&#39;, &#39;o&#39;, &#39; &#39;, &#39;B&#39;, &#39;e&#39;, &#39;i&#39;, &#39;j&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39;!&#39;]
[&#39;北&#39;, &#39;京&#39;, &#39;欢&#39;, &#39;迎&#39;, &#39;您&#39;, &#39;！&#39;]
[&#39;我&#39;, &#39;喜&#39;, &#39;欢&#39;, &#39;E&#39;, &#39;n&#39;, &#39;g&#39;, &#39;l&#39;, &#39;i&#39;, &#39;s&#39;, &#39;h&#39;, &#39;!&#39;]
</pre></div>
</div>
</section>
<section id="whitespacetokenizer">
<h3>WhitespaceTokenizer<a class="headerlink" href="#whitespacetokenizer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> performs tokenization based on spaces.</p>
<p>The following example builds a text dataset, uses <code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Welcome to Beijing!&quot;</span><span class="p">,</span> <span class="s2">&quot;北京欢迎您！&quot;</span><span class="p">,</span> <span class="s2">&quot;我喜欢English!&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
Welcome to Beijing!
北京欢迎您！
我喜欢English!
------------------------after tokenization-----------------------------
[&#39;Welcome&#39;, &#39;to&#39;, &#39;Beijing!&#39;]
[&#39;北京欢迎您！&#39;]
[&#39;我喜欢English!&#39;]
</pre></div>
</div>
</section>
<section id="wordpiecetokenizer">
<h3>WordpieceTokenizer<a class="headerlink" href="#wordpiecetokenizer" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code> performs tokenization based on the word set. A token can be a single word in the word set or a combination of words.</p>
<p>The following example builds a text dataset, creates a <code class="docutils literal notranslate"><span class="pre">vocab</span></code> object from the word list, uses <code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code> to perform tokenization on the dataset, and displays the text results before and after tokenization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;my&quot;</span><span class="p">,</span> <span class="s2">&quot;favorite&quot;</span><span class="p">,</span> <span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;during&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;cholera&quot;</span><span class="p">,</span> <span class="s2">&quot;era&quot;</span><span class="p">,</span> <span class="s2">&quot;what&quot;</span><span class="p">,</span>
    <span class="s2">&quot;我&quot;</span><span class="p">,</span> <span class="s2">&quot;最&quot;</span><span class="p">,</span> <span class="s2">&quot;喜&quot;</span><span class="p">,</span> <span class="s2">&quot;欢&quot;</span><span class="p">,</span> <span class="s2">&quot;的&quot;</span><span class="p">,</span> <span class="s2">&quot;书&quot;</span><span class="p">,</span> <span class="s2">&quot;是&quot;</span><span class="p">,</span> <span class="s2">&quot;霍&quot;</span><span class="p">,</span> <span class="s2">&quot;乱&quot;</span><span class="p">,</span> <span class="s2">&quot;时&quot;</span><span class="p">,</span> <span class="s2">&quot;期&quot;</span><span class="p">,</span> <span class="s2">&quot;的&quot;</span><span class="p">,</span> <span class="s2">&quot;爱&quot;</span><span class="p">,</span> <span class="s2">&quot;情&quot;</span><span class="p">,</span> <span class="s2">&quot;您&quot;</span><span class="p">]</span>
<span class="n">vocab_english</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;cholera&quot;</span><span class="p">,</span> <span class="s2">&quot;era&quot;</span><span class="p">,</span> <span class="s2">&quot;favor&quot;</span><span class="p">,</span> <span class="s2">&quot;##ite&quot;</span><span class="p">,</span> <span class="s2">&quot;my&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;dur&quot;</span><span class="p">,</span> <span class="s2">&quot;##ing&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">]</span>
<span class="n">vocab_chinese</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;我&quot;</span><span class="p">,</span> <span class="s1">&#39;最&#39;</span><span class="p">,</span> <span class="s1">&#39;喜&#39;</span><span class="p">,</span> <span class="s1">&#39;欢&#39;</span><span class="p">,</span> <span class="s1">&#39;的&#39;</span><span class="p">,</span> <span class="s1">&#39;书&#39;</span><span class="p">,</span> <span class="s1">&#39;是&#39;</span><span class="p">,</span> <span class="s1">&#39;霍&#39;</span><span class="p">,</span> <span class="s1">&#39;乱&#39;</span><span class="p">,</span> <span class="s1">&#39;时&#39;</span><span class="p">,</span> <span class="s1">&#39;期&#39;</span><span class="p">,</span> <span class="s1">&#39;爱&#39;</span><span class="p">,</span> <span class="s1">&#39;情&#39;</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">vocab_english</span><span class="o">+</span><span class="n">vocab_chinese</span><span class="p">)</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WordpieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
my
favorite
book
is
love
during
the
cholera
era
what
我
最
喜
欢
的
书
是
霍
乱
时
期
的
爱
情
您
------------------------after tokenization-----------------------------
[&#39;my&#39;]
[&#39;favor&#39; &#39;##ite&#39;]
[&#39;book&#39;]
[&#39;is&#39;]
[&#39;love&#39;]
[&#39;dur&#39; &#39;##ing&#39;]
[&#39;the&#39;]
[&#39;cholera&#39;]
[&#39;era&#39;]
[&#39;[UNK]&#39;]
[&#39;我&#39;]
[&#39;最&#39;]
[&#39;喜&#39;]
[&#39;欢&#39;]
[&#39;的&#39;]
[&#39;书&#39;]
[&#39;是&#39;]
[&#39;霍&#39;]
[&#39;乱&#39;]
[&#39;时&#39;]
[&#39;期&#39;]
[&#39;的&#39;]
[&#39;爱&#39;]
[&#39;情&#39;]
[&#39;[UNK]&#39;]
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="augmentation.html" class="btn btn-neutral float-left" title="Data Augmentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dataset_conversion.html" class="btn btn-neutral float-right" title="MindSpore Data Format Conversion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>