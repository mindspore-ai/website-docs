<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>分布式并行 &mdash; MindSpore r1.1 documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="进阶用法" href="advanced_usage.html" />
    <link rel="prev" title="Callback机制" href="callback.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore API概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">数据类型</a></li>
<li class="toctree-l1"><a class="reference internal" href="compute_component.html">计算组件</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_pipeline.html">数据管道</a></li>
<li class="toctree-l1"><a class="reference internal" href="execution_management.html">执行管理</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">分布式并行</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id3">分布式并行配置</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">通用配置</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#device-num">device_num</a></li>
<li class="toctree-l4"><a class="reference internal" href="#global-rank">global_rank</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gradients-mean">gradients_mean</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parallel-mode">parallel_mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#all-reduce-fusion-config">all_reduce_fusion_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="#enable-parallel-optimizer">enable_parallel_optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameter-broadcast">parameter_broadcast</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">自动并行配置</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gradient-fp32-sync">gradient_fp32_sync</a></li>
<li class="toctree-l4"><a class="reference internal" href="#auto-parallel-search-mode">auto_parallel_search_mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#strategy-ckpt-load-file">strategy_ckpt_load_file</a></li>
<li class="toctree-l4"><a class="reference internal" href="#strategy-ckpt-save-file">strategy_ckpt_save_file</a></li>
<li class="toctree-l4"><a class="reference internal" href="#full-batch">full_batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pipeline-stages">pipeline_stages</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id6">分布式通信接口</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#init">init</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-group-size">get_group_size</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-rank">get_rank</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id7">分布式属性配置</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cross-batch">cross_batch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fusion">fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#layerwise-parallel">layerwise_parallel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id8">数据并行</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id9">自动并行</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="advanced_usage.html">进阶用法</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_list.html">网络支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list.html">算子支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntax_list.html">语法支持</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>分布式并行</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/auto_parallel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>分布式并行<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/docs/programming_guide/source_zh_cn/auto_parallel.md" target="_blank"><img src="./_static/logo_source.png"></a></p>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>在深度学习中，当数据集和参数量的规模越来越大，训练所需的时间和硬件资源会随之增加，最后会变成制约训练的瓶颈。分布式并行训练，可以降低对内存、计算性能等硬件的需求，是进行训练的重要优化手段。</p>
<p>MindSpore提供了分布式并行训练的功能，它支持了包括数据并行和自动并行在内的多种并行模式。</p>
</section>
<section id="id3">
<h2>分布式并行配置<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>MindSpore的分布式并行配置通过<code class="docutils literal notranslate"><span class="pre">auto_parallel_context</span></code>来进行集中管理，用户可根据自身需求和实际情况来进行个性化的配置。这些配置可分为三大类：</p>
<ul class="simple">
<li><p>通用配置：对数据并行、自动并行以及混合并行均起作用的配置，如：<code class="docutils literal notranslate"><span class="pre">device_num</span></code>、<code class="docutils literal notranslate"><span class="pre">global_rank</span></code>等。</p></li>
<li><p>自动并行配置：仅在自动并行模式下起作用的配置，如：<code class="docutils literal notranslate"><span class="pre">auto_parallel_search_mode</span></code>、<code class="docutils literal notranslate"><span class="pre">gradient_fp32_sync</span></code>等。</p></li>
</ul>
<p>用户可利用<code class="docutils literal notranslate"><span class="pre">context.set_auto_parallel_context</span></code>配置上述参数，同时可通过<code class="docutils literal notranslate"><span class="pre">context.get_auto_parallel_context</span></code>来获取上述参数。</p>
<section id="id4">
<h3>通用配置<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<section id="device-num">
<h4>device_num<a class="headerlink" href="#device-num" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">device_num</span></code>表示可用的机器数，其值为int型，默认值是0，且必须在1~4096范围内。若用户不配置，<code class="docutils literal notranslate"><span class="pre">Model</span></code>接口内部则会通过<code class="docutils literal notranslate"><span class="pre">get_group_size</span></code>方法获取，若用户进行了配置，则遵循用户的配置。这个配置可以在用户不使用<code class="docutils literal notranslate"><span class="pre">Model</span></code>接口的情况下，手动传递<code class="docutils literal notranslate"><span class="pre">device_num</span></code>。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;device_num&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="global-rank">
<h4>global_rank<a class="headerlink" href="#global-rank" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">global_rank</span></code>表示当前卡的逻辑序号，其值为int型，默认值是0，且必须在0~4095范围内。若用户不配置，<code class="docutils literal notranslate"><span class="pre">Model</span></code>接口内部则会通过<code class="docutils literal notranslate"><span class="pre">get_rank</span></code>方法获取，若用户进行了配置，则遵循用户的配置。这个配置可以在用户不使用<code class="docutils literal notranslate"><span class="pre">Model</span></code>接口的情况下，手动传递<code class="docutils literal notranslate"><span class="pre">global_rank</span></code>。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">global_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;global_rank&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gradients-mean">
<h4>gradients_mean<a class="headerlink" href="#gradients-mean" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">gradients_mean</span></code>表示在反向梯度进行聚合时，是否进行平均操作。其值为bool型，默认为False，即梯度聚合仅进行AllReduce的SUM操作，不做平均操作。<code class="docutils literal notranslate"><span class="pre">gradients_mean</span></code>会影响网络的收敛，不同场景，<code class="docutils literal notranslate"><span class="pre">gradients_mean</span></code>的设置可能不同。因此，MindSpore提供这个接口让用户根据实际情况来配置。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradients_mean</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;gradients_mean&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parallel-mode">
<h4>parallel_mode<a class="headerlink" href="#parallel-mode" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code>表示并行模式，其值为字符串类型。用户可选择的模式有：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">stand_alone</span></code>：单机模式。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>：数据并行模式。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hybrid_parallel</span></code>：混合并行模式。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code>：半自动并行模式，即用户可通过<code class="docutils literal notranslate"><span class="pre">shard</span></code>方法给算子配置切分策略，若不配置策略，则默认是数据并行策略。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>：自动并行模式，即框架会自动建立代价模型，为用户选择最优的切分策略。</p></li>
</ul>
<p>其中<code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>和<code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>在MindSpore教程中有完整样例：</p>
<p><a class="reference external" href="https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_tutorials.html">https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_tutorials.html</a>。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;semi_auto_parallel&quot;</span><span class="p">)</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="all-reduce-fusion-config">
<h4>all_reduce_fusion_config<a class="headerlink" href="#all-reduce-fusion-config" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">all_reduce_fusion_config</span></code>可以让用户自定义梯度AllReduce融合切分策略。出于减少资源消耗及算子执行间隙的目的，框架默认将所有反向梯度聚合的AllReduce融合成一个算子运算，但当模型较大时，这会造成迭代拖尾耗时增加。用户可结合具体网络，通过设置该参数，手动调优找到性能最好的融合切分策略。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">35</span><span class="p">])</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;all_reduce_fusion_config&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>样例中，<code class="docutils literal notranslate"><span class="pre">all_reduce_fusion_config</span></code>的值为[20, 35]，将前20个AllReduce融合成1个，第20～35个AllReduce融合成1个，剩下的AllReduce融合成1个。</p>
</section>
<section id="enable-parallel-optimizer">
<h4>enable_parallel_optimizer<a class="headerlink" href="#enable-parallel-optimizer" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">enable_parallel_optimizer</span></code>是一个开发中特性，参数默认值是False。数据并行时参数更新部分在各卡间存在冗余计算，优化器并行通过将优化器的计算量分散到各个卡上，在大规模网络上（比如Bert、GPT）可以有效减少内存消耗并提升网络性能。</p>
<p>在<code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>模式下使能优化器并行，框架会将需要更新的参数进行分组到不同卡上，各自更新后再通过<code class="docutils literal notranslate"><span class="pre">Broadcast</span></code>算子在集群间做权重共享。需要注意的是参数量应当大于机器数，当前只支持<code class="docutils literal notranslate"><span class="pre">Lamb</span></code>和<code class="docutils literal notranslate"><span class="pre">AdamWeightDecay</span></code>优化器。</p>
<p>在<code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>或者<code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code>模式下使能优化器并行，如果经过策略切分后的参数在机器间存在重复切片，并且shape的最高维可以被卡数整除，框架会以最小切片的方式保存参数并在优化器中更新。该模式下支持所有优化器。</p>
<p>无论是哪种模式，优化器并行不会影响原有正反向网络的计算图，只会影响参数更新的计算量和计算逻辑。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;enable_parallel_optimizer&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parameter-broadcast">
<h4>parameter_broadcast<a class="headerlink" href="#parameter-broadcast" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">parameter_broadcast</span></code>将数据并行参数在0号卡上的权值广播到其他卡上，达到同步初始化权重的目的。参数默认值是False，当前仅支持图模式。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parameter_broadcast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parameter_broadcast&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id5">
<h3>自动并行配置<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<section id="gradient-fp32-sync">
<h4>gradient_fp32_sync<a class="headerlink" href="#gradient-fp32-sync" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">gradient_fp32_sync</span></code>表示梯度是否以float32类型进行聚合，其值为bool类型，默认为True，即梯度以float32类型进行聚合。由于<code class="docutils literal notranslate"><span class="pre">Ascend</span></code>AI处理器的特殊构造，float32类型的数据进行聚合的速度要高于float16，但可能会影响精度。因此，MindSpore提供<code class="docutils literal notranslate"><span class="pre">gradient_fp32_sync</span></code>接口，让用户自己根据实际情况去进行取舍。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;gradient_fp32_sync&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="auto-parallel-search-mode">
<h4>auto_parallel_search_mode<a class="headerlink" href="#auto-parallel-search-mode" title="Permalink to this headline"></a></h4>
<p>MindSpore提供了<code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>和<code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code>两种搜索策略的算法，默认是<code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>。<code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code>能够搜索出代价模型刻画的最优策略，但在搜索巨大网络模型的并行策略时耗时较长；而<code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code>能瞬间搜索出并行策略，同时在已验证的常用网络中搜索出来的策略是最优策略，但在未经验证的某些特殊网络中可能找到次优策略。为此，MindSpore提供了参数，让用户自由选择搜索算法。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="s2">&quot;recursive_programming&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;auto_parallel_search_mode&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="strategy-ckpt-load-file">
<h4>strategy_ckpt_load_file<a class="headerlink" href="#strategy-ckpt-load-file" title="Permalink to this headline"></a></h4>
<p>指定加载路径，加载自动并行中所有带有权重的算子的切分信息。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="s2">&quot;./&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;strategy_ckpt_load_file&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="strategy-ckpt-save-file">
<h4>strategy_ckpt_save_file<a class="headerlink" href="#strategy-ckpt-save-file" title="Permalink to this headline"></a></h4>
<p>指定存储路径，存储自动并行中所有带有权重的算子的切分信息。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;./&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;strategy_ckpt_save_file&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="full-batch">
<h4>full_batch<a class="headerlink" href="#full-batch" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">full_batch</span></code>可以让用户决定数据集是否以全量导入。默认是False。即数据集以数据并行的方式导入。在特殊场景下，数据集全量导入的性能要优于数据并行方式导入，比如WideDeep网络的非均匀切分场景。因此，MindSpore提供<code class="docutils literal notranslate"><span class="pre">full_batch</span></code>可配置接口。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;full_batch&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pipeline-stages">
<h4>pipeline_stages<a class="headerlink" href="#pipeline-stages" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">pipeline_stages</span></code>是用来设置<code class="docutils literal notranslate"><span class="pre">pipeline</span></code>并行的<code class="docutils literal notranslate"><span class="pre">stage</span></code>信息。用来表明机器在<code class="docutils literal notranslate"><span class="pre">pipeline</span></code>并行下是如何分布的。目前<code class="docutils literal notranslate"><span class="pre">pipeline</span></code>并行仍在开发中。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">pipeline_stage</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;pipeline_stage&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="id6">
<h2>分布式通信接口<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">mindspore.communication.management</span></code>中封装了分布式并行用到的集合通信接口，方便用户配置分布式信息。</p>
<section id="init">
<h3>init<a class="headerlink" href="#init" title="Permalink to this headline"></a></h3>
<p>使能MindSpore通信，并完成分布式训练初始化操作。<code class="docutils literal notranslate"><span class="pre">init</span></code>要在<code class="docutils literal notranslate"><span class="pre">context.set_context</span></code>之后调用。用户可给<code class="docutils literal notranslate"><span class="pre">init</span></code>传入通信后端信息，<code class="docutils literal notranslate"><span class="pre">init</span></code>会根据不同的后端来进行不同初始化。</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">hccl</span></code>：全名为<code class="docutils literal notranslate"><span class="pre">Huawei</span> <span class="pre">Collective</span> <span class="pre">Communication</span> <span class="pre">Library</span></code>。用于<code class="docutils literal notranslate"><span class="pre">Ascend</span></code>处理器平台。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nccl</span></code>：全名为<code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Collective</span> <span class="pre">Communication</span> <span class="pre">Library</span></code>。用于<code class="docutils literal notranslate"><span class="pre">GPU</span></code>处理器平台。</p></li>
</ul>
<p>若用户不配置通信后端，MindSpore会根据<code class="docutils literal notranslate"><span class="pre">context</span></code>中的<code class="docutils literal notranslate"><span class="pre">device_target</span></code>信息进行自动配置。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="get-group-size">
<h3>get_group_size<a class="headerlink" href="#get-group-size" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">get_group_size</span></code>可让用户获取集群数量。在用<code class="docutils literal notranslate"><span class="pre">get_group_size</span></code>接口之前，要先调用<code class="docutils literal notranslate"><span class="pre">init</span></code>。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">group_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="get-rank">
<h3>get_rank<a class="headerlink" href="#get-rank" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">get_rank</span></code>可让用户获取当前设备在集群中的ID。在用<code class="docutils literal notranslate"><span class="pre">get_rank</span></code>接口之前，要先调用<code class="docutils literal notranslate"><span class="pre">init</span></code>。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s1">&#39;GPU&#39;</span><span class="p">)</span>
<span class="n">init</span><span class="p">()</span>
<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="id7">
<h2>分布式属性配置<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<section id="cross-batch">
<h3>cross_batch<a class="headerlink" href="#cross-batch" title="Permalink to this headline"></a></h3>
<p>在特定场景下，<code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>的计算逻辑和<code class="docutils literal notranslate"><span class="pre">stand_alone</span></code>是不一样的，<code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>在任何场景下都是和<code class="docutils literal notranslate"><span class="pre">stand_alone</span></code>的计算逻辑保持一致。而<code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>的收敛效果可能更好，因此MindSpore提供了<code class="docutils literal notranslate"><span class="pre">cross_batch</span></code>这个参数，可以使<code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>的计算逻辑和<code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>保持一致，用户可通过<code class="docutils literal notranslate"><span class="pre">add_prim_attr</span></code>方法进行配置，默认值是False。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cross_batch&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fusion">
<h3>fusion<a class="headerlink" href="#fusion" title="Permalink to this headline"></a></h3>
<p>出于性能考虑，MindSpore提供了<code class="docutils literal notranslate"><span class="pre">AllGather</span></code>和<code class="docutils literal notranslate"><span class="pre">AllReduce</span></code>算子的融合功能，<code class="docutils literal notranslate"><span class="pre">fusion</span></code>值相同的同类算子（算子类型以及通信域相同）会融合在一起，<code class="docutils literal notranslate"><span class="pre">fusion</span></code>的值必须大于等于0，且当<code class="docutils literal notranslate"><span class="pre">fusion</span></code>值为0时，表示不融合。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">allreduce1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">()</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;fusion&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">allreduce2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">()</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;fusion&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="layerwise-parallel">
<h3>layerwise_parallel<a class="headerlink" href="#layerwise-parallel" title="Permalink to this headline"></a></h3>
<p>在<code class="docutils literal notranslate"><span class="pre">HYBRID_PARALLEL</span></code>模式下用户需要手动切分模型，其中对于模型并行的参数用户需要手动打上标记<code class="docutils literal notranslate"><span class="pre">layerwise_parallel</span></code>，框架会根据此标记为模型并行参数过滤掉梯度聚合操作。</p>
<p>代码样例如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">imoprt</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Tensor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])),</span> <span class="n">layerwise_parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id8">
<h2>数据并行<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h2>
<p>数据并行是对数据进行切分的并行模式，一般按照batch维度切分，将数据分配到各个计算单元（worker）中，进行模型计算。在数据并行模式下，数据集要以数据并行的方式导入，并且<code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code>要设置为<code class="docutils literal notranslate"><span class="pre">data_parallel</span></code>。</p>
<p>具体用例请参考MindSpore分布式并行训练教程：</p>
<p><a class="reference external" href="https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_tutorials.html">https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_tutorials.html</a>。</p>
</section>
<section id="id9">
<h2>自动并行<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<p>自动并行是融合了数据并行、模型并行及混合并行的一种分布式并行模式，可以自动建立代价模型，为用户选择一种并行模式。其中，代价模型指基于内存的计算开销和通信开销对训练时间建模，并设计高效的算法找到训练时间较短的并行策略。在自动并行模式下，<code class="docutils literal notranslate"><span class="pre">parallel_mode</span></code>要设置为<code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code>。</p>
<p>具体用例请参考MindSpore分布式并行训练教程：</p>
<p><a class="reference external" href="https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_tutorials.html">https://www.mindspore.cn/tutorial/training/zh-CN/r1.1/advanced_use/distributed_training_tutorials.html</a>。</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="callback.html" class="btn btn-neutral float-left" title="Callback机制" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="advanced_usage.html" class="btn btn-neutral float-right" title="进阶用法" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>