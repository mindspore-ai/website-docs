<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>åˆ†è¯å™¨ &mdash; MindSpore r1.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="MindSporeæ•°æ®æ ¼å¼è½¬æ¢" href="dataset_conversion.html" />
    <link rel="prev" title="æ•°æ®å¢å¼º" href="augmentation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="api_structure.html">MindSpore APIæ¦‚è¿°</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_type.html">æ•°æ®ç±»å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="compute_component.html">è®¡ç®—ç»„ä»¶</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="data_pipeline.html">æ•°æ®ç®¡é“</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dataset_loading.html">æ•°æ®é›†åŠ è½½</a></li>
<li class="toctree-l2"><a class="reference internal" href="sampler.html">é‡‡æ ·å™¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeline.html">æ•°æ®å¤„ç†</a></li>
<li class="toctree-l2"><a class="reference internal" href="augmentation.html">æ•°æ®å¢å¼º</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">åˆ†è¯å™¨</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">æ¦‚è¿°</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mindspore">MindSporeåˆ†è¯å™¨</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#berttokenizer">BertTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jiebatokenizer">JiebaTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sentencepiecetokenizer">SentencePieceTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unicodechartokenizer">UnicodeCharTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#whitespacetokenizer">WhitespaceTokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="#wordpiecetokenizer">WordpieceTokenizer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dataset_conversion.html">MindSporeæ•°æ®æ ¼å¼è½¬æ¢</a></li>
<li class="toctree-l2"><a class="reference internal" href="auto_augmentation.html">è‡ªåŠ¨æ•°æ®å¢å¼º</a></li>
<li class="toctree-l2"><a class="reference internal" href="cache.html">å•èŠ‚ç‚¹æ•°æ®ç¼“å­˜</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="execution_management.html">æ‰§è¡Œç®¡ç†</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_parallel.html">åˆ†å¸ƒå¼å¹¶è¡Œ</a></li>
<li class="toctree-l1"><a class="reference internal" href="advanced_usage.html">è¿›é˜¶ç”¨æ³•</a></li>
<li class="toctree-l1"><a class="reference internal" href="network_list.html">ç½‘ç»œæ”¯æŒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list.html">ç®—å­æ”¯æŒ</a></li>
<li class="toctree-l1"><a class="reference internal" href="syntax_list.html">è¯­æ³•æ”¯æŒ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="data_pipeline.html">æ•°æ®ç®¡é“</a> &raquo;</li>
      <li>åˆ†è¯å™¨</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tokenizer.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>åˆ†è¯å™¨<a class="headerlink" href="#id1" title="Permalink to this headline">ïƒ</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.1/docs/programming_guide/source_zh_cn/tokenizer.md" target="_blank"><img src="./_static/logo_source.png"></a>
Â Â 
<a href="https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.1/programming_guide/mindspore_tokenizer.ipynb"><img src="./_static/logo_notebook.png"></a>
Â Â 
<a href="https://console.huaweicloud.com/modelarts/?region=cn-north-4#/notebook/loading?share-url-b64=aHR0cHM6Ly9vYnMuZHVhbHN0YWNrLmNuLW5vcnRoLTQubXlodWF3ZWljbG91ZC5jb20vbWluZHNwb3JlLXdlYnNpdGUvbm90ZWJvb2svbW9kZWxhcnRzL3Byb2dyYW1taW5nX2d1aWRlL21pbmRzcG9yZV90b2tlbml6ZXIuaXB5bmI=&image_id=65f636a0-56cf-49df-b941-7d2a07ba8c8c" target="_blank"><img src="./_static/logo_modelarts.png"></a></p>
<section id="id2">
<h2>æ¦‚è¿°<a class="headerlink" href="#id2" title="Permalink to this headline">ïƒ</a></h2>
<p>åˆ†è¯å°±æ˜¯å°†è¿ç»­çš„å­—åºåˆ—æŒ‰ç…§ä¸€å®šçš„è§„èŒƒé‡æ–°ç»„åˆæˆè¯åºåˆ—çš„è¿‡ç¨‹ï¼Œåˆç†çš„è¿›è¡Œåˆ†è¯æœ‰åŠ©äºè¯­ä¹‰çš„ç†è§£ã€‚</p>
<p>MindSporeæä¾›äº†å¤šç§ç”¨é€”çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·é«˜æ€§èƒ½åœ°å¤„ç†æ–‡æœ¬ï¼Œç”¨æˆ·å¯ä»¥æ„å»ºè‡ªå·±çš„å­—å…¸ï¼Œä½¿ç”¨é€‚å½“çš„æ ‡è®°å™¨å°†å¥å­æ‹†åˆ†ä¸ºä¸åŒçš„æ ‡è®°ï¼Œå¹¶é€šè¿‡æŸ¥æ‰¾æ“ä½œè·å–å­—å…¸ä¸­æ ‡è®°çš„ç´¢å¼•ã€‚</p>
<p>MindSporeç›®å‰æä¾›çš„åˆ†è¯å™¨å¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚æ­¤å¤–ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å®ç°è‡ªå®šä¹‰çš„åˆ†è¯å™¨ã€‚</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>åˆ†è¯å™¨</p></th>
<th class="head"><p>åˆ†è¯å™¨è¯´æ˜</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>BasicTokenizer</p></td>
<td><p>æ ¹æ®æŒ‡å®šè§„åˆ™å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚</p></td>
</tr>
<tr class="row-odd"><td><p>BertTokenizer</p></td>
<td><p>ç”¨äºå¤„ç†Bertæ–‡æœ¬æ•°æ®çš„åˆ†è¯å™¨ã€‚</p></td>
</tr>
<tr class="row-even"><td><p>JiebaTokenizer</p></td>
<td><p>åŸºäºå­—å…¸çš„ä¸­æ–‡å­—ç¬¦ä¸²åˆ†è¯å™¨ã€‚</p></td>
</tr>
<tr class="row-odd"><td><p>RegexTokenizer</p></td>
<td><p>æ ¹æ®æŒ‡å®šæ­£åˆ™è¡¨è¾¾å¼å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚</p></td>
</tr>
<tr class="row-even"><td><p>SentencePieceTokenizer</p></td>
<td><p>åŸºäºSentencePieceå¼€æºå·¥å…·åŒ…è¿›è¡Œåˆ†è¯ã€‚</p></td>
</tr>
<tr class="row-odd"><td><p>UnicodeCharTokenizer</p></td>
<td><p>å°†æ ‡é‡æ–‡æœ¬æ•°æ®åˆ†è¯ä¸ºUnicodeå­—ç¬¦ã€‚</p></td>
</tr>
<tr class="row-even"><td><p>UnicodeScriptTokenizer</p></td>
<td><p>æ ¹æ®Unicodeè¾¹ç•Œå¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚</p></td>
</tr>
<tr class="row-odd"><td><p>WhitespaceTokenizer</p></td>
<td><p>æ ¹æ®ç©ºæ ¼ç¬¦å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚</p></td>
</tr>
<tr class="row-even"><td><p>WordpieceTokenizer</p></td>
<td><p>æ ¹æ®å•è¯é›†å¯¹æ ‡é‡æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ã€‚</p></td>
</tr>
</tbody>
</table>
<p>æ›´å¤šåˆ†è¯å™¨çš„è¯¦ç»†è¯´æ˜ï¼Œå¯ä»¥å‚è§<a class="reference external" href="https://www.mindspore.cn/doc/api_python/zh-CN/r1.1/mindspore/mindspore.dataset.text.html">APIæ–‡æ¡£</a>ã€‚</p>
</section>
<section id="mindspore">
<h2>MindSporeåˆ†è¯å™¨<a class="headerlink" href="#mindspore" title="Permalink to this headline">ïƒ</a></h2>
<p>ä¸‹é¢ä»‹ç»å‡ ç§å¸¸ç”¨åˆ†è¯å™¨çš„ä½¿ç”¨æ–¹æ³•ã€‚</p>
<section id="berttokenizer">
<h3>BertTokenizer<a class="headerlink" href="#berttokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code>æ˜¯é€šè¿‡è°ƒç”¨<code class="docutils literal notranslate"><span class="pre">BasicTokenizer</span></code>å’Œ<code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code>æ¥è¿›è¡Œåˆ†è¯çš„ã€‚</p>
<p>ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†å’Œå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œç„¶åé€šè¿‡<code class="docutils literal notranslate"><span class="pre">BertTokenizer</span></code>å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;åºŠå‰æ˜æœˆå…‰&quot;</span><span class="p">,</span> <span class="s2">&quot;ç–‘æ˜¯åœ°ä¸Šéœœ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¸¾å¤´æœ›æ˜æœˆ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä½å¤´æ€æ•…ä¹¡&quot;</span><span class="p">,</span> <span class="s2">&quot;I am making small mistakes during working hours&quot;</span><span class="p">,</span>
                <span class="s2">&quot;ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»&quot;</span><span class="p">,</span> <span class="s2">&quot;ç¹é«”å­—&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">vocab_list</span> <span class="o">=</span> <span class="p">[</span>
  <span class="s2">&quot;åºŠ&quot;</span><span class="p">,</span> <span class="s2">&quot;å‰&quot;</span><span class="p">,</span> <span class="s2">&quot;æ˜&quot;</span><span class="p">,</span> <span class="s2">&quot;æœˆ&quot;</span><span class="p">,</span> <span class="s2">&quot;å…‰&quot;</span><span class="p">,</span> <span class="s2">&quot;ç–‘&quot;</span><span class="p">,</span> <span class="s2">&quot;æ˜¯&quot;</span><span class="p">,</span> <span class="s2">&quot;åœ°&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¸Š&quot;</span><span class="p">,</span> <span class="s2">&quot;éœœ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¸¾&quot;</span><span class="p">,</span> <span class="s2">&quot;å¤´&quot;</span><span class="p">,</span> <span class="s2">&quot;æœ›&quot;</span><span class="p">,</span> <span class="s2">&quot;ä½&quot;</span><span class="p">,</span> <span class="s2">&quot;æ€&quot;</span><span class="p">,</span> <span class="s2">&quot;æ•…&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¹¡&quot;</span><span class="p">,</span>
  <span class="s2">&quot;ç¹&quot;</span><span class="p">,</span> <span class="s2">&quot;é«”&quot;</span><span class="p">,</span> <span class="s2">&quot;å­—&quot;</span><span class="p">,</span> <span class="s2">&quot;å˜¿&quot;</span><span class="p">,</span> <span class="s2">&quot;å“ˆ&quot;</span><span class="p">,</span> <span class="s2">&quot;å¤§&quot;</span><span class="p">,</span> <span class="s2">&quot;ç¬‘&quot;</span><span class="p">,</span> <span class="s2">&quot;å˜»&quot;</span><span class="p">,</span> <span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="s2">&quot;am&quot;</span><span class="p">,</span> <span class="s2">&quot;mak&quot;</span><span class="p">,</span> <span class="s2">&quot;make&quot;</span><span class="p">,</span> <span class="s2">&quot;small&quot;</span><span class="p">,</span> <span class="s2">&quot;mistake&quot;</span><span class="p">,</span>
  <span class="s2">&quot;##s&quot;</span><span class="p">,</span> <span class="s2">&quot;during&quot;</span><span class="p">,</span> <span class="s2">&quot;work&quot;</span><span class="p">,</span> <span class="s2">&quot;##ing&quot;</span><span class="p">,</span> <span class="s2">&quot;hour&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜€&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜ƒ&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜„&quot;</span><span class="p">,</span> <span class="s2">&quot;ğŸ˜&quot;</span><span class="p">,</span> <span class="s2">&quot;+&quot;</span><span class="p">,</span> <span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="s2">&quot;12&quot;</span><span class="p">,</span>
  <span class="s2">&quot;28&quot;</span><span class="p">,</span> <span class="s2">&quot;40&quot;</span><span class="p">,</span> <span class="s2">&quot;16&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;I&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[unused1]&quot;</span><span class="p">,</span> <span class="s2">&quot;[unused10]&quot;</span><span class="p">]</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">)</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
åºŠå‰æ˜æœˆå…‰
ç–‘æ˜¯åœ°ä¸Šéœœ
ä¸¾å¤´æœ›æ˜æœˆ
ä½å¤´æ€æ•…ä¹¡
I am making small mistakes during working hours
ğŸ˜€å˜¿å˜¿ğŸ˜ƒå“ˆå“ˆğŸ˜„å¤§ç¬‘ğŸ˜å˜»å˜»
ç¹é«”å­—
------------------------after tokenization-----------------------------
[&#39;åºŠ&#39; &#39;å‰&#39; &#39;æ˜&#39; &#39;æœˆ&#39; &#39;å…‰&#39;]
[&#39;ç–‘&#39; &#39;æ˜¯&#39; &#39;åœ°&#39; &#39;ä¸Š&#39; &#39;éœœ&#39;]
[&#39;ä¸¾&#39; &#39;å¤´&#39; &#39;æœ›&#39; &#39;æ˜&#39; &#39;æœˆ&#39;]
[&#39;ä½&#39; &#39;å¤´&#39; &#39;æ€&#39; &#39;æ•…&#39; &#39;ä¹¡&#39;]
[&#39;I&#39; &#39;am&#39; &#39;mak&#39; &#39;##ing&#39; &#39;small&#39; &#39;mistake&#39; &#39;##s&#39; &#39;during&#39; &#39;work&#39; &#39;##ing&#39;
 &#39;hour&#39; &#39;##s&#39;]
[&#39;ğŸ˜€&#39; &#39;å˜¿&#39; &#39;å˜¿&#39; &#39;ğŸ˜ƒ&#39; &#39;å“ˆ&#39; &#39;å“ˆ&#39; &#39;ğŸ˜„&#39; &#39;å¤§&#39; &#39;ç¬‘&#39; &#39;ğŸ˜&#39; &#39;å˜»&#39; &#39;å˜»&#39;]
[&#39;ç¹&#39; &#39;é«”&#39; &#39;å­—&#39;]
</pre></div>
</div>
</section>
<section id="jiebatokenizer">
<h3>JiebaTokenizer<a class="headerlink" href="#jiebatokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code>æ˜¯åŸºäºjiebaçš„ä¸­æ–‡åˆ†è¯ã€‚</p>
<p>ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä½¿ç”¨HMMä¸MPå­—å…¸æ–‡ä»¶åˆ›å»º<code class="docutils literal notranslate"><span class="pre">JiebaTokenizer</span></code>å¯¹è±¡ï¼Œå¹¶å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œæœ€åå±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="c1"># files from open source repository https://github.com/yanyiwu/cppjieba/tree/master/dict</span>
<span class="n">HMM_FILE</span> <span class="o">=</span> <span class="s2">&quot;hmm_model.utf8&quot;</span>
<span class="n">MP_FILE</span> <span class="o">=</span> <span class="s2">&quot;jieba.dict.utf8&quot;</span>
<span class="n">jieba_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">JiebaTokenizer</span><span class="p">(</span><span class="n">HMM_FILE</span><span class="p">,</span> <span class="n">MP_FILE</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">jieba_op</span><span class="p">,</span> <span class="n">input_columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">num_parallel_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
ä»Šå¤©å¤©æ°”å¤ªå¥½äº†æˆ‘ä»¬ä¸€èµ·å»å¤–é¢ç©å§
------------------------after tokenization-----------------------------
[&#39;ä»Šå¤©å¤©æ°”&#39; &#39;å¤ªå¥½äº†&#39; &#39;æˆ‘ä»¬&#39; &#39;ä¸€èµ·&#39; &#39;å»&#39; &#39;å¤–é¢&#39; &#39;ç©å§&#39;]
</pre></div>
</div>
</section>
<section id="sentencepiecetokenizer">
<h3>SentencePieceTokenizer<a class="headerlink" href="#sentencepiecetokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">SentencePieceTokenizer</span></code>æ˜¯åŸºäº<a class="reference external" href="https://github.com/google/sentencepiece">SentencePiece</a>è¿™ä¸ªå¼€æºçš„è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…ã€‚</p>
<p>ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»<code class="docutils literal notranslate"><span class="pre">vocab_file</span></code>æ–‡ä»¶ä¸­æ„å»ºä¸€ä¸ª<code class="docutils literal notranslate"><span class="pre">vocab</span></code>å¯¹è±¡ï¼Œå†é€šè¿‡<code class="docutils literal notranslate"><span class="pre">SentencePieceTokenizer</span></code>å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>
<span class="kn">from</span> <span class="nn">mindspore.dataset.text</span> <span class="kn">import</span> <span class="n">SentencePieceModel</span><span class="p">,</span> <span class="n">SPieceTokenizerOutType</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;I saw a girl with a telescope.&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="c1"># file from MindSpore repository https://gitee.com/mindspore/mindspore/blob/r1.1/tests/ut/data/dataset/test_sentencepiece/botchan.txt</span>
<span class="n">vocab_file</span> <span class="o">=</span> <span class="s2">&quot;botchan.txt&quot;</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">SentencePieceVocab</span><span class="o">.</span><span class="n">from_file</span><span class="p">([</span><span class="n">vocab_file</span><span class="p">],</span> <span class="mi">5000</span><span class="p">,</span> <span class="mf">0.9995</span><span class="p">,</span> <span class="n">SentencePieceModel</span><span class="o">.</span><span class="n">UNIGRAM</span><span class="p">,</span> <span class="p">{})</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">SentencePieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">SPieceTokenizerOutType</span><span class="o">.</span><span class="n">STRING</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
I saw a girl with a telescope.
------------------------after tokenization-----------------------------
[&#39;â–I&#39; &#39;â–sa&#39; &#39;w&#39; &#39;â–a&#39; &#39;â–girl&#39; &#39;â–with&#39; &#39;â–a&#39; &#39;â–te&#39; &#39;les&#39; &#39;co&#39; &#39;pe&#39; &#39;.&#39;]
</pre></div>
</div>
</section>
<section id="unicodechartokenizer">
<h3>UnicodeCharTokenizer<a class="headerlink" href="#unicodechartokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">UnicodeCharTokenizer</span></code>æ˜¯æ ¹æ®Unicodeå­—ç¬¦é›†æ¥åˆ†è¯çš„ã€‚</p>
<p>ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡<code class="docutils literal notranslate"><span class="pre">UnicodeCharTokenizer</span></code>å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Welcome to Beijing!&quot;</span><span class="p">,</span> <span class="s2">&quot;åŒ—äº¬æ¬¢è¿æ‚¨ï¼&quot;</span><span class="p">,</span> <span class="s2">&quot;æˆ‘å–œæ¬¢English!&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">UnicodeCharTokenizer</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
[&#39;W&#39;, &#39;e&#39;, &#39;l&#39;, &#39;c&#39;, &#39;o&#39;, &#39;m&#39;, &#39;e&#39;, &#39; &#39;, &#39;t&#39;, &#39;o&#39;, &#39; &#39;, &#39;B&#39;, &#39;e&#39;, &#39;i&#39;, &#39;j&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39;!&#39;]
[&#39;åŒ—&#39;, &#39;äº¬&#39;, &#39;æ¬¢&#39;, &#39;è¿&#39;, &#39;æ‚¨&#39;, &#39;ï¼&#39;]
[&#39;æˆ‘&#39;, &#39;å–œ&#39;, &#39;æ¬¢&#39;, &#39;E&#39;, &#39;n&#39;, &#39;g&#39;, &#39;l&#39;, &#39;i&#39;, &#39;s&#39;, &#39;h&#39;, &#39;!&#39;]
</pre></div>
</div>
</section>
<section id="whitespacetokenizer">
<h3>WhitespaceTokenizer<a class="headerlink" href="#whitespacetokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code>æ˜¯æ ¹æ®ç©ºæ ¼æ¥è¿›è¡Œåˆ†è¯çš„ã€‚</p>
<p>ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åé€šè¿‡<code class="docutils literal notranslate"><span class="pre">WhitespaceTokenizer</span></code>å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Welcome to Beijing!&quot;</span><span class="p">,</span> <span class="s2">&quot;åŒ—äº¬æ¬¢è¿æ‚¨ï¼&quot;</span><span class="p">,</span> <span class="s2">&quot;æˆ‘å–œæ¬¢English!&quot;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WhitespaceTokenizer</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
<p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
Welcome to Beijing!
åŒ—äº¬æ¬¢è¿æ‚¨ï¼
æˆ‘å–œæ¬¢English!
------------------------after tokenization-----------------------------
[&#39;Welcome&#39;, &#39;to&#39;, &#39;Beijing!&#39;]
[&#39;åŒ—äº¬æ¬¢è¿æ‚¨ï¼&#39;]
[&#39;æˆ‘å–œæ¬¢English!&#39;]
</pre></div>
</div>
</section>
<section id="wordpiecetokenizer">
<h3>WordpieceTokenizer<a class="headerlink" href="#wordpiecetokenizer" title="Permalink to this headline">ïƒ</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code>æ˜¯åŸºäºå•è¯é›†æ¥è¿›è¡Œåˆ’åˆ†çš„ï¼Œåˆ’åˆ†ä¾æ®å¯ä»¥æ˜¯å•è¯é›†ä¸­çš„å•ä¸ªå•è¯ï¼Œæˆ–è€…å¤šä¸ªå•è¯çš„ç»„åˆå½¢å¼ã€‚</p>
<p>ä¸‹é¢çš„æ ·ä¾‹é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªæ–‡æœ¬æ•°æ®é›†ï¼Œç„¶åä»å•è¯åˆ—è¡¨ä¸­æ„å»º<code class="docutils literal notranslate"><span class="pre">vocab</span></code>å¯¹è±¡ï¼Œé€šè¿‡<code class="docutils literal notranslate"><span class="pre">WordpieceTokenizer</span></code>å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯ï¼Œå¹¶å±•ç¤ºäº†åˆ†è¯å‰åçš„æ–‡æœ¬ç»“æœã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.dataset</span> <span class="k">as</span> <span class="nn">ds</span>
<span class="kn">import</span> <span class="nn">mindspore.dataset.text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">input_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;my&quot;</span><span class="p">,</span> <span class="s2">&quot;favorite&quot;</span><span class="p">,</span> <span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;during&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;cholera&quot;</span><span class="p">,</span> <span class="s2">&quot;era&quot;</span><span class="p">,</span> <span class="s2">&quot;what&quot;</span><span class="p">,</span>
    <span class="s2">&quot;æˆ‘&quot;</span><span class="p">,</span> <span class="s2">&quot;æœ€&quot;</span><span class="p">,</span> <span class="s2">&quot;å–œ&quot;</span><span class="p">,</span> <span class="s2">&quot;æ¬¢&quot;</span><span class="p">,</span> <span class="s2">&quot;çš„&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¹¦&quot;</span><span class="p">,</span> <span class="s2">&quot;æ˜¯&quot;</span><span class="p">,</span> <span class="s2">&quot;éœ&quot;</span><span class="p">,</span> <span class="s2">&quot;ä¹±&quot;</span><span class="p">,</span> <span class="s2">&quot;æ—¶&quot;</span><span class="p">,</span> <span class="s2">&quot;æœŸ&quot;</span><span class="p">,</span> <span class="s2">&quot;çš„&quot;</span><span class="p">,</span> <span class="s2">&quot;çˆ±&quot;</span><span class="p">,</span> <span class="s2">&quot;æƒ…&quot;</span><span class="p">,</span> <span class="s2">&quot;æ‚¨&quot;</span><span class="p">]</span>
<span class="n">vocab_english</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;book&quot;</span><span class="p">,</span> <span class="s2">&quot;cholera&quot;</span><span class="p">,</span> <span class="s2">&quot;era&quot;</span><span class="p">,</span> <span class="s2">&quot;favor&quot;</span><span class="p">,</span> <span class="s2">&quot;##ite&quot;</span><span class="p">,</span> <span class="s2">&quot;my&quot;</span><span class="p">,</span> <span class="s2">&quot;is&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span> <span class="s2">&quot;dur&quot;</span><span class="p">,</span> <span class="s2">&quot;##ing&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">]</span>
<span class="n">vocab_chinese</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;æˆ‘&quot;</span><span class="p">,</span> <span class="s1">&#39;æœ€&#39;</span><span class="p">,</span> <span class="s1">&#39;å–œ&#39;</span><span class="p">,</span> <span class="s1">&#39;æ¬¢&#39;</span><span class="p">,</span> <span class="s1">&#39;çš„&#39;</span><span class="p">,</span> <span class="s1">&#39;ä¹¦&#39;</span><span class="p">,</span> <span class="s1">&#39;æ˜¯&#39;</span><span class="p">,</span> <span class="s1">&#39;éœ&#39;</span><span class="p">,</span> <span class="s1">&#39;ä¹±&#39;</span><span class="p">,</span> <span class="s1">&#39;æ—¶&#39;</span><span class="p">,</span> <span class="s1">&#39;æœŸ&#39;</span><span class="p">,</span> <span class="s1">&#39;çˆ±&#39;</span><span class="p">,</span> <span class="s1">&#39;æƒ…&#39;</span><span class="p">]</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">NumpySlicesDataset</span><span class="p">(</span><span class="n">input_list</span><span class="p">,</span> <span class="n">column_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------before tokenization----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Vocab</span><span class="o">.</span><span class="n">from_list</span><span class="p">(</span><span class="n">vocab_english</span><span class="o">+</span><span class="n">vocab_chinese</span><span class="p">)</span>
<span class="n">tokenizer_op</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">WordpieceTokenizer</span><span class="p">(</span><span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">operations</span><span class="o">=</span><span class="n">tokenizer_op</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;------------------------after tokenization-----------------------------&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">create_dict_iterator</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_numpy</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">to_str</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]))</span>
</pre></div>
</div>
<p>è¾“å‡ºç»“æœå¦‚ä¸‹ï¼š</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------------------------before tokenization----------------------------
my
favorite
book
is
love
during
the
cholera
era
what
æˆ‘
æœ€
å–œ
æ¬¢
çš„
ä¹¦
æ˜¯
éœ
ä¹±
æ—¶
æœŸ
çš„
çˆ±
æƒ…
æ‚¨
------------------------after tokenization-----------------------------
[&#39;my&#39;]
[&#39;favor&#39; &#39;##ite&#39;]
[&#39;book&#39;]
[&#39;is&#39;]
[&#39;love&#39;]
[&#39;dur&#39; &#39;##ing&#39;]
[&#39;the&#39;]
[&#39;cholera&#39;]
[&#39;era&#39;]
[&#39;[UNK]&#39;]
[&#39;æˆ‘&#39;]
[&#39;æœ€&#39;]
[&#39;å–œ&#39;]
[&#39;æ¬¢&#39;]
[&#39;çš„&#39;]
[&#39;ä¹¦&#39;]
[&#39;æ˜¯&#39;]
[&#39;éœ&#39;]
[&#39;ä¹±&#39;]
[&#39;æ—¶&#39;]
[&#39;æœŸ&#39;]
[&#39;çš„&#39;]
[&#39;çˆ±&#39;]
[&#39;æƒ…&#39;]
[&#39;[UNK]&#39;]
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="augmentation.html" class="btn btn-neutral float-left" title="æ•°æ®å¢å¼º" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dataset_conversion.html" class="btn btn-neutral float-right" title="MindSporeæ•°æ®æ ¼å¼è½¬æ¢" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>