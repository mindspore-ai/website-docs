<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.scipy.linalg &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script><script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/js/theme.js"></script><script src="../../../_static/underscore.js"></script><script src="../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.boost.html">mindspore.boost (experimental)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.6/api_cpp/mindspore.html">MindSpore Liteâ†—</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.scipy.linalg</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.scipy.linalg</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;Linear algebra submodule&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">mnp</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="n">SolveTriangular</span>
<span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="n">CholeskySolver</span>
<span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="n">Cholesky</span>
<span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="n">LU</span>
<span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="n">LUSolver</span>
<span class="kn">from</span> <span class="nn">.ops</span> <span class="kn">import</span> <span class="n">EighNet</span>
<span class="kn">from</span> <span class="nn">..ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">..ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">.utils_const</span> <span class="kn">import</span> <span class="n">_raise_value_error</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;block_diag&#39;</span><span class="p">,</span> <span class="s1">&#39;inv&#39;</span><span class="p">,</span> <span class="s1">&#39;eigh&#39;</span><span class="p">,</span> <span class="s1">&#39;lu_factor&#39;</span><span class="p">,</span> <span class="s1">&#39;lu&#39;</span><span class="p">]</span>


<div class="viewcode-block" id="block_diag"><a class="viewcode-back" href="../../../api_python/scipy/mindspore.scipy.linalg.block_diag.html#mindspore.scipy.linalg.block_diag">[docs]</a><span class="k">def</span> <span class="nf">block_diag</span><span class="p">(</span><span class="o">*</span><span class="n">arrs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a block diagonal matrix from provided arrays.</span>

<span class="sd">    Given the list of Tensors `A`, `B`, and `C`, the output will have these</span>
<span class="sd">    Tensors arranged on the diagonal:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        [[A, 0, 0],</span>
<span class="sd">         [0, B, 0],</span>
<span class="sd">         [0, 0, C]]</span>

<span class="sd">    Args:</span>
<span class="sd">        arrs (list): up to 2-D Input Tensors.</span>
<span class="sd">            A 1-D Tensor or a 2-D Tensor with shape :math:`(1,n)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor with `A`, `B`, `C`, ... on the diagonal which has the same dtype as `A`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If there are Tensors with dimensions higher than 2 in all arguments.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import block_diag</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[1, 0], [0, 1]]))</span>
<span class="sd">        &gt;&gt;&gt; B = Tensor(onp.array([[3, 4, 5], [6, 7, 8]]))</span>
<span class="sd">        &gt;&gt;&gt; C = Tensor(onp.array([[7]]))</span>
<span class="sd">        &gt;&gt;&gt; P = Tensor(onp.zeros((2, ), dtype=&#39;int32&#39;))</span>
<span class="sd">        &gt;&gt;&gt; block_diag(A, B, C)</span>
<span class="sd">        Tensor(shape=[5, 6], dtype=Int64, value=</span>
<span class="sd">        [[1, 0, 0, 0, 0, 0],</span>
<span class="sd">         [0, 1, 0, 0, 0, 0],</span>
<span class="sd">         [0, 0, 3, 4, 5, 0],</span>
<span class="sd">         [0, 0, 6, 7, 8, 0],</span>
<span class="sd">         [0, 0, 0, 0, 0, 7]])</span>
<span class="sd">        &gt;&gt;&gt; block_diag(A, P, B, C)</span>
<span class="sd">        Tensor(shape=[6, 8], dtype=Int64, value=</span>
<span class="sd">        [[1, 0, 0 ... 0, 0, 0],</span>
<span class="sd">         [0, 1, 0 ... 0, 0, 0],</span>
<span class="sd">         [0, 0, 0 ... 0, 0, 0],</span>
<span class="sd">         [0, 0, 0 ... 4, 5, 0],</span>
<span class="sd">         [0, 0, 0 ... 7, 8, 0],</span>
<span class="sd">         [0, 0, 0 ... 0, 0, 7]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">arrs</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">bad_shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">arrs</span><span class="p">)</span> <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">bad_shapes</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;Arguments to mindspore.scipy.linalg.block_diag must have at most 2 dimensions.&quot;</span><span class="p">)</span>

    <span class="n">accum</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">arrs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">arrs</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">accum</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)))(</span><span class="n">arr</span><span class="p">)</span>
        <span class="n">accum</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">c</span><span class="p">)))(</span><span class="n">accum</span><span class="p">)</span>
        <span class="n">accum</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">accum</span><span class="p">,</span> <span class="n">arr</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accum</span></div>


<span class="k">def</span> <span class="nf">solve_triangular</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">unit_diagonal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">overwrite_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve the equation `a x = b` for `x`, assuming a is a triangular matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        A (Tensor): A triangular matrix of shape :math:`(N, N)`.</span>
<span class="sd">        b (Tensor): A Tensor of shape :math:`(M,)` or :math:`(M, N)`.</span>
<span class="sd">            Right-hand side matrix in :math:`A x = b`.</span>
<span class="sd">        lower (bool, optional): Use only data contained in the lower triangle of `a`.</span>
<span class="sd">            Default is to use upper triangle.</span>
<span class="sd">        trans (0, 1, 2, &#39;N&#39;, &#39;T&#39;, &#39;C&#39;, optional):</span>
<span class="sd">            Type of system to solve:</span>
<span class="sd">            trans:        system:</span>
<span class="sd">                0 or &#39;N&#39;        a x  = b</span>
<span class="sd">                1 or &#39;T&#39;        a^T x = b</span>
<span class="sd">                2 or &#39;C&#39;        a^H x = b</span>
<span class="sd">        unit_diagonal (bool, optional): If True, diagonal elements of :math:`A` are assumed to be 1 and</span>
<span class="sd">            will not be referenced.</span>
<span class="sd">        overwrite_b (bool, optional): Allow overwriting data in :math:`b` (may enhance performance)</span>
<span class="sd">        check_finite (bool, optional): Whether to check that the input matrices contain only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems</span>
<span class="sd">            (crashes, non-termination) if the inputs do contain infinities or NaNs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape :math:`(M,)` or :math:`(M, N)`,</span>
<span class="sd">        which is the solution to the system :math:`A x = b`.</span>
<span class="sd">        Shape of :math:`x` matches :math:`b`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        LinAlgError: If :math:`A` is singular</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        Solve the lower triangular system :math:`A x = b`, where:</span>

<span class="sd">                 [3  0  0  0]       [4]</span>
<span class="sd">            A =  [2  1  0  0]   b = [2]</span>
<span class="sd">                 [1  0  1  0]       [4]</span>
<span class="sd">                 [1  1  1  1]       [2]</span>

<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as mnp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import solve_triangular</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[3, 0, 0, 0], [2, 1, 0, 0], [1, 0, 1, 0], [1, 1, 1, 1]], onp.float64))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(onp.array([4, 2, 4, 2], onp.float64))</span>
<span class="sd">        &gt;&gt;&gt; x = solve_triangular(A, b, lower=True, unit_diagonal=False, trans=&#39;N&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x</span>
<span class="sd">        Tensor(shape=[4], dtype=Float32, value= [ 1.33333337e+00, -6.66666746e-01,  2.66666651e+00, -1.33333313e+00])</span>
<span class="sd">        &gt;&gt;&gt; mnp.dot(A, x)  # Check the result</span>
<span class="sd">        Tensor(shape=[4], dtype=Float32, value= [ 4.00000000e+00,  2.00000000e+00,  4.00000000e+00,  2.00000000e+00])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trans</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">trans_table</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;T&#39;</span><span class="p">,</span> <span class="s1">&#39;C&#39;</span><span class="p">]</span>
        <span class="n">trans</span> <span class="o">=</span> <span class="n">trans_table</span><span class="p">[</span><span class="n">trans</span><span class="p">]</span>
    <span class="n">solve</span> <span class="o">=</span> <span class="n">SolveTriangular</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">unit_diagonal</span><span class="p">,</span> <span class="n">trans</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>


<div class="viewcode-block" id="inv"><a class="viewcode-back" href="../../../api_python/scipy/mindspore.scipy.linalg.inv.html#mindspore.scipy.linalg.inv">[docs]</a><span class="k">def</span> <span class="nf">inv</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the inverse of a matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): Square matrix to be inverted. Note that if the input tensor is not a `float`,</span>
<span class="sd">            then it will be casted to :class:`mstype.float32`.</span>
<span class="sd">        overwrite_a (bool, optional): Discard data in `a` (may improve performance). Default: False.</span>
<span class="sd">        check_finite (bool, optional): Whether to check that the input matrix contains only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems (crashes, non-termination)</span>
<span class="sd">            if the inputs do contain infinities or NaNs. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, inverse of the matrix `a`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        LinAlgError: If :math:`a` is singular.</span>
<span class="sd">        ValueError: If :math:`a` is not square, or not 2D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as mnp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import inv</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(onp.array([[1., 2.], [3., 4.]]))</span>
<span class="sd">        &gt;&gt;&gt; inv(a)</span>
<span class="sd">        Tensor(shape=[2, 2], dtype=Float64, value=</span>
<span class="sd">        [[-2.00000000e+00,  1.00000000e+00],</span>
<span class="sd">         [ 1.50000000e+00, -5.00000000e-01]])</span>
<span class="sd">        &gt;&gt;&gt; mnp.dot(a, inv(a))</span>
<span class="sd">        Tensor(shape=[2, 2], dtype=Float64, value=</span>
<span class="sd">        [[ 1.00000000e+00,  0.00000000e+00],</span>
<span class="sd">         [ 8.88178420e-16,  1.00000000e+00]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">matrix_inverse</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatrixInverse</span><span class="p">(</span><span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_inverse</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">cho_factor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the Cholesky decomposition of a matrix, to use in cho_solve</span>
<span class="sd">    Note that if the input tensor is not a `float`, then it will be casted to :class:&#39;mstype.float32&#39;.</span>
<span class="sd">    Returns a matrix containing the Cholesky decomposition,</span>
<span class="sd">    ``A = L L*`` or ``A = U* U`` of a Hermitian positive-definite matrix `a`.</span>
<span class="sd">    The return value can be directly used as the first parameter to cho_solve.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The returned matrix also contains random data in the entries not</span>
<span class="sd">        used by the Cholesky decomposition. If you need to zero these</span>
<span class="sd">        entries, use the function `cholesky` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): square Matrix of (M, M) to be decomposed. Note that if the input tensor is not a `float`,</span>
<span class="sd">            then it will be casted to :class:&#39;mstype.float32&#39;.</span>
<span class="sd">        lower (bool, optional): Whether to compute the upper or lower triangular Cholesky factorization</span>
<span class="sd">            (Default: upper-triangular)</span>
<span class="sd">        overwrite_a(bool, optional): Whether to overwrite data in a (may improve performance)</span>
<span class="sd">        check_finite(bool, optional): Whether to check that the input matrix contains only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems</span>
<span class="sd">            (crashes, non-termination) if the inputs do contain infinities or NaNs.</span>

<span class="sd">    Returns:</span>
<span class="sd">         - Tensor, matrix whose upper or lower triangle contains the Cholesky factor of `a`.</span>
<span class="sd">        Other parts of the matrix contain random data.</span>
<span class="sd">         - bool, flag indicating whether the factor is in the lower or upper triangle</span>

<span class="sd">    Raises:</span>
<span class="sd">        LinAlgError: Raised if decomposition fails.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import cho_factor</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[9, 3, 1, 5], [3, 7, 5, 1], [1, 5, 9, 2], [5, 1, 2, 6]]).astype(onp.float32))</span>
<span class="sd">        &gt;&gt;&gt; c, low = cho_factor(A)</span>
<span class="sd">        &gt;&gt;&gt; c</span>
<span class="sd">        Tensor(shape=[4, 4], dtype=Float32, value=</span>
<span class="sd">        [[ 3.00000000e+00,  1.00000000e+00,  3.33333343e-01,  1.66666663e+00],</span>
<span class="sd">         [ 3.00000000e+00,  2.44948983e+00,  1.90515852e+00, -2.72165507e-01],</span>
<span class="sd">         [ 1.00000000e+00,  5.00000000e+00,  2.29330778e+00,  8.55952621e-01],</span>
<span class="sd">         [ 5.00000000e+00,  1.00000000e+00,  2.00000000e+00,  1.55418575e+00]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">cholesky_net</span> <span class="o">=</span> <span class="n">Cholesky</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">clean</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">cholesky_net</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span><span class="p">,</span> <span class="n">lower</span>


<span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the Cholesky decomposition of a matrix.</span>

<span class="sd">    Returns the Cholesky decomposition, :math:`A = L L^*` or</span>
<span class="sd">    :math:`A = U^* U` of a Hermitian positive-definite matrix A.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): square Matrix of (M, M) to be decomposed</span>
<span class="sd">        lower (bool, optional): Whether to compute the upper- or lower-triangular Cholesky</span>
<span class="sd">            factorization.  Default is upper-triangular.</span>
<span class="sd">        overwrite_a (bool, optional): Whether to overwrite data in `a` (may improve performance).</span>
<span class="sd">        check_finite (bool, optional): Whether to check that the input matrix contains only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems</span>
<span class="sd">            (crashes, non-termination) if the inputs do contain infinities or NaNs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, upper- or lower-triangular Cholesky factor of `a`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        LinAlgError: if decomposition fails.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import cholesky</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(onp.array([[1, -2],[2, 5]]).astype(onp.float32))</span>
<span class="sd">        &gt;&gt;&gt; L = cholesky(a, lower=True)</span>
<span class="sd">        &gt;&gt;&gt; L</span>
<span class="sd">        Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00,  0.00000000e+00],</span>
<span class="sd">         [ 2.00000000e+00,  1.00000000e+00]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">cholesky_net</span> <span class="o">=</span> <span class="n">Cholesky</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">clean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">cholesky_net</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span>


<span class="k">def</span> <span class="nf">cho_solve</span><span class="p">(</span><span class="n">c_and_lower</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">overwrite_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Solve the linear equations Ax = b, given the Cholesky factorization of A.</span>

<span class="sd">    Args:</span>
<span class="sd">        c_and_lower ((Tensor, bool)): Cholesky factorization of a, as given by cho_factor</span>
<span class="sd">        b (Tensor): Right-hand side</span>
<span class="sd">        overwrite_b (bool, optional): Whether to overwrite data in b (may improve performance)</span>
<span class="sd">        check_finite (bool, optional): Whether to check that the input matrices contain only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems</span>
<span class="sd">            (crashes, non-termination) if the inputs do contain infinities or NaNs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the solution to the system A x = b</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import cho_factor, cho_solve</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[9, 3, 1, 5], [3, 7, 5, 1], [1, 5, 9, 2], [5, 1, 2, 6]]).astype(onp.float32))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(onp.array([1, 1, 1, 1]).astype(onp.float32))</span>
<span class="sd">        &gt;&gt;&gt; c, low = cho_factor(A)</span>
<span class="sd">        &gt;&gt;&gt; x = cho_solve((c, low), b)</span>
<span class="sd">        &gt;&gt;&gt; x</span>
<span class="sd">        Tensor(shape=[4], dtype=Float32, value= [-1.74926575e-02,  1.19533479e-01,  1.16618462e-02,  1.57434344e-01])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">lower</span><span class="p">)</span> <span class="o">=</span> <span class="n">c_and_lower</span>
    <span class="n">cholesky_solver_net</span> <span class="o">=</span> <span class="n">CholeskySolver</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cholesky_solver_net</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="eigh"><a class="viewcode-back" href="../../../api_python/scipy/mindspore.scipy.linalg.eigh.html#mindspore.scipy.linalg.eigh">[docs]</a><span class="k">def</span> <span class="nf">eigh</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eigvals_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
         <span class="n">overwrite_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">turbo</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eigvals</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_type</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
         <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve a standard or generalized eigenvalue problem for a complex Hermitian or real symmetric matrix.</span>

<span class="sd">    Find eigenvalues Tensor `w` and optionally eigenvectors Tensor `v` of Tensor `a`,</span>
<span class="sd">    where `b` is positive definite such that for every eigenvalue `Î»` (i-th entry of w) and</span>
<span class="sd">    its eigenvector `vi` (i-th column of `v`) satisfies::</span>

<span class="sd">                      a @ vi = Î» * b @ vi</span>
<span class="sd">        vi.conj().T @ a @ vi = Î»</span>
<span class="sd">        vi.conj().T @ b @ vi = 1</span>

<span class="sd">    In the standard problem, `b` is assumed to be the identity matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): A :math:`(M, M)` complex Hermitian or real symmetric matrix whose eigenvalues and</span>
<span class="sd">            eigenvectors will be computed.</span>
<span class="sd">        b (Tensor, optional): A :math:`(M, M)` complex Hermitian or real symmetric definite positive matrix in.</span>
<span class="sd">            If omitted, identity matrix is assumed. Default: None.</span>
<span class="sd">        lower (bool, optional): Whether the pertinent Tensor data is taken from the lower or upper</span>
<span class="sd">            triangle of `a` and, if applicable, `b`. Default: True.</span>
<span class="sd">        eigvals_only (bool, optional): Whether to calculate only eigenvalues and no eigenvectors.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        _type (int, optional): For the generalized problems, this keyword specifies the problem type</span>
<span class="sd">            to be solved for `w` and `v` (only takes 1, 2, 3 as possible inputs)::</span>

<span class="sd">                1 =&gt;     a @ v = w @ b @ v</span>
<span class="sd">                2 =&gt; a @ b @ v = w @ v</span>
<span class="sd">                3 =&gt; b @ a @ v = w @ v</span>

<span class="sd">            This keyword is ignored for standard problems. Default: 1.</span>
<span class="sd">        overwrite_a (bool, optional): Whether to overwrite data in `a` (may improve performance). Default: False.</span>
<span class="sd">        overwrite_b (bool, optional): Whether to overwrite data in `b` (may improve performance). Default: False.</span>
<span class="sd">        check_finite (bool, optional): Whether to check that the input matrices contain only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems (crashes, non-termination)</span>
<span class="sd">            if the inputs do contain infinities or NaNs. Default: True.</span>
<span class="sd">        turbo (bool, optional): use divide and conquer algorithm (faster but expensive in memory, only</span>
<span class="sd">            for generalized eigenvalue problem and if full set of eigenvalues are requested.).</span>
<span class="sd">            Has no significant effect if eigenvectors are not requested. Default: True.</span>
<span class="sd">        eigvals (tuple, optional): Indexes of the smallest and largest (in ascending order) eigenvalues</span>
<span class="sd">            and corresponding eigenvectors to be returned: :math:`0 &lt;= lo &lt;= hi &lt;= M-1`. If omitted, all eigenvalues</span>
<span class="sd">            and eigenvectors are returned. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - Tensor with shape :math:`(N,)`, the :math:`N (1&lt;=N&lt;=M)` selected eigenvalues, in ascending order,</span>
<span class="sd">          each repeated according to its multiplicity.</span>

<span class="sd">        - Tensor with shape :math:`(M, N)`, (if ``eigvals_only == False``)</span>

<span class="sd">    Raises:</span>
<span class="sd">        LinAlgError: If eigenvalue computation does not converge, an error occurred, or b matrix is not</span>
<span class="sd">            definite positive. Note that if input matrices are not symmetric or Hermitian, no error will</span>
<span class="sd">            be reported but results will be wrong.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as mnp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import eigh</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor([[6., 3., 1., 5.], [3., 0., 5., 1.], [1., 5., 6., 2.], [5., 1., 2., 2.]])</span>
<span class="sd">        &gt;&gt;&gt; w, v = eigh(A)</span>
<span class="sd">        &gt;&gt;&gt; mnp.sum(mnp.dot(A, v) - mnp.dot(v, mnp.diag(w))) &lt; 1e-10</span>
<span class="sd">        Tensor(shape=[], dtype=Bool, value= True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eigh_net</span> <span class="o">=</span> <span class="n">EighNet</span><span class="p">(</span><span class="ow">not</span> <span class="n">eigvals_only</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eigh_net</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">lu_pivots_to_permutation</span><span class="p">(</span><span class="n">pivots</span><span class="p">,</span> <span class="n">permutation_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;transfer pivots to permutation&quot;&quot;&quot;</span>
    <span class="n">batch_dims</span> <span class="o">=</span> <span class="n">pivots</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">pivots</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">per</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">permutation_size</span><span class="p">)</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">per</span><span class="p">,</span> <span class="n">batch_dims</span> <span class="o">+</span> <span class="p">(</span><span class="n">permutation_size</span><span class="p">,))</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">permutation</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">permutation_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">permutation</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">pivots</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">mnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch_dims</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">permutation</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">permutation</span><span class="p">[</span><span class="n">loc</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span><span class="p">,)]</span>
        <span class="n">permutation</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">permutation</span><span class="p">[</span><span class="n">loc</span> <span class="o">+</span> <span class="p">(</span><span class="n">j</span><span class="p">,)]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">permutation</span>


<span class="k">def</span> <span class="nf">lu_solve_core</span><span class="p">(</span><span class="n">in_lu</span><span class="p">,</span> <span class="n">permutation</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">trans</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; core implementation of lu solve&quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">in_lu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">res_shape</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">prod_result</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">sh</span> <span class="ow">in</span> <span class="n">res_shape</span><span class="p">:</span>
        <span class="n">prod_result</span> <span class="o">*=</span> <span class="n">sh</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">prod_result</span><span class="p">))</span>
    <span class="n">trans_str</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">trans</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">trans_str</span> <span class="o">=</span> <span class="s2">&quot;N&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">permutation</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">elif</span> <span class="n">trans</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">trans_str</span> <span class="o">=</span> <span class="s2">&quot;T&quot;</span>
    <span class="k">elif</span> <span class="n">trans</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">trans_str</span> <span class="o">=</span> <span class="s2">&quot;C&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;trans error, it&#39;s value must be 0, 1, 2&quot;</span><span class="p">)</span>
    <span class="n">ms_lu_solve</span> <span class="o">=</span> <span class="n">LUSolver</span><span class="p">(</span><span class="n">trans_str</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">ms_lu_solve</span><span class="p">(</span><span class="n">in_lu</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_lu_shape</span><span class="p">(</span><span class="n">in_lu</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; check lu input shape&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_lu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">in_lu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">in_lu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;last two dimensions of LU decomposition must be equal.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot; LU decomposition input b&#39;s rank must &gt;=1.&quot;</span><span class="p">)</span>

    <span class="n">rhs_vector</span> <span class="o">=</span> <span class="n">in_lu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">rhs_vector</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">in_lu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;LU decomposition: lu matrix and b must have same number of dimensions&quot;</span><span class="p">)</span>
        <span class="n">mnp</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">in_lu</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;LU decomposition: lu matrix and b must have same number of dimensions&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="lu_factor"><a class="viewcode-back" href="../../../api_python/scipy/mindspore.scipy.linalg.lu_factor.html#mindspore.scipy.linalg.lu_factor">[docs]</a><span class="k">def</span> <span class="nf">lu_factor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute pivoted LU decomposition of a square matrix,</span>
<span class="sd">    and its outputs can be directly used as the inputs of `lu_solve`.</span>
<span class="sd">    The decomposition is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        A = P L U</span>

<span class="sd">    where :math:`P` is a permutation matrix, :math:`L` lower triangular with unit diagonal elements,</span>
<span class="sd">    and :math:`U` upper triangular.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): square matrix of :math:`(M, M)` to decompose. Note that if the input tensor is not a `float`,</span>
<span class="sd">            then it will be casted to :class:&#39;mstype.float32&#39;.</span>
<span class="sd">        overwrite_a (bool, optional): Whether to overwrite data in :math:`A` (may increase performance). Default: False.</span>
<span class="sd">        check_finite (bool, optional): Whether to check that the input matrix contains only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems</span>
<span class="sd">            (crashes, non-termination) if the inputs do contain infinities or NaNs. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - Tensor, a square matrix of :math:`(N, N)` containing `U` in its upper triangle, and `L` in its lower triangle.</span>
<span class="sd">          The unit diagonal elements of `L` are not stored.</span>

<span class="sd">        - Tensor, :math:`(N,)` pivot indices representing the permutation matrix `P`:</span>
<span class="sd">          the i-th element value j in the indices indicates that row i of matrix was interchanged with row j.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If :math:`a` is not square.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import lu_factor</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[2, 5, 8, 7], [5, 2, 2, 8], [7, 5, 6, 6], [5, 4, 4, 8]]).astype(onp.float64))</span>
<span class="sd">        &gt;&gt;&gt; lu, piv = lu_factor(A)</span>
<span class="sd">        &gt;&gt;&gt; lu</span>
<span class="sd">        Tensor(shape=[4, 4], dtype=Float64, value=</span>
<span class="sd">        [[ 7.00000000e+00,  5.00000000e+00,  6.00000000e+00,  6.00000000e+00],</span>
<span class="sd">         [ 2.85714286e-01,  3.57142857e+00,  6.28571429e+00,  5.28571429e+00],</span>
<span class="sd">         [ 7.14285714e-01,  1.20000000e-01, -1.04000000e+00,  3.08000000e+00],</span>
<span class="sd">         [ 7.14285714e-01, -4.40000000e-01, -4.61538462e-01,  7.46153846e+00]])</span>
<span class="sd">        &gt;&gt;&gt; piv</span>
<span class="sd">        Tensor(shape=[4], dtype=Int32, value= [2, 2, 3, 3])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]):</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;input of lu matrix must be square.&quot;</span><span class="p">)</span>
    <span class="n">msp_lu</span> <span class="o">=</span> <span class="n">LU</span><span class="p">()</span>
    <span class="n">m_lu</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">msp_lu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m_lu</span><span class="p">,</span> <span class="n">pivots</span></div>


<div class="viewcode-block" id="lu"><a class="viewcode-back" href="../../../api_python/scipy/mindspore.scipy.linalg.lu.html#mindspore.scipy.linalg.lu">[docs]</a><span class="k">def</span> <span class="nf">lu</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">permute_l</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute pivoted LU decomposition of a general matrix.</span>

<span class="sd">    The decomposition is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        A = P L U</span>

<span class="sd">    where :math:`P` is a permutation matrix, :math:`L` lower triangular with unit</span>
<span class="sd">    diagonal elements, and :math:`U` upper triangular.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Tensor): a :math:`(M, N)` matrix to decompose. Note that if the input tensor is not a `float`,</span>
<span class="sd">            then it will be casted to :class:&#39;mstype.float32&#39;.</span>
<span class="sd">        permute_l (bool, optional): Perform the multiplication :math:`P L` (Default: do not permute). Default: False.</span>
<span class="sd">        overwrite_a (bool, optional): Whether to overwrite data in :math:`A` (may improve performance). Default: False.</span>
<span class="sd">        check_finite (bool, optional):  Whether to check that the input matrix contains</span>
<span class="sd">            only finite numbers. Disabling may give a performance gain, but may result</span>
<span class="sd">            in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        **If permute_l == False**</span>

<span class="sd">        - Tensor, :math:`(M, M)` permutation matrix.</span>
<span class="sd">        - Tensor, :math:`(M, K)` lower triangular or trapezoidal matrix with unit diagonal. :math:`K = min(M, N)`.</span>
<span class="sd">        - Tensor, :math:`(K, N)` upper triangular or trapezoidal matrix.</span>

<span class="sd">        **If permute_l == True**</span>

<span class="sd">        - Tensor, :math:`(M, K)` permuted L matrix. :math:`K = min(M, N)`.</span>
<span class="sd">        - Tensor, :math:`(K, N)` upper triangular or trapezoidal matrix.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import lu</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[2, 5, 8, 7], [5, 2, 2, 8], [7, 5, 6, 6], [5, 4, 4, 8]]).astype(onp.float64))</span>
<span class="sd">        &gt;&gt;&gt; p, l, u = lu(A)</span>
<span class="sd">        &gt;&gt;&gt; p</span>
<span class="sd">        Tensor(shape=[4, 4], dtype=Int32, value=</span>
<span class="sd">        [[0, 1, 0, 0],</span>
<span class="sd">         [0, 0, 0, 1],</span>
<span class="sd">         [1, 0, 0, 0],</span>
<span class="sd">         [0, 0, 1, 0]])</span>
<span class="sd">        &gt;&gt;&gt; l</span>
<span class="sd">        Tensor(shape=[4, 4], dtype=Float64, value=</span>
<span class="sd">        [[ 1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00],</span>
<span class="sd">         [ 2.85714298e-01,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00],</span>
<span class="sd">         [ 7.14285731e-01,  1.19999997e-01,  1.00000000e+00,  0.00000000e+00],</span>
<span class="sd">         [ 7.14285731e-01, -4.39999998e-01, -4.61538464e-01,  1.00000000e+00]])</span>
<span class="sd">        &gt;&gt;&gt; u</span>
<span class="sd">        Tensor(shape=[4, 4], dtype=Float64, value=</span>
<span class="sd">        [[ 7.00000000e+00,  5.00000000e+00,  6.00000000e+00,  6.00000000e+00],</span>
<span class="sd">         [ 0.00000000e+00,  3.57142854e+00,  6.28571415e+00,  5.28571415e+00],</span>
<span class="sd">         [ 0.00000000e+00,  0.00000000e+00, -1.03999996e+00,  3.07999992e+00],</span>
<span class="sd">         [ 0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  7.46153831e+00]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">msp_lu</span> <span class="o">=</span> <span class="n">LU</span><span class="p">()</span>
    <span class="n">m_lu</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">msp_lu</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">m</span> <span class="o">&gt;</span> <span class="n">n</span><span class="p">:</span>
        <span class="n">_raise_value_error</span><span class="p">(</span><span class="s2">&quot;last two dimensions of LU decomposition must be row less or equal to col.&quot;</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">a_dtype</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">dtype</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">m_lu</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">mnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a_dtype</span><span class="p">)</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">mnp</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">m_lu</span><span class="p">)[:</span><span class="n">k</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">if</span> <span class="n">permute_l</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">l</span><span class="p">),</span> <span class="n">u</span>
    <span class="k">return</span> <span class="n">p</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span></div>


<span class="k">def</span> <span class="nf">lu_solve</span><span class="p">(</span><span class="n">lu_and_piv</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">overwrite_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">check_finite</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Solve an equation system, a x = b, given the LU factorization of a</span>

<span class="sd">    Args:</span>
<span class="sd">        lu_and_piv (Tensor, Tensor): Factorization of the coefficient matrix a, as given by lu_factor</span>
<span class="sd">        b (Tensor): Right-hand side</span>
<span class="sd">        trans (int, optional): {0, 1, 2}</span>
<span class="sd">            Type of system to solve:</span>
<span class="sd">            =====  =========</span>
<span class="sd">            trans  system</span>
<span class="sd">            =====  =========</span>
<span class="sd">            0      a x   = b</span>
<span class="sd">            1      a^T x = b</span>
<span class="sd">            2      a^H x = b</span>
<span class="sd">            =====  =========</span>
<span class="sd">        overwrite_b (bool, optional): Whether to overwrite data in b (may increase performance)</span>
<span class="sd">        check_finite ( bool, optional): Whether to check that the input matrices contain only finite numbers.</span>
<span class="sd">            Disabling may give a performance gain, but may result in problems (crashes, non-termination)</span>
<span class="sd">            if the inputs do contain infinities or NaNs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tesnor, solution to the system</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as onp</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.scipy.linalg import lu_factor, lu_solve</span>
<span class="sd">        &gt;&gt;&gt; A = Tensor(onp.array([[2, 5, 8, 7], [5, 2, 2, 8], [7, 5, 6, 6], [5, 4, 4, 8]]).astype(onp.float64))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(onp.array([1, 1, 1, 1]).astype(onp.float64))</span>
<span class="sd">        &gt;&gt;&gt; lu, piv = lu_factor(A)</span>
<span class="sd">        &gt;&gt;&gt; lu_solve((lu, piv), b)</span>
<span class="sd">        [ 0.05154639, -0.08247423,  0.08247423,  0.09278351]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m_lu</span><span class="p">,</span> <span class="n">pivots</span> <span class="o">=</span> <span class="n">lu_and_piv</span>
    <span class="c1"># 1. check shape</span>
    <span class="n">check_lu_shape</span><span class="p">(</span><span class="n">m_lu</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="c1"># here permutation array has been calculated, just use it.</span>
    <span class="c1"># 2. calculate permutation</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="n">lu_pivots_to_permutation</span><span class="p">(</span><span class="n">pivots</span><span class="p">,</span> <span class="n">pivots</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
    <span class="c1"># 3. rhs_vector</span>
    <span class="n">rhs_vector</span> <span class="o">=</span> <span class="n">m_lu</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">lu_solve_core</span><span class="p">(</span><span class="n">m_lu</span><span class="p">,</span> <span class="n">permutation</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">trans</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">rhs_vector</span> <span class="k">else</span> <span class="n">x</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>