<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn.DistributedGradReducer &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.nn.DynamicLossScaleUpdateCell" href="mindspore.nn.DynamicLossScaleUpdateCell.html" />
    <link rel="prev" title="mindspore.nn.thor" href="mindspore.nn.thor.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#cell">Cell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#containers">Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#convolution-layers">Convolution Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#recurrent-layers">Recurrent Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#sparse-layers">Sparse Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#non-linear-activations">Non-linear Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#utilities">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#images-functions">Images Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#normalization-layers">Normalization Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#pooling-layers">Pooling layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#thor-layers">Thor Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#quantized-functions">Quantized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#optimizer-functions">Optimizer Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.nn.html#wrapper-functions">Wrapper Functions</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.nn.DistributedGradReducer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.DynamicLossScaleUpdateCell.html">mindspore.nn.DynamicLossScaleUpdateCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.FixedLossScaleUpdateCell.html">mindspore.nn.FixedLossScaleUpdateCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.ForwardValueAndGrad.html">mindspore.nn.ForwardValueAndGrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.GetNextSingleOp.html">mindspore.nn.GetNextSingleOp</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.ParameterUpdate.html">mindspore.nn.ParameterUpdate</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.PipelineCell.html">mindspore.nn.PipelineCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.TimeDistributed.html">mindspore.nn.TimeDistributed</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.TrainOneStepCell.html">mindspore.nn.TrainOneStepCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.TrainOneStepWithLossScaleCell.html">mindspore.nn.TrainOneStepWithLossScaleCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.WithEvalCell.html">mindspore.nn.WithEvalCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.WithGradCell.html">mindspore.nn.WithGradCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.WithLossCell.html">mindspore.nn.WithLossCell</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#math-functions">Math Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#metrics">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.3/api_cpp/class_list.html">Lite</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.nn.html">mindspore.nn</a> &raquo;</li>
      <li>mindspore.nn.DistributedGradReducer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/nn/mindspore.nn.DistributedGradReducer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-nn-distributedgradreducer">
<h1>mindspore.nn.DistributedGradReducer<a class="headerlink" href="#mindspore-nn-distributedgradreducer" title="Permalink to this headline">ÔÉÅ</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.DistributedGradReducer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">DistributedGradReducer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fusion_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/wrap/grad_reducer.html#DistributedGradReducer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DistributedGradReducer" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>A distributed optimizer.</p>
<p>Constructs a gradient reducer Cell, which applies communication and average operations on
single-process gradient values.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) ‚Äì the parameters to be updated.</p></li>
<li><p><strong>mean</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) ‚Äì When mean is true, the mean coefficient (degree) would apply on gradients. Default: False.</p></li>
<li><p><strong>degree</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) ‚Äì The mean coefficient. Usually it equals to device number. Default: None.</p></li>
<li><p><strong>fusion_type</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) ‚Äì The type of all reduce fusion. Default: 1.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> ‚Äì If degree is not a int or less than 0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># This example should be run with multiple processes.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Please refer to the tutorial &gt; Distributed Training on mindspore.cn.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.wrap.cell_wrapper</span> <span class="kn">import</span> <span class="n">WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="p">(</span><span class="n">_get_device_num</span><span class="p">,</span> <span class="n">_get_gradients_mean</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">context</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">reset_auto_parallel_context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">TrainingWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">TrainingWrapper</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">network</span> <span class="o">=</span> <span class="n">network</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="o">.</span><span class="n">add_flags</span><span class="p">(</span><span class="n">defer_inline</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="n">sens</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">HYBRID_PARALLEL</span><span class="p">]:</span>
<span class="gp">... </span>            <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span> <span class="o">=</span> <span class="kc">True</span>
<span class="gp">... </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
<span class="gp">... </span>            <span class="n">mean</span> <span class="o">=</span> <span class="n">_get_gradients_mean</span><span class="p">()</span>
<span class="gp">... </span>            <span class="n">degree</span> <span class="o">=</span> <span class="n">_get_device_num</span><span class="p">()</span>
<span class="gp">... </span>            <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DistributedGradReducer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">degree</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
<span class="gp">... </span>        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">sens</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
<span class="gp">... </span>            <span class="c1"># apply grad reducer on grads</span>
<span class="gp">... </span>            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_cell</span> <span class="o">=</span> <span class="n">TrainingWrapper</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="n">train_cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
<span class="go">256.0</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.nn.DistributedGradReducer.construct">
<span class="sig-name descname"><span class="pre">construct</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/wrap/grad_reducer.html#DistributedGradReducer.construct"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.DistributedGradReducer.construct" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Under certain circumstances, the data precision of grads could be mixed with float16 and float32. Thus, the
result of AllReduce is unreliable. To solve the problem, grads must be cast to float32 before AllReduce,
and cast back after the operation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>grads</strong> (<em>Union</em><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em><em>]</em>) ‚Äì The gradient tensor or tuple before operation.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>new_grads (Union[Tensor, tuple[Tensor]]), the gradient tensor or tuple after operation.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.nn.thor.html" class="btn btn-neutral float-left" title="mindspore.nn.thor" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.nn.DynamicLossScaleUpdateCell.html" class="btn btn-neutral float-right" title="mindspore.nn.DynamicLossScaleUpdateCell" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>