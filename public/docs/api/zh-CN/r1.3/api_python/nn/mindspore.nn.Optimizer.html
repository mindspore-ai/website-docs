

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.nn.Optimizer &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.nn.ProximalAdagrad" href="mindspore.nn.ProximalAdagrad.html" />
    <link rel="prev" title="mindspore.nn.Momentum" href="mindspore.nn.Momentum.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#cell">Cell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#containers">Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#convolution-layers">Convolution Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#recurrent-layers">Recurrent Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#sparse-layers">Sparse Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#non-linear-activations">Non-linear Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#utilities">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#images-functions">Images Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#normalization-layers">Normalization Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#pooling-layers">Pooling layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#thor-layers">Thor Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#quantized-functions">Quantized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.nn.html#optimizer-functions">Optimizer Functions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Adagrad.html">mindspore.nn.Adagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Adam.html">mindspore.nn.Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.AdamOffload.html">mindspore.nn.AdamOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.AdamWeightDecay.html">mindspore.nn.AdamWeightDecay</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.FTRL.html">mindspore.nn.FTRL</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Lamb.html">mindspore.nn.Lamb</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LARS.html">mindspore.nn.LARS</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LazyAdam.html">mindspore.nn.LazyAdam</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Momentum.html">mindspore.nn.Momentum</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.nn.Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.ProximalAdagrad.html">mindspore.nn.ProximalAdagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.RMSProp.html">mindspore.nn.RMSProp</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.SGD.html">mindspore.nn.SGD</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.thor.html">mindspore.nn.thor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#wrapper-functions">Wrapper Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#math-functions">Math Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#metrics">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.3/api_cpp/class_list.html">Lite</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore.nn.html">mindspore.nn</a> &raquo;</li>
        
      <li>mindspore.nn.Optimizer</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/api_python/nn/mindspore.nn.Optimizer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-nn-optimizer">
<h1>mindspore.nn.Optimizer<a class="headerlink" href="#mindspore-nn-optimizer" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindspore.nn.Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.nn.</code><code class="sig-name descname">Optimizer</code><span class="sig-paren">(</span><em class="sig-param">learning_rate</em>, <em class="sig-param">parameters</em>, <em class="sig-param">weight_decay=0.0</em>, <em class="sig-param">loss_scale=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for all optimizers.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class defines the API to add Ops to train a model. Never use
this class directly, but instead instantiate one of its subclasses.</p>
<p>Different parameter groups can set different <cite>learning_rate</cite>, <cite>weight_decay</cite> and <cite>grad_centralization</cite>.</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight_decay is positive. For most optimizer, when not separating parameters, the <cite>weight_decay</cite> in the API will
be applied on the parameters without ‘beta’ or ‘gamma’ in their names if <cite>weight_decay</cite> is positive.</p>
<p>When separating parameter groups, if you want to centralize the gradient, set grad_centralization to True,
but the gradient centralization can only be applied to the parameters of the convolution layer.
If the parameters of the non convolution layer are set to True, an error will be reported.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning
rate. When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate must be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.</p></li>
<li><p><strong>parameters</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>parameters</cite> is a list of <cite>Parameter</cite> which will be
updated, the element in <cite>parameters</cite> must be class <cite>Parameter</cite>. When the <cite>parameters</cite> is a list of <cite>dict</cite>,
the “params”, “lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value must be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>weight_decay: Optional. If “weight_decay” in the keys, the value of corresponding weight decay
will be used. If not, the <cite>weight_decay</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value must be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ must be in one of group parameters.</p></li>
<li><p>grad_centralization: Optional. The data type of “grad_centralization” is Bool. If “grad_centralization”
is in the keys, the set value will be used. If not, the <cite>grad_centralization</cite> is False by default.
This parameter only works on the convolution layer.</p></li>
</ul>
</p></li>
<li><p><strong>weight_decay</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – An int or a floating point value for the weight decay.
It must be equal to or greater than 0.
If the type of <cite>weight_decay</cite> input is int, it will be converted to float. Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale. It must be greater than 0. If the
type of <cite>loss_scale</cite> input is int, it will be converted to float. In general, use the default value. Only
when <cite>FixedLossScaleManager</cite> is used for training and the <cite>drop_overflow_update</cite> in
<cite>FixedLossScaleManager</cite> is set to False, then this value needs to be the same as the <cite>loss_scale</cite> in
<cite>FixedLossScaleManager</cite>. Refer to class <a class="reference internal" href="../mindspore.html#mindspore.FixedLossScaleManager" title="mindspore.FixedLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.FixedLossScaleManager</span></code></a> for more details.
Default: 1.0.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>learning_rate</cite> is not one of int, float, Tensor, Iterable, LearningRateSchedule.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If element of <cite>parameters</cite> is neither Parameter nor dict.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>loss_scale</cite> is not a float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_decay</cite> is neither float nor int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>loss_scale</cite> is less than or equal to 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>weight_decay</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>learning_rate</cite> is a Tensor, but the dimension of tensor is greater than 1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<dl class="method">
<dt id="mindspore.nn.Optimizer.broadcast_params">
<code class="sig-name descname">broadcast_params</code><span class="sig-paren">(</span><em class="sig-param">optim_result</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.broadcast_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer.broadcast_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply Broadcast operations in the sequential order of parameter groups.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>bool, the status flag.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.decay_weight">
<code class="sig-name descname">decay_weight</code><span class="sig-paren">(</span><em class="sig-param">gradients</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.decay_weight"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer.decay_weight" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight decay.</p>
<p>An approach to reduce the overfitting of a deep learning neural network model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of <cite>self.parameters</cite>, and have the same shape as
<cite>self.parameters</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after weight decay.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.get_lr">
<code class="sig-name descname">get_lr</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_lr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_lr" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the learning rate of current step.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>float, the learning rate of current step.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.get_lr_parameter">
<code class="sig-name descname">get_lr_parameter</code><span class="sig-paren">(</span><em class="sig-param">param</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.get_lr_parameter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer.get_lr_parameter" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the learning rate of parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>param</strong> (<em>Union</em><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>]</em>) – The <cite>Parameter</cite> or list of <cite>Parameter</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Parameter, single <cite>Parameter</cite> or <cite>list[Parameter]</cite> according to the input type.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.gradients_centralization">
<code class="sig-name descname">gradients_centralization</code><span class="sig-paren">(</span><em class="sig-param">gradients</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.gradients_centralization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer.gradients_centralization" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradients centralization.</p>
<p>A method for optimizing convolutional layer parameters to impore the training speed of a deep learning neural
network model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of <cite>self.parameters</cite>, and have the same shape as
<cite>self.parameters</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after gradients centralization.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.scale_grad">
<code class="sig-name descname">scale_grad</code><span class="sig-paren">(</span><em class="sig-param">gradients</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/optimizer.html#Optimizer.scale_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.Optimizer.scale_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss scale for mixed precision.</p>
<p>An approach of mixed precision training to improve the speed and energy efficiency of training deep neural
network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>gradients</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – The gradients of <cite>self.parameters</cite>, and have the same shape as
<cite>self.parameters</cite>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tuple[Tensor], The gradients after loss scale.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.target">
<em class="property">property </em><code class="sig-name descname">target</code><a class="headerlink" href="#mindspore.nn.Optimizer.target" title="Permalink to this definition">¶</a></dt>
<dd><p>The method is used to determine whether the parameter is updated on host or device. The input type is str
and can only be ‘CPU’, ‘Ascend’ or ‘GPU’.</p>
</dd></dl>

<dl class="method">
<dt id="mindspore.nn.Optimizer.unique">
<em class="property">property </em><code class="sig-name descname">unique</code><a class="headerlink" href="#mindspore.nn.Optimizer.unique" title="Permalink to this definition">¶</a></dt>
<dd><p>The method is to see whether to make unique. The input type is bool. The method is read-only.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.nn.ProximalAdagrad.html" class="btn btn-neutral float-right" title="mindspore.nn.ProximalAdagrad" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.nn.Momentum.html" class="btn btn-neutral float-left" title="mindspore.nn.Momentum" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>