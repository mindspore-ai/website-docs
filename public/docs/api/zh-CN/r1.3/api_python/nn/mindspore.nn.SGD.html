

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.nn.SGD &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.nn.thor" href="mindspore.nn.thor.html" />
    <link rel="prev" title="mindspore.nn.RMSProp" href="mindspore.nn.RMSProp.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#cell">Cell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#containers">Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#convolution-layers">Convolution Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#recurrent-layers">Recurrent Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#sparse-layers">Sparse Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#non-linear-activations">Non-linear Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#utilities">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#images-functions">Images Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#normalization-layers">Normalization Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#pooling-layers">Pooling layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#thor-layers">Thor Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#quantized-functions">Quantized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.nn.html#optimizer-functions">Optimizer Functions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Adagrad.html">mindspore.nn.Adagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Adam.html">mindspore.nn.Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.AdamOffload.html">mindspore.nn.AdamOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.AdamWeightDecay.html">mindspore.nn.AdamWeightDecay</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.FTRL.html">mindspore.nn.FTRL</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Lamb.html">mindspore.nn.Lamb</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LARS.html">mindspore.nn.LARS</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LazyAdam.html">mindspore.nn.LazyAdam</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Momentum.html">mindspore.nn.Momentum</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Optimizer.html">mindspore.nn.Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.ProximalAdagrad.html">mindspore.nn.ProximalAdagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.RMSProp.html">mindspore.nn.RMSProp</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.nn.SGD</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.thor.html">mindspore.nn.thor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#wrapper-functions">Wrapper Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#math-functions">Math Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#metrics">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.3/api_cpp/class_list.html">Lite</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore.nn.html">mindspore.nn</a> &raquo;</li>
        
      <li>mindspore.nn.SGD</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/api_python/nn/mindspore.nn.SGD.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-nn-sgd">
<h1>mindspore.nn.SGD<a class="headerlink" href="#mindspore-nn-sgd" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindspore.nn.SGD">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.nn.</code><code class="sig-name descname">SGD</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/sgd.html#SGD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.nn.SGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements stochastic gradient descent. Momentum is optional.</p>
<p>Introduction to SGD can be found at <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">https://en.wikipedia.org/wiki/Stochastic_gradient_descent</a>.
Nesterov momentum is based on the formula from paper <a class="reference external" href="http://proceedings.mlr.press/v28/sutskever13.html">On the importance of initialization and
momentum in deep learning</a>.</p>
<div class="math notranslate nohighlight">
\[v_{t+1} = u \ast v_{t} + gradient \ast (1-dampening)\]</div>
<p>If nesterov is True:</p>
<div class="math notranslate nohighlight">
\[p_{t+1} = p_{t} - lr \ast (gradient + u \ast v_{t+1})\]</div>
<p>If nesterov is False:</p>
<div class="math notranslate nohighlight">
\[p_{t+1} = p_{t} - lr \ast v_{t+1}\]</div>
<p>To be noticed, for the first step, <span class="math notranslate nohighlight">\(v_{t+1} = gradient\)</span></p>
<p>Here : where p, v and u denote the parameters, accum, and momentum respectively.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, if you want to centralize the gradient, set grad_centralization to True,
but the gradient centralization can only be applied to the parameters of the convolution layer.
If the parameters of the non convolution layer are set to True, an error will be reported.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>]</em><em>]</em>) – <p>When the <cite>params</cite> is a list of <cite>Parameter</cite> which will be updated,
the element in <cite>params</cite> must be class <cite>Parameter</cite>. When the <cite>params</cite> is a list of <cite>dict</cite>, the “params”,
“lr”, “weight_decay” and “order_params” are the keys can be parsed.</p>
<ul>
<li><p>params: Required. The value must be a list of <cite>Parameter</cite>.</p></li>
<li><p>lr: Optional. If “lr” in the keys, the value of corresponding learning rate will be used.
If not, the <cite>learning_rate</cite> in the API will be used.</p></li>
<li><p>order_params: Optional. If “order_params” in the keys, the value must be the order of parameters and
the order will be followed in optimizer. There are no other keys in the <cite>dict</cite> and the parameters which
in the value of ‘order_params’ must be in one of group parameters.</p></li>
<li><p>grad_centralization: Optional. The data type of “grad_centralization” is Bool. If “grad_centralization”
is in the keys, the set value will be used. If not, the <cite>grad_centralization</cite> is False by default.
This parameter only works on the convolution layer.</p></li>
</ul>
</p></li>
<li><p><strong>learning_rate</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a><em>, </em><a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><em>Iterable</em><em>, </em><em>LearningRateSchedule</em><em>]</em>) – A value or a graph for the learning rate.
When the learning_rate is an Iterable or a Tensor in a 1D dimension, use dynamic learning rate, then
the i-th step will take the i-th value as the learning rate. When the learning_rate is LearningRateSchedule,
use dynamic learning rate, the i-th learning rate will be calculated during the process of training
according to the formula of LearningRateSchedule. When the learning_rate is a float or a Tensor in a zero
dimension, use fixed learning rate. Other cases are not supported. The float learning rate must be
equal to or greater than 0. If the type of <cite>learning_rate</cite> is int, it will be converted to float.
Default: 0.1.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value the momentum. must be at least 0.0. Default: 0.0.</p></li>
<li><p><strong>dampening</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value of dampening for momentum. must be at least 0.0. Default: 0.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Weight decay (L2 penalty). It must be equal to or greater than 0. Default: 0.0.</p></li>
<li><p><strong>nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enables the Nesterov momentum. If use nesterov, momentum must be positive,
and dampening must equal to 0.0. Default: False.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – A floating point value for the loss scale, which must be larger than 0.0. In general, use
the default value. Only when <cite>FixedLossScaleManager</cite> is used for training and the <cite>drop_overflow_update</cite> in
<cite>FixedLossScaleManager</cite> is set to False, then this value needs to be the same as the <cite>loss_scale</cite> in
<cite>FixedLossScaleManager</cite>. Refer to class <a class="reference internal" href="../mindspore.html#mindspore.FixedLossScaleManager" title="mindspore.FixedLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.FixedLossScaleManager</span></code></a> for more details.
Default: 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor[bool], the value is True.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the momentum, dampening or weight_decay value is less than 0.0.</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) All parameters use the same learning rate and weight decay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Use parameter groups and set different values</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span><span class="s1">&#39;grad_centralization&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">},</span>
<span class="gp">... </span>                <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span>
<span class="gp">... </span>                <span class="p">{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()}]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The conv_params&#39;s parameters will use default learning rate of 0.1 default weight decay of 0.0 and grad</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># centralization of True.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The no_conv_params&#39;s parameters will use learning rate of 0.01 and default weight decay of 0.0 and grad</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># centralization of False.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The final parameters order in which the optimizer will be followed is the value of &#39;order_params&#39;.</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.nn.thor.html" class="btn btn-neutral float-right" title="mindspore.nn.thor" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.nn.RMSProp.html" class="btn btn-neutral float-left" title="mindspore.nn.RMSProp" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>