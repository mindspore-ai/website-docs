<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn.thor &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.nn.DistributedGradReducer" href="mindspore.nn.DistributedGradReducer.html" />
    <link rel="prev" title="mindspore.nn.SGD" href="mindspore.nn.SGD.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.explainer.html">mindspore.explainer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#cell">Cell</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#containers">Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#convolution-layers">Convolution Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#recurrent-layers">Recurrent Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#sparse-layers">Sparse Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#non-linear-activations">Non-linear Activations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#utilities">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#images-functions">Images Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#normalization-layers">Normalization Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#pooling-layers">Pooling layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#thor-layers">Thor Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#quantized-functions">Quantized Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#loss-functions">Loss Functions</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.nn.html#optimizer-functions">Optimizer Functions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Adagrad.html">mindspore.nn.Adagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Adam.html">mindspore.nn.Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.AdamOffload.html">mindspore.nn.AdamOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.AdamWeightDecay.html">mindspore.nn.AdamWeightDecay</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.FTRL.html">mindspore.nn.FTRL</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Lamb.html">mindspore.nn.Lamb</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LARS.html">mindspore.nn.LARS</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.LazyAdam.html">mindspore.nn.LazyAdam</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Momentum.html">mindspore.nn.Momentum</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.Optimizer.html">mindspore.nn.Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.ProximalAdagrad.html">mindspore.nn.ProximalAdagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.RMSProp.html">mindspore.nn.RMSProp</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.nn.SGD.html">mindspore.nn.SGD</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.nn.thor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#wrapper-functions">Wrapper Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#math-functions">Math Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#metrics">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.nn.html#dynamic-learning-rate">Dynamic Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.3/api_cpp/class_list.html">Lite</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.nn.html">mindspore.nn</a> &raquo;</li>
      <li>mindspore.nn.thor</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/nn/mindspore.nn.thor.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-nn-thor">
<h1>mindspore.nn.thor<a class="headerlink" href="#mindspore-nn-thor" title="Permalink to this headline">ÔÉÅ</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="mindspore.nn.thor">
<span class="sig-prename descclassname"><span class="pre">mindspore.nn.</span></span><span class="sig-name descname"><span class="pre">thor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">net</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">damping</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_decay=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale=1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size=32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_nesterov=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decay_filter=&lt;lambda</span> <span class="pre">x:</span> <span class="pre">x.name</span> <span class="pre">not</span> <span class="pre">in</span> <span class="pre">[]&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_indices=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_clip_grad=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency=100</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/nn/optim/thor.html#thor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.nn.thor" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Updates gradients by second-order algorithm‚ÄìTHOR.</p>
<p>Trace-based Hardware-driven layer-ORiented Natural Gradient Descent Computation (THOR) algorithm is proposed in:</p>
<p><a class="reference external" href="https://www.aaai.org/AAAI21Papers/AAAI-6611.ChenM.pdf">THOR: Trace-based Hardware-driven layer-ORiented Natural Gradient Descent Computation</a></p>
<p>The updating formulas are as follows,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    A_i = a_i{a_i}^T \\
    G_i = D_{s_i}{ D_{s_i}}^T \\
    m_i = \beta * m_i + ({G_i^{(k)}}+\lambda I)^{-1}) g_i ({\overline A_{i-1}^{(k)}}+\lambda I)^{-1} \\
    w_i = w_i - \alpha * m_i \\
\end{array}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(D_{s_i}\)</span> represents the derivative of the loss function of the output of the i-th layer,
<span class="math notranslate nohighlight">\(a_{i-1}\)</span> represents the input of i-th layer,and which is the activations of previous layer,
<span class="math notranslate nohighlight">\(\beta\)</span> represents momentum, <span class="math notranslate nohighlight">\(I\)</span> represents the identity matrix,
<span class="math notranslate nohighlight">\(\overline A\)</span> represents the transpose of matrix A,
<span class="math notranslate nohighlight">\(\lambda\)</span> represents ‚Äòdamping‚Äô, <span class="math notranslate nohighlight">\(g_i\)</span> represents gradients of the i-th layer,
<span class="math notranslate nohighlight">\(\otimes\)</span> represents Kronecker product, <span class="math notranslate nohighlight">\(\alpha\)</span> represents ‚Äòlearning rate‚Äô</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When separating parameter groups, the weight decay in each group will be applied on the parameters if the
weight decay is positive. When not separating parameter groups, the <cite>weight_decay</cite> in the API will be applied
on the parameters without ‚Äòbeta‚Äô or ‚Äògamma‚Äô in their names if <cite>weight_decay</cite> is positive.</p>
<p>When separating parameter groups, if you want to centralize the gradient, set grad_centralization to True,
but the gradient centralization can only be applied to the parameters of the convolution layer.
If the parameters of the non convolution layer are set to True, an error will be reported.</p>
<p>To improve parameter groups performance, the customized order of parameters can be supported.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>net</strong> (<a class="reference internal" href="mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) ‚Äì The training network.</p></li>
<li><p><strong>learning_rate</strong> (<a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) ‚Äì A value for the learning rate.</p></li>
<li><p><strong>damping</strong> (<a class="reference internal" href="../mindspore.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) ‚Äì A value for the damping.</p></li>
<li><p><strong>momentum</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) ‚Äì Hyper-parameter of type float, means momentum for the moving average. It must be at least 0.0.</p></li>
<li><p><strong>weight_decay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) ‚Äì Weight decay (L2 penalty). It must be equal to or greater than 0.0. Default: 0.0.</p></li>
<li><p><strong>loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) ‚Äì A value for the loss scale. It must be greater than 0.0. In general, use the
default value. Default: 1.0.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) ‚Äì The size of a batch. Default: 32</p></li>
<li><p><strong>use_nesterov</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) ‚Äì Enable Nesterov momentum. Default: False.</p></li>
<li><p><strong>decay_filter</strong> (<em>function</em>) ‚Äì A function to determine which layers the weight decay applied to. And it
only works when the weight_decay &gt; 0. Default: lambda x: x.name not in []</p></li>
<li><p><strong>split_indices</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) ‚Äì Set allreduce fusion strategy by A/G layer indices . Only works when distributed
computing. ResNet50 as an example, there are 54 layers of A/G respectively, when split_indices is set
to [26, 53], it means A/G is divided into two groups to allreduce,  one is 0~26 layer, and the other
is 27~53. Default: None</p></li>
<li><p><strong>enable_clip_grad</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) ‚Äì Whether to clip the gradients. Default: False</p></li>
<li><p><strong>frequency</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) ‚Äì The update interval of A/G and $A^{-1}/G^{-1}$. When frequency equals N (N is greater than 1),
A/G and $A^{-1}/G^{-1}$ will be updated  every N steps, and other steps will use the stale A/G and
$A^{-1}/G^{-1}$ to update weights. Default: 100.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>gradients</strong> (tuple[Tensor]) - The gradients of <cite>params</cite>, the shape is the same as <cite>params</cite>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>tuple[bool], all elements are True.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> ‚Äì If <cite>learning_rate</cite> is not Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> ‚Äì If <cite>loss_scale</cite>,`momentum` or <cite>frequency</cite> is not a float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> ‚Äì If <cite>weight_decay</cite> is neither float nor int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> ‚Äì If <cite>use_nesterov</cite> is not a bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> ‚Äì If <cite>loss_scale</cite> is less than or equal to 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> ‚Äì If <cite>weight_decay</cite> or <cite>momentum</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> ‚Äì If <cite>frequency</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> ‚Äì If <cite>frequency</cite> is less than 2.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">thor</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">damping</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ConvertModelUtils</span><span class="p">()</span><span class="o">.</span><span class="n">convert_to_thor_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">network</span><span class="o">=</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
<span class="gp">... </span><span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">},</span> <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O2&quot;</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">cb</span><span class="p">,</span> <span class="n">sink_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.nn.SGD.html" class="btn btn-neutral float-left" title="mindspore.nn.SGD" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.nn.DistributedGradReducer.html" class="btn btn-neutral float-right" title="mindspore.nn.DistributedGradReducer" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>