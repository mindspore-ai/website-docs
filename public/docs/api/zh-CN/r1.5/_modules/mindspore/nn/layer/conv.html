<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn.layer.conv &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost (experimental)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.5/api_cpp/mindspore.html">MindSpore Liteâ†—</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.nn.layer.conv</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.nn.layer.conv</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;conv&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span><span class="p">,</span> <span class="n">Rel</span><span class="p">,</span> <span class="n">twice</span><span class="p">,</span> <span class="n">_check_3d_int_or_tuple</span>
<span class="kn">from</span> <span class="nn">mindspore._extends</span> <span class="kn">import</span> <span class="n">cell_attr_register</span>
<span class="kn">from</span> <span class="nn">..cell</span> <span class="kn">import</span> <span class="n">Cell</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Conv2d&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv2dTranspose&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv1d&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv1dTranspose&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv3d&#39;</span><span class="p">,</span> <span class="s1">&#39;Conv3dTranspose&#39;</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">_Conv</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a N-D convolution over an input signal composed of several input planes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="p">,</span>
                 <span class="n">group</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span>
                 <span class="n">transposed</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _Conv.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Conv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="s1">&#39;in_channels&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="s1">&#39;out_channels&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span> <span class="o">=</span> <span class="n">weight_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span> <span class="o">=</span> <span class="n">bias_init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the </span><span class="se">\&quot;</span><span class="s2">NHWC</span><span class="se">\&quot;</span><span class="s2"> format only support in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCDHW&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the </span><span class="se">\&quot;</span><span class="s2">NCDHW</span><span class="se">\&quot;</span><span class="s2"> format only support in Ascend and GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">pad</span> <span class="ow">in</span> <span class="n">padding</span><span class="p">:</span>
                <span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="s1">&#39;padding item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;padding&#39; must be int or tuple(int), &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">has_bias</span>
        <span class="k">for</span> <span class="n">kernel_size_elem</span> <span class="ow">in</span> <span class="n">kernel_size</span><span class="p">:</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">kernel_size_elem</span><span class="p">,</span> <span class="s1">&#39;kernel_size item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">stride_elem</span> <span class="ow">in</span> <span class="n">stride</span><span class="p">:</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">stride_elem</span><span class="p">,</span> <span class="s1">&#39;stride item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">dilation_elem</span> <span class="ow">in</span> <span class="n">dilation</span><span class="p">:</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">dilation_elem</span><span class="p">,</span> <span class="s1">&#39;dilation item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">group</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the attr &#39;in_channels&#39; must be divisible by attr &#39;group&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;in_channels&#39;: </span><span class="si">{</span><span class="n">in_channels</span><span class="si">}</span><span class="s2"> and &#39;group&#39;: </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">group</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;out_channels&#39; must be divisible by attr &#39;group&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;out_channels&#39;: </span><span class="si">{</span><span class="n">out_channels</span><span class="si">}</span><span class="s2"> and &#39;group&#39;: </span><span class="si">{</span><span class="n">group</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">transposed</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> \
                <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="n">group</span><span class="p">,</span> <span class="o">*</span><span class="n">kernel_size</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">has_bias</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Value of &#39;has_bias&#39; is False, value of &#39;bias_init&#39; will be ignored.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Must be overridden by all subclasses.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<div class="viewcode-block" id="Conv2d"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d">[docs]</a><span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">_Conv</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Applies a 2D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C_{in}` is channel number, and :math:`H_{in}, W_{in}` are height and width.</span>
<span class="sd">    For each batch of shape :math:`(C_{in}, H_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is the cross-correlation operator, :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to the :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where :math:`\text{kernel_size[0]}` and :math:`\text{kernel_size[1]}` are the height and width of</span>
<span class="sd">    the convolution kernel. The full kernel has shape</span>
<span class="sd">    :math:`(C_{out}, C_{in} // \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where group is the group number to split the input `x` in the channel dimension.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + \text{padding[0]} + \text{padding[1]} - \text{kernel_size[0]} -</span>
<span class="sd">    (\text{kernel_size[0]} - 1) \times (\text{dilation[0]} - 1) }{\text{stride[0]}}} \right \rfloor`    and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + \text{padding[2]} + \text{padding[3]} - \text{kernel_size[1]} -</span>
<span class="sd">    (\text{kernel_size[1]} - 1) \times (\text{dilation[1]} - 1) }{\text{stride[1]}}} \right \rfloor`    respectively.</span>

<span class="sd">    The first introduction can be found in paper `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of input channel :math:`C_{in}`.</span>
<span class="sd">        out_channels (int): The number of output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The data type is int or a tuple of 2 integers. Specifies the height</span>
<span class="sd">            and width of the 2D convolution window. Single int means the value is for both the height and the width of</span>
<span class="sd">            the kernel. A tuple of 2 ints means the first value is for the height and the other is for the</span>
<span class="sd">            width of the kernel.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;same&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input `x`. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top and bottom, left and right if possible. Otherwise, the</span>
<span class="sd">              last extra padding will be done from the bottom and the right side. If this mode is set, `padding`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded. If this mode is set, `padding`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`. The number of `padding` will be padded to the input</span>
<span class="sd">              Tensor borders. `padding` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union[int, tuple[int]]): Implicit paddings on both sides of the input `x`. If `padding` is one integer,</span>
<span class="sd">                    the paddings of top, bottom, left and right are the same, equal to padding. If `padding` is a tuple</span>
<span class="sd">                    with four integers, the paddings of top, bottom, left and right will be equal to padding[0],</span>
<span class="sd">                    padding[1], padding[2], and padding[3] accordingly. Default: 0.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): The data type is int or a tuple of 2 integers. Specifies the dilation rate</span>
<span class="sd">                                      to use for dilated convolution. If set to be :math:`k &gt; 1`, there will</span>
<span class="sd">                                      be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">                                      be greater or equal to 1 and bounded by the height and width of the</span>
<span class="sd">                                      input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_ channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. If the group is equal to `in_channels` and `out_channels`,</span>
<span class="sd">            this 2D convolution layer also can be called 2D depthwise convolution layer. Default: 1.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the convolution kernel.</span>
<span class="sd">            It can be a Tensor, a string, an Initializer or a number. When a string is specified,</span>
<span class="sd">            values from &#39;TruncatedNormal&#39;, &#39;Normal&#39;, &#39;Uniform&#39;, &#39;HeUniform&#39; and &#39;XavierUniform&#39; distributions as well</span>
<span class="sd">            as constant &#39;One&#39; and &#39;Zero&#39; distributions are possible. Alias &#39;xavier_uniform&#39;, &#39;he_uniform&#39;, &#39;ones&#39;</span>
<span class="sd">            and &#39;zeros&#39; are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Possible</span>
<span class="sd">            Initializer and string are the same as &#39;weight_init&#39;. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;zeros&#39;.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})` \</span>
<span class="sd">          or :math:`(N, H_{in}, W_{in}, C_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(N, H_{out}, W_{out}, C_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels`, `out_channels` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `padding` or `dilation` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `padding` is not equal to (0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; not &#39;NHWC&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Conv2d(120, 240, 4, has_bias=False, weight_init=&#39;normal&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([1, 120, 1024, 640]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x).shape</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1, 240, 1024, 640)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@cell_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2d.&quot;&quot;&quot;</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">twice</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">twice</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">twice</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="p">,</span>
            <span class="n">data_format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                               <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                               <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">pad_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                               <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                               <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                               <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">(</span><span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;input_channels=</span><span class="si">{}</span><span class="s1">, output_channels=</span><span class="si">{}</span><span class="s1">, kernel_size=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;stride=</span><span class="si">{}</span><span class="s1">, pad_mode=</span><span class="si">{}</span><span class="s1">, padding=</span><span class="si">{}</span><span class="s1">, dilation=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;group=</span><span class="si">{}</span><span class="s1">, has_bias=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;weight_init=</span><span class="si">{}</span><span class="s1">, bias_init=</span><span class="si">{}</span><span class="s1">, format=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_3d</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">op_name</span><span class="si">}</span><span class="s2">&#39;, the dimension of input should be 3d, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="Conv1d"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.Conv1d.html#mindspore.nn.Conv1d">[docs]</a><span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_Conv</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    1D convolution layer.</span>

<span class="sd">    Applies a 1D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size and :math:`C_{in}` is channel number. For each batch of shape</span>
<span class="sd">    :math:`(C_{in}, W_{in})`, the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_j = \sum_{i=0}^{C_{in} - 1} ccor(W_{ij}, X_i) + b_j,</span>

<span class="sd">    where :math:`ccor` is the cross correlation operator, :math:`C_{in}` is the input channel number, :math:`j` ranges</span>
<span class="sd">    from :math:`0` to :math:`C_{out} - 1`, :math:`W_{ij}` corresponds to the :math:`i`-th channel of the :math:`j`-th</span>
<span class="sd">    filter and :math:`out_{j}` corresponds to the :math:`j`-th channel of the output. :math:`W_{ij}` is a slice</span>
<span class="sd">    of kernel and it has shape :math:`(\text{ks_w})`, where :math:`\text{ks_w}` is the width of the convolution kernel.</span>
<span class="sd">    The full kernel has shape :math:`(C_{out}, C_{in} // \text{group}, \text{ks_w})`, where group is the group number</span>
<span class="sd">    to split the input `x` in the channel dimension.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -</span>
<span class="sd">    (\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor`    respectively.</span>

<span class="sd">    The first introduction of convolution layer can be found in paper `Gradient Based Learning Applied to Document</span>
<span class="sd">    Recognition &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of input channel :math:`C_{in}`.</span>
<span class="sd">        out_channels (int): The number of output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (int): The data type is int. Specifies the</span>
<span class="sd">            width of the 1D convolution window.</span>
<span class="sd">        stride (int): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the width of movement. Default: 1.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;same&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The output width will be the same as the input `x`.</span>
<span class="sd">              The total number of padding will be calculated in the horizontal</span>
<span class="sd">              direction and evenly distributed to left and right if possible. Otherwise, the</span>
<span class="sd">              last extra padding will be done from the bottom and the right side. If this mode is set, `padding`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest width of the output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded. If this mode is set, `padding`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`. The number of `padding` will be padded to the input</span>
<span class="sd">              Tensor borders. `padding` must be greater than or equal to 0.</span>

<span class="sd">        padding (int): Implicit paddings on both sides of the input `x`. Default: 0.</span>
<span class="sd">        dilation (int): The data type is int. Specifies the dilation rate</span>
<span class="sd">                                      to use for dilated convolution. If set to be :math:`k &gt; 1`, there will</span>
<span class="sd">                                      be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">                                      be greater or equal to 1 and bounded by the height and width of the</span>
<span class="sd">                                      input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_ channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. Default: 1.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): An initializer for the convolution kernel.</span>
<span class="sd">            It can be a Tensor, a string, an Initializer or a number. When a string is specified,</span>
<span class="sd">            values from &#39;TruncatedNormal&#39;, &#39;Normal&#39;, &#39;Uniform&#39;, &#39;HeUniform&#39; and &#39;XavierUniform&#39; distributions as well</span>
<span class="sd">            as constant &#39;One&#39; and &#39;Zero&#39; distributions are possible. Alias &#39;xavier_uniform&#39;, &#39;he_uniform&#39;, &#39;ones&#39;</span>
<span class="sd">            and &#39;zeros&#39; are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Possible</span>
<span class="sd">            Initializer and string are the same as &#39;weight_init&#39;. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;zeros&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` or `dilation` is not an int.</span>
<span class="sd">        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Conv1d(120, 240, 4, has_bias=False, weight_init=&#39;normal&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([1, 120, 640]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x).shape</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1, 240, 640)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@cell_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv1d.&quot;&quot;&quot;</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>
        <span class="n">get_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="n">get_dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">weight_init_shape</span> <span class="o">=</span> <span class="n">get_shape</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_init_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;weight_init_shape&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">weight_init_dtype</span> <span class="o">=</span> <span class="n">get_dtype</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>
            <span class="n">weight_init_value</span> <span class="o">=</span> <span class="n">weight_init</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">weight_init_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">weight_init_value</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">weight_init</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init_value</span><span class="p">,</span> <span class="n">weight_init_dtype</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                               <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                               <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">pad_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                               <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                               <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_check_input_3d</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;input_channels=</span><span class="si">{}</span><span class="s1">, output_channels=</span><span class="si">{}</span><span class="s1">, kernel_size=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;stride=</span><span class="si">{}</span><span class="s1">, pad_mode=</span><span class="si">{}</span><span class="s1">, padding=</span><span class="si">{}</span><span class="s1">, dilation=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;group=</span><span class="si">{}</span><span class="s1">, has_bias=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;weight_init=</span><span class="si">{}</span><span class="s1">, bias_init=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_5dims</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">op_name</span><span class="si">}</span><span class="s2">&#39;, the dimension of input should be 5d, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="Conv3d"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.Conv3d.html#mindspore.nn.Conv3d">[docs]</a><span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_Conv</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D convolution layer.</span>

<span class="sd">    Applies a 3D convolution over an input tensor which is typically of shape</span>
<span class="sd">    :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` and output shape</span>
<span class="sd">    :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. where :math:`N` is batch size. :math:`C` is channel number.</span>
<span class="sd">    the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \operatorname{out}\left(N_{i}, C_{\text {out}_j}\right)=\operatorname{bias}\left(C_{\text {out}_j}\right)+</span>
<span class="sd">        \sum_{k=0}^{C_{in}-1} ccor(\text {weight}\left(C_{\text {out}_j}, k\right),</span>
<span class="sd">        \operatorname{input}\left(N_{i}, k\right))</span>

<span class="sd">    where :math:`ccor` is the cross-correlation operator.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output depth, height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{D_{in} + \text{padding[0]} + \text{padding[1]} - \text{kernel_size[0]} -</span>
<span class="sd">    (\text{kernel_size[0]} - 1) \times (\text{dilation[0]} - 1) }{\text{stride[0]}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + \text{padding[2]} + \text{padding[3]} - \text{kernel_size[1]} -</span>
<span class="sd">    (\text{kernel_size[1]} - 1) \times (\text{dilation[1]} - 1) }{\text{stride[1]}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + \text{padding[4]} + \text{padding[5]} - \text{kernel_size[2]} -</span>
<span class="sd">    (\text{kernel_size[2]} - 1) \times (\text{dilation[2]} - 1) }{\text{stride[2]}}} \right \rfloor` respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of input channel :math:`C_{in}`.</span>
<span class="sd">        out_channels (int): The number of output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers.</span>
<span class="sd">            Specifies the depth, height and width of the 3D convolution window.</span>
<span class="sd">            Single int means the value is for the depth, height and the width of the kernel.</span>
<span class="sd">            A tuple of 3 ints means the first value is for the depth, second value is for height and the</span>
<span class="sd">            other is for the width of the kernel.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;same&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be the same as</span>
<span class="sd">              the input `x`. The total number of padding will be calculated in depth, horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to head and tail, top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `padding` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `padding`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x` in depth, height, width. The number of `padding`</span>
<span class="sd">              will be padded to the input Tensor borders. `padding` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union(int, tuple[int])): Implicit paddings on both sides of the input `x`.</span>
<span class="sd">            The data type is int or a tuple of 6 integers. Default: 0. If `padding` is an integer,</span>
<span class="sd">            the paddings of head, tail, top, bottom, left and right are the same, equal to padding.</span>
<span class="sd">            If `paddings` is a tuple of six integers, the padding of head, tail, top, bottom, left and right equal to</span>
<span class="sd">            padding[0], padding[1], padding[2], padding[3], padding[4] and padding[5] correspondingly.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers</span>
<span class="sd">            : math:`(dilation_d, dilation_h, dilation_w)`. Currently, dilation on depth only supports the case of 1.</span>
<span class="sd">            Specifies the dilation rate to use for dilated convolution. If set to be :math:`k &gt; 1`,</span>
<span class="sd">            there will be :math:`k - 1` pixels skipped for each sampling location.</span>
<span class="sd">            Its value must be greater or equal to 1 and bounded by the height and width of the input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_ channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the convolution kernel.</span>
<span class="sd">            It can be a Tensor, a string, an Initializer or a number. When a string is specified,</span>
<span class="sd">            values from &#39;TruncatedNormal&#39;, &#39;Normal&#39;, &#39;Uniform&#39;, &#39;HeUniform&#39; and &#39;XavierUniform&#39; distributions as well</span>
<span class="sd">            as constant &#39;One&#39; and &#39;Zero&#39; distributions are possible. Alias &#39;xavier_uniform&#39;, &#39;he_uniform&#39;, &#39;ones&#39;</span>
<span class="sd">            and &#39;zeros&#39; are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Possible</span>
<span class="sd">            Initializer and string are the same as &#39;weight_init&#39;. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;zeros&#39;.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &quot;NCDHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently input data type only support float16 and float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 3D convolution. The shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels`, `out_channels` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `padding` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `padding` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 3, 10, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = nn.Conv3d(in_channels=3, out_channels=32, kernel_size=(4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 10, 32, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@cell_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3d.&quot;&quot;&quot;</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;padding size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="p">,</span>
            <span class="n">data_format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv3D</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                               <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                               <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">pad_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                               <span class="n">pad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                               <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                               <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                               <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                               <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">(</span><span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_check_input_5dims</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;input_channels=</span><span class="si">{}</span><span class="s1">, output_channels=</span><span class="si">{}</span><span class="s1">, kernel_size=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;stride=</span><span class="si">{}</span><span class="s1">, pad_mode=</span><span class="si">{}</span><span class="s1">, padding=</span><span class="si">{}</span><span class="s1">, dilation=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;group=</span><span class="si">{}</span><span class="s1">, has_bias=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;weight_init=</span><span class="si">{}</span><span class="s1">, bias_init=</span><span class="si">{}</span><span class="s1">, format=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>


<div class="viewcode-block" id="Conv3dTranspose"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.Conv3dTranspose.html#mindspore.nn.Conv3dTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv3dTranspose</span><span class="p">(</span><span class="n">_Conv</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>
<span class="sd">    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,</span>
<span class="sd">    and sums over the outputs from all input feature planes.</span>
<span class="sd">    This module can be seen as the gradient of Conv3d with respect to its input.</span>

<span class="sd">    `x` is typically of shape :math:`(N, C, D, H, W)`, where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`D` is the characteristic depth, :math:`H` is the height of the characteristic layer,</span>
<span class="sd">    and :math:`W` is the width of the characteristic layer.</span>
<span class="sd">    The calculation process of transposed convolution is equivalent to the reverse calculation of convolution.</span>

<span class="sd">    The pad_mode argument effectively adds :math:`dilation * (kernel\_size - 1) - padding` amount of zero padding</span>
<span class="sd">    to both sizes of the input. So that when a Conv3d and a ConvTranspose3d are initialized with same parameters,</span>
<span class="sd">    they are inverses of each other in regard to the input and output shapes.</span>
<span class="sd">    However, when stride &gt; 1, Conv3d maps multiple input shapes to the same output shape.</span>
<span class="sd">    ConvTranspose3d provide padding argument to  increase the calculated output shape on one or more side.</span>

<span class="sd">    The height and width of output are defined as:</span>

<span class="sd">    if the &#39;pad_mode&#39; is set to be &quot;pad&quot;,</span>

<span class="sd">    .. math::</span>
<span class="sd">        D_{out} = (D_{in} - 1) \times \text{stride_d} - 2 \times \text{padding_d} + \text{dilation_d} \times</span>
<span class="sd">        (\text{kernel_size_d} - 1) + \text{output_padding_d} + 1</span>

<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride_h} - 2 \times \text{padding_h} + \text{dilation_h} \times</span>
<span class="sd">        (\text{kernel_size_h} - 1) + \text{output_padding_h} + 1</span>

<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride_w} - 2 \times \text{padding_w} + \text{dilation_w} \times</span>
<span class="sd">        (\text{kernel_size_w} - 1) + \text{output_padding_w} + 1</span>

<span class="sd">    if the &#39;pad_mode&#39; is set to be &quot;same&quot;,</span>

<span class="sd">    .. math::</span>

<span class="sd">        D_{out} = (D_{in} + \text{stride_d} - 1)/\text{stride_d} \\</span>
<span class="sd">        H_{out} = (H_{in} + \text{stride_h} - 1)/\text{stride_h} \\</span>
<span class="sd">        W_{out} = (W_{in} + \text{stride_w} - 1)/\text{stride_w}</span>

<span class="sd">    if the &#39;pad_mode&#39; is set to be &quot;valid&quot;,</span>

<span class="sd">    .. math::</span>

<span class="sd">        D_{out} = (D_{in} - 1) \times \text{stride_d} + \text{dilation_d} \times</span>
<span class="sd">        (\text{kernel_size_d} - 1) + 1 \\</span>
<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride_h} + \text{dilation_h} \times</span>
<span class="sd">        (\text{kernel_size_h} - 1) + 1 \\</span>
<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride_w} + \text{dilation_w} \times</span>
<span class="sd">        (\text{kernel_size_w} - 1) + 1</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of input channel :math:`C_{in}`.</span>
<span class="sd">        out_channels (int): The number of output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 3D convolution.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Its value must be equal to or greater than 1.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        pad_mode (str): Select the mode of the pad. The optional values are</span>
<span class="sd">            &quot;pad&quot;, &quot;same&quot;, &quot;valid&quot;. Default: &quot;same&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be the same as</span>
<span class="sd">              the input `x`. The total number of padding will be calculated in depth, horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to head and tail, top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `padding` and `output_padding` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `padding`</span>
<span class="sd">              and `output_padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x` in depth, height, width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `padding` must be greater than or equal to 0.</span>

<span class="sd">        padding (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `padding` is an integer,</span>
<span class="sd">            the paddings of head, tail, top, bottom, left and right are the same, equal to padding.</span>
<span class="sd">            If `padding` is a tuple of six integers, the padding of head, tail, top, bottom, left and right equal to</span>
<span class="sd">            padding[0], padding[1], padding[2], padding[3], padding[4] and padding[5] correspondingly.</span>
<span class="sd">        dilation (Union(int, tuple[int])): The data type is int or a tuple of 3 integers</span>
<span class="sd">            : math:`(dilation_d, dilation_h, dilation_w)`. Currently, dilation on depth only supports the case of 1.</span>
<span class="sd">            Specifies the dilation rate to use for dilated convolution. If set to be :math:`k &gt; 1`,</span>
<span class="sd">            there will be :math:`k - 1` pixels skipped for each sampling location.</span>
<span class="sd">            Its value must be greater or equal to 1 and bounded by the height and width of the input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_ channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        output_padding (Union(int, tuple[int])): Add extra size to each dimension of the output. Default: 0.</span>
<span class="sd">            Must be greater than or equal to 0.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the convolution kernel.</span>
<span class="sd">            It can be a Tensor, a string, an Initializer or a number. When a string is specified,</span>
<span class="sd">            values from &#39;TruncatedNormal&#39;, &#39;Normal&#39;, &#39;Uniform&#39;, &#39;HeUniform&#39; and &#39;XavierUniform&#39; distributions as well</span>
<span class="sd">            as constant &#39;One&#39; and &#39;Zero&#39; distributions are possible. Alias &#39;xavier_uniform&#39;, &#39;he_uniform&#39;, &#39;ones&#39;</span>
<span class="sd">            and &#39;zeros&#39; are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Possible</span>
<span class="sd">            Initializer and string are the same as &#39;weight_init&#39;. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;zeros&#39;.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently input data type only support float16 and float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels`, `out_channels` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `padding` , `dilation` or `output_padding`</span>
<span class="sd">                   is neither an int not a tuple of three.</span>
<span class="sd">        TypeError: If input data type is not float16 or float32.</span>
<span class="sd">        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `padding` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d_transpose = nn.Conv3dTranspose(in_channels=16, out_channels=3, kernel_size=(4, 6, 2),</span>
<span class="sd">        ...                                       pad_mode=&#39;pad&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3dTranspose.&quot;&quot;&quot;</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;padding size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;output_padding&quot;</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv3dTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="p">,</span>
            <span class="n">data_format</span><span class="p">,</span>
            <span class="n">transposed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3d_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv3DTranspose</span><span class="p">(</span><span class="n">in_channel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                                                  <span class="n">out_channel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                                                  <span class="n">kernel_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                                                  <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                  <span class="n">pad_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                                                  <span class="n">pad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                  <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                                  <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                  <span class="n">group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                                                  <span class="n">output_padding</span><span class="o">=</span><span class="n">output_padding</span><span class="p">,</span>
                                                  <span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">(</span><span class="n">data_format</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_check_input_5dims</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3d_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;input_channels=</span><span class="si">{}</span><span class="s1">, output_channels=</span><span class="si">{}</span><span class="s1">, kernel_size=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;stride=</span><span class="si">{}</span><span class="s1">, pad_mode=</span><span class="si">{}</span><span class="s1">, padding=</span><span class="si">{}</span><span class="s1">, dilation=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;group=</span><span class="si">{}</span><span class="s1">, has_bias=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;weight_init=</span><span class="si">{}</span><span class="s1">, bias_init=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>


<span class="k">def</span> <span class="nf">_deconv_output_length</span><span class="p">(</span><span class="n">is_valid</span><span class="p">,</span> <span class="n">is_same</span><span class="p">,</span> <span class="n">is_pad</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">stride_size</span><span class="p">,</span> <span class="n">dilation_size</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the width and height of output.&quot;&quot;&quot;</span>
    <span class="n">length</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">filter_size</span> <span class="o">=</span> <span class="n">filter_size</span> <span class="o">+</span> <span class="p">(</span><span class="n">filter_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_valid</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span> <span class="o">+</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span>
    <span class="k">elif</span> <span class="n">is_same</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span>
    <span class="k">elif</span> <span class="n">is_pad</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span> <span class="o">-</span> <span class="n">padding</span> <span class="o">+</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span>

    <span class="k">return</span> <span class="n">length</span>


<div class="viewcode-block" id="Conv2dTranspose"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.Conv2dTranspose.html#mindspore.nn.Conv2dTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv2dTranspose</span><span class="p">(</span><span class="n">_Conv</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D transposed convolution layer.</span>

<span class="sd">    Compute a 2D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>
<span class="sd">    This module can be seen as the gradient of Conv2d with respect to its input.</span>

<span class="sd">    `x` is typically of shape :math:`(N, C, H, W)`, where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`H` is the height of the characteristic layer and :math:`W` is the width of the characteristic layer.</span>

<span class="sd">    The pad_mode argument effectively adds :math:`dilation * (kernel\_size - 1) - padding` amount of zero padding</span>
<span class="sd">    to both sizes of the input. So that when a Conv2d and a ConvTranspose2d are initialized with same parameters,</span>
<span class="sd">    they are inverses of each other in regard to the input and output shapes.</span>
<span class="sd">    However, when stride &gt; 1, Conv2d maps multiple input shapes to the same output shape.</span>
<span class="sd">    ConvTranspose2d provide padding argument to  increase the calculated output shape on one or more side.</span>

<span class="sd">    The height and width of output are defined as:</span>

<span class="sd">    if the &#39;pad_mode&#39; is set to be &quot;pad&quot;,</span>

<span class="sd">    .. math::</span>

<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride[0]} - \left (\text{padding[0]} + \text{padding[1]}\right ) +</span>
<span class="sd">        \text{dilation[0]} \times (\text{kernel_size[0]} - 1) + 1</span>

<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride[1]} - \left (\text{padding[2]} + \text{padding[3]}\right ) +</span>
<span class="sd">        \text{dilation[1]} \times (\text{kernel_size[1]} - 1) + 1</span>

<span class="sd">    if the &#39;pad_mode&#39; is set to be &quot;same&quot;,</span>

<span class="sd">    .. math::</span>

<span class="sd">        H_{out} = (H_{in} + \text{stride[0]} - 1)/\text{stride[0]} \\</span>
<span class="sd">        W_{out} = (W_{in} + \text{stride[1]} - 1)/\text{stride[1]}</span>

<span class="sd">    if the &#39;pad_mode&#39; is set to be &quot;valid&quot;,</span>

<span class="sd">    .. math::</span>

<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride[0]} + \text{dilation[0]} \times</span>
<span class="sd">        (\text{ks_w[0]} - 1) + 1 \\</span>
<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride[1]} + \text{dilation[1]} \times</span>
<span class="sd">        (\text{ks_w[1]} - 1) + 1</span>

<span class="sd">    where :math:`\text{kernel_size[0]}` is the height of the convolution kernel and :math:`\text{kernel_size[1]}`</span>
<span class="sd">    is the width of the convolution kernel.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of channels in the input space.</span>
<span class="sd">        out_channels (int): The number of channels in the output space.</span>
<span class="sd">        kernel_size (Union[int, tuple]): int or a tuple of 2 integers, which specifies the height</span>
<span class="sd">            and width of the 2D convolution window. Single int means the value is for both the height and the width of</span>
<span class="sd">            the kernel. A tuple of 2 ints means the first value is for the height and the other is for the</span>
<span class="sd">            width of the kernel.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Its value must be equal to or greater than 1.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        pad_mode (str): Select the mode of the pad. The optional values are</span>
<span class="sd">            &quot;pad&quot;, &quot;same&quot;, &quot;valid&quot;. Default: &quot;same&quot;.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`.</span>

<span class="sd">            - same: Adopted the way of completion.</span>

<span class="sd">            - valid: Adopted the way of discarding.</span>
<span class="sd">        padding (Union[int, tuple[int]]): Implicit paddings on both sides of the input `x`. If `padding` is one integer,</span>
<span class="sd">                    the paddings of top, bottom, left and right are the same, equal to padding. If `padding` is a tuple</span>
<span class="sd">                    with four integers, the paddings of top, bottom, left and right will be equal to padding[0],</span>
<span class="sd">                    padding[1], padding[2], and padding[3] accordingly. Default: 0.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): The data type is int or a tuple of 2 integers. Specifies the dilation rate</span>
<span class="sd">                                      to use for dilated convolution. If set to be :math:`k &gt; 1`, there will</span>
<span class="sd">                                      be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">                                      be greater than or equal to 1 and bounded by the height and width of the</span>
<span class="sd">                                      input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. This does not support for Davinci devices when group &gt; 1. Default: 1.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the convolution kernel.</span>
<span class="sd">            It can be a Tensor, a string, an Initializer or a number. When a string is specified,</span>
<span class="sd">            values from &#39;TruncatedNormal&#39;, &#39;Normal&#39;, &#39;Uniform&#39;, &#39;HeUniform&#39; and &#39;XavierUniform&#39; distributions as well</span>
<span class="sd">            as constant &#39;One&#39; and &#39;Zero&#39; distributions are possible. Alias &#39;xavier_uniform&#39;, &#39;he_uniform&#39;, &#39;ones&#39;</span>
<span class="sd">            and &#39;zeros&#39; are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Possible</span>
<span class="sd">            Initializer and string are the same as &#39;weight_init&#39;. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;zeros&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels`, `out_channels` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `padding` or `dilation` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `padding` is not equal to (0, 0, 0, 0).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Conv2dTranspose(3, 64, 4, has_bias=False, weight_init=&#39;normal&#39;, pad_mode=&#39;pad&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([1, 3, 16, 50]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x).shape</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1, 64, 19, 53)</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2dTranspose.&quot;&quot;&quot;</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">twice</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">twice</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">twice</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;padding size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="c1"># out_channels and in_channels swap.</span>
        <span class="c1"># cause Conv2DBackpropInput&#39;s out_channel refers to Conv2D&#39;s out_channel,</span>
        <span class="c1"># then Conv2dTranspose&#39;s out_channel refers to Conv2DBackpropInput&#39;s in_channel.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2dTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="p">,</span>
            <span class="n">transposed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_valid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;valid&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_same</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span>
        <span class="k">if</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">has_bias</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>

        <span class="c1"># cause Conv2DTranspose&#39;s out_channel refers to Conv2D&#39;s out_channel.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2DTranspose</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                                                  <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                                  <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                  <span class="n">pad_mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">,</span>
                                                  <span class="n">pad</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                                                  <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                                  <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                                  <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_top</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_bottom</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_left</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_right</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_top</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_bottom</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_left</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>

    <span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_valid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_same</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pad</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_top</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_bottom</span><span class="p">)</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_valid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_same</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pad</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_left</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_right</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">)),</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;input_channels=</span><span class="si">{}</span><span class="s1">, output_channels=</span><span class="si">{}</span><span class="s1">, kernel_size=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;stride=</span><span class="si">{}</span><span class="s1">, pad_mode=</span><span class="si">{}</span><span class="s1">, padding=</span><span class="si">{}</span><span class="s1">, dilation=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;group=</span><span class="si">{}</span><span class="s1">, has_bias=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;weight_init=</span><span class="si">{}</span><span class="s1">, bias_init=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>


<div class="viewcode-block" id="Conv1dTranspose"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.Conv1dTranspose.html#mindspore.nn.Conv1dTranspose">[docs]</a><span class="k">class</span> <span class="nc">Conv1dTranspose</span><span class="p">(</span><span class="n">_Conv</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    1D transposed convolution layer.</span>

<span class="sd">    Compute a 1D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>
<span class="sd">    This module can be seen as the gradient of Conv1d with respect to its input.</span>

<span class="sd">    `x` is typically of shape :math:`(N, C, W)`, where :math:`N` is batch size, :math:`C` is channel number and</span>
<span class="sd">    :math:`W` is the characteristic length.</span>

<span class="sd">    The padding argument effectively adds :math:`dilation * (kernel\_size - 1) - padding` amount of zero padding to</span>
<span class="sd">    both sizes of the input. So that when a Conv1d and a ConvTranspose1d are initialized with same parameters,</span>
<span class="sd">    they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1,</span>
<span class="sd">    Conv1d maps multiple input shapes to the same output shape.</span>

<span class="sd">    The width of output is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        W_{out} = \begin{cases}</span>
<span class="sd">        (W_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation} \times</span>
<span class="sd">        (\text{ks_w} - 1) + 1, &amp; \text{if pad_mode=&#39;pad&#39;}\\</span>
<span class="sd">        (W_{in} + \text{stride} - 1)/\text{stride}, &amp; \text{if pad_mode=&#39;same&#39;}\\</span>
<span class="sd">        (W_{in} - 1) \times \text{stride} + \text{dilation} \times</span>
<span class="sd">        (\text{ks_w} - 1) + 1, &amp; \text{if pad_mode=&#39;valid&#39;}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`\text{ks_w}` is the width of the convolution kernel.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): The number of channels in the input space.</span>
<span class="sd">        out_channels (int): The number of channels in the output space.</span>
<span class="sd">        kernel_size (int): int, which specifies the width of the 1D convolution window.</span>
<span class="sd">        stride (int): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the width of movement. Default: 1.</span>
<span class="sd">        pad_mode (str): Select the mode of the pad. The optional values are</span>
<span class="sd">            &quot;pad&quot;, &quot;same&quot;, &quot;valid&quot;. Default: &quot;same&quot;.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input `x`.</span>

<span class="sd">            - same: Adopted the way of completion.</span>

<span class="sd">            - valid: Adopted the way of discarding.</span>
<span class="sd">        padding (int): Implicit paddings on both sides of the input `x`. Default: 0.</span>
<span class="sd">        dilation (int): The data type is int. Specifies the dilation rate</span>
<span class="sd">                                      to use for dilated convolution. If set to be :math:`k &gt; 1`, there will</span>
<span class="sd">                                      be :math:`k - 1` pixels skipped for each sampling location. Its value must</span>
<span class="sd">                                      be greater or equal to 1 and bounded by the width of the</span>
<span class="sd">                                      input `x`. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. This is not support for Davinci devices when group &gt; 1. Default: 1.</span>
<span class="sd">        has_bias (bool): Specifies whether the layer uses a bias vector. Default: False.</span>
<span class="sd">        weight_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the convolution kernel.</span>
<span class="sd">            It can be a Tensor, a string, an Initializer or a numbers.Number. When a string is specified,</span>
<span class="sd">            values from &#39;TruncatedNormal&#39;, &#39;Normal&#39;, &#39;Uniform&#39;, &#39;HeUniform&#39; and &#39;XavierUniform&#39; distributions as well</span>
<span class="sd">            as constant &#39;One&#39; and &#39;Zero&#39; distributions are possible. Alias &#39;xavier_uniform&#39;, &#39;he_uniform&#39;, &#39;ones&#39;</span>
<span class="sd">            and &#39;zeros&#39; are acceptable. Uppercase and lowercase are both acceptable. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;normal&#39;.</span>
<span class="sd">        bias_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the bias vector. Possible</span>
<span class="sd">            Initializer and string are the same as &#39;weight_init&#39;. Refer to the values of</span>
<span class="sd">            Initializer for more details. Default: &#39;zeros&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` or `dilation` is not an int.</span>
<span class="sd">        ValueError: If `in_channels`, `out_channels`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `padding` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; net = nn.Conv1dTranspose(3, 64, 4, has_bias=False, weight_init=&#39;normal&#39;, pad_mode=&#39;pad&#39;)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([1, 3, 50]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x).shape</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1, 64, 53)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channels</span><span class="p">,</span>
                 <span class="n">out_channels</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">,</span>
                 <span class="n">bias_init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv1dTranspose.&quot;&quot;&quot;</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>
        <span class="n">get_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="n">get_dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight_init</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">weight_init_shape</span> <span class="o">=</span> <span class="n">get_shape</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_init_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;weight_init_shape&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">weight_init_dtype</span> <span class="o">=</span> <span class="n">get_dtype</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>
            <span class="n">weight_init_value</span> <span class="o">=</span> <span class="n">weight_init</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">weight_init_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">weight_init_value</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">weight_init</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">weight_init_value</span><span class="p">,</span> <span class="n">weight_init_dtype</span><span class="p">)</span>
        <span class="c1"># out_channels and in_channels swap.</span>
        <span class="c1"># cause Conv2DBackpropInput&#39;s out_channel refers to Conv2D&#39;s out_channel,</span>
        <span class="c1"># then Conv1dTranspose&#39;s out_channel refers to Conv2DBackpropInput&#39;s in_channel.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1dTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">pad_mode</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">group</span><span class="p">,</span>
            <span class="n">has_bias</span><span class="p">,</span>
            <span class="n">weight_init</span><span class="p">,</span>
            <span class="n">bias_init</span><span class="p">,</span>
            <span class="n">transposed</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_valid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;valid&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_same</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;same&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_pad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span>
        <span class="k">if</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">has_bias</span><span class="p">,</span> <span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">bias_init</span><span class="p">,</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>

        <span class="c1"># cause Conv2DBackpropInput&#39;s out_channel refers to Conv2D&#39;s out_channel.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Conv2DBackpropInput</span><span class="p">(</span><span class="n">out_channel</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
                                                      <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                                      <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                      <span class="n">pad_mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">,</span>
                                                      <span class="n">pad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                      <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                                      <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                                      <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BiasAdd</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">_check_input_3d</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">h_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_valid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_same</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pad</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="n">_deconv_output_length</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_valid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_same</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_pad</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                      <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2d_transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">extend_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="s1">&#39;input_channels=</span><span class="si">{}</span><span class="s1">, output_channels=</span><span class="si">{}</span><span class="s1">, kernel_size=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;stride=</span><span class="si">{}</span><span class="s1">, pad_mode=</span><span class="si">{}</span><span class="s1">, padding=</span><span class="si">{}</span><span class="s1">, dilation=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;group=</span><span class="si">{}</span><span class="s1">, has_bias=</span><span class="si">{}</span><span class="s1">, &#39;</span> \
            <span class="s1">&#39;weight_init=</span><span class="si">{}</span><span class="s1">, bias_init=</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">weight_init</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">bias_init</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>