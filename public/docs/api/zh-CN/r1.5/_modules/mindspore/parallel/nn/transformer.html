<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.parallel.nn.transformer &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">MindSpore Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.parallel.nn.html">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost (experimental)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.5/api_cpp/mindspore.html">MindSpore Liteâ†—</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.parallel.nn.transformer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.parallel.nn.transformer</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Note:</span>
<span class="sd">    Transformer Networks. This is an experimental interface that is subject to change and/or deletion.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.cell</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="n">_get_parallel_mode</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">.layers</span> <span class="kn">import</span> <span class="n">_LayerNorm</span><span class="p">,</span> <span class="n">_Linear</span><span class="p">,</span> <span class="n">_check_input_shape</span><span class="p">,</span> \
    <span class="n">_args_type_validator_check</span><span class="p">,</span> <span class="n">_valid_type_checks</span><span class="p">,</span> <span class="n">_valid_value_checks</span><span class="p">,</span> \
    <span class="n">_check_shape_equal</span><span class="p">,</span> <span class="n">_check_past_none_input_none</span><span class="p">,</span> <span class="n">_check_input_dtype</span><span class="p">,</span> <span class="n">_check_input_shape_value</span>
<span class="kn">from</span> <span class="nn">.op_parallel_config</span> <span class="kn">import</span> <span class="n">default_dpmp_config</span><span class="p">,</span> <span class="n">_PipeLineConfig</span><span class="p">,</span> <span class="n">OpParallelConfig</span><span class="p">,</span> <span class="n">_Config</span><span class="p">,</span> <span class="n">_check_config</span>
<span class="kn">from</span> <span class="nn">.moe</span> <span class="kn">import</span> <span class="n">default_moe_config</span><span class="p">,</span> <span class="n">MoE</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;AttentionMask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;VocabEmbedding&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FeedForward&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Transformer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerOpParallelConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;EmbeddingOpParallelConfig&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="EmbeddingOpParallelConfig"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.EmbeddingOpParallelConfig">[docs]</a><span class="k">class</span> <span class="nc">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">_Config</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        EmbeddingOpParallelConfig for the setting data parallel or row slice for the embedding table.</span>

<span class="sd">        Args:</span>
<span class="sd">            data_parallel (int): The data parallel way. Default: 1</span>
<span class="sd">            model_parallel (int): The model parallel way. Default: 1</span>
<span class="sd">            vocab_emb_dp (bool): Shard embedding in model parallel or data parallel. Default: True</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; config=EmbeddingOpParallelConfig(data_parallel=1, model_parallel=1, vocab_emb_dp=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span> <span class="o">=</span> <span class="n">OpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="n">model_parallel</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">vocab_emb_dp</span><span class="p">,</span> <span class="s2">&quot;vocab_emb_dp&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_emb_dp</span> <span class="o">=</span> <span class="n">vocab_emb_dp</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span>

    <span class="nd">@data_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span>

    <span class="nd">@model_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_emb_dp</span>

    <span class="nd">@vocab_emb_dp</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;vocab_emb_dp&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_emb_dp</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dp_mp_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            To obtain the DPMPlConfig for the setting data parallel, model parallel</span>

<span class="sd">            Supported Platforms:</span>
<span class="sd">                ``Ascend`` ``GPU``</span>

<span class="sd">            Examples:</span>
<span class="sd">                &gt;&gt;&gt; config=EmbeddingOpParallelConfig(data_parallel=1, model_parallel=1, vocab_emb_dp=True)</span>
<span class="sd">                &gt;&gt;&gt; parallel_config = config.dp_mp_config</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span></div>


<div class="viewcode-block" id="TransformerOpParallelConfig"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.TransformerOpParallelConfig">[docs]</a><span class="k">class</span> <span class="nc">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">_Config</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TransformerOpParallelConfig for the setting global data parallel, model parallel and fusion group.</span>
<span class="sd">        The parallel configure setting.</span>

<span class="sd">        Note:</span>
<span class="sd">            Except the recompute argument, other arguments will not be effective when the user doesn&#39;t set</span>
<span class="sd">            auto_parallel_context to `SEMI_AUTO_PARALLEL` or `AUTO_PARALLEL`.</span>
<span class="sd">            The micro_batch_num must be greater than or equal to pipeline_stage. The data_parallel\*model_parallel</span>
<span class="sd">            \*pipeline_stage must be equal or less equal to the device. When setting the pipeline stage and</span>
<span class="sd">            optimizer_shard, the config will overwrite the auto_parallel_context.</span>

<span class="sd">        Args:</span>
<span class="sd">            data_parallel (int): The data parallel way. Default: 1.</span>
<span class="sd">            model_parallel (int): The model parallel way. Default: 1.</span>
<span class="sd">            pipeline_stage (int): The number of the pipeline stage. Should be a positive value. Default: 1.</span>
<span class="sd">            micro_batch_num (int): The microe size of the batches for the pipeline training. Default: 1.</span>
<span class="sd">            optimizer_shard (bool): Whether to enable optimizer shard. Default False.</span>
<span class="sd">            gradient_aggregation_group (int): The fusion group size of the optimizer state sharding. Default: 4.</span>
<span class="sd">            recompute (bool): Enable recomputation of the transformer block or not. Default: False.</span>
<span class="sd">            vocab_emb_dp (bool): Shard embedding in model parallel or data parallel. Default: True.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; config=TransformerOpParallelConfig(data_parallel=1, model_parallel=1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pipeline_stage</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">micro_batch_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">recompute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">optimizer_shard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_aggregation_group</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recompute</span> <span class="o">=</span> <span class="n">recompute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_shard</span> <span class="o">=</span> <span class="n">optimizer_shard</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_aggregation_group</span> <span class="o">=</span> <span class="n">gradient_aggregation_group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span> <span class="o">=</span> <span class="n">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="n">model_parallel</span><span class="p">,</span>
                                                             <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="n">vocab_emb_dp</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span> <span class="o">=</span> <span class="n">_PipeLineConfig</span><span class="p">(</span><span class="n">pipeline_stage</span><span class="o">=</span><span class="n">pipeline_stage</span><span class="p">,</span> <span class="n">micro_batch_num</span><span class="o">=</span><span class="n">micro_batch_num</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span>

    <span class="nd">@recompute</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;recompute&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span>

    <span class="nd">@vocab_emb_dp</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">gradient_aggregation_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_aggregation_group</span>

    <span class="nd">@gradient_aggregation_group</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">gradient_aggregation_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;gradient_aggregation_group&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_aggregation_group</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">micro_batch_num</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">micro_batch_num</span>

    <span class="nd">@micro_batch_num</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">micro_batch_num</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">micro_batch_num</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span>

    <span class="nd">@model_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span>

    <span class="nd">@data_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pipeline_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">pipeline_stage</span>

    <span class="nd">@pipeline_stage</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pipeline_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">optimizer_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_shard</span>

    <span class="nd">@optimizer_shard</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">optimizer_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;optimizer_shard&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_shard</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">embedding_dp_mp_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            To obtain the EmbeddingParallelConfig for the setting data parallel, model parallel and embedding</span>
<span class="sd">            parallel.</span>

<span class="sd">            Supported Platforms:</span>
<span class="sd">                ``Ascend`` ``GPU``</span>

<span class="sd">            Examples:</span>
<span class="sd">                &gt;&gt;&gt; config=TransformerOpParallelConfig(data_parallel=1, model_parallel=1, vocab_emb_dp=True)</span>
<span class="sd">                &gt;&gt;&gt; parallel_config = config.embedding_dp_mp_config</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dp_mp_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            To obtain the EmbeddingParallelConfig for the setting data parallel, model parallel and embedding</span>
<span class="sd">            parallel.</span>

<span class="sd">            Supported Platforms:</span>
<span class="sd">                ``Ascend`` ``GPU``</span>

<span class="sd">            Examples:</span>
<span class="sd">                &gt;&gt;&gt; config=TransformerOpParallelConfig(data_parallel=1, model_parallel=1, vocab_emb_dp=True)</span>
<span class="sd">                &gt;&gt;&gt; parallel_config = config.dp_mp_config</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">dp_mp_config</span></div>


<span class="n">default_transformer_config</span> <span class="o">=</span> <span class="n">TransformerOpParallelConfig</span><span class="p">()</span>
<span class="n">default_embedding_parallel_config</span> <span class="o">=</span> <span class="n">EmbeddingOpParallelConfig</span><span class="p">()</span>


<div class="viewcode-block" id="FeedForward"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.FeedForward">[docs]</a><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The multilayer perceptron with two linear layers with dropout applied at final output. The first linear</span>
<span class="sd">    will project the input dimension from hidden_size to ffn_hidden_size, the second linear will project the</span>
<span class="sd">    dimension from ffn_hidden_size to hidden_size. The first linear is sharded on the relative dimension,</span>
<span class="sd">    the second linear is sharded on the output dimension. The overview process can be</span>

<span class="sd">    .. math::</span>
<span class="sd">        Dropout((xW_1+b_1)W_2 + b_2))</span>

<span class="sd">    where the :math:`W_1, W_2, b_1` and :math:`b_2` are trainable parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_size (int): The dimension of the inputs.</span>
<span class="sd">        ffn_hidden_size (int): The intermediate hidden size.</span>
<span class="sd">        dropout_rate (float): The dropout rate for the second linear&#39;s output.</span>
<span class="sd">        hidden_act (str): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                         &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                         &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. Default: gelu.</span>
<span class="sd">        expert_num (int): The number of experts used in Linear. For the case expert_num &gt; 1, BatchMatMul is used</span>
<span class="sd">            and the first dimension in BatchMatMul indicate expert_num. Default: 1.</span>
<span class="sd">        param_init_type (dtype.Number): The parameter initialization type. Should be dtype.float32 or dtype.float16.</span>
<span class="sd">                                        Default: dtype.float32.</span>
<span class="sd">        parallel_config(OpParallelConfig): The config of parallel setting, see `OpParallelConfig`.</span>
<span class="sd">                                           Default `default_dpmp_config`, an instance of `OpParallelConfig` with</span>
<span class="sd">                                           default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - should be `[batch, seq_length, hidden_size] or [batch * seq_length, hidden_size]`.</span>
<span class="sd">          Float tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the output of this layer after mapping. The shape is `[batch, seq_length, hidden_size]</span>
<span class="sd">        or [batch * seq_length, hidden_size]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: `hidden_act` is not a string.</span>
<span class="sd">        TypeError: `parallel_config` is not a subclass of OpParallelConfig.</span>
<span class="sd">        ValueError: `ffn_hidden_size` is not a multiple of the model parallel way.</span>
<span class="sd">        ValueError: `hidden_size` is not a multiple of the model parallel way.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import FeedForward</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = FeedForward(hidden_size=15, ffn_hidden_size=30, dropout_rate=0.1)</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor(np.ones((2, 20, 15)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = model(tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 20, 15)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;FeedForward&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;FeedForward&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;FeedForward&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">expert_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">dp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span>
        <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ffn_hidden_size </span><span class="si">{</span><span class="n">ffn_hidden_size</span><span class="si">}</span><span class="s2"> should be a multiple of the model parallel way </span><span class="si">{</span><span class="n">mp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;hidden_size </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2"> should be a multiple of the model parallel way </span><span class="si">{</span><span class="n">mp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;dropout_rate probability should be a number in range [0, 1.0), &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">dropout_rate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>
        <span class="c1"># Here, &#39;ep&#39; stands for expert parallel number, which is equal to data parallel number.</span>
        <span class="n">ep</span> <span class="o">=</span> <span class="n">dp</span>
        <span class="c1"># Project to ffn_hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                               <span class="n">out_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                               <span class="n">activation</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                               <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                               <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                               <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                               <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,)),</span>
                               <span class="n">strategy_activation</span><span class="o">=</span><span class="p">((</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                               <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,)),</span>
                               <span class="n">strategy_activation</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),))</span>
        <span class="c1"># Project back to hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                  <span class="n">out_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                                  <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                  <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                  <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">_check_input_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="c1"># returned shape is [bs, seq_length, ffn_hidden_size] or [bs * seq_length, ffn_hidden_size]</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># returned shape is [bs, seq_length, ffn_hidden_size] or [bs * seq_length, ffn_hidden_size]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="AttentionMask"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.AttentionMask">[docs]</a><span class="k">class</span> <span class="nc">AttentionMask</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the Lower triangular matrix from the input mask. The input mask is a 2D tensor (batch_size, seq_length)</span>
<span class="sd">    with 1 and 0. 1 indicates the current position is a valid token, otherwise not.</span>

<span class="sd">    Args:</span>
<span class="sd">        seq_length(int): The sequence length of the input tensor.</span>
<span class="sd">        parallel_config(OpParallelConfig): The parallel configure. Default `default_dpmp_config`,</span>
<span class="sd">                                           an instance of `OpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_mask** (Tensor) - The mask indicating whether each position is a valid input with</span>
<span class="sd">          (batch_size, seq_length).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor. The attention mask matrix with shape (batch_size, seq_length, seq_length).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: `seq_length` is not an integer.</span>
<span class="sd">        ValueError: `seq_length` is not a positive value.</span>
<span class="sd">        TypeError: `parallel_config` is not a subclass of OpParallelConfig.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import AttentionMask</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; mask = AttentionMask(seq_length=4)</span>
<span class="sd">        &gt;&gt;&gt; mask_array = np.array([[1, 1, 1, 0]], np.float32)</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(mask_array)</span>
<span class="sd">        &gt;&gt;&gt; res = mask(inputs)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        [[[1. 0. 0. 0],</span>
<span class="sd">          [1. 1. 0. 0],</span>
<span class="sd">          [1. 1. 1. 0],</span>
<span class="sd">          [0. 0. 0. 0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;AttentionMask&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionMask</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dim</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>
        <span class="c1"># Default lower triangle mask matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lower_triangle_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">):</span>
        <span class="n">_check_input_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_shape_value</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)</span>
        <span class="n">input_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="n">shape_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">shape_left</span> <span class="o">=</span> <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
        <span class="c1"># Mask the padded inputs</span>
        <span class="n">mask_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">shape_left</span><span class="p">)</span>
        <span class="n">mask_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">shape_right</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">mask_left</span><span class="p">,</span> <span class="n">mask_right</span><span class="p">)</span>
        <span class="n">lower_traiangle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lower_triangle_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, seq_length, seq_length]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">lower_traiangle</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_mask</span></div>


<div class="viewcode-block" id="VocabEmbedding"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.VocabEmbedding">[docs]</a><span class="k">class</span> <span class="nc">VocabEmbedding</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The embedding lookup table from the 0-th dim of the parameter table. When the parallel_config.vocab_emb_dp is</span>
<span class="sd">    True and in the `AUTO_PARALLEL_MODE`, the embedding lookup will be a `parallel_config.data_parallel`</span>
<span class="sd">    data parallel way, or will shard the parameter at the 0-th dimension in `parallel_config.model_parallel`, so-called</span>
<span class="sd">    row slice of the embedding table.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size (int): Size of the dictionary of embeddings.</span>
<span class="sd">        embedding_size (int): The size of each embedding vector.</span>
<span class="sd">        param_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the embedding_table.</span>
<span class="sd">            Refer to class `initializer` for the values of string when a string</span>
<span class="sd">            is specified. Default: &#39;normal&#39;.</span>
<span class="sd">        parallel_config(EmbeddingOpParallelConfig): The parallel config of network. Default</span>
<span class="sd">            `default_embedding_parallel_config`, an instance of `EmbeddingOpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        **input_ids** (Tensor) - The tokenized inputs with datatype int32 with shape (batch_size, seq_length)</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains (`output`, `embedding_table`)</span>

<span class="sd">        - **output** (Tensor) - The embedding vector for the input with shape (batch_size,</span>
<span class="sd">          seq_length, embedding_size).</span>
<span class="sd">        - **weight** (Tensor) - The embedding table with shape (vocab_size, embedding_size).</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the parallel_config.vocab_emb_dp is True, the vocab size is not a multiple of</span>
<span class="sd">            parallel_config.model_parallel</span>
<span class="sd">        ValueError: `vocab_size` is not a positive value.</span>
<span class="sd">        ValueError: `embedding_size` is not a positive value.</span>
<span class="sd">        TypeError: `parallel_config` is not a subclass of OpParallelConfig.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import VocabEmbedding</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; model = VocabEmbedding(vocab_size=30, embedding_size=30)</span>
<span class="sd">        &gt;&gt;&gt; tensor = Tensor(np.ones((20, 15)), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output, table = model(tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (20, 15, 30)</span>
<span class="sd">        &gt;&gt;&gt; print(table.shape)</span>
<span class="sd">        (30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">embedding_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">EmbeddingOpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;VocabEmbedding&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_embedding_parallel_config</span><span class="p">,</span>
                 <span class="n">param_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VocabEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">]),</span>
                                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">,</span> <span class="n">parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherV2</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="si">}</span><span class="s2"> data parallel for the embedding lookup.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The vocab size of the embedding </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> must be a &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;multiple of parallel_config.model_parallel </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherV2</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="si">}</span><span class="s2"> model parallel for the embedding lookup.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="n">_check_input_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span></div>


<div class="viewcode-block" id="MultiHeadAttention"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.MultiHeadAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This is an implementation of multihead attention in the paper `Attention is all you need</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1706.03762v5.pdf&gt;`_. Given the query vector with source length, and the</span>
<span class="sd">    key and value vector with target length, the attention will be performed as the following</span>

<span class="sd">    .. math::</span>
<span class="sd">           MultiHeadAttention(query, key, vector) = Concat(head_1, \dots, head_h)W^O</span>

<span class="sd">    where :math:`head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)`. The default is with a bias.</span>

<span class="sd">    if query, key and value tensor is same, then it will be self attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): The batch size of the input tensor.</span>
<span class="sd">        src_seq_length(int): The sequence length of the query vector.</span>
<span class="sd">        tgt_seq_length(int): The sequence length of the key and value vector.</span>
<span class="sd">        hidden_size(int): The hidden size of the input.</span>
<span class="sd">        num_heads(int): The number of the heads.</span>
<span class="sd">        hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1</span>
<span class="sd">        attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1</span>
<span class="sd">        compute_dtype(dtype.Number): The computation type of dense. Default dtype.float16.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16.</span>
<span class="sd">        param_init_type(dtype.Number): The parameter initialization type of the module. Default dtype.float32.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16.</span>
<span class="sd">        softmax_compute_type(dtype.Number): The type of softmax computation module. Default dtype.float32.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16.</span>
<span class="sd">        use_past(bool): Use the past state to compute, used for incremental prediction. For example, if we have two</span>
<span class="sd">            words and want to generate the ten more words. We just need to compute the two words&#39;s state only once,</span>
<span class="sd">            and generate the next word one by one. When use_past is True, there are two steps to run the prediction.</span>
<span class="sd">            The first step, set the is_first_iteration to be True by</span>
<span class="sd">            `model.add_flags_recursive(is_first_iteration=True)`, and pass the full inputs. Then, set the</span>
<span class="sd">            is_first_iteration to be False by `model.add_flags_recursive(is_first_iteration=False)`. At this moment,</span>
<span class="sd">            pass the single step&#39;s input tensor, and loop it. Default False.</span>
<span class="sd">        parallel_config(OpParallelConfig): The parallel configure. Default `default_dpmp_config`,</span>
<span class="sd">                                           an instance of `OpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **query_tensor** (Tensor) - the query vector with shape (batch_size, src_seq_length, hidden_size) or</span>
<span class="sd">          (batch_size * src_seq_length, hidden_size), if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">          must be (batch_size, 1, hidden_size)</span>
<span class="sd">        - **key_tensor** (Tensor) - the key vector with shape (batch_size, tgt_seq_length, hidden_size) or</span>
<span class="sd">          (batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">          must be (batch_size, 1, hidden_size)</span>
<span class="sd">        - **value_tensor** (Tensor) - the value vector with shape (batch_size, tgt_seq_length, hidden_size) or</span>
<span class="sd">          (batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">          must be (batch_size, 1, hidden_size)</span>
<span class="sd">        - **attention_mask** (Tensor) - the attention mask matrix with shape (batch_size, src_seq_length,</span>
<span class="sd">          tgt_seq_length), if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">          must be (batch_size, 1, tgt_seq_length)</span>
<span class="sd">        - **key_past** (Tensor) - Float16 tensor with shape (batch_size, num_heads, size_per_head, tgt_seq_length).</span>
<span class="sd">          The past calculated key vector. Used for incremental prediction when the use_past is True.</span>
<span class="sd">          Default None.</span>
<span class="sd">        - **value_past** (Tensor) - Float16 tensor with shape (batch_size, num_heads, tgt_seq_length, size_per_head).</span>
<span class="sd">          The past calculated value vector. Used for incremental prediction when the use_past is True.</span>
<span class="sd">          Default None.</span>
<span class="sd">        - **batch_valid_length** (Tensor) - Int32 tensor with shape (batch_size,) the past calculated the index.</span>
<span class="sd">          Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">        - **output** (Tensor) - Tensor, the float tensor of the output of the layer with</span>
<span class="sd">          shape (batch_size, src_seq_length, hidden_size) or (batch_size * src_seq_length, hidden_size),</span>
<span class="sd">          if the use_past is False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</span>

<span class="sd">        - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with</span>
<span class="sd">          ((batch_size, num_heads, size_per_head, tgt_seq_length),</span>
<span class="sd">          (batch_size, num_heads, tgt_seq_length, size_per_head)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import MultiHeadAttention</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = MultiHeadAttention(batch_size=2, hidden_size=15, src_seq_length=20, tgt_seq_length=20,</span>
<span class="sd">        ...                            num_heads=3)</span>
<span class="sd">        &gt;&gt;&gt; from_tensor = Tensor(np.ones((2, 20, 15)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; to_tensor = Tensor(np.ones((2, 20, 15)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; attention_mask = Tensor(np.ones((2, 20, 20)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; attn_out, past = model(from_tensor, to_tensor, to_tensor, attention_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(attn_out.shape)</span>
<span class="sd">        (2, 20, 15)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 3, 5, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 3, 20, 5)</span>
<span class="sd">        # When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="sd">        # Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="sd">        # We need to prepare the memory parameters for saving key and value states firstly.</span>
<span class="sd">        &gt;&gt;&gt; model = MultiHeadAttention(batch_size=2, hidden_size=15, src_seq_length=20, tgt_seq_length=20,</span>
<span class="sd">        ...                            num_heads=3, use_past=True)</span>
<span class="sd">        &gt;&gt;&gt; key_past = Tensor(np.zeros(shape=(2, 3, 5, 20)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; value_past = Tensor(np.zeros(shape=(2, 3, 20, 5)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; batch_valid_length = Tensor(np.ones((2,)), mstype.int32)</span>
<span class="sd">        # Set is_first_iteration=True to generate the full memory states</span>
<span class="sd">        &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=True)</span>
<span class="sd">        &gt;&gt;&gt; attn_out, past = model(from_tensor, to_tensor, to_tensor, attention_mask, key_past, value_past,</span>
<span class="sd">        ...                        batch_valid_length)</span>
<span class="sd">        &gt;&gt;&gt; print(attn_out.shape)</span>
<span class="sd">        (2, 20, 15)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 3, 5, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 3, 20, 5)</span>
<span class="sd">        &gt;&gt;&gt; from_tensor = Tensor(np.ones((2, 1, 15)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; to_tensor = Tensor(np.ones((2, 1, 15)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; attention_mask = Tensor(np.ones((2, 1, 20)), mstype.float16)</span>
<span class="sd">        # Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the full</span>
<span class="sd">        # sequence.</span>
<span class="sd">        &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=False)</span>
<span class="sd">        &gt;&gt;&gt; attn_out, past = model(from_tensor, to_tensor, to_tensor, attention_mask, key_past, value_past,</span>
<span class="sd">        ...                        batch_valid_length)</span>
<span class="sd">        &gt;&gt;&gt; print(attn_out.shape)</span>
<span class="sd">        (2, 1, 15)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 3, 5, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 3, 20, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">compute_dtype</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                  <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_mode</span> <span class="o">=</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="k">if</span> <span class="n">hidden_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">hidden_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;hidden_dropout_rate probability should be a number in range [0, 1.0), &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">hidden_dropout_rate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">attention_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;attention_dropout_rate probability should be a number in range [0, 1.0), &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">attention_dropout_rate</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The hidden size </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2"> should be a multiple of num_heads </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The number of heads </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2"> must be a &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;multiple of parallel_config.model_parallel </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_mode</span> <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The batch size </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> must be a &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;multiple of parallel_config.data_parallel </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="c1"># Output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
                              <span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                               <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="c1"># embedding size per head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_k</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_v</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span>
            <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span>
        <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="c1"># Normalize factor for attention, sqrt(dk) as widely used</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_dropout_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

        <span class="c1"># Query</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                          <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>
        <span class="c1"># Key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                          <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>

        <span class="c1"># Value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">compute_dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                          <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># operators used for state reuse</span>
            <span class="n">seq_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">src_seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">range</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">seq_range</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">))),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">less</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">value_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span>
                           <span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">ori_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_to_2d_tensor</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span>
                                                                                                   <span class="n">key_tensor</span><span class="p">,</span>
                                                                                                   <span class="n">value_tensor</span><span class="p">,</span>
                                                                                                   <span class="n">attention_mask</span><span class="p">)</span>

        <span class="c1"># multi head attention: query, key, value are derived from the same inputs</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="c1"># the returned shape is [bs, size_per_head, seq_length, num_heads]</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># the returned shape is [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="c1"># support input shape is [bs, seq, seq] or [bs, heads, seq, seq]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># expand attention mask from [bs, seq, seq] -&gt; [bs, 1, seq, seq]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># key and value for current token(s)</span>
        <span class="n">key_present</span> <span class="o">=</span> <span class="n">key</span>
        <span class="n">value_present</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># The first graph with the input size of (bs, seq_length)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
                <span class="c1"># Get the valid input length without padding</span>
                <span class="n">valid_length_vector</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="c1"># Cover the key and value numbers corresponding to the padding position</span>
                <span class="n">key_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">value_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="c1"># The second graph with the inpus size of (bs, 1)</span>
            <span class="c1"># the shape of query is (bs, num_heads, 1, size_per_head)</span>
            <span class="c1"># the shape of key is   (bs, num_heads, size_per_head, 1)</span>
            <span class="c1"># the shape of value is (bs, num_heads, 1, size_per_head)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Get the current token position index</span>
                <span class="n">valid_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                                                               <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                                                <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">),</span>
                                                                               <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                                    <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="n">valid_length</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">valid_length</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">valid_length_vector</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">valid_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="c1"># Pad the key and value to seq_length with only the position index not zero</span>
                <span class="n">current_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)),</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">current_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="c1"># Concat the previous saved state and current state</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="n">current_key</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value_past</span><span class="p">,</span> <span class="n">current_value</span><span class="p">)</span>
                <span class="c1"># Update key_present and value_present for state update</span>
                <span class="n">key_present</span> <span class="o">=</span> <span class="n">key</span>
                <span class="n">value_present</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">layer_present</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
        <span class="c1"># multi head attention considering attention mask</span>
        <span class="c1"># the return shape is [bs * seq_length, hidden_size]</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="c1"># Output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ori_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span>

    <span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">value_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">):</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span>
                                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span>
                                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span>
                                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>

        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">key_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">value_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value_past</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">key_is_default</span> <span class="o">=</span> <span class="n">key_past</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">value_is_default</span> <span class="o">=</span> <span class="n">value_past</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">key_is_tensor</span><span class="p">,</span>
                                    <span class="n">key_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">value_is_tensor</span><span class="p">,</span>
                                    <span class="n">value_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_past</span><span class="p">),</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">key_past</span><span class="p">),</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_past</span><span class="p">),</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">value_past</span><span class="p">),</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_convert_to_2d_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;convert a nd tensor to a 2d tensor&quot;&quot;&quot;</span>
        <span class="n">query_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">query_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">query_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">key_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">key_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">key_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">value_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">)</span>
        <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">value_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">return</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">query_shape</span>

    <span class="k">def</span> <span class="nf">_merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        convert a 4d input to a 2d output</span>

<span class="sd">        Inputs:</span>
<span class="sd">            x: input tensor</span>

<span class="sd">        Output:</span>
<span class="sd">            x_merge: the 2d output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># bs, seq_length, head, size_per_head</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_merge</span>

    <span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the weighted score along the seq_length</span>

<span class="sd">        Inputs:</span>
<span class="sd">            query: the query matrix</span>
<span class="sd">            key: the key matrix</span>
<span class="sd">            value: the value matrix</span>
<span class="sd">            attention_mask: the attention mask matrix with shape (batch_size,</span>
<span class="sd">            1, seq_length, seq_length)</span>
<span class="sd">        Outputs:</span>
<span class="sd">            weighted_values: Tensor, the weighted sum scores</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Normalize query and key before MatMul, default off</span>
        <span class="c1"># Attention score [bs, num_heads, seq_length, seq_length]</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="c1"># Normalize after query and key MatMul</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span>
            <span class="n">score</span><span class="p">,</span>
            <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)))</span>

        <span class="n">ori_dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span><span class="p">)</span>

        <span class="c1"># for input size of (bs, 1) namely the second graph,</span>
        <span class="c1"># the shape of attention_mask matrix should be (bs, 1, 1, seq_length)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
            <span class="c1"># Calculate the current total token</span>
            <span class="n">current_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                                                            <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">),</span>
                                                                            <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                                 <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="c1"># Get the precise position index</span>
            <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">current_index</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="c1"># Calculate the attention_mask matrix via the position index</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">,</span> <span class="n">index</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Minus 10000 for the position where masked to exclude them from softmax</span>
        <span class="n">multiplu_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
            <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">F</span><span class="o">.</span><span class="n">tuple_to_array</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,)),</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)),</span>
            <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)))</span>

        <span class="n">adder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">multiplu_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span><span class="p">)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">adder</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
        <span class="c1"># attention probs</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span>
                      <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">ori_dtype</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
        <span class="c1"># Weighted sum output [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">weighted_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">weighted_values</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_merge</span></div>


<div class="viewcode-block" id="TransformerEncoderLayer"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.TransformerEncoderLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer Encoder Layer. This is an implementation of the single layer of the transformer</span>
<span class="sd">    encoder layer, including multihead attention and feedward layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): The batch size of the input tensor.</span>
<span class="sd">        hidden_size(int): The hidden size of the input.</span>
<span class="sd">        seq_length(int): The input sequence length.</span>
<span class="sd">        ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">        num_heads(int): The number of the heads.</span>
<span class="sd">        hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1</span>
<span class="sd">        attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1</span>
<span class="sd">        post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">        hidden_act(str): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                         &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                         &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. Default: gelu.</span>
<span class="sd">        layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">        param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        use_past(bool): Use the past state to compute, used for incremental prediction. For example, if we have two</span>
<span class="sd">            words and want to generate the ten more words. We just need to compute the two words&#39;s state only once,</span>
<span class="sd">            and generate the next word one by one. When use_past is True, there are two steps to run the prediction.</span>
<span class="sd">            The first step, set the is_first_iteration to be True by</span>
<span class="sd">            `model.add_flags_recursive(is_first_iteration=True)`, and pass the full inputs. Then, set the</span>
<span class="sd">            is_first_iteration to be False by `model.add_flags_recursive(is_first_iteration=False)`. At this moment,</span>
<span class="sd">            pass the single step&#39;s input tensor, and loop it. Default False.</span>
<span class="sd">        moe_config(MoEConfig): The configuration of MoE (Mixture of Expert).</span>
<span class="sd">        parallel_config(OpParallelConfig): The parallel configure. Default `default_dpmp_config`,</span>
<span class="sd">                                           an instance of `OpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Float Tensor, shape should be [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">          should be [batch_size, 1, hidden_size]</span>
<span class="sd">        - **input_mask** (Tensor) - Float Tensor, attention mask with shape [batch_size, seq_length, seq_length],</span>
<span class="sd">          if the use_past is False or is_first_iteration=True. Otherwise, should be [batch_size, 1, hidden_size]</span>
<span class="sd">        - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">          past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">        - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used</span>
<span class="sd">          for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains(`output`, `layer_present`).</span>

<span class="sd">        - **output** (Tensor) - The float tensor of the output of the layer with</span>
<span class="sd">          shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is</span>
<span class="sd">          False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size)</span>

<span class="sd">        - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with</span>
<span class="sd">          ((batch_size, num_heads, size_per_head, seq_length),</span>
<span class="sd">          (batch_size, num_heads, seq_length, size_per_head)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import TransformerEncoderLayer</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = TransformerEncoderLayer(batch_size=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">        ...                                 num_heads=2)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 16, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 16, 16)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output, past = model(encoder_input_value, encoder_input_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 16, 8)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 2, 4, 16)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 2, 16, 4)</span>
<span class="sd">        # When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="sd">        # Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="sd">        &gt;&gt;&gt; batch_valid_length = Tensor(np.ones((2,)), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; init_reset = Tensor([True], mstype.bool_)</span>
<span class="sd">        # Set is_first_iteration=True to generate the full memory states</span>
<span class="sd">        &gt;&gt;&gt; model = TransformerEncoderLayer(batch_size=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">        ...                                 num_heads=2, use_past=True)</span>
<span class="sd">        &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=True)</span>
<span class="sd">        &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">        &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">        (2, 16, 8)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 2, 4, 16)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 2, 16, 4)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 1, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 1, 16)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; init_reset = Tensor([False], mstype.bool_)</span>
<span class="sd">        # Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the full</span>
<span class="sd">        # sequence.</span>
<span class="sd">        &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=False)</span>
<span class="sd">        &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">        &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">        (2, 1, 8)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 2, 4, 16)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 2, 16, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;num heads must be divisibled by the model parallel way </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but found </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;hidden_size must be divisibled by the model parallel way </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but found </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;ffn_hidden_size must be divisibled by the model parallel way </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but found </span><span class="si">{</span><span class="n">ffn_hidden_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">src_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                            <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                            <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                            <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                            <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                              <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                              <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                              <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Feed Forward Network, FFN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                      <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                      <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                      <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># operator used for state reuse</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="n">size_per_head</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
            <span class="c1"># parameters saving key and value states</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># indicate whether reset saved states</span>
        <span class="n">key_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">value_reset</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># reset states, init_reset True for reuse and False for reset</span>
            <span class="n">key_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">value_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="n">attention</span><span class="p">,</span> <span class="n">layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># For post-layernorm the inputs for residual path are output of self-attention and output of layernorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># For pre-layernorm the inputs for residual path are output of self-attention and input of this layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>

        <span class="n">output_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">aux_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">mlp_logit</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>

        <span class="n">value_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">key_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># current key and value</span>
            <span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span> <span class="o">=</span> <span class="n">layer_present</span>
            <span class="c1"># update key and value calculated this step</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">key_present</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">key_update</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">value_update</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="c1"># add dependency for desired execution order</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">value_update</span><span class="p">)</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">key_update</span><span class="p">)</span>

        <span class="c1"># if shape is 3d, we reshape the inputs of the add</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">aux_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">):</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span>
                                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">init_reset_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">init_reset_is_default</span> <span class="o">=</span> <span class="n">init_reset</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_reset_is_tensor</span><span class="p">,</span>
                                    <span class="n">init_reset_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="TransformerDecoderLayer"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.TransformerDecoderLayer">[docs]</a><span class="k">class</span> <span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer Decoder Layer. This is an implementation of the single layer of the transformer</span>
<span class="sd">    decoder layer, including self-attention, cross attention and feedward layer. When the encoder_output is None,</span>
<span class="sd">    the cross attention will not be effective.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): The batch size of the input tensor.</span>
<span class="sd">        hidden_size(int): The hidden size of the input.</span>
<span class="sd">        src_seq_length(int): The input source sequence length.</span>
<span class="sd">        tgt_seq_length(int): The input target sequence length.</span>
<span class="sd">        ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">        num_heads(int): The number of the heads.</span>
<span class="sd">        hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">        attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">        post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">        hidden_act(str): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                         &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                         &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. Default: gelu.</span>
<span class="sd">        layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">        param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        use_past(bool): Use the past state to compute, used for incremental prediction. Default False.</span>
<span class="sd">        moe_config(MoEConfig): The configuration of MoE (Mixture of Expert).</span>
<span class="sd">        parallel_config(OpParallelConfig): The parallel configure. Default `default_dpmp_config`,</span>
<span class="sd">                                           an instance of `OpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **hidden_stats** (Tensor) - the input tensor with shape [batch_size, tgt_seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * tgt_seq_length, hidden_size].</span>
<span class="sd">        - **decoder_mask** (Tensor) - the attention mask for decoder with shape [batch_size, src_seq_length,</span>
<span class="sd">          seq_length].</span>
<span class="sd">        - **encoder_output** (Tensor) - the output of the encoder with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size]. Note this args can not be passed by None when the net is in outermost</span>
<span class="sd">          layer. Default None.</span>
<span class="sd">        - **memory_mask** (Tensor) - the memory mask of the cross attention with shape [batch, tgt_seq_length,</span>
<span class="sd">          src_seq_length] where tgt_seq_length is the length of the decoder. Note this args can not be passed by</span>
<span class="sd">          None when the net is in outermost layer. Default None.</span>
<span class="sd">        - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">          past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">        - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used</span>
<span class="sd">          for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">        - **output** (Tensor) - the output logit of this layer. The shape is [batch, seq_length, hidden_size] or</span>
<span class="sd">          [batch * seq_length, hidden_size].</span>
<span class="sd">        - **layer_present** (Tensor) - A tuple, where each tuple is the tensor of the projected key and value</span>
<span class="sd">          vector in self attention with shape ((batch_size, num_heads, size_per_head, tgt_seq_length),</span>
<span class="sd">          (batch_size, num_heads, tgt_seq_length, size_per_head), and of the projected key and value vector</span>
<span class="sd">          in cross attention with shape  (batch_size, num_heads, size_per_head, src_seq_length),</span>
<span class="sd">          (batch_size, num_heads, src_seq_length, size_per_head)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import TransformerDecoderLayer</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = TransformerDecoderLayer(batch_size=2, hidden_size=64, ffn_hidden_size=64, num_heads=2,</span>
<span class="sd">        ...                                 src_seq_length=20, tgt_seq_length=10)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 20, 64)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; decoder_input_value = Tensor(np.ones((2, 10, 64)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; decoder_input_mask = Tensor(np.ones((2, 10, 10)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; memory_mask = Tensor(np.ones((2, 10, 20)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output, past = model(decoder_input_value, decoder_input_mask, encoder_input_value, memory_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 10, 64)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 2, 32, 10)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 2, 10, 32)</span>
<span class="sd">        &gt;&gt;&gt; print(past[2].shape)</span>
<span class="sd">        (2, 2, 32, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(past[3].shape)</span>
<span class="sd">        (2, 2, 20, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;num heads must be divisibled by the model parallel way </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but found </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;hidden_size must be divisibled by the model parallel way </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but found </span><span class="si">{</span><span class="n">hidden_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;ffn_hidden_size must be divisibled by the model parallel way </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but found </span><span class="si">{</span><span class="n">ffn_hidden_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_past</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support use_past=True.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_compute_type</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                            <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                            <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                            <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                            <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="c1"># Cross attention with the output of encoder as memory tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                  <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                  <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                  <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                  <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                  <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span>
            <span class="n">layernorm_compute_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                              <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                              <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                              <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                              <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Feed Forward Network, FFN</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                      <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                      <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                      <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># operator used for state reuse</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="n">size_per_head</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
            <span class="c1"># parameters saving key and value states</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_stats</span><span class="p">,</span>
                  <span class="n">decoder_mask</span><span class="p">,</span>
                  <span class="n">encoder_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="n">decoder_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, seq_length, embedding_size] or [bs * seq_length, embedding_size]</span>
        <span class="n">hidden_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">)</span>
        <span class="n">hidden_stats</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># indicate whether reset saved states</span>
        <span class="n">key_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">value_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># reset states, init_reset True for reuse and False for reset</span>
            <span class="n">key_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">value_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="n">attention</span><span class="p">,</span> <span class="n">layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">decoder_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># For post-layernorm the inputs for residual path are output of self-attention and output of layernorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># For pre-layernorm the inputs for residual path are output of self-attention and input of this layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>

        <span class="n">middle_output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">encoder_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">middle_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">middle_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">cross_attn_output</span><span class="p">,</span> <span class="n">cross_layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span>
                                                                          <span class="n">encoder_output</span><span class="p">,</span>
                                                                          <span class="n">memory_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span>
                                                                          <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">layer_present</span> <span class="o">+=</span> <span class="n">cross_layer_present</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="n">cross_attn_output</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cross_attn_output</span><span class="p">)</span>

        <span class="n">output_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">aux_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">mlp_logit</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>

        <span class="n">value_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">key_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># current key and value</span>
            <span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span> <span class="o">=</span> <span class="n">layer_present</span>
            <span class="c1"># update key and value calculated this step</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">key_present</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">key_update</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">value_update</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="c1"># add dependency for desired execution order</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">value_update</span><span class="p">)</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">key_update</span><span class="p">)</span>

        <span class="c1"># if shape is 3d, we reshape the inputs of the add</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">aux_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">):</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span>
                                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">),</span> <span class="s2">&quot;encoder_output&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span>
                                <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">),</span> <span class="s2">&quot;encoder_output&quot;</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">memory_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">memory_mask</span><span class="p">),</span> <span class="s2">&quot;memory_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">memory_mask</span><span class="p">),</span> <span class="s2">&quot;memory_mask&quot;</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">init_reset_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">init_reset_is_default</span> <span class="o">=</span> <span class="n">init_reset</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_reset_is_tensor</span><span class="p">,</span>
                                    <span class="n">init_reset_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span></div>


<span class="k">def</span> <span class="nf">_get_lambda_func</span><span class="p">(</span><span class="n">total_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A wrapper function of specifying pipeline stage and gradient aggregation fusion. If the total layer</span>
<span class="sd">        is not None, for example, set in the transformer model, the pipeline stage setting function will be</span>
<span class="sd">        `(layer_id + 0) // (total_layers / parallel_config.pipeline_stage)` for the encoder and,</span>
<span class="sd">        `(layer_id + offset) //</span>
<span class="sd">        (total_layers / parallel_config.pipeline_stage)` for the decoder, where `offset` is the layers in the encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_set_parallel_configure_for_layer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>

<span class="sd">            Args:</span>
<span class="sd">                network(Cell) - Represents the transformer block</span>
<span class="sd">                layer_id(int) - Means the layer index for the current module, counts from zero.</span>
<span class="sd">                offset(int) - Means the layer_index needs an offset, if there are other modules in the net.</span>
<span class="sd">                layers(int) - The total layers used for the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># override the layers</span>
        <span class="k">if</span> <span class="n">total_layer</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="n">total_layer</span>
        <span class="c1"># Used for the pipeline&#39;s stages setting</span>
        <span class="k">if</span> <span class="n">layers</span> <span class="o">&lt;</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layers </span><span class="si">{</span><span class="n">layers</span><span class="si">}</span><span class="s2"> must be larger than pipeline stage </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">pp_dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">layers</span> <span class="o">/</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># the pipeline stage must be in [0, parallel_config.pipeline_stage - 1]</span>
        <span class="n">pp_id</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_dis</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="n">pp_id</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;pipeline stage id is </span><span class="si">{</span><span class="n">pp_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Used for optimizer&#39;s fusion tag</span>
        <span class="n">dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">layers</span> <span class="o">/</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">gradient_aggregation_group</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">(</span><span class="nb">int</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">/</span> <span class="n">dis</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Used for enabling recomputation of the block</span>
        <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="p">:</span>
            <span class="n">network</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">_set_parallel_configure_for_layer</span>


<div class="viewcode-block" id="TransformerEncoder"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.TransformerEncoder">[docs]</a><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer Encoder module with multi-layer stacked of `TransformerEncoderLayer`, including multihead self</span>
<span class="sd">    attention and feedforward layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): The batch size of the input tensor.</span>
<span class="sd">        num_layers(int): The layers of the `TransformerEncoderLayer`</span>
<span class="sd">        hidden_size(int): The hidden size of the input.</span>
<span class="sd">        ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">        seq_length(int): The seq_length of the input tensor.</span>
<span class="sd">        num_heads(int): The number of the heads.</span>
<span class="sd">        hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1</span>
<span class="sd">        attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1</span>
<span class="sd">        post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">        hidden_act(str): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                         &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                         &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. Default: gelu.</span>
<span class="sd">        layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">        param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        use_past(bool): Use the past state to compute, used for incremental prediction. For example, if we have two</span>
<span class="sd">            words and want to generate the ten more words. We just need to compute the two words&#39;s state only once,</span>
<span class="sd">            and generate the next word one by one. When use_past is True, there are two steps to run the prediction.</span>
<span class="sd">            The first step, set the is_first_iteration to be True by</span>
<span class="sd">            `model.add_flags_recursive(is_first_iteration=True)`, and pass the full inputs. Then, set the</span>
<span class="sd">            is_first_iteration to be False by `model.add_flags_recursive(is_first_iteration=False)`. At this moment,</span>
<span class="sd">            pass the single step&#39;s input tensor, and loop it. Default False.</span>
<span class="sd">        lambda_func: A function can determine the fusion index, pipeline stages and recompute attribute. If the user</span>
<span class="sd">            wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function</span>
<span class="sd">            that accepts `network`, `layer_id`, `offset`, `parallel_config`, `layers`. The `network(Cell)`</span>
<span class="sd">            represents the transformer block, `layer_id(int)` means the layer index for the current module, counts from</span>
<span class="sd">            zero, `offset(int)` means the layer_index needs an offset, if there are other modules in the net. The</span>
<span class="sd">            default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>
<span class="sd">        offset(int): The initial layer index for the `decoder`. Used for setting the fusion id and stage id, to not</span>
<span class="sd">            overlap with the encoder layer.</span>
<span class="sd">        moe_config(MoEConfig): The configuration of MoE (Mixture of Expert).</span>
<span class="sd">        parallel_config(TransformerOpParallelConfig): The parallel configure. Default `default_transformer_config`,</span>
<span class="sd">                                           an instance of `TransformerOpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **hidden_states** (Tensor) - Tensor, shape should be [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">          should be [batch_size, 1, hidden_size].</span>
<span class="sd">        - **attention_mask** (Tensor) - Tensor, attention mask with shape [batch_size, seq_length, seq_length]</span>
<span class="sd">        - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">          past value parameter used in the incremental prediction. Only valid when use_past is True. Default True</span>
<span class="sd">        - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used</span>
<span class="sd">          for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">        - **output** (Tensor) - The float tensor of the output of the layer with</span>
<span class="sd">          shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is</span>
<span class="sd">          False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</span>
<span class="sd">        - **layer_present** (Tuple) - A tuple with size of num_layers, where each tuple contains the Tensor the</span>
<span class="sd">          projected key and value vector with shape ((batch_size, num_heads, size_per_head, seq_length),</span>
<span class="sd">          and (batch_size, num_heads, seq_length, size_per_head)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import TransformerEncoder</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = TransformerEncoder(batch_size=2, num_layers=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">        ...                            num_heads=2)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 16, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 16, 16)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output, past = model(encoder_input_value, encoder_input_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 16, 8)</span>
<span class="sd">        &gt;&gt;&gt; print(len(past))</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; print(past[0][0].shape)</span>
<span class="sd">        (2, 2, 4, 16)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0][1].shape)</span>
<span class="sd">        (2, 2, 16, 4)</span>
<span class="sd">        # When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="sd">        # Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="sd">        &gt;&gt;&gt; batch_valid_length = Tensor(np.ones((2,)), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; init_reset = Tensor([True], mstype.bool_)</span>
<span class="sd">        # Set is_first_iteration=True to generate the full memory states</span>
<span class="sd">        &gt;&gt;&gt; model = TransformerEncoder(batch_size=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">        ...                            num_heads=2, num_layers=2, use_past=True)</span>
<span class="sd">        &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=True)</span>
<span class="sd">        &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">        &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">        (2, 16, 8)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 2, 4, 16)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 2, 16, 4)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 1, 8)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 1, 16)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; init_reset = Tensor([False], mstype.bool_)</span>
<span class="sd">        # Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the full</span>
<span class="sd">        # sequence.</span>
<span class="sd">        &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=False)</span>
<span class="sd">        &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">        &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">        (2, 1, 8)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">        (2, 2, 4, 16)</span>
<span class="sd">        &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">        (2, 2, 16, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">offset</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">TransformerOpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support auto parallel mode now.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                            <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                            <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                            <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                            <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                            <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                            <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                            <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                            <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span><span class="p">)</span>
            <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

            <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                        <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">present_layer</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                  <span class="n">attention_mask</span><span class="p">,</span>
                                                                  <span class="n">init_reset</span><span class="p">,</span>
                                                                  <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span><span class="p">,</span> <span class="n">accum_loss</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                    <span class="n">attention_mask</span><span class="p">,</span>
                                                    <span class="n">init_reset</span><span class="p">,</span>
                                                    <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span></div>


<div class="viewcode-block" id="TransformerDecoder"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.TransformerDecoder">[docs]</a><span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer Decoder module with multi-layer stacked of `TransformerDecoderLayer`, including multihead self</span>
<span class="sd">    attention, cross attention and feedforward layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): The batch size of the input tensor.</span>
<span class="sd">        num_layers(int): The layers of the `TransformerDecoderLayer`.</span>
<span class="sd">        hidden_size(int): The hidden size of the input.</span>
<span class="sd">        ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">        src_seq_length(int): The input source sequence length.</span>
<span class="sd">        tgt_seq_length(int): The input target sequence length.</span>
<span class="sd">        num_heads(int): The number of the heads.</span>
<span class="sd">        hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">        attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">        post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">        hidden_act(str): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                         &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                         &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. Default: gelu.</span>
<span class="sd">        layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">        param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        offset(int): The initial layer index for the `decoder`. Used for setting the fusion id and stage id, to not</span>
<span class="sd">            overlap with the encoder layer.</span>
<span class="sd">        lambda_func: A function can determine the fusion index, pipeline stages and recompute attribute. If the user</span>
<span class="sd">            wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function</span>
<span class="sd">            that accepts `network`, `layer_id`, `offset`, `parallel_config`, `layers`. The `network(Cell)`</span>
<span class="sd">            represents the transformer block, `layer_id(int)` means the layer index for the current module, counts from</span>
<span class="sd">            zero, `offset(int)` means the layer_index needs an offset, if there are other modules in the net. The</span>
<span class="sd">            default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>
<span class="sd">            Default: None</span>
<span class="sd">        moe_config(MoEConfig): The configuration of MoE (Mixture of Expert).</span>
<span class="sd">        parallel_config(TransformerOpParallelConfig): The parallel configure. Default `default_transformer_config`,</span>
<span class="sd">                                           an instance of `TransformerOpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **hidden_stats** (Tensor) - the input tensor with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size]</span>
<span class="sd">        - **attention_mask** (Tensor) - the attention mask for decoder with shape [batch_size, seq_length, seq_length]</span>
<span class="sd">        - **encoder_output** (Tensor) - the output of the encoder with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size]. Note this args can not be passed by None when the net is in outermost</span>
<span class="sd">          layer. Default None.</span>
<span class="sd">        - **memory_mask** (Tensor) - the memory mask of the cross attention with shape [batch, tgt_seq_length,</span>
<span class="sd">          src_seq_length] where tgt_seq_length is the length of the decoder. Note this args can not be passed by</span>
<span class="sd">          None when the net is in outermost layer. Default None.</span>
<span class="sd">        - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">          past value parameter used in the incremental prediction. Only valid when use_past is True. Default True</span>
<span class="sd">        - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.</span>
<span class="sd">          Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">        - **output** (Tensor) - The output logit of this layer. The shape is [batch, tgt_seq_length, hidden_size] or</span>
<span class="sd">          [batch * tgt_seq_length, hidden_size]</span>
<span class="sd">        - **layer_present** (Tuple) - A tuple with size of num_layers, where each tuple is the tensor of the projected</span>
<span class="sd">          key and value vector in self attention with shape ((batch_size, num_heads, size_per_head, tgt_seq_length),</span>
<span class="sd">          (batch_size, num_heads, tgt_seq_length, size_per_head), and of the projected key and value vector</span>
<span class="sd">          in cross attention with shape  (batch_size, num_heads, size_per_head, src_seq_length),</span>
<span class="sd">          (batch_size, num_heads, src_seq_length, size_per_head)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import TransformerDecoder</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = TransformerDecoder(batch_size=2, num_layers=1, hidden_size=64, ffn_hidden_size=64,</span>
<span class="sd">        ...                            num_heads=2, src_seq_length=20, tgt_seq_length=10)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 20, 64)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; decoder_input_value = Tensor(np.ones((2, 10, 64)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; decoder_input_mask = Tensor(np.ones((2, 10, 10)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; memory_mask = Tensor(np.ones((2, 10, 20)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output, past = model(decoder_input_value, decoder_input_mask, encoder_input_value, memory_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 10, 64)</span>
<span class="sd">        &gt;&gt;&gt; print(len(past))</span>
<span class="sd">        1</span>
<span class="sd">        &gt;&gt;&gt; print(past[0][0].shape)</span>
<span class="sd">        (2, 2, 32, 10)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0][1].shape)</span>
<span class="sd">        (2, 2, 10, 32)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0][2].shape)</span>
<span class="sd">        (2, 2, 32, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(past[0][3].shape)</span>
<span class="sd">        (2, 2, 20, 32)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">offset</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">TransformerOpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support auto parallel mode now.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                            <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                            <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                            <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                            <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                            <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                            <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                            <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                            <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                            <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                            <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                            <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                            <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                            <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span><span class="p">)</span>
            <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

            <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                        <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">present_layer</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                  <span class="n">attention_mask</span><span class="p">,</span>
                                                                  <span class="n">encoder_output</span><span class="p">,</span>
                                                                  <span class="n">memory_mask</span><span class="p">,</span>
                                                                  <span class="n">init_reset</span><span class="p">,</span>
                                                                  <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span><span class="p">,</span> <span class="n">accum_loss</span>

        <span class="c1"># Loop through each self-attention layer</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                    <span class="n">attention_mask</span><span class="p">,</span>
                                                    <span class="n">encoder_output</span><span class="p">,</span>
                                                    <span class="n">memory_mask</span><span class="p">,</span>
                                                    <span class="n">init_reset</span><span class="p">,</span>
                                                    <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span></div>


<div class="viewcode-block" id="Transformer"><a class="viewcode-back" href="../../../../api_python/mindspore.parallel.nn.html#mindspore.parallel.nn.Transformer">[docs]</a><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transformer module including encoder and decoder. The difference with the original implements is the module use</span>
<span class="sd">    the residual addition before the layer normalization. And the default hidden act is `gelu`.</span>
<span class="sd">    The details can be found in `Attention is all you need &lt;https://arxiv.org/pdf/1706.03762v5.pdf&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        This is an experimental interface that is subject to change and/or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_size(int): The batch size of the input tensor.</span>
<span class="sd">        encoder_layers(int): The layers of the `TransformerEncoderLayer`.</span>
<span class="sd">        decoder_layers(int): The layers of the `TransformerDecoderLayer`.</span>
<span class="sd">        hidden_size(int): The hidden size of the input.</span>
<span class="sd">        ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">        src_seq_length(int): The seq_length of the encoder&#39;s input tensor.</span>
<span class="sd">        tgt_seq_length(int): The seq_length of the decoder&#39;s input tensor.</span>
<span class="sd">        num_heads(int): The number of the heads. Default: 2.</span>
<span class="sd">        hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1</span>
<span class="sd">        attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1</span>
<span class="sd">        post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">        layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">        param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">            Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">        hidden_act(str): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                         &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                         &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. Default: gelu.</span>
<span class="sd">        moe_config(MoEConfig): The configuration of MoE (Mixture of Expert).</span>
<span class="sd">        lambda_func: A function can determine the fusion index, pipeline stages and recompute attribute. If the user</span>
<span class="sd">            wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function</span>
<span class="sd">            that accepts `network`, `layer_id`, `offset`, `parallel_config`, `layers`. The `network(Cell)`</span>
<span class="sd">            represents the transformer block, `layer_id(int)` means the layer index for the current module, counts from</span>
<span class="sd">            zero, `offset(int)` means the layer_index needs an offset, if there are other modules in the net. The</span>
<span class="sd">            default setting for the pipeline is: `(layer_id + offset) // ((encoder_layers + decoder_length)</span>
<span class="sd">            / pipeline_stage)`.</span>
<span class="sd">        parallel_config(TransformerOpParallelConfig): The parallel configure. Default `default_transformer_config`,</span>
<span class="sd">                                           an instance of `TransformerOpParallelConfig` with default args.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **encoder_inputs** (Tensor) - the input tensor with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size].</span>
<span class="sd">        - **encoder_masks** (Tensor) - the attention mask for decoder with shape [batch_size, seq_length, seq_length].</span>
<span class="sd">        - **decoder_inputs** (Tensor) - the output of the encoder with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">          [batch_size * seq_length, hidden_size],</span>
<span class="sd">          this should be none if the decoder layer is 0.</span>
<span class="sd">        - **decoder_masks** (Tensor) - the attention mask for decoder with shape [batch_size, seq_length, seq_length]</span>
<span class="sd">        - **memory_mask** (Tensor) - the memory mask of the cross attention with shape [batch, tgt_seq_length,</span>
<span class="sd">          src_seq_length]</span>
<span class="sd">          where tgt_seq_length is the length of the decoder. the output of the encoder with shape [batch_size,</span>
<span class="sd">          seq_length, hidden_size], this should be none if the decoder layer is 0.</span>
<span class="sd">        - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">          past value parameter used in the incremental prediction. Only valid when use_past is True. Default True</span>
<span class="sd">        - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used</span>
<span class="sd">          for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains(`output`, `encoder_layer_present`, `encoder_layer_present`)</span>

<span class="sd">        - **output** (Tensor) - If there is only encoder, the output logit of the encoder layer. The shape is</span>
<span class="sd">          [batch, src_seq_length, hidden_size] or [batch * src_seq_length, hidden_size], if there are encoder and</span>
<span class="sd">          decoders, the output is from the decoder layer. The shape is [batch, tgt_seq_length, hidden_size] or</span>
<span class="sd">          [batch * tgt_seq_length, hidden_size].</span>
<span class="sd">        - **encoder_layer_present** (Tuple) - A tuple with size of num_layers, where each tuple is the tensor the</span>
<span class="sd">          projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,</span>
<span class="sd">          src_seq_length), (batch_size, num_heads, src_seq_length, size_per_head)).</span>
<span class="sd">        - **decoder_layer_present** (Tuple) - A tuple with size of num_layers, where each tuple is the tensor</span>
<span class="sd">          of the projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,</span>
<span class="sd">          tgt_seq_length), (batch_size, num_heads, tgt_seq_length, size_per_head)), and the</span>
<span class="sd">          projected key and value vector in cross attention with shape</span>
<span class="sd">          (batch_size, num_heads, size_per_head, src_seq_length),</span>
<span class="sd">          (batch_size, num_heads, src_seq_length, size_per_head)). If the decoder is not set, the</span>
<span class="sd">          returned value will be None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.parallel.nn import Transformer</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; model = Transformer(batch_size=2, encoder_layers=1, decoder_layers=2, hidden_size=64, ffn_hidden_size=64,</span>
<span class="sd">        ...         src_seq_length=20, tgt_seq_length=10)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 20, 64)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 20, 20)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; decoder_input_value = Tensor(np.ones((2, 10, 64)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; decoder_input_mask = Tensor(np.ones((2, 10, 10)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; memory_mask = Tensor(np.ones((2, 10, 20)), mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output, en_past, de_past = model(encoder_input_value, encoder_input_mask, decoder_input_value,</span>
<span class="sd">        ...                                  decoder_input_mask, memory_mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 10, 64)</span>
<span class="sd">        &gt;&gt;&gt; print(len(en_past))</span>
<span class="sd">        1</span>
<span class="sd">        &gt;&gt;&gt; print(len(de_past))</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; print(en_past[0][0].shape)</span>
<span class="sd">        (2, 2, 32, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(en_past[0][1].shape)</span>
<span class="sd">        (2, 2, 20, 32)</span>
<span class="sd">        &gt;&gt;&gt; print(de_past[0][0].shape)</span>
<span class="sd">        (2, 2, 32, 10)</span>
<span class="sd">        &gt;&gt;&gt; print(de_past[0][1].shape)</span>
<span class="sd">        (2, 2, 10, 32)</span>
<span class="sd">        &gt;&gt;&gt; print(de_past[0][2].shape)</span>
<span class="sd">        (2, 2, 32, 20)</span>
<span class="sd">        &gt;&gt;&gt; print(de_past[0][3].shape)</span>
<span class="sd">        (2, 2, 20, 32)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">encoder_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">decoder_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="nb">str</span><span class="p">],</span> <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">TransformerOpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
        <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">decoder_layers</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformer doest support encoder layer </span><span class="si">{</span><span class="n">encoder_layers</span><span class="si">}</span><span class="s2"> and decoder&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;layer </span><span class="si">{</span><span class="n">decoder_layers</span><span class="si">}</span><span class="s2">, please use TransformerDecoder&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">decoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">use_past</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> with encoder and decoder does not support use_past=True.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support auto parallel mode now.&quot;</span><span class="p">)</span>
        <span class="c1"># The shard setting of Transformer is set within the TransformerEncoderLayer</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
            <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">(</span><span class="n">total_layer</span><span class="o">=</span><span class="n">encoder_layers</span> <span class="o">+</span> <span class="n">decoder_layers</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                              <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                              <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                              <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                              <span class="n">seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                              <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                              <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                              <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                              <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                              <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                              <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                              <span class="n">lambda_func</span><span class="o">=</span><span class="n">lambda_func</span><span class="p">,</span>
                                              <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                              <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Offset is needed as the encoder has consumed some flags.</span>
        <span class="c1"># so the decoder need to increase the flags based on the encoder layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">decoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">decoder_layers</span><span class="p">,</span>
                                              <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                              <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                              <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                              <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                              <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                              <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                              <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                              <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                              <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                              <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                              <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                              <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                              <span class="n">lambda_func</span><span class="o">=</span><span class="n">lambda_func</span><span class="p">,</span>
                                              <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                              <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                              <span class="n">offset</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
                                              <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                              <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_inputs</span><span class="p">,</span>
                  <span class="n">encoder_masks</span><span class="p">,</span>
                  <span class="n">decoder_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">decoder_masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="n">encoder_output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">encoder_layer_present</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">decoder_layer_present</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_layer_present</span><span class="p">,</span> <span class="n">encoder_aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_masks</span><span class="p">,</span>
                                                                                       <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">encoder_aux_loss</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_masks</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span>
                                                                     <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">encoder_output</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># decoder mask should be created outside of the model</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
                <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_layer_present</span><span class="p">,</span> <span class="n">decoder_aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">decoder_masks</span><span class="p">,</span>
                                                                                       <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span>
                                                                                       <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">decoder_aux_loss</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span>
                                                                     <span class="n">decoder_masks</span><span class="p">,</span>
                                                                     <span class="n">encoder_output</span><span class="p">,</span>
                                                                     <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span>
                                                                     <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">decoder_output</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">encoder_layer_present</span><span class="p">,</span> <span class="n">decoder_layer_present</span><span class="p">,</span> <span class="n">accum_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">encoder_layer_present</span><span class="p">,</span> <span class="n">decoder_layer_present</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>