

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.parallel.nn &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore.profiler" href="mindspore.profiler.html" />
    <link rel="prev" title="mindspore.parallel" href="mindspore.parallel.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">MindSpore Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.compression.html">mindspore.compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.context.html">mindspore.context</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.parallel.html">mindspore.parallel</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.parallel.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.profiler.html">mindspore.profiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.boost.html">mindspore.boost (experimental)</a></li>
</ul>
<p class="caption"><span class="caption-text">MindSpore C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.5/api_cpp/mindspore.html">MindSpore Lite↗</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>mindspore.parallel.nn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/api_python/mindspore.parallel.nn.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-mindspore.parallel.nn">
<span id="mindspore-parallel-nn"></span><h1>mindspore.parallel.nn<a class="headerlink" href="#module-mindspore.parallel.nn" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Transformer Networks
This is an experimental interface that is subject to change and/or deletion.</p>
</div>
<dl class="class">
<dt id="mindspore.parallel.nn.AttentionMask">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">AttentionMask</code><span class="sig-paren">(</span><em class="sig-param">seq_length</em>, <em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#AttentionMask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.AttentionMask" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Lower triangular matrix from the input mask. The input mask is a 2D tensor (batch_size, seq_length)
with 1 and 0. 1 indicates the current position is a valid token, otherwise not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The sequence length of the input tensor.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.OpParallelConfig" title="mindspore.parallel.nn.OpParallelConfig"><em>OpParallelConfig</em></a>) – The parallel configure. Default <cite>default_dpmp_config</cite>,
an instance of <cite>OpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>input_mask</strong> (Tensor) - The mask indicating whether each position is a valid input with
(batch_size, seq_length).</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. The attention mask matrix with shape (batch_size, seq_length, seq_length).</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>seq_length</cite> is not an integer.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>seq_length</cite> is not a positive value.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>parallel_config</cite> is not a subclass of OpParallelConfig.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">AttentionMask</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">AttentionMask</span><span class="p">(</span><span class="n">seq_length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mask_array</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="go">[[[1. 0. 0. 0],</span>
<span class="go">  [1. 1. 0. 0],</span>
<span class="go">  [1. 1. 1. 0],</span>
<span class="go">  [0. 0. 0. 0]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.VocabEmbedding">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">VocabEmbedding</code><span class="sig-paren">(</span><em class="sig-param">vocab_size</em>, <em class="sig-param">embedding_size</em>, <em class="sig-param">parallel_config=default_embedding_parallel_config</em>, <em class="sig-param">param_init=&quot;normal&quot;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#VocabEmbedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.VocabEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>The embedding lookup table from the 0-th dim of the parameter table. When the parallel_config.vocab_emb_dp is
True and in the <cite>AUTO_PARALLEL_MODE</cite>, the embedding lookup will be a <cite>parallel_config.data_parallel</cite>
data parallel way, or will shard the parameter at the 0-th dimension in <cite>parallel_config.model_parallel</cite>, so-called
row slice of the embedding table.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vocab_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Size of the dictionary of embeddings.</p></li>
<li><p><strong>embedding_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The size of each embedding vector.</p></li>
<li><p><strong>param_init</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, </em><a class="reference internal" href="mindspore.common.initializer.html#mindspore.common.initializer.Initializer" title="mindspore.common.initializer.Initializer"><em>Initializer</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a><em>]</em>) – Initializer for the embedding_table.
Refer to class <cite>initializer</cite> for the values of string when a string
is specified. Default: ‘normal’.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.EmbeddingOpParallelConfig" title="mindspore.parallel.nn.EmbeddingOpParallelConfig"><em>EmbeddingOpParallelConfig</em></a>) – The parallel config of network. Default
<cite>default_embedding_parallel_config</cite>, an instance of <cite>EmbeddingOpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><p><strong>input_ids</strong> (Tensor) - The tokenized inputs with datatype int32 with shape (batch_size, seq_length)</p>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains (<cite>output</cite>, <cite>embedding_table</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - The embedding vector for the input with shape (batch_size,
seq_length, embedding_size).</p></li>
<li><p><strong>weight</strong> (Tensor) - The embedding table with shape (vocab_size, embedding_size).</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the parallel_config.vocab_emb_dp is True, the vocab size is not a multiple of
    parallel_config.model_parallel</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>vocab_size</cite> is not a positive value.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>embedding_size</cite> is not a positive value.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>parallel_config</cite> is not a subclass of OpParallelConfig.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">VocabEmbedding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">VocabEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">embedding_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">table</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(20, 15, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(30, 30)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.MultiHeadAttention">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">MultiHeadAttention</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">src_seq_length</em>, <em class="sig-param">tgt_seq_length</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">hidden_dropout_rate=0.1</em>, <em class="sig-param">attention_dropout_rate=0.1</em>, <em class="sig-param">compute_dtype=mstype.float16</em>, <em class="sig-param">softmax_compute_type=mstype.float32</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">use_past=False</em>, <em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#MultiHeadAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.MultiHeadAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>This is an implementation of multihead attention in the paper <a class="reference external" href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention is all you need</a>. Given the query vector with source length, and the
key and value vector with target length, the attention will be performed as the following</p>
<div class="math notranslate nohighlight">
\[MultiHeadAttention(query, key, vector) = Concat(head_1, \dots, head_h)W^O\]</div>
<p>where <span class="math notranslate nohighlight">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span>. The default is with a bias.</p>
<p>if query, key and value tensor is same, then it will be self attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor.</p></li>
<li><p><strong>src_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The sequence length of the query vector.</p></li>
<li><p><strong>tgt_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The sequence length of the key and value vector.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1</p></li>
<li><p><strong>compute_dtype</strong> (<em>dtype.Number</em>) – The computation type of dense. Default dtype.float16.
Should be dtype.float32 or dtype.float16.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module. Default dtype.float32.
Should be dtype.float32 or dtype.float16.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The type of softmax computation module. Default dtype.float32.
Should be dtype.float32 or dtype.float16.</p></li>
<li><p><strong>use_past</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use the past state to compute, used for incremental prediction. For example, if we have two
words and want to generate the ten more words. We just need to compute the two words’s state only once,
and generate the next word one by one. When use_past is True, there are two steps to run the prediction.
The first step, set the is_first_iteration to be True by
<cite>model.add_flags_recursive(is_first_iteration=True)</cite>, and pass the full inputs. Then, set the
is_first_iteration to be False by <cite>model.add_flags_recursive(is_first_iteration=False)</cite>. At this moment,
pass the single step’s input tensor, and loop it. Default False.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.OpParallelConfig" title="mindspore.parallel.nn.OpParallelConfig"><em>OpParallelConfig</em></a>) – The parallel configure. Default <cite>default_dpmp_config</cite>,
an instance of <cite>OpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>query_tensor</strong> (Tensor) - the query vector with shape (batch_size, src_seq_length, hidden_size) or
(batch_size * src_seq_length, hidden_size), if the use_past is False or is_first_iteration=True. Otherwise,
must be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>key_tensor</strong> (Tensor) - the key vector with shape (batch_size, tgt_seq_length, hidden_size) or
(batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True. Otherwise,
must be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>value_tensor</strong> (Tensor) - the value vector with shape (batch_size, tgt_seq_length, hidden_size) or
(batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True. Otherwise,
must be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - the attention mask matrix with shape (batch_size, src_seq_length,
tgt_seq_length), if the use_past is False or is_first_iteration=True. Otherwise,
must be (batch_size, 1, tgt_seq_length)</p></li>
<li><p><strong>key_past</strong> (Tensor) - Float16 tensor with shape (batch_size, num_heads, size_per_head, tgt_seq_length).
The past calculated key vector. Used for incremental prediction when the use_past is True.
Default None.</p></li>
<li><p><strong>value_past</strong> (Tensor) - Float16 tensor with shape (batch_size, num_heads, tgt_seq_length, size_per_head).
The past calculated value vector. Used for incremental prediction when the use_past is True.
Default None.</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape (batch_size,) the past calculated the index.
Used for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - Tensor, the float tensor of the output of the layer with
shape (batch_size, src_seq_length, hidden_size) or (batch_size * src_seq_length, hidden_size),
if the use_past is False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</p></li>
<li><p><strong>layer_present</strong> (Tuple) - A tuple of the Tensor of the projected key and value vector with
((batch_size, num_heads, size_per_head, tgt_seq_length),
(batch_size, num_heads, tgt_seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">MultiHeadAttention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">from_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
<span class="go"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="go"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="go"># We need to prepare the memory parameters for saving key and value states firstly.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="go"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span> <span class="n">value_past</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">from_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="go"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the full</span>
<span class="go"># sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span> <span class="n">value_past</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.FeedForward">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">FeedForward</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">ffn_hidden_size</em>, <em class="sig-param">dropout_rate</em>, <em class="sig-param">hidden_act=&quot;gelu&quot;</em>, <em class="sig-param">expert_num=1</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#FeedForward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.FeedForward" title="Permalink to this definition">¶</a></dt>
<dd><p>The multilayer perceptron with two linear layers with dropout applied at final output. The first linear
will project the input dimension from hidden_size to ffn_hidden_size, the second linear will project the
dimension from ffn_hidden_size to hidden_size. The first linear is sharded on the relative dimension,
the second linear is sharded on the output dimension. The overview process can be</p>
<div class="math notranslate nohighlight">
\[Dropout((xW_1+b_1)W_2 + b_2))\]</div>
<p>where the <span class="math notranslate nohighlight">\(W_1, W_2, b_1\)</span> and <span class="math notranslate nohighlight">\(b_2\)</span> are trainable parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The dimension of the inputs.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The intermediate hidden size.</p></li>
<li><p><strong>dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate for the second linear’s output.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. Default: gelu.</p></li>
<li><p><strong>expert_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of experts used in Linear. For the case expert_num &gt; 1, BatchMatMul is used
and the first dimension in BatchMatMul indicate expert_num. Default: 1.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type. Should be dtype.float32 or dtype.float16.
Default: dtype.float32.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.OpParallelConfig" title="mindspore.parallel.nn.OpParallelConfig"><em>OpParallelConfig</em></a>) – The config of parallel setting, see <cite>OpParallelConfig</cite>.
Default <cite>default_dpmp_config</cite>, an instance of <cite>OpParallelConfig</cite> with
default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - should be <cite>[batch, seq_length, hidden_size] or [batch * seq_length, hidden_size]</cite>.
Float tensor.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, the output of this layer after mapping. The shape is <cite>[batch, seq_length, hidden_size]
or [batch * seq_length, hidden_size]</cite>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>hidden_act</cite> is not a string.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>parallel_config</cite> is not a subclass of OpParallelConfig.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>ffn_hidden_size</cite> is not a multiple of the model parallel way.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>hidden_size</cite> is not a multiple of the model parallel way.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">FeedForward</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.TransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">TransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">num_layers</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">ffn_hidden_size</em>, <em class="sig-param">seq_length</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">attention_dropout_rate=0.1</em>, <em class="sig-param">hidden_dropout_rate=0.1</em>, <em class="sig-param">hidden_act=&quot;gelu&quot;</em>, <em class="sig-param">post_layernorm_residual=False</em>, <em class="sig-param">layernorm_compute_type=mstype.float32</em>, <em class="sig-param">softmax_compute_type=mstype.float32</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">lambda_func=None</em>, <em class="sig-param">offset=0</em>, <em class="sig-param">use_past=False</em>, <em class="sig-param">moe_config=default_moe_config</em>, <em class="sig-param">parallel_config=default_transformer_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#TransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.TransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Encoder module with multi-layer stacked of <cite>TransformerEncoderLayer</cite>, including multihead self
attention and feedforward layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The layers of the <cite>TransformerEncoderLayer</cite></p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of bottleneck in the feedforward layer.</p></li>
<li><p><strong>seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The seq_length of the input tensor.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Do residuals adds before the layernorm. Default False.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. Default: gelu.</p></li>
<li><p><strong>layernorm_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the layernorm.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the softmax in the attention.
Should be dtype.float32 or dtype.float16. Default mstype.float32.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>use_past</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use the past state to compute, used for incremental prediction. For example, if we have two
words and want to generate the ten more words. We just need to compute the two words’s state only once,
and generate the next word one by one. When use_past is True, there are two steps to run the prediction.
The first step, set the is_first_iteration to be True by
<cite>model.add_flags_recursive(is_first_iteration=True)</cite>, and pass the full inputs. Then, set the
is_first_iteration to be False by <cite>model.add_flags_recursive(is_first_iteration=False)</cite>. At this moment,
pass the single step’s input tensor, and loop it. Default False.</p></li>
<li><p><strong>lambda_func</strong> – A function can determine the fusion index, pipeline stages and recompute attribute. If the user
wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function
that accepts <cite>network</cite>, <cite>layer_id</cite>, <cite>offset</cite>, <cite>parallel_config</cite>, <cite>layers</cite>. The <cite>network(Cell)</cite>
represents the transformer block, <cite>layer_id(int)</cite> means the layer index for the current module, counts from
zero, <cite>offset(int)</cite> means the layer_index needs an offset, if there are other modules in the net. The
default setting for the pipeline is: <cite>(layer_id + offset) // (layers / pipeline_stage)</cite>.</p></li>
<li><p><strong>offset</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The initial layer index for the <cite>decoder</cite>. Used for setting the fusion id and stage id, to not
overlap with the encoder layer.</p></li>
<li><p><strong>moe_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.MoEConfig" title="mindspore.parallel.nn.MoEConfig"><em>MoEConfig</em></a>) – The configuration of MoE (Mixture of Expert).</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.TransformerOpParallelConfig" title="mindspore.parallel.nn.TransformerOpParallelConfig"><em>TransformerOpParallelConfig</em></a>) – The parallel configure. Default <cite>default_transformer_config</cite>,
an instance of <cite>TransformerOpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>hidden_states</strong> (Tensor) - Tensor, shape should be [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,
should be [batch_size, 1, hidden_size].</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - Tensor, attention mask with shape [batch_size, seq_length, seq_length]</p></li>
<li><p><strong>init_reset</strong> (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
past value parameter used in the incremental prediction. Only valid when use_past is True. Default True</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used
for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - The float tensor of the output of the layer with
shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is
False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</p></li>
<li><p><strong>layer_present</strong> (Tuple) - A tuple with size of num_layers, where each tuple contains the Tensor the
projected key and value vector with shape ((batch_size, num_heads, size_per_head, seq_length),
and (batch_size, num_heads, seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">TransformerEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">past</span><span class="p">))</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="go"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="go"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="go"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="go"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the full</span>
<span class="go"># sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.TransformerDecoder">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">TransformerDecoder</code><span class="sig-paren">(</span><em class="sig-param">num_layers</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">ffn_hidden_size</em>, <em class="sig-param">src_seq_length</em>, <em class="sig-param">tgt_seq_length</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">attention_dropout_rate=0.1</em>, <em class="sig-param">hidden_dropout_rate=0.1</em>, <em class="sig-param">post_layernorm_residual=False</em>, <em class="sig-param">layernorm_compute_type=mstype.float32</em>, <em class="sig-param">softmax_compute_type=mstype.float32</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">hidden_act=&quot;gelu&quot;</em>, <em class="sig-param">lambda_func=None</em>, <em class="sig-param">use_past=False</em>, <em class="sig-param">offset=0</em>, <em class="sig-param">moe_config=default_moe_config</em>, <em class="sig-param">parallel_config=default_transformer_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#TransformerDecoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.TransformerDecoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Decoder module with multi-layer stacked of <cite>TransformerDecoderLayer</cite>, including multihead self
attention, cross attention and feedforward layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The layers of the <cite>TransformerDecoderLayer</cite>.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of bottleneck in the feedforward layer.</p></li>
<li><p><strong>src_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The input source sequence length.</p></li>
<li><p><strong>tgt_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The input target sequence length.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1.</p></li>
<li><p><strong>post_layernorm_residual</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Do residuals adds before the layernorm. Default False.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. Default: gelu.</p></li>
<li><p><strong>layernorm_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the layernorm.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the softmax in the attention.
Should be dtype.float32 or dtype.float16. Default mstype.float32.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>offset</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The initial layer index for the <cite>decoder</cite>. Used for setting the fusion id and stage id, to not
overlap with the encoder layer.</p></li>
<li><p><strong>lambda_func</strong> – A function can determine the fusion index, pipeline stages and recompute attribute. If the user
wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function
that accepts <cite>network</cite>, <cite>layer_id</cite>, <cite>offset</cite>, <cite>parallel_config</cite>, <cite>layers</cite>. The <cite>network(Cell)</cite>
represents the transformer block, <cite>layer_id(int)</cite> means the layer index for the current module, counts from
zero, <cite>offset(int)</cite> means the layer_index needs an offset, if there are other modules in the net. The
default setting for the pipeline is: <cite>(layer_id + offset) // (layers / pipeline_stage)</cite>.
Default: None</p></li>
<li><p><strong>moe_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.MoEConfig" title="mindspore.parallel.nn.MoEConfig"><em>MoEConfig</em></a>) – The configuration of MoE (Mixture of Expert).</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.TransformerOpParallelConfig" title="mindspore.parallel.nn.TransformerOpParallelConfig"><em>TransformerOpParallelConfig</em></a>) – The parallel configure. Default <cite>default_transformer_config</cite>,
an instance of <cite>TransformerOpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>hidden_stats</strong> (Tensor) - the input tensor with shape [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size]</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - the attention mask for decoder with shape [batch_size, seq_length, seq_length]</p></li>
<li><p><strong>encoder_output</strong> (Tensor) - the output of the encoder with shape [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size]. Note this args can not be passed by None when the net is in outermost
layer. Default None.</p></li>
<li><p><strong>memory_mask</strong> (Tensor) - the memory mask of the cross attention with shape [batch, tgt_seq_length,
src_seq_length] where tgt_seq_length is the length of the decoder. Note this args can not be passed by
None when the net is in outermost layer. Default None.</p></li>
<li><p><strong>init_reset</strong> (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
past value parameter used in the incremental prediction. Only valid when use_past is True. Default True</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.
Used for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - The output logit of this layer. The shape is [batch, tgt_seq_length, hidden_size] or
[batch * tgt_seq_length, hidden_size]</p></li>
<li><p><strong>layer_present</strong> (Tuple) - A tuple with size of num_layers, where each tuple is the tensor of the projected
key and value vector in self attention with shape ((batch_size, num_heads, size_per_head, tgt_seq_length),
(batch_size, num_heads, tgt_seq_length, size_per_head), and of the projected key and value vector
in cross attention with shape  (batch_size, num_heads, size_per_head, src_seq_length),
(batch_size, num_heads, src_seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">TransformerDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">memory_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">decoder_input_value</span><span class="p">,</span> <span class="n">decoder_input_mask</span><span class="p">,</span> <span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 10, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">past</span><span class="p">))</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 10, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.TransformerEncoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">TransformerEncoderLayer</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">hidden_size</em>, <em class="sig-param">ffn_hidden_size</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">seq_length</em>, <em class="sig-param">attention_dropout_rate=0.1</em>, <em class="sig-param">hidden_dropout_rate=0.1</em>, <em class="sig-param">post_layernorm_residual=False</em>, <em class="sig-param">layernorm_compute_type=mstype.float32</em>, <em class="sig-param">softmax_compute_type=mstype.float32</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">hidden_act=&quot;gelu&quot;</em>, <em class="sig-param">use_past=False</em>, <em class="sig-param">moe_config=default_moe_config</em>, <em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#TransformerEncoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.TransformerEncoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Encoder Layer. This is an implementation of the single layer of the transformer
encoder layer, including multihead attention and feedward layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The input sequence length.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of bottleneck in the feedforward layer.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Do residuals adds before the layernorm. Default False.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. Default: gelu.</p></li>
<li><p><strong>layernorm_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the layernorm.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the softmax in the attention.
Should be dtype.float32 or dtype.float16. Default mstype.float32.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>use_past</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use the past state to compute, used for incremental prediction. For example, if we have two
words and want to generate the ten more words. We just need to compute the two words’s state only once,
and generate the next word one by one. When use_past is True, there are two steps to run the prediction.
The first step, set the is_first_iteration to be True by
<cite>model.add_flags_recursive(is_first_iteration=True)</cite>, and pass the full inputs. Then, set the
is_first_iteration to be False by <cite>model.add_flags_recursive(is_first_iteration=False)</cite>. At this moment,
pass the single step’s input tensor, and loop it. Default False.</p></li>
<li><p><strong>moe_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.MoEConfig" title="mindspore.parallel.nn.MoEConfig"><em>MoEConfig</em></a>) – The configuration of MoE (Mixture of Expert).</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.OpParallelConfig" title="mindspore.parallel.nn.OpParallelConfig"><em>OpParallelConfig</em></a>) – The parallel configure. Default <cite>default_dpmp_config</cite>,
an instance of <cite>OpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Float Tensor, shape should be [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,
should be [batch_size, 1, hidden_size]</p></li>
<li><p><strong>input_mask</strong> (Tensor) - Float Tensor, attention mask with shape [batch_size, seq_length, seq_length],
if the use_past is False or is_first_iteration=True. Otherwise, should be [batch_size, 1, hidden_size]</p></li>
<li><p><strong>init_reset</strong> (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used
for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>).</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - The float tensor of the output of the layer with
shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is
False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size)</p></li>
<li><p><strong>layer_present</strong> (Tuple) - A tuple of the Tensor of the projected key and value vector with
((batch_size, num_heads, size_per_head, seq_length),
(batch_size, num_heads, seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">TransformerEncoderLayer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="go"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="go"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="go"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="go"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the full</span>
<span class="go"># sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.TransformerDecoderLayer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">TransformerDecoderLayer</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">ffn_hidden_size</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">src_seq_length</em>, <em class="sig-param">tgt_seq_length</em>, <em class="sig-param">attention_dropout_rate=0.1</em>, <em class="sig-param">hidden_dropout_rate=0.1</em>, <em class="sig-param">post_layernorm_residual=False</em>, <em class="sig-param">use_past=False</em>, <em class="sig-param">layernorm_compute_type=mstype.float32</em>, <em class="sig-param">softmax_compute_type=mstype.float32</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">hidden_act=&quot;gelu&quot;</em>, <em class="sig-param">moe_config=default_moe_config</em>, <em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#TransformerDecoderLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.TransformerDecoderLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer Decoder Layer. This is an implementation of the single layer of the transformer
decoder layer, including self-attention, cross attention and feedward layer. When the encoder_output is None,
the cross attention will not be effective.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>src_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The input source sequence length.</p></li>
<li><p><strong>tgt_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The input target sequence length.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of bottleneck in the feedforward layer.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1.</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1.</p></li>
<li><p><strong>post_layernorm_residual</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Do residuals adds before the layernorm. Default False.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. Default: gelu.</p></li>
<li><p><strong>layernorm_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the layernorm.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the softmax in the attention.
Should be dtype.float32 or dtype.float16. Default mstype.float32.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>use_past</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use the past state to compute, used for incremental prediction. Default False.</p></li>
<li><p><strong>moe_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.MoEConfig" title="mindspore.parallel.nn.MoEConfig"><em>MoEConfig</em></a>) – The configuration of MoE (Mixture of Expert).</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.OpParallelConfig" title="mindspore.parallel.nn.OpParallelConfig"><em>OpParallelConfig</em></a>) – The parallel configure. Default <cite>default_dpmp_config</cite>,
an instance of <cite>OpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>hidden_stats</strong> (Tensor) - the input tensor with shape [batch_size, tgt_seq_length, hidden_size] or
[batch_size * tgt_seq_length, hidden_size].</p></li>
<li><p><strong>decoder_mask</strong> (Tensor) - the attention mask for decoder with shape [batch_size, src_seq_length,
seq_length].</p></li>
<li><p><strong>encoder_output</strong> (Tensor) - the output of the encoder with shape [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size]. Note this args can not be passed by None when the net is in outermost
layer. Default None.</p></li>
<li><p><strong>memory_mask</strong> (Tensor) - the memory mask of the cross attention with shape [batch, tgt_seq_length,
src_seq_length] where tgt_seq_length is the length of the decoder. Note this args can not be passed by
None when the net is in outermost layer. Default None.</p></li>
<li><p><strong>init_reset</strong> (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used
for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - the output logit of this layer. The shape is [batch, seq_length, hidden_size] or
[batch * seq_length, hidden_size].</p></li>
<li><p><strong>layer_present</strong> (Tensor) - A tuple, where each tuple is the tensor of the projected key and value
vector in self attention with shape ((batch_size, num_heads, size_per_head, tgt_seq_length),
(batch_size, num_heads, tgt_seq_length, size_per_head), and of the projected key and value vector
in cross attention with shape  (batch_size, num_heads, size_per_head, src_seq_length),
(batch_size, num_heads, src_seq_length, size_per_head)).</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">TransformerDecoderLayer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">memory_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">decoder_input_value</span><span class="p">,</span> <span class="n">decoder_input_mask</span><span class="p">,</span> <span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 10, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 10, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.Transformer">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">Transformer</code><span class="sig-paren">(</span><em class="sig-param">hidden_size</em>, <em class="sig-param">batch_size</em>, <em class="sig-param">ffn_hidden_size</em>, <em class="sig-param">src_seq_length</em>, <em class="sig-param">tgt_seq_length</em>, <em class="sig-param">encoder_layers=3</em>, <em class="sig-param">decoder_layers=3</em>, <em class="sig-param">num_heads=2</em>, <em class="sig-param">attention_dropout_rate=0.1</em>, <em class="sig-param">hidden_dropout_rate=0.1</em>, <em class="sig-param">hidden_act=&quot;gelu&quot;</em>, <em class="sig-param">post_layernorm_residual=False</em>, <em class="sig-param">layernorm_compute_type=mstype.float32</em>, <em class="sig-param">softmax_compute_type=mstype.float32</em>, <em class="sig-param">param_init_type=mstype.float32</em>, <em class="sig-param">lambda_func=None</em>, <em class="sig-param">use_past=False</em>, <em class="sig-param">moe_config=default_moe_config</em>, <em class="sig-param">parallel_config=default_transformer_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#Transformer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.Transformer" title="Permalink to this definition">¶</a></dt>
<dd><p>Transformer module including encoder and decoder. The difference with the original implements is the module use
the residual addition before the layer normalization. And the default hidden act is <cite>gelu</cite>.
The details can be found in <a class="reference external" href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention is all you need</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an experimental interface that is subject to change and/or deletion.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The batch size of the input tensor.</p></li>
<li><p><strong>encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The layers of the <cite>TransformerEncoderLayer</cite>.</p></li>
<li><p><strong>decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The layers of the <cite>TransformerDecoderLayer</cite>.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of the input.</p></li>
<li><p><strong>ffn_hidden_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The hidden size of bottleneck in the feedforward layer.</p></li>
<li><p><strong>src_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The seq_length of the encoder’s input tensor.</p></li>
<li><p><strong>tgt_seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The seq_length of the decoder’s input tensor.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the heads. Default: 2.</p></li>
<li><p><strong>hidden_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the final output of the layer. Default:0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The dropout rate of the attention scores. Default:0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Do residuals adds before the layernorm. Default False.</p></li>
<li><p><strong>layernorm_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the layernorm.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>softmax_compute_type</strong> (<em>dtype.Number</em>) – The computation type of the softmax in the attention.
Should be dtype.float32 or dtype.float16. Default mstype.float32.</p></li>
<li><p><strong>param_init_type</strong> (<em>dtype.Number</em>) – The parameter initialization type of the module.
Should be dtype.float32 or dtype.float16. Default dtype.float32.</p></li>
<li><p><strong>hidden_act</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – The activation of the internal feedforward layer. Supports ‘relu’,
‘relu6’, ‘tanh’, ‘gelu’, ‘fast_gelu’, ‘elu’, ‘sigmoid’, ‘prelu’, ‘leakyrelu’, ‘hswish’,
‘hsigmoid’, ‘logsigmoid’ and so on. Default: gelu.</p></li>
<li><p><strong>moe_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.MoEConfig" title="mindspore.parallel.nn.MoEConfig"><em>MoEConfig</em></a>) – The configuration of MoE (Mixture of Expert).</p></li>
<li><p><strong>lambda_func</strong> – A function can determine the fusion index, pipeline stages and recompute attribute. If the user
wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function
that accepts <cite>network</cite>, <cite>layer_id</cite>, <cite>offset</cite>, <cite>parallel_config</cite>, <cite>layers</cite>. The <cite>network(Cell)</cite>
represents the transformer block, <cite>layer_id(int)</cite> means the layer index for the current module, counts from
zero, <cite>offset(int)</cite> means the layer_index needs an offset, if there are other modules in the net. The
default setting for the pipeline is: <cite>(layer_id + offset) // ((encoder_layers + decoder_length)
/ pipeline_stage)</cite>.</p></li>
<li><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.TransformerOpParallelConfig" title="mindspore.parallel.nn.TransformerOpParallelConfig"><em>TransformerOpParallelConfig</em></a>) – The parallel configure. Default <cite>default_transformer_config</cite>,
an instance of <cite>TransformerOpParallelConfig</cite> with default args.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>encoder_inputs</strong> (Tensor) - the input tensor with shape [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size].</p></li>
<li><p><strong>encoder_masks</strong> (Tensor) - the attention mask for decoder with shape [batch_size, seq_length, seq_length].</p></li>
<li><p><strong>decoder_inputs</strong> (Tensor) - the output of the encoder with shape [batch_size, seq_length, hidden_size] or
[batch_size * seq_length, hidden_size],
this should be none if the decoder layer is 0.</p></li>
<li><p><strong>decoder_masks</strong> (Tensor) - the attention mask for decoder with shape [batch_size, seq_length, seq_length]</p></li>
<li><p><strong>memory_mask</strong> (Tensor) - the memory mask of the cross attention with shape [batch, tgt_seq_length,
src_seq_length]
where tgt_seq_length is the length of the decoder. the output of the encoder with shape [batch_size,
seq_length, hidden_size], this should be none if the decoder layer is 0.</p></li>
<li><p><strong>init_reset</strong> (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and
past value parameter used in the incremental prediction. Only valid when use_past is True. Default True</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index. Used
for incremental prediction when the use_past is True. Default None.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple, a tuple contains(<cite>output</cite>, <cite>encoder_layer_present</cite>, <cite>encoder_layer_present</cite>)</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - If there is only encoder, the output logit of the encoder layer. The shape is
[batch, src_seq_length, hidden_size] or [batch * src_seq_length, hidden_size], if there are encoder and
decoders, the output is from the decoder layer. The shape is [batch, tgt_seq_length, hidden_size] or
[batch * tgt_seq_length, hidden_size].</p></li>
<li><p><strong>encoder_layer_present</strong> (Tuple) - A tuple with size of num_layers, where each tuple is the tensor the
projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,
src_seq_length), (batch_size, num_heads, src_seq_length, size_per_head)).</p></li>
<li><p><strong>decoder_layer_present</strong> (Tuple) - A tuple with size of num_layers, where each tuple is the tensor
of the projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,
tgt_seq_length), (batch_size, num_heads, tgt_seq_length, size_per_head)), and the
projected key and value vector in cross attention with shape
(batch_size, num_heads, size_per_head, src_seq_length),
(batch_size, num_heads, src_seq_length, size_per_head)). If the decoder is not set, the
returned value will be None.</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">Transformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>        <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">memory_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">en_past</span><span class="p">,</span> <span class="n">de_past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">decoder_input_value</span><span class="p">,</span>
<span class="gp">... </span>                                 <span class="n">decoder_input_mask</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 10, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">en_past</span><span class="p">))</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">de_past</span><span class="p">))</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">en_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">en_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 10, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.TransformerOpParallelConfig">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">TransformerOpParallelConfig</code><span class="sig-paren">(</span><em class="sig-param">data_parallel=1</em>, <em class="sig-param">model_parallel=1</em>, <em class="sig-param">pipeline_stage=1</em>, <em class="sig-param">micro_batch_num=1</em>, <em class="sig-param">recompute=False</em>, <em class="sig-param">optimizer_shard=False</em>, <em class="sig-param">gradient_aggregation_group=4</em>, <em class="sig-param">vocab_emb_dp=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#TransformerOpParallelConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.TransformerOpParallelConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>TransformerOpParallelConfig for the setting global data parallel, model parallel and fusion group.
The parallel configure setting.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Except the recompute argument, other arguments will not be effective when the user doesn’t set
auto_parallel_context to <cite>SEMI_AUTO_PARALLEL</cite> or <cite>AUTO_PARALLEL</cite>.
The micro_batch_num must be greater than or equal to pipeline_stage. The data_parallel*model_parallel
*pipeline_stage must be equal or less equal to the device. When setting the pipeline stage and
optimizer_shard, the config will overwrite the auto_parallel_context.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The data parallel way. Default: 1.</p></li>
<li><p><strong>model_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The model parallel way. Default: 1.</p></li>
<li><p><strong>pipeline_stage</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of the pipeline stage. Should be a positive value. Default: 1.</p></li>
<li><p><strong>micro_batch_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The microe size of the batches for the pipeline training. Default: 1.</p></li>
<li><p><strong>optimizer_shard</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether to enable optimizer shard. Default False.</p></li>
<li><p><strong>gradient_aggregation_group</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The fusion group size of the optimizer state sharding. Default: 4.</p></li>
<li><p><strong>recompute</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Enable recomputation of the transformer block or not. Default: False.</p></li>
<li><p><strong>vocab_emb_dp</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Shard embedding in model parallel or data parallel. Default: True.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.parallel.nn.TransformerOpParallelConfig.dp_mp_config">
<em class="property">property </em><code class="sig-name descname">dp_mp_config</code><a class="headerlink" href="#mindspore.parallel.nn.TransformerOpParallelConfig.dp_mp_config" title="Permalink to this definition">¶</a></dt>
<dd><p>To obtain the EmbeddingParallelConfig for the setting data parallel, model parallel and embedding
parallel.</p>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dp_mp_config</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindspore.parallel.nn.TransformerOpParallelConfig.embedding_dp_mp_config">
<em class="property">property </em><code class="sig-name descname">embedding_dp_mp_config</code><a class="headerlink" href="#mindspore.parallel.nn.TransformerOpParallelConfig.embedding_dp_mp_config" title="Permalink to this definition">¶</a></dt>
<dd><p>To obtain the EmbeddingParallelConfig for the setting data parallel, model parallel and embedding
parallel.</p>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">embedding_dp_mp_config</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.EmbeddingOpParallelConfig">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">EmbeddingOpParallelConfig</code><span class="sig-paren">(</span><em class="sig-param">data_parallel=1</em>, <em class="sig-param">model_parallel=1</em>, <em class="sig-param">vocab_emb_dp=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/transformer.html#EmbeddingOpParallelConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.EmbeddingOpParallelConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>EmbeddingOpParallelConfig for the setting data parallel or row slice for the embedding table.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The data parallel way. Default: 1</p></li>
<li><p><strong>model_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The model parallel way. Default: 1</p></li>
<li><p><strong>vocab_emb_dp</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Shard embedding in model parallel or data parallel. Default: True</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.parallel.nn.EmbeddingOpParallelConfig.dp_mp_config">
<em class="property">property </em><code class="sig-name descname">dp_mp_config</code><a class="headerlink" href="#mindspore.parallel.nn.EmbeddingOpParallelConfig.dp_mp_config" title="Permalink to this definition">¶</a></dt>
<dd><p>To obtain the DPMPlConfig for the setting data parallel, model parallel</p>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dp_mp_config</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.CrossEntropyLoss">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">CrossEntropyLoss</code><span class="sig-paren">(</span><em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/loss.html#CrossEntropyLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.CrossEntropyLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the cross entropy loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>parallel_config</strong> (<a class="reference internal" href="#mindspore.parallel.nn.OpParallelConfig" title="mindspore.parallel.nn.OpParallelConfig"><em>OpParallelConfig</em></a>) – The parallel configure. Default <cite>default_dpmp_config</cite>,
an instance of <cite>OpParallelConfig</cite> with default args.</p>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - Tensor of shape (N, C). Data type must be float16 or float32. the output logits of
the backbone.</p></li>
<li><p><strong>labels</strong> (Tensor) - Tensor of shape (N, ). The ground truth label of the sample.</p></li>
<li><p><strong>input_mask</strong> (Tensor) - Tensor of shape (N, ). input_mask indicates whether there is padded inputs and for
padded inputs it will not be counted into loss.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor. the corresponding cross entropy loss</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">72</span><span class="p">]]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">labels_np</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1,)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.OpParallelConfig">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">OpParallelConfig</code><span class="sig-paren">(</span><em class="sig-param">data_parallel=1</em>, <em class="sig-param">model_parallel=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/op_parallel_config.html#OpParallelConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.OpParallelConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>OpParallelConfig for the setting data parallel and model parallel.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The data parallel way. Default: 1</p></li>
<li><p><strong>model_parallel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The model parallel way. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">OpParallelConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">OpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.FixedSparseAttention">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">FixedSparseAttention</code><span class="sig-paren">(</span><em class="sig-param">batch_size</em>, <em class="sig-param">num_heads</em>, <em class="sig-param">size_per_head</em>, <em class="sig-param">block_size</em>, <em class="sig-param">seq_length=1024</em>, <em class="sig-param">num_different_global_patterns=4</em>, <em class="sig-param">parallel_config=default_dpmp_config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/layers.html#FixedSparseAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.FixedSparseAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Fixed Sparse Attention Layer</p>
<p>This function contains the sparse attention primitives used in Sparse Transformers (see paper).
<a class="reference external" href="https://arxiv.org/abs/1904.10509">https://arxiv.org/abs/1904.10509</a>
Specifically, it includes the following:
1. A faster implementation of normal attention (the upper triangle is not computed, and many operations are fused).
2. An implementation of “strided” and “fixed” attention, as in the Sparse Transformers paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of input batch size.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of attention heads.</p></li>
<li><p><strong>block_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An integer determining the block size. Current implementation of sparse self-attention
is based on blocked sparse matrices. In which this parameter defines size of such blocks,
Block X Block. only supports 64 for now</p></li>
<li><p><strong>seq_length</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – length of input sequence, only supports 1024 for now</p></li>
<li><p><strong>num_different_global_patterns</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An integer determining number of different global attentions layouts.
While global attention can be fixed by which block/s are representative of
any local window, since there are multi-heads, each head can use a
different global representative, only supports 4 for now</p></li>
<li><p><strong>size_per_head</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – An integer determining embedding size of each attention head,
only supports 64, 128 for now</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>q</strong> (Tensor) - Tensor query (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [batch_size, seq_length, hidden_size]): Sequence of
queries to query the context.</p></li>
<li><p><strong>k</strong> (Tensor) - Tensor key (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [batch_size, seq_length, hidden_size]): Sequence of
queries to query the context.</p></li>
<li><p><strong>v</strong> (Tensor) - Tensor value (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [batch size, sequence length, Embedding Size]):
Sequence of queries to query the context.</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - Float Tensor the mask of (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp32</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code>
[batch_size, seq_length, seq_length]): Lower triangular matrix to pass masked information.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>A Tensor. The output of the attention with shape [batch_size, seq_length, hidden_size]</p>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.parallel.nn</span> <span class="kn">import</span> <span class="n">FixedSparseAttention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FixedSparseAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">size_per_head</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1024, 512)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.parallel.nn.MoEConfig">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.parallel.nn.</code><code class="sig-name descname">MoEConfig</code><span class="sig-paren">(</span><em class="sig-param">expert_num=1</em>, <em class="sig-param">capacity_factor=1.1</em>, <em class="sig-param">aux_loss_factor=0.05</em>, <em class="sig-param">num_experts_chosen=1</em>, <em class="sig-param">noisy_policy=None</em>, <em class="sig-param">noisy_epsilon=1e-2</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/parallel/nn/moe.html#MoEConfig"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.parallel.nn.MoEConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>The configuration of MoE (Mixture of Expert).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>expert_num</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of experts employed. Default: 1</p></li>
<li><p><strong>capacity_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The factor is used to indicate how much to expand expert capacity,
which is &gt;=1.0. Default: 1.1.</p></li>
<li><p><strong>aux_loss_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The factor is used to indicate how much the load balance loss (produced by the
router) to be added to the entire model loss, which is &lt; 1.0. Default: 0.05.</p></li>
<li><p><strong>num_experts_chosen</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of experts is chosen by each token. Default: 1.</p></li>
<li><p><strong>noisy_policy</strong> (<em>string</em>) – The noisy policy is used in routing tokens to experts. Default: None.</p></li>
<li><p><strong>noisy_epsilon</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The parameter is used in adding noises in routing tokens to experts. Default: 1e-2.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.profiler.html" class="btn btn-neutral float-right" title="mindspore.profiler" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.parallel.html" class="btn btn-neutral float-left" title="mindspore.parallel" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>