<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn.loss.loss &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Native Distributed Parallel Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">Static Graph Syntax —— Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">Static Graph Syntax —— Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax —— Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.nn.loss.loss</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.nn.loss.loss</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;loss&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span><span class="p">,</span> <span class="n">division</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations</span> <span class="kn">import</span> <span class="n">_inner_ops</span> <span class="k">as</span> <span class="n">inner</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">MultiMarginLoss</span> <span class="k">as</span> <span class="n">MultiMarginLossOp</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.nn_ops</span> <span class="kn">import</span> <span class="n">MultilabelMarginLoss</span> <span class="k">as</span> <span class="n">MultilabelMarginLossOp</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span><span class="p">,</span> <span class="n">_primexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.cell</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.layer.activation</span> <span class="kn">import</span> <span class="n">get_activation</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>


<div class="viewcode-block" id="LossBase"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.LossBase.html#mindspore.nn.LossBase">[docs]</a><span class="k">class</span> <span class="nc">LossBase</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for other losses.</span>

<span class="sd">    Other losses derived from this should implement their own `construct` and use method `self.get_loss`</span>
<span class="sd">    to apply reduction to loss values.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the (weighted) mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Loss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LossBase</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;none&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;reduction&#39; must be in [&#39;mean&#39;, &#39;sum&#39;, &#39;none&#39;], &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_mean</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

<div class="viewcode-block" id="LossBase.get_axis"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.LossBase.html#mindspore.nn.LossBase.get_axis">[docs]</a>    <span class="k">def</span> <span class="nf">get_axis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get a range of axis for input.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): Tensor of any shape.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import ops, Tensor, nn</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; class Net(nn.LossBase):</span>
<span class="sd">            ...     def __init__(self, reduction=&#39;mean&#39;):</span>
<span class="sd">            ...         super(Net, self).__init__(reduction)</span>
<span class="sd">            ...         self.abs = ops.Abs()</span>
<span class="sd">            ...</span>
<span class="sd">            ...     def construct(self, logits, labels):</span>
<span class="sd">            ...         x = self.abs(logits - labels)</span>
<span class="sd">            ...         axis = self.get_axis(x)</span>
<span class="sd">            ...         return axis</span>
<span class="sd">            &gt;&gt;&gt; net = Net()</span>
<span class="sd">            &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">            &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = net(logits, labels)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            (0,)</span>
<span class="sd">            &gt;&gt;&gt; # Case 2: logits.shape = labels.shape = (3, 3)</span>
<span class="sd">            &gt;&gt;&gt; logits = Tensor(np.array([[1, 2, 3],[1, 2, 3],[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; labels = Tensor(np.array([[1, 2, 3],[1, 2, 3],[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = net(logits, labels)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            (0, 1)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">length</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">perm</span></div>

<div class="viewcode-block" id="LossBase.get_loss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.LossBase.html#mindspore.nn.LossBase.get_loss">[docs]</a>    <span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Computes the weighted loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            x (Tensor): Tensor of shape :math:`(N, *)` where :math:`*` means, any number of</span>
<span class="sd">                additional dimensions.</span>
<span class="sd">            weights (Union[float, Tensor]): Optional `Tensor` whose rank is either 0, or the same rank as inputs,</span>
<span class="sd">                and must be broadcastable to inputs (i.e., all dimensions must be either `1`,</span>
<span class="sd">                or the same as the corresponding inputs dimension). Default: ``1.0`` .</span>

<span class="sd">        Returns:</span>
<span class="sd">            Return the weighted loss.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import ops, Tensor, nn</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; class Net(nn.LossBase):</span>
<span class="sd">            ...     def __init__(self, reduction=&#39;mean&#39;):</span>
<span class="sd">            ...         super(Net, self).__init__(reduction)</span>
<span class="sd">            ...         self.abs = ops.Abs()</span>
<span class="sd">            ...</span>
<span class="sd">            ...     def construct(self, logits, labels):</span>
<span class="sd">            ...         x = self.abs(logits - labels)</span>
<span class="sd">            ...         output = self.get_loss(x)</span>
<span class="sd">            ...         return output</span>
<span class="sd">            &gt;&gt;&gt; net = Net()</span>
<span class="sd">            &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">            &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = net(logits, labels)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            0.33333334</span>
<span class="sd">            &gt;&gt;&gt; # Case 2: logits.shape = labels.shape = (3, 3)</span>
<span class="sd">            &gt;&gt;&gt; logits = Tensor(np.array([[1, 2, 3],[1, 2, 3],[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; labels = Tensor(np.array([[1, 2, 2],[1, 2, 3],[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = net(logits, labels)</span>
<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            0.11111111</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_axis</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Base class for other losses.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _Loss.&quot;&quot;&quot;</span>
        <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;&#39;_Loss&#39; is deprecated from version 1.3 and &quot;</span>
                    <span class="s2">&quot;will be removed in a future version, use &#39;LossBase&#39; instead.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<span class="nd">@constexpr</span><span class="p">(</span><span class="n">check</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_check_is_tensor</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the input data is Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">input_data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">input_data</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;</span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2">&#39; must be &#39;</span><span class="si">{</span><span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">F</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="L1Loss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.L1Loss.html#mindspore.nn.L1Loss">[docs]</a><span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L1Loss is used to calculate the mean absolute error between the predicted value and the target value.</span>

<span class="sd">    Assuming that the :math:`x` and :math:`y` are 1-D Tensor, length :math:`N`, then calculate the loss of :math:`x` and</span>
<span class="sd">    :math:`y` without dimensionality reduction (the reduction parameter is set to &quot;none&quot;). The formula is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad \text{with } l_n = \left| x_n - y_n \right|,</span>

<span class="sd">    where :math:`N` is the batch size. If `reduction` is not &#39;none&#39;, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Predicted value, Tensor of any dimension.</span>
<span class="sd">        - **labels** (Tensor) - Target value, same shape as the `logits` in common cases.</span>
<span class="sd">          However, it supports the shape of `logits` is different from the shape of `labels`</span>
<span class="sd">          and they should be broadcasted to each other.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, data type is float.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        ValueError: If `logits` and `labels` have different shapes and cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.L1Loss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.33333334</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: logits.shape = (3,), labels.shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.L1Loss(reduction=&#39;none&#39;)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 2.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L1Loss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="MSELoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.MSELoss.html#mindspore.nn.MSELoss">[docs]</a><span class="k">class</span> <span class="nc">MSELoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the mean squared error between the predicted value and the label value.</span>

<span class="sd">    For simplicity, let :math:`x` and :math:`y` be 1-dimensional Tensor with length :math:`N`,</span>
<span class="sd">    the unreduced loss (i.e. with argument reduction set to &#39;none&#39;) of :math:`x` and :math:`y` is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad \text{with} \quad l_n = (x_n - y_n)^2.</span>

<span class="sd">    where :math:`N` is the batch size. If `reduction` is not &#39;none&#39;, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The predicted value of the input. Tensor of any dimension.</span>
<span class="sd">        - **labels** (Tensor) - The input label. Tensor of any dimension, same shape as the `logits` in common cases.</span>
<span class="sd">          However, it supports the shape of `logits` is different from the shape of `labels`</span>
<span class="sd">          and they should be broadcasted to each other.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, loss of type float, the shape is zero if `reduction` is &#39;mean&#39; or &#39;sum&#39;,</span>
<span class="sd">        while the shape of output is the broadcasted shape if `reduction` is &#39;none&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>
<span class="sd">        ValueError: If `logits` and `labels` have different shapes and cannot be broadcasted.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.6666667</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: logits.shape = (3,), labels.shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss(reduction=&#39;none&#39;)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 4.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_rmseloss_dtype</span><span class="p">(</span><span class="n">param_dtype</span><span class="p">,</span> <span class="n">not_supported_dtype</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check RMSELoss not supported data type&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">param_dtype</span> <span class="ow">in</span> <span class="n">not_supported_dtype</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the parameters data type must not be in </span><span class="si">{</span><span class="n">not_supported_dtype</span><span class="si">}</span><span class="s2">, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got mindspore.</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">param_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="RMSELoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.RMSELoss.html#mindspore.nn.RMSELoss">[docs]</a><span class="k">class</span> <span class="nc">RMSELoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RMSELoss creates a criterion to measure the root mean square error between :math:`x` and :math:`y`</span>
<span class="sd">    element-wise, where :math:`x` is the input and :math:`y` is the labels.</span>

<span class="sd">    For simplicity, let :math:`x` and :math:`y` be 1-dimensional Tensor with length :math:`N`,</span>
<span class="sd">    the loss of :math:`x` and :math:`y` is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss = \sqrt{\frac{1}{N}\sum_{i=1}^{N}{(x_i-y_i)^2}}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(N, *)` where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(N, *)`, same shape as the `logits` in common cases.</span>
<span class="sd">          However, it supports the shape of `logits` is different from the shape of `labels`</span>
<span class="sd">          and they should be broadcasted to each other.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, weighted loss float tensor and its shape is :math:`()`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.RMSELoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.57735026</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: logits.shape = (3,), labels.shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.RMSELoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize RMSELoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RMSELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">MSELoss</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">logits_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">label_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
        <span class="n">not_supported_dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">]</span>
        <span class="n">_check_rmseloss_dtype</span><span class="p">(</span><span class="n">logits_dtype</span><span class="p">,</span> <span class="n">not_supported_dtype</span><span class="p">,</span> <span class="s1">&#39;RMSELoss&#39;</span><span class="p">)</span>
        <span class="n">_check_rmseloss_dtype</span><span class="p">(</span><span class="n">label_dtype</span><span class="p">,</span> <span class="n">not_supported_dtype</span><span class="p">,</span> <span class="s2">&quot;RMSELoss&quot;</span><span class="p">)</span>

        <span class="n">rmse_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">rmse_loss</span></div>


<span class="k">class</span> <span class="nc">MAELoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MAELoss creates a criterion to measure the average absolute error between :math:`x` and :math:`y`</span>
<span class="sd">    element-wise, where :math:`x` is the input and :math:`y` is the labels.</span>

<span class="sd">    For simplicity, let :math:`x` and :math:`y` be 1-dimensional Tensor with length :math:`N`,</span>
<span class="sd">    the unreduced loss (i.e. with argument reduction set to &#39;none&#39;) of :math:`x` and :math:`y` is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad \text{with } l_n = \left| x_n - y_n \right|,</span>

<span class="sd">    where :math:`N` is the batch size. If `reduction` is not &#39;none&#39;, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(M, *)` where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(N, *)`, same shape as the `logits` in common cases.</span>
<span class="sd">          However, it supports the shape of `logits` is different from the shape of `labels`</span>
<span class="sd">          and they should be broadcasted to each other.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, weighted loss float tensor, the shape is zero if `reduction` is &#39;mean&#39; or &#39;sum&#39;,</span>
<span class="sd">        while the shape of output is the broadcasted shape if `reduction` is &#39;none&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MAELoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.33333334</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: logits.shape = (3,), labels.shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MAELoss(reduction=&#39;none&#39;)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[1, 1, 1], [1, 2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 2.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MAELoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MAELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">abs</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">label</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="MarginRankingLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.MarginRankingLoss.html#mindspore.nn.MarginRankingLoss">[docs]</a><span class="k">class</span> <span class="nc">MarginRankingLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MarginRankingLoss creates a criterion that measures the loss.</span>

<span class="sd">    Given two tensors :math:`input1`, :math:`input2` and a Tensor label :math:`target` with values 1 or -1,</span>
<span class="sd">    the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(input1, input2, target) = \max(0, -target * (input1 - input2) + \text{margin})</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Specify the adjustment factor of the operation. Default: ``0.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input1** (Tensor) - Tensor of shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions.</span>
<span class="sd">        - **input2** (Tensor) - Tensor of shape :math:`(N, *)`, same shape and dtype as `input1`.</span>
<span class="sd">        - **target** (Tensor) - Contains value 1 or -1. Suppose the shape of `input1` is</span>
<span class="sd">          :math:`(x_1, x_2, x_3, ..., x_R)`, then the shape of `target` must be :math:`(x_1, x_2, x_3, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar. if `reduction` is &quot;none&quot;, its shape is the same as `labels`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `margin` is not a float.</span>
<span class="sd">        TypeError: If `input1`, `input2` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If the types of `input1` and `input2` are inconsistent.</span>
<span class="sd">        TypeError: If the types of `input1` and `target` are inconsistent.</span>
<span class="sd">        ValueError: If the shape of `input1` and `input2` are inconsistent.</span>
<span class="sd">        ValueError: If the shape of `input1` and `target` are inconsistent.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss1 = nn.MarginRankingLoss(reduction=&#39;none&#39;)</span>
<span class="sd">        &gt;&gt;&gt; loss2 = nn.MarginRankingLoss(reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; loss3 = nn.MarginRankingLoss(reduction=&#39;sum&#39;)</span>
<span class="sd">        &gt;&gt;&gt; sign = ops.Sign()</span>
<span class="sd">        &gt;&gt;&gt; input1 = Tensor(np.array([0.3864, -2.4093, -1.4076]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; input2 = Tensor(np.array([-0.6012, -1.6681, 1.2928]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = sign(Tensor(np.array([-2, -2, 3]), ms.float32))</span>
<span class="sd">        &gt;&gt;&gt; output1 = loss1(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [0.98759997 0.         2.7003999 ]</span>
<span class="sd">        &gt;&gt;&gt; output2 = loss2(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        1.2293333</span>
<span class="sd">        &gt;&gt;&gt; output3 = loss3(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output3)</span>
<span class="sd">        3.6879997</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MarginRankingLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MarginRankingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.SmoothL1Loss.html#mindspore.nn.SmoothL1Loss">[docs]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SmoothL1 loss function, if the absolute error element-wise between the predicted value and the target value</span>
<span class="sd">    is less than the set threshold `beta`, the square term is used, otherwise the absolute error term is used.</span>

<span class="sd">    Given two input :math:`x,\  y`, the SmoothL1Loss can be described as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{i} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        \frac{0.5 (x_i - y_i)^{2}}{\beta}, &amp; \text{if } |x_i - y_i| &lt; {\beta} \\</span>
<span class="sd">        |x_i - y_i| - 0.5 {\beta}, &amp; \text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Where :math:`{\beta}` represents the threshold `beta`.</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L_{i}), &amp;  \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L_{i}),  &amp;  \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - On the Ascend platform, float64 data type will result in low operator performance.</span>
<span class="sd">        - SmoothL1Loss can be regarded as modified version of L1Loss or a combination of L1Loss and L2Loss.</span>
<span class="sd">        - L1Loss computes the element-wise absolute difference between two input tensors while L2Loss computes the</span>
<span class="sd">        - squared difference between two input tensors. L2Loss often leads to faster convergence but it is less</span>
<span class="sd">        - robust to outliers, and the loss function has better robustness.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta (float): The loss function calculates the threshold of the transformation between L1Loss and L2Loss.</span>
<span class="sd">            Default: ``1.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Predictive value. Tensor of any dimension. Data type must be one of float16,</span>
<span class="sd">          float32 and float64.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth data, same shape and dtype as the `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if `reduction` is &#39;none&#39;, then output is a tensor with the same shape as `logits`.</span>
<span class="sd">        Otherwise the shape of output tensor is :math:`()`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `beta` is not a float.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        TypeError: If `logits` or `labels` are not Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 not float32.</span>
<span class="sd">        TypeError: If dtype of `logits` is not the same as `labels`.</span>
<span class="sd">        ValueError: If `beta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SmoothL1Loss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SmoothL1Loss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SmoothL1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth_l1_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SmoothL1Loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftMarginLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.SoftMarginLoss.html#mindspore.nn.SoftMarginLoss">[docs]</a><span class="k">class</span> <span class="nc">SoftMarginLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A loss class for two-class classification problems.</span>

<span class="sd">    SoftMarginLoss creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and labels tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    :math:`x.nelement()` represents the number of element of `x` .</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Predict data. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth data, with the same type and shape as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &quot;none&quot;, its shape is the same as `logits`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` or `labels` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SoftMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[-1, 1], [1, -1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6764238</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">soft_margin_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SoftMarginLoss</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.SoftmaxCrossEntropyWithLogits.html#mindspore.nn.SoftmaxCrossEntropyWithLogits">[docs]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes softmax cross entropy between logits and labels.</span>

<span class="sd">    Measures the distribution error between the probabilities of the input (computed with softmax function) and the</span>
<span class="sd">    labels where the classes are mutually exclusive (only one class is positive) using cross entropy loss.</span>

<span class="sd">    Typical input into this function is unnormalized scores denoted as x whose shape is :math:`(N, C)` ,</span>
<span class="sd">    and the corresponding targets.</span>

<span class="sd">    Typically, the input to this function is the fractional value of each category and the corresponding target value,</span>
<span class="sd">    and the input format is :math:`(N, C)` .</span>

<span class="sd">    For each instance :math:`x_i`, i ranges from 0 to N-1, the loss is given as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x_i, c) = - \log\left(\frac{\exp(x_i[c])}{\sum_j \exp(x_i[j])}\right)</span>
<span class="sd">        =  -x_i[c] + \log\left(\sum_j \exp(x_i[j])\right)</span>

<span class="sd">    where :math:`x_i` is a 1D score Tensor, :math:`c` is the index of 1 in one-hot.</span>

<span class="sd">    Note:</span>
<span class="sd">        While the labels classes are mutually exclusive, i.e., only one class is positive in the labels, the predicted</span>
<span class="sd">        probabilities does not need to be exclusive. It is only required that the predicted probability distribution</span>
<span class="sd">        of entry is a valid one.</span>

<span class="sd">    Args:</span>
<span class="sd">        sparse (bool, optional): Specifies whether labels use sparse format or not. Default: ``False`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(N, C)` . Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(N, )` . If `sparse` is True, The type of</span>
<span class="sd">          `labels` is int32 or int64. Otherwise, the type of `labels` is the same as the type of `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor of the same shape and type as logits with the component-wise logistic losses.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sparse` is not a bool.</span>
<span class="sd">        TypeError: If `sparse` is True and dtype of `labels` is neither int32 not int64.</span>
<span class="sd">        TypeError: If `sparse` is False and dtype of `labels` is neither float16 not float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # case 1: sparse=True</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[3, 5, 6, 9, 12, 33, 42, 12, 32, 72]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels_np = np.array([1]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(labels_np)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [67.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: sparse=False</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SoftmaxCrossEntropyWithLogits(sparse=False)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[3, 5, 6, 9, 12, 33, 42, 12, 32, 72]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels_np = np.array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(labels_np)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [30.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SoftmaxCrossEntropyWithLogits.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">sparse</span><span class="p">,</span> <span class="s2">&quot;sparse&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_cross_entropy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_cpugpu</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">x</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logits</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">on_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">off_value</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_label_dtype</span><span class="p">(</span><span class="n">labels_dtype</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the data type of labels meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="n">labels_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>


<div class="viewcode-block" id="DiceLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.DiceLoss.html#mindspore.nn.DiceLoss">[docs]</a><span class="k">class</span> <span class="nc">DiceLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Dice coefficient is a set similarity loss, which is used to calculate the similarity between two samples. The</span>
<span class="sd">    value of the Dice coefficient is 1 when the segmentation result is the best and is 0 when the segmentation result</span>
<span class="sd">    is the worst. The Dice coefficient indicates the ratio of the area between two objects to the total area.</span>
<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        dice = 1 - \frac{2 * |pred \bigcap true|}{|pred| + |true| + smooth}</span>

<span class="sd">    :math:`pred` represent `logits`, :math:`true` represent `labels` .</span>

<span class="sd">    Args:</span>
<span class="sd">        smooth (float): A term added to the denominator to improve numerical stability. Should be greater than 0.</span>
<span class="sd">                        Default: ``1e-5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input predicted value. The data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Input target value. Same shape as the `logits`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor of shape with the per-example sampled Dice losses.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the dimension of `logits` is different from `labels`.</span>
<span class="sd">        TypeError: If the type of `logits` or `labels` is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.DiceLoss(smooth=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.2, 0.5], [0.3, 0.1], [0.9, 0.6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[0, 1], [1, 0], [0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38596618</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DiceLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DiceLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">smooth</span><span class="p">,</span> <span class="s2">&quot;smooth&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_shape</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the dtype of &#39;logits&#39; can not be uint8.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">label</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the dtype of &#39;labels&#39; can not be uint8.&quot;</span><span class="p">)</span>
        <span class="n">intersection</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">unionset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="o">+</span> \
                   <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

        <span class="n">single_dice_coeff</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">intersection</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">unionset</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">)</span>
        <span class="n">dice_loss</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">single_dice_coeff</span>

        <span class="k">return</span> <span class="n">dice_loss</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_shape</span><span class="p">(</span><span class="n">logits_shape</span><span class="p">,</span> <span class="n">label_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the shape of logits and labels meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;logits_shape&#39;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="s1">&#39;label_shape&#39;</span><span class="p">,</span> <span class="n">label_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="n">prim_name</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_ndim_multi</span><span class="p">(</span><span class="n">logits_dim</span><span class="p">,</span> <span class="n">label_dim</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the dimension of logits and label meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the&#39;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">logits_dim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;logits&#39; dimension must be greater than 1, but got </span><span class="si">{</span><span class="n">logits_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label_dim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;labels&#39; dimension must be greater than 1, but got </span><span class="si">{</span><span class="n">label_dim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_weights</span><span class="p">(</span><span class="n">weight_shape</span><span class="p">,</span> <span class="n">label_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the reduced shape meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the&#39;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">weight_shape</span> <span class="o">!=</span> <span class="n">label_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> weight_shape[0] must be equal to label_shape[1], &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got weight_shape[0]: </span><span class="si">{</span><span class="n">weight_shape</span><span class="si">}</span><span class="s2"> and label_shape[1]: </span><span class="si">{</span><span class="n">label_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="MultiClassDiceLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.MultiClassDiceLoss.html#mindspore.nn.MultiClassDiceLoss">[docs]</a><span class="k">class</span> <span class="nc">MultiClassDiceLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    When there are multiple classifications, label is transformed into multiple binary classifications by one hot.</span>
<span class="sd">    For each channel section in the channel, it can be regarded as a binary classification problem, so it can be</span>
<span class="sd">    obtained through the binary :class:`mindspore.nn.DiceLoss` losses of each category,</span>
<span class="sd">    and then the average value of the binary losses.</span>

<span class="sd">    Args:</span>
<span class="sd">        weights (Union[Tensor, None]): Tensor of shape :math:`(num\_classes, dim)`. The weight shape[0] should be</span>
<span class="sd">            equal to labels shape[1].</span>
<span class="sd">            Default: ``None`` .</span>
<span class="sd">        ignore_indiex (Union[int, None]): Class index to ignore.</span>
<span class="sd">            Default: ``None`` .</span>
<span class="sd">        activation (Union[str, Cell]): Activate function applied to the output of the fully connected layer, eg. &#39;ReLU&#39;.</span>
<span class="sd">            Default: ``&#39;softmax&#39;`` . Choose from: [ ``&#39;softmax&#39;`` , ``&#39;logsoftmax&#39;`` , ``&#39;relu&#39;`` , ``&#39;relu6&#39;`` ,</span>
<span class="sd">            ``&#39;tanh&#39;`` , ``&#39;Sigmoid&#39;`` ]</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(N, C, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions. The logits dimension should be greater than 1. The data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(N, C, *)`, same shape as the `logits`.</span>
<span class="sd">          The labels dimension should be greater than 1. The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor of shape with the per-example sampled MultiClass Dice Losses.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the shape of `logits` is different from `labels`.</span>
<span class="sd">        TypeError: If the type of `logits` or `labels` is not a tensor.</span>
<span class="sd">        ValueError: If the dimension of `logits` or `labels` is less than 2.</span>
<span class="sd">        ValueError: If the weights.shape[0] is not equal to labels.shape[1].</span>
<span class="sd">        ValueError: If `weights` is a tensor, but its dimension is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MultiClassDiceLoss(weights=None, ignore_indiex=None, activation=&quot;softmax&quot;)</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.2, 0.5, 0.7], [0.3, 0.1, 0.5], [0.9, 0.6, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.54958105</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_indiex</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultiClassDiceLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiClassDiceLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">activation_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="s1">&#39;logsoftmax&#39;</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="s1">&#39;relu6&#39;</span><span class="p">,</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">binarydiceloss</span> <span class="o">=</span> <span class="n">DiceLoss</span><span class="p">(</span><span class="n">smooth</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="p">[</span><span class="n">Tensor</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the dimension of &#39;weights&#39; must be 2, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_indiex</span> <span class="o">=</span> <span class="n">ignore_indiex</span> <span class="k">if</span> <span class="n">ignore_indiex</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_indiex&quot;</span><span class="p">,</span>
                                                                                                    <span class="n">ignore_indiex</span><span class="p">,</span>
                                                                                                    <span class="p">[</span><span class="nb">int</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">and</span> <span class="n">activation</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">activation_list</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;activation&#39; must be in </span><span class="si">{</span><span class="n">activation_list</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">activation</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">get_activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">activation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">,</span> <span class="n">Cell</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;activation&#39; must be str or Cell, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_shape</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_ndim_multi</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_indiex</span><span class="p">:</span>
                <span class="n">dice_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binarydiceloss</span><span class="p">(</span><span class="n">logits</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_check_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
                    <span class="n">dice_loss</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">dice_loss</span>

        <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">label</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></div>


<div class="viewcode-block" id="SampledSoftmaxLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.SampledSoftmaxLoss.html#mindspore.nn.SampledSoftmaxLoss">[docs]</a><span class="k">class</span> <span class="nc">SampledSoftmaxLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sampled softmax training loss. This operator can accelerate the training of the softmax classifier</span>
<span class="sd">    over a large number of classes. It is generally an underestimate of the full softmax loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_sampled (int): The number of classes to randomly sample per batch.</span>
<span class="sd">        num_classes (int): The number of possible classes.</span>
<span class="sd">        num_true (int): The number of labels classes per training example. Default: ``1`` .</span>
<span class="sd">        sampled_values (Union[list, tuple]):  List or tuple of (`sampled_candidates`, `true_expected_count`,</span>
<span class="sd">            `sampled_expected_count`) returned by a `*CandidateSampler` function.</span>
<span class="sd">            Default to None, `UniformCandidateSampler` is applied. Default: ``None`` .</span>
<span class="sd">        remove_accidental_hits (bool): Whether to remove &quot;accidental hits&quot;</span>
<span class="sd">            where a sampled class equals to one of the labels classes. Default: ``True`` .</span>
<span class="sd">        seed (int): Random seed for candidate sampling. Default: 0</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weights** (Tensor) - Tensor of shape :math:`(C, dim)`.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`. The class biases.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(N, num\_true)`, type `int64, int32`. The labels classes.</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(N, dim)`. The forward activations of the input network.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor with shape :math:`(N,)`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sampled_values` is not a list or tuple.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 not int64.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        ValueError: If `num_sampled` or `num_true` is greater than `num_classes`.</span>
<span class="sd">        ValueError: If length of `sampled_values` is not equal to 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; mindspore.set_seed(1)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.SampledSoftmaxLoss(num_sampled=4, num_classes=7, num_true=1)</span>
<span class="sd">        &gt;&gt;&gt; weights = Tensor(np.random.randint(0, 9, [7, 10]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; biases = Tensor(np.random.randint(0, 9, [7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1, 2])</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.random.randint(0, 9, [3, 10]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(weights, biases, labels, logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4.6051701e+01 1.4000047e+01 6.1989022e-06]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_sampled</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">sampled_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">remove_accidental_hits</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SampledSoftmaxLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SampledSoftmaxLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_true</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;num_true&#39; must be greater than or equal to 1, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">num_true</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;seed&#39; must be greater than or equal to 0, but got </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_sampled</span> <span class="o">&gt;</span> <span class="n">num_classes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;num_sampled&#39; must be smaller than or &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;equal to &#39;num_classes&#39;, but got &#39;num_sampled&#39;: </span><span class="si">{</span><span class="n">num_sampled</span><span class="si">}</span><span class="s2"> &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;and &#39;num_classes&#39;: </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_true</span> <span class="o">&gt;</span> <span class="n">num_classes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;num_true&#39; must be smaller than or equal to &#39;num_classes&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;num_true&#39;: </span><span class="si">{</span><span class="n">num_true</span><span class="si">}</span><span class="s2"> amd &#39;num_classes&#39;: </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sampled_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sampled_values</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;sampled_values&#39; must be a list or tuple, &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">sampled_values</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sampled_values</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the length of &#39;sampled_values&#39; must be equal to 3,&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">sampled_values</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_sampled</span> <span class="o">=</span> <span class="n">num_sampled</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span> <span class="o">=</span> <span class="n">num_true</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampled_values</span> <span class="o">=</span> <span class="n">sampled_values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remove_accidental_hits</span> <span class="o">=</span> <span class="n">remove_accidental_hits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UniformCandidateSampler</span><span class="p">(</span>
            <span class="n">num_true</span><span class="p">,</span>
            <span class="n">num_sampled</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span>
            <span class="n">num_classes</span><span class="p">,</span>
            <span class="n">seed</span><span class="p">,</span>
            <span class="n">remove_accidental_hits</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exp</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gather_v2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_max_true</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum_true</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim0</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;weights&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;biases&#39;</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_label_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_sampled_logits</span><span class="p">(</span>
            <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span>
            <span class="n">biases</span><span class="o">=</span><span class="n">biases</span><span class="p">,</span>
            <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span>
            <span class="n">num_true</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_true</span><span class="p">,</span>
            <span class="n">sampled_values</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampled_values</span><span class="p">,</span>
            <span class="n">subtract_log_q</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_softmax_cross_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
        <span class="n">stable_exp_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_max_true</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">stable_exp_logits</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum_true</span><span class="p">(</span><span class="n">stable_exp_logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">targets</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred</span> <span class="o">+</span> <span class="mf">1.0e-20</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compute_sampled_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span>
                                <span class="n">biases</span><span class="p">,</span>
                                <span class="n">labels</span><span class="p">,</span>
                                <span class="n">logits</span><span class="p">,</span>
                                <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">sampled_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">subtract_log_q</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Helper function for SampledSoftmaxLoss functions.</span>

<span class="sd">        Computes sampled output training logits and labels suitable</span>

<span class="sd">        Note: In the case where num_true &gt; 1, we assign to each labels class</span>
<span class="sd">        with the labels probability (1/num_true) so that the labels probabilities</span>
<span class="sd">        sum to 1 per-example.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights (Tensor): Tensor of shape `[num_classes, dim]`.</span>
<span class="sd">            biases (Tensor): Tensor of shape `[num_classes]`.</span>
<span class="sd">            labels (Tensor): Tensor of shape `[batch_size, num_true]`. The labels classes.</span>
<span class="sd">            logits (Tensor): Tensor of shape `[batch_size, dim]`. The forward</span>
<span class="sd">                activations of the input network.</span>
<span class="sd">            num_true (int): The number of labels classes per training example.</span>
<span class="sd">            sampled_values: A tuple of (`sampled_candidates`, `true_expected_count`,</span>
<span class="sd">                `sampled_expected_count`) returned by a `UniformCandidateSampler` function.</span>
<span class="sd">            subtract_log_q: A `bool`. whether to subtract the log expected count of</span>
<span class="sd">                the labels in the sample to get the logits of the true labels. Default: ``True`` .</span>
<span class="sd">        Returns:</span>
<span class="sd">            out_logits: `Tensor` object with shape</span>
<span class="sd">                `[batch_size, num_true + num_sampled]`</span>
<span class="sd">            out_labels: A tensor object with the same shape as `out_logits`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">))</span>
        <span class="n">labels_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>

        <span class="c1"># Sample the negative labels.</span>
        <span class="c1">#   sampled shape: [num_sampled] tensor</span>
        <span class="c1">#   true_expected_count shape is [batch_size, 1] tensor</span>
        <span class="c1">#   sampled_expected_count shape is [num_sampled] tensor</span>
        <span class="k">if</span> <span class="n">sampled_values</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampled_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampler</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

        <span class="p">(</span><span class="n">sampled</span><span class="p">,</span> <span class="n">true_expected_count</span><span class="p">,</span> <span class="n">sampled_expected_count</span><span class="p">)</span> <span class="o">=</span> <span class="n">sampled_values</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">sampled</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="n">sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">sampled</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">all_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim0</span><span class="p">((</span><span class="n">labels_flat</span><span class="p">,</span> <span class="n">sampled</span><span class="p">))</span>
        <span class="n">all_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_v2</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">all_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">n_true</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">labels_flat</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">sampled</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">all_w</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">true_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_w</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">n_true</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">])</span>
        <span class="n">sampled_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_w</span><span class="p">,</span> <span class="p">[</span><span class="n">n_true</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">n_sampled</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">])</span>
        <span class="n">sampled_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">sampled_w</span><span class="p">)</span>

        <span class="n">all_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_v2</span><span class="p">(</span><span class="n">biases</span><span class="p">,</span> <span class="n">all_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">true_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_b</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="n">n_true</span><span class="p">])</span>
        <span class="n">sampled_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice_op</span><span class="p">(</span><span class="n">all_b</span><span class="p">,</span> <span class="p">[</span><span class="n">n_true</span><span class="p">],</span> <span class="p">[</span><span class="n">n_sampled</span><span class="p">])</span>

        <span class="n">new_true_w_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">)</span>
        <span class="n">row_wise_dots</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                 <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">true_w</span><span class="p">,</span> <span class="n">new_true_w_shape</span><span class="p">))</span>

        <span class="c1"># We want the row-wise dot plus biases which yields a</span>
        <span class="c1"># [batch_size, num_true] tensor of true_logits.</span>
        <span class="n">dots_as_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">row_wise_dots</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_dim</span><span class="p">))</span>
        <span class="n">true_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">dots_as_matrix</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">))</span>
        <span class="n">true_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">true_b</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_true</span><span class="p">))</span>
        <span class="n">true_logits</span> <span class="o">+=</span> <span class="n">true_b</span>
        <span class="n">sampled_logits</span> <span class="o">+=</span> <span class="n">sampled_b</span>

        <span class="k">if</span> <span class="n">subtract_log_q</span><span class="p">:</span>
            <span class="c1"># Subtract log of Q(l), prior probability that l appears in sampled.</span>
            <span class="n">true_logits</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">true_expected_count</span><span class="p">)</span>
            <span class="n">sampled_logits</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">sampled_expected_count</span><span class="p">)</span>

        <span class="c1"># Construct output logits and labels. The true labels/logits start at col 0.</span>
        <span class="n">out_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim1</span><span class="p">((</span><span class="n">true_logits</span><span class="p">,</span> <span class="n">sampled_logits</span><span class="p">))</span>

        <span class="c1"># true_logits is a float tensor, ones_like(true_logits) is a float</span>
        <span class="c1"># tensor of ones. We then divide by num_true to ensure the per-example</span>
        <span class="c1"># labels sum to 1.0, i.e. form a proper probability distribution.</span>
        <span class="n">out_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_dim1</span><span class="p">((</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">true_logits</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_true</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sampled_logits</span><span class="p">)</span>
        <span class="p">))</span>
        <span class="k">return</span> <span class="n">out_logits</span><span class="p">,</span> <span class="n">out_labels</span></div>


<span class="k">class</span> <span class="nc">TripletMarginWithDistanceLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TripletMarginWithDistanceLoss operation.</span>

<span class="sd">    Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses` by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    Args:</span>
<span class="sd">        distance_function (callable): The distance function needed to calculate the margin loss of a triplet.</span>
<span class="sd">            if no distance metric is specified, the pairwise distance will be used. Default: ``None`` .</span>
<span class="sd">        swap (bool): The distance swap is described in detail in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. Default: ``False`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        margin (float): Make a margin between the positive pair and the negative pair. Default: ``1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A sample randomly selected from the training set. Data type must be BasicType.</span>
<span class="sd">          The shape should be :math:`(N, D)`.</span>
<span class="sd">        - **positive** (Tensor) - A sample belonging to the same category as x,</span>
<span class="sd">          with the same type and shape as `x`.</span>
<span class="sd">        - **negative** (Tensor) - A sample belonging to the different class from x,</span>
<span class="sd">          with the same type and shape as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Union[Tensor, Scalar], if `reduction` is &#39;none&#39;, its shape is :math:`(N)`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `positive` or `negative` is not a Tensor.</span>
<span class="sd">        TypeError: If `swap` is not a bool.</span>
<span class="sd">        ValueError: If dimensions of input `x`, `positive` and `negative` are less than or equal to 1 at the same time.</span>
<span class="sd">        ValueError: If length of shape of `margin` is not 0.</span>
<span class="sd">        ValueError: If shape of `x`, `positive` and `negative` cannot broadcast.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0.3, 0.7], [0.5, 0.5]])</span>
<span class="sd">        &gt;&gt;&gt; positive = Tensor([[0.4, 0.6], [0.4, 0.6]])</span>
<span class="sd">        &gt;&gt;&gt; negative = Tensor([[0.2, 0.9], [0.3, 0.7]])</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.TripletMarginWithDistanceLoss()</span>
<span class="sd">        &gt;&gt;&gt; out = loss(x, positive, negative)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        0.8881968</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distance_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TripletMarginWithDistanceLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletMarginWithDistanceLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_float</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">swap</span><span class="p">,</span> <span class="s2">&quot;swap&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">distance_function</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                <span class="n">d</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;For &#39;pairwise_distance&#39; in &#39;TripletMarginWithDistanceLoss&#39;, &quot;</span>
                        <span class="s2">&quot;&#39;ndim&#39; of the input must be positive, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">d</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">LpNorm</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">d</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span> <span class="o">=</span> <span class="n">pairwise_distance</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span> <span class="o">=</span> <span class="n">distance_function</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minimum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;positive&quot;</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;negative&quot;</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">positive</span><span class="p">)</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">negative</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">:</span>
            <span class="n">d2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">d2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">))</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">d1</span> <span class="o">-</span> <span class="n">d2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>


<div class="viewcode-block" id="PoissonNLLLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.PoissonNLLLoss.html#mindspore.nn.PoissonNLLLoss">[docs]</a><span class="k">class</span> <span class="nc">PoissonNLLLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Poisson negative log likelihood loss.</span>

<span class="sd">    The loss is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{L}_{D} = \sum_{i = 0}^{|D|}\left( x_{i} - y_{i}\ln x_{i} + \ln{y_{i}!} \right)</span>

<span class="sd">    where :math:`\mathcal{L}_{D}` is the loss, :math:`y_{i}` is the `target`,</span>
<span class="sd">    :math:`x_{i}` is the `input`.</span>

<span class="sd">    If `log_input` is True, use :math:`e^{x_{i}} - y_{i} x_{i}` instead of :math:`x_{i} - y_{i}\ln x_{i}`.</span>
<span class="sd">    When calculating logarithms, the lower bound of `input` is set to `eps` to avoid numerical errors.</span>

<span class="sd">    If `full` is False, the last term :math:`\ln{y_{i}!}` will be omitted,</span>
<span class="sd">    otherwise the last term will be approximated using Stirling formula:</span>

<span class="sd">    .. math::</span>
<span class="sd">        n! \approx \sqrt{2\pi n}\left( \frac{n}{e} \right)^{n}</span>

<span class="sd">    Note:</span>
<span class="sd">        Calculating the logarithm of a negative number or the exponent of a large positive number under Ascend</span>
<span class="sd">        will have a different range of return values and results different from those under GPU and CPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_input (bool, optional): Whether use log input. Default: ``True`` .</span>
<span class="sd">        full (bool, optional): Whether include the Stirling approximation term in the loss calculation.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        eps (float, optional): Lower bound of `input` when calculating logarithms. Default: ``1e-08`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input Tensor. The shape can be any number of dimensions.</span>
<span class="sd">        - **target** (Tensor) - The label Tensor which has the same shape as `input`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `input`.</span>
<span class="sd">        Otherwise it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `input` nor `target` is a tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `target` is not currently supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([[0.3, 0.7], [0.5, 0.5]])</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor([[1.0, 2.0], [3.0, 4.0]])</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.PoissonNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        0.3652635</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PoissonNLLLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoissonNLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_input</span> <span class="o">=</span> <span class="n">log_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">target</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;For &#39;PoissonNLLLoss&#39;, the inputs must be non-scalar, but got shapes: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;input: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, target: </span><span class="si">{</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_input</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="nb">input</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">target</span> <span class="o">*</span> <span class="p">((</span><span class="nb">input</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
            <span class="n">stirling_term</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">target</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span><span class="o">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="n">target</span> <span class="o">+</span> <span class="n">get_half_ln_2_pi</span><span class="p">())</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">F</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">stirling_term</span><span class="p">,</span> <span class="n">target</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">get_half_ln_2_pi</span><span class="p">():</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>


<div class="viewcode-block" id="MultiLabelSoftMarginLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.MultiLabelSoftMarginLoss.html#mindspore.nn.MultiLabelSoftMarginLoss">[docs]</a><span class="k">class</span> <span class="nc">MultiLabelSoftMarginLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the MultiLabelSoftMarginLoss.</span>
<span class="sd">    The multi-label soft margin loss is a commonly used loss function in multi-label classification tasks</span>
<span class="sd">    where an input sample can belong to multiple classes.</span>
<span class="sd">    Given an input :math:`x` and binary labels :math:`y` of size :math:`(N,C)`, where :math:`N` denotes</span>
<span class="sd">    the number of samples and :math:`C` denotes the number of classes.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{loss\left( x , y \right)} = - \frac{1}{N}\frac{1}{C}\sum_{i = 1}^{N}</span>
<span class="sd">        \sum_{j = 1}^{C}\left(y_{ij}\log\frac{1}{1 + e^{- x_{ij}}} + \left( 1 - y_{ij}</span>
<span class="sd">        \right)\log\frac{e^{-x_{ij}}}{1 + e^{-x_{ij}}} \right)</span>

<span class="sd">    where :math:`x{ij}` represents the predicted score of sample :math:`i` for class :math:`j`. :math:`y{ij}`</span>
<span class="sd">    represents the binary label of sample :math:`i` for class :math:`j`, where sample :math:`i` belongs to</span>
<span class="sd">    class :math:`j` if :math:`y{ij}=1` , and sample :math:`i` does not belong to class :math:`j` if :math:`y{ij}=0`.</span>
<span class="sd">    For a multi-label classification task, each sample may have multiple labels with a value of 1 in the binary</span>
<span class="sd">    label :math:`y`. `weight` will multiply to the loss of each class if given.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Union[Tensor, int, float]): The manual rescaling weight given to each class. Default: ``None`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A tensor of shape (N, C), where N is batch size and C is number</span>
<span class="sd">          of classes.</span>
<span class="sd">        - **target** (Tensor) - The label target Tensor which has the same shape as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the data type is the same as x, if the reduction is &#39;none&#39;, its shape is (N), otherwise it is zero.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the rank of `x` or `target` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([[0.3, 0.6, 0.6], [0.9, 0.4, 0.2]])</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor([[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]])</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MultiLabelSoftMarginLoss(reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; out = loss(x, target)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        0.84693956</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultiLabelSoftMarginLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiLabelSoftMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_soft_margin_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiMarginLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.MultiMarginLoss.html#mindspore.nn.MultiMarginLoss">[docs]</a><span class="k">class</span> <span class="nc">MultiMarginLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a criterion that optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span>
<span class="sd">    output :math:`y` (which is a 1D tensor of target class indices,</span>
<span class="sd">    :math:`0 \leq y \leq \text{x.size}(1)-1`):</span>

<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span>
<span class="sd">    output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`</span>
<span class="sd">    and :math:`i \neq y`.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Should be 1 or 2. Default: ``1`` .</span>
<span class="sd">        margin (float, optional): A parameter to change pairwise distance. Default: 1.0.</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        weight (Tensor, optional): The rescaling weight to each class with shape :math:`(C,)`. Data type only</span>
<span class="sd">            support float32, float16 or float64. Default: ``None`` , all classes are weighted equally.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input x, with shape :math:`(N, C)`. Data type only support float32, float16 or float64.</span>
<span class="sd">          x is :math:`x` in the above formula.</span>
<span class="sd">        - **target** (Tensor) - Ground truth labels, with shape :math:`(N,)`. Data type only support int64. The</span>
<span class="sd">          value of target should be non-negative, less than C. `target` is :math:`y` in the above formula.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, When `reduction` is &#39;none&#39;, the shape is :math:`(N,)`.</span>
<span class="sd">        Otherwise, it is a scalar. Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `p` or `target` is not int.</span>
<span class="sd">        TypeError: If dtype of `margin` is not float.</span>
<span class="sd">        TypeError: If dtype of `reduction` is not str.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float or float64.</span>
<span class="sd">        TypeError: If dtype of `weight` and `x` is not the same.</span>
<span class="sd">        ValueError: If &#39;p&#39; is not 1 or 2.</span>
<span class="sd">        ValueError: If &#39;reduction&#39; is not one of {&#39;none&#39;,&#39;sum&#39;,&#39;mean&#39;}.</span>
<span class="sd">        ValueError: If shape[0] of `x` is not equal to shape[0] of `target`.</span>
<span class="sd">        ValueError: If shape[1] of `x` is not equal to shape[0] of `weight`.</span>
<span class="sd">        ValueError: IF rank of `weight` is not 1.</span>
<span class="sd">        ValueError: If rank of `x` is not 2 or rank of &#39;target&#39; is not 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.ones(shape=[3, 3]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(np.array([1, 2, 1]), ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MultiMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6666667</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultiMarginLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multi_margin_loss</span> <span class="o">=</span> <span class="n">MultiMarginLossOp</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">weight_one</span> <span class="o">=</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">weight_one</span><span class="p">:</span>
            <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="BCELoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.BCELoss.html#mindspore.nn.BCELoss">[docs]</a><span class="k">class</span> <span class="nc">BCELoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    BCELoss creates a criterion to measure the binary cross entropy between the true labels and predicted labels.</span>

<span class="sd">    Set the predicted labels as :math:`x`, true labels as :math:`y`, the output loss as :math:`\ell(x, y)`.</span>
<span class="sd">    The formula is as follow:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    where N is the batch size. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        Note that the predicted labels should always be the output of sigmoid. Because it is a two-class</span>
<span class="sd">        classification, the true labels should be numbers between 0 and 1.</span>
<span class="sd">        And if input is either 0 or 1, one of the log terms would be mathematically undefined in the above loss</span>
<span class="sd">        equation.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            And it must have the same shape and data type as `inputs`. Default: ``None`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input tensor with shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions. The data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - The label tensor with shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions. The same shape and data type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `logits`. if `reduction` is &#39;none&#39;, then it has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits`, `labels` or `weight` (if given) is neither float16 not float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels` or `weight` (if given).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; weight = ms.Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 3.3, 2.2]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.BCELoss(weight=weight, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(np.array([[0.1, 0.2, 0.3], [0.5, 0.7, 0.9]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([[0, 1, 0], [0, 0, 1]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.8952923</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BCELoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BinaryCrossEntropy</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_one</span> <span class="o">=</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_one</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ones</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_one</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_reduced_shape_valid</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">,</span> <span class="n">reduced_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">arg_name1</span><span class="p">,</span> <span class="n">arg_name2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the reduced shape meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_reduce_shape</span><span class="p">(</span><span class="n">ori_shape</span><span class="p">,</span> <span class="n">reduced_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">,</span> <span class="n">arg_name1</span><span class="p">,</span> <span class="n">arg_name2</span><span class="p">)</span>


<div class="viewcode-block" id="CosineEmbeddingLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.CosineEmbeddingLoss.html#mindspore.nn.CosineEmbeddingLoss">[docs]</a><span class="k">class</span> <span class="nc">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    CosineEmbeddingLoss creates a criterion to measure the similarity between two tensors using cosine distance.</span>

<span class="sd">    Given two tensors :math:`x1`, :math:`x2`, and a Tensor label :math:`y` with values 1 or -1:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss(x_1, x_2, y) = \begin{cases}</span>
<span class="sd">        1-cos(x_1, x_2), &amp; \text{if } y = 1\\</span>
<span class="sd">        \max(0, cos(x_1, x_2)-margin), &amp; \text{if } y = -1\\</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float): Should be in [-1.0, 1.0]. Default: ``0.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits_x1** (Tensor) - Tensor of shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions.</span>
<span class="sd">        - **logits_x2** (Tensor) - Tensor of shape :math:`(N, *)`, same shape and dtype as `logits_x1`.</span>
<span class="sd">        - **labels** (Tensor) - Contains value 1 or -1. Suppose the shape of `logits_x1` is</span>
<span class="sd">          :math:`(x_1, x_2, x_3, ..., x_R)`, then the shape of `labels` must be :math:`(x_1, x_3, x_4, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &quot;none&quot;, its shape is the same as `labels`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `margin` is not a float.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        ValueError: If `margin` is not in range [-1, 1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; logits_x1 = ms.Tensor(np.array([[0.3, 0.8], [0.4, 0.3]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; logits_x2 = ms.Tensor(np.array([[0.4, 1.2], [-0.4, -0.9]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([1, -1]), ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; cosine_embedding_loss = nn.CosineEmbeddingLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = cosine_embedding_loss(logits_x1, logits_x2, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.0003425479</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CosineEmbeddingLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">margin</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_x1</span><span class="p">,</span> <span class="n">logits_x2</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits_x1&#39;</span><span class="p">,</span> <span class="n">logits_x1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits_x2&#39;</span><span class="p">,</span> <span class="n">logits_x2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">inner</span><span class="o">.</span><span class="n">same_type_shape_</span><span class="p">(</span><span class="n">logits_x1</span><span class="p">,</span> <span class="n">logits_x2</span><span class="p">)</span>
        <span class="n">_check_reduced_shape_valid</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">logits_x1</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="s2">&quot;logits_x1&quot;</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">)</span>
        <span class="c1"># if labels &gt; 0, 1-cosine(logits_x1, logits_x2)</span>
        <span class="c1"># else, max(0, cosine(logits_x1, logits_x2)-margin)</span>
        <span class="n">prod_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">logits_x1</span> <span class="o">*</span> <span class="n">logits_x2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">square1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">logits_x1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">square2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">logits_x2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">square1</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">square2</span><span class="p">)</span>
        <span class="n">cosine</span> <span class="o">=</span> <span class="n">prod_sum</span> <span class="o">/</span> <span class="n">denom</span>

        <span class="n">pos_value</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">cosine</span>
        <span class="n">neg_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">cosine</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="n">zeros</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">cosine</span><span class="p">)</span>
        <span class="n">pos_part</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">pos_value</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
        <span class="n">neg_part</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">neg_value</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
        <span class="n">output_unreduced</span> <span class="o">=</span> <span class="n">pos_part</span> <span class="o">+</span> <span class="n">neg_part</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">output_unreduced</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultilabelMarginLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.MultilabelMarginLoss.html#mindspore.nn.MultilabelMarginLoss">[docs]</a><span class="k">class</span> <span class="nc">MultilabelMarginLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a loss criterion that minimizes the hinge loss for multi-class</span>
<span class="sd">    classification tasks.</span>
<span class="sd">    It takes a 2D mini-batch Tensor :math:`x` as input and a 2D</span>
<span class="sd">    Tensor :math:`y` containing target class indices as output.</span>

<span class="sd">    Each sample in the mini-batch, the loss is computed as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and for all :math:`i` and :math:`j`, :math:`i` does not equal to :math:`y[j]`.</span>

<span class="sd">    Furthermore, both :math:`y` and :math:`x` should have identical sizes.</span>

<span class="sd">    Note:</span>
<span class="sd">        For this operator, only a contiguous sequence of non-negative targets that starts at</span>
<span class="sd">        the beginning is taken into consideration, which means that different samples can have different</span>
<span class="sd">        number of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Predict data. Tensor of shape :math:`(C)` or :math:`(N, C)`, where :math:`N`</span>
<span class="sd">          is the batch size and :math:`C` is the number of classes. Data type must be float16 or float32.</span>
<span class="sd">        - **target** (Tensor) - Ground truth data, with the same shape as `x`, data type must be int32 and</span>
<span class="sd">          label targets padded by -1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Union[Tensor, Scalar]) - The loss of MultilabelMarginLoss. If `reduction` is &quot;none&quot;, its shape</span>
<span class="sd">          is :math:`(N)`. Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `target` is not int32.</span>
<span class="sd">        ValueError: If length of shape of `x` is neither 1 nor 2.</span>
<span class="sd">        ValueError: If shape of `x` is not the same as `target`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MultilabelMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.array([[0.1, 0.2, 0.4, 0.8], [0.2, 0.3, 0.5, 0.7]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(np.array([[1, 2, 0, 3], [2, 3, -1, 1]]), ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, target)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.325</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultilabelMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multilabel_margin_loss</span> <span class="o">=</span> <span class="n">MultilabelMarginLossOp</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="BCEWithLogitsLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.BCEWithLogitsLoss.html#mindspore.nn.BCEWithLogitsLoss">[docs]</a><span class="k">class</span> <span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input logits, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the logits and the labels.</span>

<span class="sd">    Sets input `logits` as :math:`X`, input `labels` as :math:`Y`, output as :math:`L`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}}</span>

<span class="sd">    .. math::</span>
<span class="sd">        L_{ij} = -[Y_{ij} \cdot \log(p_{ij}) + (1 - Y_{ij}) \cdot \log(1 - p_{ij})]</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        weight (Tensor, optional): A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">            If not None, it can be broadcast to a tensor with shape of `logits`,</span>
<span class="sd">            data type must be float16 or float32. Default: ``None`` .</span>
<span class="sd">        pos_weight (Tensor, optional): A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">            number of classes. If not None, it must be broadcast to a tensor with shape of `logits`, data type</span>
<span class="sd">            must be float16 or float32. Default: ``None`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits with shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions. The data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth label with shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions. The same shape and data type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, its shape is the same as `logits`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `logits` or `labels` is not Tensor.</span>
<span class="sd">        TypeError: If data type of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `weight` or `pos_weight` is a parameter.</span>
<span class="sd">        TypeError: If data type of `weight` or `pos_weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of `reduction` is not string.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BCEWithLogitsLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCEWithLogitsLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bce_with_logits_loss</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;weight&#39; can not be a Parameter.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pos_weight</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pos_weight&#39; can not be a Parameter.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span> <span class="o">=</span> <span class="n">pos_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ones</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">ones_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">ones_input</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pos_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pos_weight</span> <span class="o">=</span> <span class="n">ones_input</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bce_with_logits_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_ndim</span><span class="p">(</span><span class="n">logits_nidm</span><span class="p">,</span> <span class="n">labels_ndim</span><span class="p">,</span> <span class="n">prime_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Internal function, used to check whether the dimension of logits and labels meets the requirements.&#39;&#39;&#39;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prime_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the&#39;</span> <span class="k">if</span> <span class="n">prime_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">logits_nidm</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">logits_nidm</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimensions of &#39;logits&#39; must be in [2, 4], but got &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;dimension of &#39;logits&#39; </span><span class="si">{</span><span class="n">logits_nidm</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">labels_ndim</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">labels_ndim</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimensions of &#39;labels&#39; must be in [2, 4], but got &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;dimension of &#39;labels&#39; </span><span class="si">{</span><span class="n">labels_ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">logits_nidm</span> <span class="o">!=</span> <span class="n">labels_ndim</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimensions of &#39;logits&#39; and &#39;labels&#39; must be equal, but got &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;dimension of &#39;logits&#39; </span><span class="si">{</span><span class="n">logits_nidm</span><span class="si">}</span><span class="s2"> and dimension of &#39;labels&#39; </span><span class="si">{</span><span class="n">labels_ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_channel_and_shape</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">prime_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Internal function, used to check whether the channels or shape of logits and labels meets the requirements.&#39;&#39;&#39;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prime_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the&#39;</span> <span class="k">if</span> <span class="n">prime_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">logits</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;logits&#39;.shape[1] cannot be one, but got </span><span class="si">{</span><span class="n">logits</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">labels</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> &#39;labels&#39;.shape[1] must be one or equal to &#39;logits&#39;.shape[1]: </span><span class="si">{</span><span class="n">logits</span><span class="si">}</span><span class="s2">, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">labels</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_dtype</span><span class="p">(</span><span class="n">labels_dtype</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the data type of labels meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="n">labels_dtype</span><span class="p">,</span>
                              <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>


<div class="viewcode-block" id="FocalLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.FocalLoss.html#mindspore.nn.FocalLoss">[docs]</a><span class="k">class</span> <span class="nc">FocalLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It is a loss function to solve the imbalance of categories and the difference of</span>
<span class="sd">    classification difficulty.</span>
<span class="sd">    The loss function proposed by Kaiming team in their paper</span>
<span class="sd">    `Focal Loss for Dense Object Detection &lt;https://arxiv.org/pdf/1708.02002.pdf&gt;`_ improves the</span>
<span class="sd">    effect of image object detection.</span>
<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        FL(p_t) = -(1-p_t)^\gamma \log(p_t)</span>

<span class="sd">    Args:</span>
<span class="sd">        gamma (float): Gamma is used to adjust the steepness of weight curve in focal loss. Default: ``2.0`` .</span>
<span class="sd">        weight (Union[Tensor, None]): A rescaling weight applied to the loss of each batch element. The dimension of</span>
<span class="sd">                                      weight should be 1. If None, no weight is applied. Default: ``None`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape should be :math:`(N, C)` or :math:`(N, C, H)` or :math:`(N, C, H, W)`.</span>
<span class="sd">          Where :math:`C` is the number of classes. Its value is greater than 1. If the shape is :math:`(N, C, H, W)`</span>
<span class="sd">          or :math:`(N, C, H)`, the :math:`H` or product of :math:`H` and :math:`W` should be the same as labels.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape should be :math:`(N, C)` or :math:`(N, C, H)` or :math:`(N, C, H, W)`.</span>
<span class="sd">          The value of :math:`C` is 1 or it needs to be the same as predict&#39;s :math:`C`. If :math:`C` is not 1,</span>
<span class="sd">          the shape of target should be the same as that of predict, where :math:`C` is the number of classes.</span>
<span class="sd">          If the shape is :math:`(N, C, H, W)` or :math:`(N, C, H)`, the :math:`H` or product of :math:`H`</span>
<span class="sd">          and :math:`W` should be the same as logits. The value of `labels` is should be in the</span>
<span class="sd">          range [-:math:`C`, :math:`C`). Where :math:`C` is the number of classes in logits.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &quot;none&quot;, its shape is the same as `logits`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `gamma` is not a float.</span>
<span class="sd">        TypeError: If `weight` is not a Tensor.</span>
<span class="sd">        ValueError: If `labels` dim is different from `logits`.</span>
<span class="sd">        ValueError: If `labels` channel is not 1 and `labels` shape is different from `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor([[0.8, 1.4], [0.5, 0.9], [1.2, 0.9]], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor([[1], [1], [0]], ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; focalloss = nn.FocalLoss(weight=Tensor([1, 2]), gamma=2.0, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = focalloss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.12516622</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FocalLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FocalLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;weight&#39; must be a Tensor, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">&#39;, the dimension of &#39;weight&#39; must be 1, but got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gather_d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logsoftmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">labelss</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="n">_check_ndim</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">labelss</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_channel_and_shape</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">labelss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">labelss</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">logits</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">labelss</span> <span class="o">=</span> <span class="n">labelss</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">labelss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labelss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">labelss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">labelss</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">log_probability</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logsoftmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">log_probability</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_d</span><span class="p">(</span><span class="n">log_probability</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">labelss</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
            <span class="n">log_probability</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">log_probability</span><span class="p">)</span>

        <span class="n">probability</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_probability</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">convert_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">convert_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">convert_weight</span><span class="p">,</span> <span class="p">(</span><span class="n">labelss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">labelss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">convert_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_d</span><span class="p">(</span><span class="n">convert_weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">labelss</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
                <span class="n">convert_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">convert_weight</span><span class="p">)</span>
            <span class="n">log_probability</span> <span class="o">=</span> <span class="n">log_probability</span> <span class="o">*</span> <span class="n">convert_weight</span>

        <span class="n">weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pows</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">probability</span> <span class="o">+</span> <span class="mf">1.0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">log_probability</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">labelss</span> <span class="o">*</span> <span class="n">log_probability</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span></div>


<div class="viewcode-block" id="HuberLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.HuberLoss.html#mindspore.nn.HuberLoss">[docs]</a><span class="k">class</span> <span class="nc">HuberLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    HuberLoss calculate the error between the predicted value and the target value.</span>
<span class="sd">    It has the advantages of both L1Loss and MSELoss.</span>

<span class="sd">    Assuming that the :math:`x` and :math:`y` are 1-D Tensor, length :math:`N`, then calculate the loss of :math:`x` and</span>
<span class="sd">    :math:`y` without dimensionality reduction (the reduction parameter is set to &quot;none&quot;). The formula is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">            0.5 * (x_n - y_n)^2, &amp; \text{if } |x_n - y_n| &lt; delta; \\</span>
<span class="sd">            delta * (|x_n - y_n| - 0.5 * delta), &amp; \text{otherwise. }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`N` is the batch size. If `reduction` is not &quot;none&quot;, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{&quot;mean&quot;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&quot;sum&quot;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        delta (Union[int, float]): The threshold to change between two type of loss.</span>
<span class="sd">            The value must be positive. Default: ``1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Predicted value, Tensor of any dimension. The data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Target value, same dtype and shape as the `logits` in common cases.</span>
<span class="sd">          However, it supports the shape of `logits` is different from the shape of `labels`</span>
<span class="sd">          and they should be broadcasted to each other.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &quot;none&quot;, return a Tensor with same shape and dtype as `logits`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of `logits` or `labels` are not the same.</span>
<span class="sd">        TypeError: If dtype of `delta` is neither float nor int.</span>
<span class="sd">        ValueError: If `delta` is less than or equal to 0.</span>
<span class="sd">        ValueError: If `reduction` is not one of &quot;none&quot;, &quot;mean&quot;, &quot;sum&quot;.</span>
<span class="sd">        ValueError: If `logits` and `labels` have different shapes and cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: logits.shape = labels.shape = (3,)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.HuberLoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(np.array([1, 2, 3]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([1, 2, 2]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.16666667</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: logits.shape = (3,), labels.shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.HuberLoss(reduction=&quot;none&quot;)</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(np.array([1, 2, 3]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([[1, 1, 1], [1, 2, 2]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.  0.5 1.5]</span>
<span class="sd">         [0.  0.  0.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize HuberLoss.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HuberLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">huber_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">)</span></div>


<div class="viewcode-block" id="TripletMarginLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.TripletMarginLoss.html#mindspore.nn.TripletMarginLoss">[docs]</a><span class="k">class</span> <span class="nc">TripletMarginLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TripletMarginLoss operation.</span>

<span class="sd">    Triple loss is used to measure the relative similarity between samples,</span>
<span class="sd">    which is measured by a triplet and a :math:`margin` with a value greater than :math:`0` .</span>
<span class="sd">    The triplet is composed by :math:`a`, :math:`p`, :math:`n` in the following formula.</span>

<span class="sd">    The shapes of all input tensors should be :math:`(N, *)` , where :math:`N` is batch size</span>
<span class="sd">    and :math:`*` means any number of additional dimensions.</span>

<span class="sd">    The distance swap is described in detail in the paper</span>
<span class="sd">    `Learning local feature descriptors with triplets and shallow convolutional neural</span>
<span class="sd">    networks &lt;http://158.109.8.37/files/BRP2016.pdf&gt;`_</span>
<span class="sd">    by V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): The degree of norm for pairwise distance. Default: ``2`` .</span>
<span class="sd">        eps (float, optional): Add small value to avoid division by zero. Default: ``1e-06`` .</span>
<span class="sd">        swap (bool, optional): The distance swap change the negative distance to the distance between positive</span>
<span class="sd">            sample and negative sample. Default: ``False`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        margin (Union[Tensor, float]) - Make a margin between the positive pair and the negative pair.</span>
<span class="sd">            Default: ``1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A sample randomly selected from the training set. Data type must be BasicType.</span>
<span class="sd">          :math:`a` in the above formula.</span>
<span class="sd">        - **positive** (Tensor) - A sample belonging to the same category as `x`, with the same type and</span>
<span class="sd">          shape as `x`. :math:`p` in the above formula.</span>
<span class="sd">        - **negative** (Tensor) - A sample belonging to the different class from `x`, with the same type and shape</span>
<span class="sd">          as `x`. :math:`n` in the above formula.</span>
<span class="sd">        - **margin** (Union[Tensor, float]) - Make a margin between the positive pair and the negative pair.</span>
<span class="sd">          Default: ``1.0`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor. If `reduction` is &quot;none&quot;, its shape is :math:`(N)`. Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `positive` or &#39;negative&#39; is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `positive` and `negative` is not the same.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `swap` is not a bool.</span>
<span class="sd">        ValueError: If dimensions of input `x`, `positive` and `negative` are less than or equal to 1 at the same time.</span>
<span class="sd">        ValueError: If the dimension of input `x` or `positive` or `negative` is bigger than or equal to 8.</span>
<span class="sd">        ValueError: If length of shape of `margin` is not 0.</span>
<span class="sd">        ValueError: If shape of `x`, `positive` and `negative` cannot broadcast.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.TripletMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; positive = ms.Tensor(np.array([[0.4, 0.6], [0.4, 0.6]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; negative = ms.Tensor(np.array([[0.2, 0.9], [0.3, 0.7]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, positive, negative)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.8881968</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">margin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span>
                                     <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_nll_loss_inputs</span><span class="p">(</span><span class="n">logits_shape</span><span class="p">,</span> <span class="n">label_shape</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">,</span> <span class="n">label_dtype</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the shape of logits and labels meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>

    <span class="n">logits_shape_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">*</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the&#39;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">logits_shape_new</span> <span class="o">!=</span> <span class="n">label_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> shape of &#39;logits&#39; should be (N, C, d_0, d_1, ...), &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;and the shape of &#39;labels&#39; should be (N, d_0, d_1, ...), &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but get &#39;logits&#39; shape: </span><span class="si">{</span><span class="n">logits_shape</span><span class="si">}</span><span class="s2"> and &#39;labels&#39; shape: </span><span class="si">{</span><span class="n">label_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="NLLLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.NLLLoss.html#mindspore.nn.NLLLoss">[docs]</a><span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between logits and labels.</span>

<span class="sd">    The nll loss with :math:`reduction = none` can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot \mathbb{1}\{c \not= \text{ignore_index}\}</span>

<span class="sd">    where :math:`x` is the logits, :math:`t` is the labels, :math:`w` is the weight,</span>
<span class="sd">    :math:`N` is the batch size, :math:`c` belonging to :math:`[0, C-1]` is class index,</span>
<span class="sd">    where :math:`C` is the number of classes.</span>

<span class="sd">    If `reduction` is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;, } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor): The rescaling weight to each class. If the value is not None, the shape is :math:`(C,)`.</span>
<span class="sd">            The data type only supports float32 or float16. Default: ``None`` .</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored (typically for padding value)</span>
<span class="sd">            and does not contribute to the gradient. Default: ``-100`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(N, C)`</span>
<span class="sd">          or :math:`(N, C, d_1, d_2, ..., d_K)` for :math:`K`-dimensional data, where `C = number of classes`.</span>
<span class="sd">          Data type must be float16 or float32. `inputs` needs to be logarithmic probability.</span>
<span class="sd">        - **labels** (Tensor) -:math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` for :math:`K`-dimensional data.</span>
<span class="sd">          Data type must be int32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed negative log likelihood loss value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `weight` is not a Tensor.</span>
<span class="sd">        TypeError: If `ignore_index` is not an int.</span>
<span class="sd">        TypeError: If the data type of `weight` is not float16 or float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        TypeError: If `logits` is not a Tensor.</span>
<span class="sd">        TypeError: If `labels` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([1, 0, 4]), ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_cross_entropy_inputs</span><span class="p">(</span><span class="n">logits_shape</span><span class="p">,</span> <span class="n">label_shape</span><span class="p">,</span>
                                <span class="n">logits_rank</span><span class="p">,</span> <span class="n">label_rank</span><span class="p">,</span>
                                <span class="n">logits_dtype</span><span class="p">,</span> <span class="n">label_dtype</span><span class="p">,</span>
                                <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the shape of logits and labels meets the requirements.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>

    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the&#39;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="n">logits_rank</span> <span class="o">==</span> <span class="n">label_rank</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logits_shape</span> <span class="o">!=</span> <span class="n">label_shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> shape of &#39;logits&#39; should be (N, C, d_0, d_1, ...), &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;and the shape of &#39;labels&#39; should be (N, C, d_0, d_1, ...), &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but get &#39;logits&#39; shape: </span><span class="si">{</span><span class="n">logits_shape</span><span class="si">}</span><span class="s2"> and &#39;labels&#39; shape: </span><span class="si">{</span><span class="n">label_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">label_rank</span> <span class="o">==</span> <span class="n">logits_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logits_rank</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">logits_shape_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">*</span><span class="n">logits_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
            <span class="k">if</span> <span class="n">logits_shape_new</span> <span class="o">!=</span> <span class="n">label_shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> shape of &#39;logits&#39; should be (N, C, d_0, d_1, ...), &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;and the shape of &#39;labels&#39; should be (N, d_0, d_1, ...), &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but get &#39;logits&#39; shape: </span><span class="si">{</span><span class="n">logits_shape</span><span class="si">}</span><span class="s2"> and &#39;labels&#39; shape: </span><span class="si">{</span><span class="n">label_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> rank of &#39;logits&#39; and &#39;labels&#39; should be:</span><span class="se">\n</span><span class="s2">&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;1. &#39;logits.ndim == labels.ndim&#39; for probabilities, </span><span class="se">\n</span><span class="s2">&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;2. &#39;logits.ndim - 1 == labels.ndim&#39; for class indices, </span><span class="se">\n</span><span class="s2">&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but get &#39;logits&#39; rank: </span><span class="si">{</span><span class="n">logits_rank</span><span class="si">}</span><span class="s2"> and &#39;labels&#39; rank: </span><span class="si">{</span><span class="n">label_rank</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_cross_entropy_ignore_index_warning</span><span class="p">(</span><span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to warning when ignore_index &gt; 0 for probabilities.&quot;&quot;&quot;</span>
    <span class="n">log</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, &#39;ignore_index&#39; does not work when &#39;labels&#39; is Probability.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="CrossEntropyLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.CrossEntropyLoss.html#mindspore.nn.CrossEntropyLoss">[docs]</a><span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The cross entropy loss between input and target.</span>

<span class="sd">    The CrossEntropyLoss support two kind of targets:</span>

<span class="sd">    - Class indices (int) in the range :math:`[0, C)` where :math:`C` is the number of classes,</span>
<span class="sd">      the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}</span>
<span class="sd">          \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">      N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore_index}\}} l_n, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    - Probabilities (float) for each class, useful when labels beyond a single class per minibatch item</span>
<span class="sd">      are required, the loss with reduction=none can be described as:</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}</span>

<span class="sd">      where :math:`x` is the inputs, :math:`t` is the target, :math:`w` is the weight,</span>
<span class="sd">      N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">      If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">      .. math::</span>

<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \frac{\sum_{n=1}^N l_n}{N}, &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;mean&#39;,}\\</span>
<span class="sd">              \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">              \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">              \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor): The rescaling weight to each class. If the value is not None, the shape is :math:`(C,)`.</span>
<span class="sd">            The data type only supports float32 or float16. Default: ``None`` .</span>
<span class="sd">        ignore_index (int): Specifies a target value that is ignored (typically for padding value)</span>
<span class="sd">            and does not contribute to the gradient. Default: ``-100`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        label_smoothing (float): Label smoothing values, a regularization tool used to prevent the model</span>
<span class="sd">            from overfitting when calculating Loss. The value range is [0.0, 1.0]. Default value: ``0.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(C,)` :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)`,</span>
<span class="sd">          where `C = number of classes`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - For class indices, tensor of shape :math:`()`, :math:`(N)` or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` , data type must be int32.</span>
<span class="sd">          For probabilities, tensor of shape :math:`(C,)` :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` ,</span>
<span class="sd">          data type must be float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the computed cross entropy loss value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `weight` is not a Tensor.</span>
<span class="sd">        TypeError: If `ignore_index` is not an int.</span>
<span class="sd">        TypeError: If the data type of `weight` is not float16 or float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>
<span class="sd">        TypeError: If `label_smoothing` is not a float.</span>
<span class="sd">        TypeError: If `logits` is not a Tensor.</span>
<span class="sd">        TypeError: If `labels` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; # Case 1: Indices labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(np.array([1, 0, 4]), ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.CrossEntropyLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(inputs, target)</span>
<span class="sd">        &gt;&gt;&gt; # Case 2: Probability labels</span>
<span class="sd">        &gt;&gt;&gt; inputs = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(np.random.randn(3, 5), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.CrossEntropyLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(inputs, target)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;label_smoothing&#39;</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">label_smoothing</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;label_smoothing&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_smoothing</span> <span class="o">=</span> <span class="n">label_smoothing</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_cross_entropy_inputs</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                    <span class="n">logits</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span>
                                    <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logits</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">labels</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">_cross_entropy_ignore_index_warning</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">)</span></div>


<div class="viewcode-block" id="KLDivLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.KLDivLoss.html#mindspore.nn.KLDivLoss">[docs]</a><span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the `logits` and the `labels`.</span>

<span class="sd">    For tensors of the same shape :math:`x` and :math:`target`,</span>
<span class="sd">    the updating formulas of KLDivLoss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(x, target) = target \cdot (\log target - x)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, target) = \begin{cases}</span>
<span class="sd">        L(x, target), &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L(x, target)), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)) / x.\operatorname{shape}[0], &amp; \text{if reduction} = \text{&#39;batchmean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `logits`,</span>
<span class="sd">    :math:`target` represents `labels`, and</span>
<span class="sd">    :math:`\ell(x, target)` represents `output`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Currently it does not support float64 input on `Ascend`.</span>
<span class="sd">        - The output aligns with the mathematical definition of Kullback-Leibler divergence</span>
<span class="sd">          only when `reduction` is set to &#39;batchmean&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output. Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - On Ascend, the value of `reduction` must be one of ``&#39;batchmean&#39;`` , ``&#39;none&#39;`` or ``&#39;sum&#39;`` .</span>
<span class="sd">            - On GPU, the value of `reduction` must be one of ``&#39;mean&#39;`` , ``&#39;none&#39;`` or ``&#39;sum&#39;`` .</span>
<span class="sd">            - On CPU, the value of `reduction` must be one of ``&#39;mean&#39;`` , ``&#39;batchmean&#39;`` , ``&#39;none&#39;`` or ``&#39;sum&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input Tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels** (Tensor) - The label Tensor which has the same shape and data type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, then output is a tensor and has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `logits` nor `labels` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is not currently supported.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        RuntimeError: If `logits` or `labels` is a scalar when `reduction` is &#39;batchmean&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(np.array([0.2, 0.7, 0.1]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(np.array([0., 1., 0.]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.KLDivLoss(reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.23333333</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_ctcloss_targets_shape</span><span class="p">(</span><span class="n">targets</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Internal function, used to check whether the shape of CTC targets meets the requirements.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">targets</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For CTCLoss, when the shape of log_probs is (T, C), the dimension of targets should&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;be 1 or 2, but got </span><span class="si">{</span><span class="n">targets</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">targets</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For CTCLoss, the first dimension of 2-D targets should be 1,&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="CTCLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.CTCLoss.html#mindspore.nn.CTCLoss">[docs]</a><span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss. It&#39;s mainly used to calculate the loss between</span>
<span class="sd">    the continuous, unsegemented time series and the target series.</span>

<span class="sd">    For the CTC algorithm, refer to `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int, optional): The blank label. Default: ``0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">        zero_infinity (bool, optional): If loss is infinite, this parameter determines whether to set that loss</span>
<span class="sd">            and its correlated gradient to zero. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **log_probs** (Tensor) - A tensor of shape :math:`(T, N, C)` or :math:`(T, C)`, where T is length of input,</span>
<span class="sd">          N is size of the batch and C is the number of classes. T, N and C are positive integers.</span>
<span class="sd">        - **targets** (Tensor) - A tensor of shape :math:`(N, S)` or (sum( `target_lengths` )),</span>
<span class="sd">          where S is max target length, means the target sequences.</span>
<span class="sd">        - **input_lengths** (Union[tuple, Tensor]) - A tuple or Tensor of shape(N). It means the lengths of the input.</span>
<span class="sd">        - **target_lengths** (Union[tuple, Tensor]) - A tuple or Tensor of shape(N). It means the lengths of the target.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **neg_log_likelihood** (Tensor) - A loss value which is differentiable with respect to each input node.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `log_probs` or `targets` is not a Tensor.</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, `reduction` is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        ValueError: If `reduction` is not &quot;none&quot;, &quot;mean&quot; or &quot;sum&quot;.</span>
<span class="sd">        ValueError: If the value of `blank` is not in range [0, C). C is number of classes of `log_probs` .</span>
<span class="sd">        ValueError: If the shape of `log_probs` is :math:`(T, C)`, the dimension of `targets` is not 1 or 2.</span>
<span class="sd">        ValueError: If the shape of `log_probs` is :math:`(T, C)`, the first dimension of 2-D `target` is not 1.</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than T. T is length of `log_probs` .</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; T = 5      # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 2      # Number of classes</span>
<span class="sd">        &gt;&gt;&gt; N = 2      # Batch size</span>
<span class="sd">        &gt;&gt;&gt; S = 3      # Target sequence length of longest target in batch (padding length)</span>
<span class="sd">        &gt;&gt;&gt; S_min = 2  # Minimum target length, for demonstration purposes</span>
<span class="sd">        &gt;&gt;&gt; arr = np.arange(T*N*C).reshape((T, N, C))</span>
<span class="sd">        &gt;&gt;&gt; ms_input = ms.Tensor(arr, dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = np.full(shape=(N), fill_value=T)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = ms.Tensor(input_lengths, dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = np.full(shape=(N), fill_value=S_min)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = ms.Tensor(target_lengths, dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; target = np.random.randint(1, C, size=(N, S))</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(target, dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss(blank=0, reduction=&#39;none&#39;, zero_infinity=False)</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(ms_input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [-45.79497  -55.794968]</span>
<span class="sd">        &gt;&gt;&gt; arr = np.arange(T*C).reshape((T, C))</span>
<span class="sd">        &gt;&gt;&gt; ms_input = ms.Tensor(arr, dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = ms.Tensor([T], dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = ms.Tensor([S_min], dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; target = np.random.randint(1, C, size=(S_min,))</span>
<span class="sd">        &gt;&gt;&gt; target = ms.Tensor(target, dtype=ms.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss(blank=0, reduction=&#39;none&#39;, zero_infinity=False)</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(ms_input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -25.794968</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blank</span> <span class="o">=</span> <span class="n">blank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span> <span class="o">=</span> <span class="n">zero_infinity</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;log_probs&#39;</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;targets&#39;</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">_check_ctcloss_targets_shape</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">targets</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">log_probs</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">neg_log_hood</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">blank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">neg_log_hood</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">neg_log_hood</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">blank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
                                     <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">neg_log_hood</span></div>


<div class="viewcode-block" id="GaussianNLLLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.GaussianNLLLoss.html#mindspore.nn.GaussianNLLLoss">[docs]</a><span class="k">class</span> <span class="nc">GaussianNLLLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian negative log likelihood loss.</span>

<span class="sd">    The target values are considered to be samples from a Gaussian distribution, where the expectation and variance are</span>
<span class="sd">    predicted by a neural network. For `labels` modeled on a Gaussian distribution, `logits` to record expectations,</span>
<span class="sd">    and the variance `var` (elements are all positive), the calculated loss is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var},</span>
<span class="sd">        \ \text{eps}\right)\right) + \frac{\left(\text{logits} - \text{labels}\right)^2}</span>
<span class="sd">        {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.}</span>

<span class="sd">    where :math:`eps` is used for stability of :math:`log`. When :math:`full=True`, a constant will be added to</span>
<span class="sd">    the loss. If the shape of :math:`var` and :math:`logits` are not the same (due to a homoscedastic assumption),</span>
<span class="sd">    their shapes must allow correct broadcasting.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        full (bool, optional): Whether include the constant term in the loss calculation. When :math:`full=True`,</span>
<span class="sd">            the constant term `const.` will be :math:`0.5 * log(2\pi)`. Default: ``False`` .</span>
<span class="sd">        eps (float, optional): Used to improve the stability of log function. Default: ``1e-6`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Tensor of shape :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(N, *)` or :math:`(*)`, same shape as the logits, or same shape</span>
<span class="sd">          as the logits but with one dimension equal to 1 (to allow for broadcasting).</span>
<span class="sd">        - **var** - Tensor of shape :math:`(N, *)` or :math:`(*)`, same shape as logits, or same shape as the logits</span>
<span class="sd">          but with one dimension equal to 1, or same shape as the logits but with one fewer dimension</span>
<span class="sd">          (to allow for broadcasting).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tensor scalar, the computed loss depending on :math:`reduction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` is not a Tensor.</span>
<span class="sd">        TypeError: If `labels` is not a Tensor.</span>
<span class="sd">        TypeError: If `full` is not a bool.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        ValueError: If `eps` is not a float within (0, inf).</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.arange(8).reshape((4, 2))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.array([2, 3, 1, 4, 6, 4, 4, 9]).reshape((4, 2))</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(arr1, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(arr2, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.GaussianNLLLoss(reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; var = ms.Tensor(np.ones((4, 1)), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels, var)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.4374993</span>

<span class="sd">    Reference:</span>
<span class="sd">        Nix, D. A. and Weigend, A. S., &quot;Estimating the mean and variance of the</span>
<span class="sd">        target probability distribution&quot;, Proceedings of 1994 IEEE International</span>
<span class="sd">        Conference on Neural Networks (ICNN&#39;94), Orlando, FL, USA, 1994, pp. 55-60</span>
<span class="sd">        vol.1, doi: 10.1109/ICNN.1994.374138.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianNLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="n">full</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="s1">&#39;gaussian_nll_loss&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">var</span><span class="p">):</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">gaussian_nll_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="HingeEmbeddingLoss"><a class="viewcode-back" href="../../../../api_python/nn/mindspore.nn.HingeEmbeddingLoss.html#mindspore.nn.HingeEmbeddingLoss">[docs]</a><span class="k">class</span> <span class="nc">HingeEmbeddingLoss</span><span class="p">(</span><span class="n">LossBase</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Hinge Embedding Loss value based on the input &#39;logits&#39; and&#39; labels&#39; (only including 1 or -1).</span>
<span class="sd">    Usually used to measure the similarity between two inputs.</span>

<span class="sd">    The loss function for :math:`n`-th sample in the mini-batch is</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        x_n, &amp; \text{if}\; y_n = 1,\\</span>
<span class="sd">        \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    and the total loss functions is</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`L = \{l_1,\dots,l_N\}^\top`.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, int): Threshold defined by Hinge Embedding Loss :math:`margin`.</span>
<span class="sd">            Represented as :math:`\Delta` in the formula. Default: ``1.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the outputelements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The predicted value, expressed as :math:`x` in the equation.</span>
<span class="sd">          Tensor of shape :math:`(*)` where :math:`*` means any number of dimensions.</span>
<span class="sd">        - **labels** (Tensor) - Label value, represented as :math:`y` in the equation.</span>
<span class="sd">          Same shape as the logits, contains -1 or 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tensor scalar, the computed loss depending on :math:`reduction`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` is not a Tensor.</span>
<span class="sd">        TypeError: If `labels` is not a Tensor.</span>
<span class="sd">        TypeError: If `margin` is not a float or int.</span>
<span class="sd">        ValueError: If `labels` does not have the same shape as `logits` or they could not broadcast to each other.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.array([0.9, -1.2, 2, 0.8, 3.9, 2, 1, 0, -1]).reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.array([1, 1, -1, 1, -1, 1, -1, 1, 1]).reshape((3, 3))</span>
<span class="sd">        &gt;&gt;&gt; logits = ms.Tensor(arr1, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = ms.Tensor(arr2, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.HingeEmbeddingLoss(reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.16666667</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HingeEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>