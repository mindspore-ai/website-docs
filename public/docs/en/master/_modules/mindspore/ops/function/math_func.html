

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.function.math_func &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/master/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.ops.function.math_func</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.ops.function.math_func</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Defines math operators with functional form.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">_primexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._inner_ops</span> <span class="kn">import</span> <span class="n">Cummin</span><span class="p">,</span> <span class="n">TileSize</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">STFT</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">Logit</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">LuUnpack</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">Roll</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">Ormqr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.array_ops</span> <span class="kn">import</span> <span class="n">MatrixSetDiagV3</span><span class="p">,</span> <span class="n">Transpose</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">layer</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">check_is_number</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Bernoulli</span><span class="p">,</span>
    <span class="n">BesselI0</span><span class="p">,</span>
    <span class="n">BesselI1</span><span class="p">,</span>
    <span class="n">BesselJ0</span><span class="p">,</span>
    <span class="n">BesselJ1</span><span class="p">,</span>
    <span class="n">BesselK0</span><span class="p">,</span>
    <span class="n">BesselK0e</span><span class="p">,</span>
    <span class="n">BesselY0</span><span class="p">,</span>
    <span class="n">BesselY1</span><span class="p">,</span>
    <span class="n">BesselK1</span><span class="p">,</span>
    <span class="n">BesselK1e</span><span class="p">,</span>
    <span class="n">LuSolve</span><span class="p">,</span>
    <span class="n">MatrixExp</span><span class="p">,</span>
    <span class="n">MatrixSolve</span><span class="p">,</span>
    <span class="n">Median</span><span class="p">,</span>
    <span class="n">Fmax</span><span class="p">,</span>
    <span class="n">Orgqr</span><span class="p">,</span>
    <span class="n">Fmin</span><span class="p">,</span>
    <span class="n">Renorm</span><span class="p">,</span>
    <span class="n">Hypot</span><span class="p">,</span>
    <span class="n">Heaviside</span><span class="p">,</span>
    <span class="n">Lcm</span><span class="p">,</span>
    <span class="n">Gcd</span><span class="p">,</span>
    <span class="n">Sinc</span><span class="p">,</span>
    <span class="n">Quantile</span><span class="p">,</span>
    <span class="n">NanToNum</span><span class="p">,</span>
    <span class="n">SparseSegmentMean</span><span class="p">,</span>
    <span class="n">TrilIndices</span><span class="p">,</span>
    <span class="n">TriuIndices</span><span class="p">,</span>
    <span class="n">InplaceIndexAdd</span><span class="p">,</span>
    <span class="n">InplaceUpdateV2</span><span class="p">,</span>
    <span class="n">Igamma</span><span class="p">,</span>
    <span class="n">Igammac</span><span class="p">,</span>
    <span class="n">Polar</span><span class="p">,</span>
    <span class="n">Angle</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">import</span> <span class="nn">mindspore.ops.function</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._sequence_ops</span> <span class="kn">import</span> <span class="n">TupleToTensor</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_make_tensor</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the tensor with value `val` and dtype `dtype`.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_x_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">i</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">,)</span>


<span class="c1">#####################################</span>
<span class="c1"># Public Operation Functions.</span>
<span class="c1">#####################################</span>
<span class="n">absolute_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()</span>
<span class="n">tensor_ceil</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ceil</span><span class="p">()</span>
<span class="n">tensor_add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
<span class="n">neg_tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>
<span class="n">tensor_sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
<span class="n">tensor_mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
<span class="n">tensor_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>
<span class="n">tensor_floordiv</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloorDiv</span><span class="p">()</span>
<span class="n">floordiv</span> <span class="o">=</span> <span class="n">tensor_floordiv</span>
<span class="n">xdivy_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Xdivy</span><span class="p">()</span>
<span class="n">tensor_pow</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">()</span>
<span class="n">pows</span> <span class="o">=</span> <span class="n">tensor_pow</span>
<span class="n">tensor_mod</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloorMod</span><span class="p">()</span>
<span class="n">floormod</span> <span class="o">=</span> <span class="n">tensor_mod</span>
<span class="n">tensor_exp</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">()</span>
<span class="n">tensor_expm1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Expm1</span><span class="p">()</span>
<span class="n">tensor_lt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span>
<span class="n">tensor_le</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span>
<span class="n">tensor_gt</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">()</span>
<span class="n">tensor_ge</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">()</span>
<span class="n">not_equal_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span>
<span class="n">transpose_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
<span class="n">cast_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

<span class="c1">#####################################</span>
<span class="c1"># Private Operation Functions.</span>
<span class="c1">#####################################</span>
<span class="n">addn_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AddN</span><span class="p">()</span>
<span class="n">angle_</span> <span class="o">=</span> <span class="n">Angle</span><span class="p">()</span>
<span class="n">log_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
<span class="n">floor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Floor</span><span class="p">()</span>
<span class="n">logical_not_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalNot</span><span class="p">()</span>
<span class="n">logical_or_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalOr</span><span class="p">()</span>
<span class="n">logical_and_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalAnd</span><span class="p">()</span>
<span class="n">sin_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sin</span><span class="p">()</span>
<span class="n">sinc_</span> <span class="o">=</span> <span class="n">Sinc</span><span class="p">()</span>
<span class="n">cos_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cos</span><span class="p">()</span>
<span class="n">tan_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tan</span><span class="p">()</span>
<span class="n">asin_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Asin</span><span class="p">()</span>
<span class="n">polar_</span> <span class="o">=</span> <span class="n">Polar</span><span class="p">()</span>
<span class="n">acos_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ACos</span><span class="p">()</span>
<span class="n">atan_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atan</span><span class="p">()</span>
<span class="n">sinh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sinh</span><span class="p">()</span>
<span class="n">cosh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cosh</span><span class="p">()</span>
<span class="n">tanh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
<span class="n">asinh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Asinh</span><span class="p">()</span>
<span class="n">acosh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Acosh</span><span class="p">()</span>
<span class="n">atanh_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Atanh</span><span class="p">()</span>
<span class="n">bitwise_and_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseAnd</span><span class="p">()</span>
<span class="n">bitwise_or_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseOr</span><span class="p">()</span>
<span class="n">bitwise_xor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BitwiseXor</span><span class="p">()</span>
<span class="n">inv_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">math_ops</span><span class="o">.</span><span class="n">Inv</span><span class="p">()</span>
<span class="n">invert_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Invert</span><span class="p">()</span>
<span class="n">erf_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erf</span><span class="p">()</span>
<span class="n">erfc_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Erfc</span><span class="p">()</span>
<span class="n">bessel_j1_</span> <span class="o">=</span> <span class="n">BesselJ1</span><span class="p">()</span>
<span class="n">bessel_j0_</span> <span class="o">=</span> <span class="n">BesselJ0</span><span class="p">()</span>
<span class="n">bessel_i0_</span> <span class="o">=</span> <span class="n">BesselI0</span><span class="p">()</span>
<span class="n">bessel_i0e_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BesselI0e</span><span class="p">()</span>
<span class="n">bessel_k0_</span> <span class="o">=</span> <span class="n">BesselK0</span><span class="p">()</span>
<span class="n">bessel_k0e_</span> <span class="o">=</span> <span class="n">BesselK0e</span><span class="p">()</span>
<span class="n">bessel_y0_</span> <span class="o">=</span> <span class="n">BesselY0</span><span class="p">()</span>
<span class="n">bessel_y1_</span> <span class="o">=</span> <span class="n">BesselY1</span><span class="p">()</span>
<span class="n">bessel_i1_</span> <span class="o">=</span> <span class="n">BesselI1</span><span class="p">()</span>
<span class="n">bessel_i1e_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BesselI1e</span><span class="p">()</span>
<span class="n">bessel_k1_</span> <span class="o">=</span> <span class="n">BesselK1</span><span class="p">()</span>
<span class="n">bessel_k1e_</span> <span class="o">=</span> <span class="n">BesselK1e</span><span class="p">()</span>
<span class="n">equal_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span>
<span class="n">isfinite_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsFinite</span><span class="p">()</span>
<span class="n">isnan_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">IsNan</span><span class="p">()</span>
<span class="n">maximum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Maximum</span><span class="p">()</span>
<span class="n">minimum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Minimum</span><span class="p">()</span>
<span class="n">lerp_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Lerp</span><span class="p">()</span>
<span class="n">tensor_round_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Round</span><span class="p">()</span>
<span class="n">linspace_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LinSpace</span><span class="p">()</span>
<span class="n">matrix_exp_</span> <span class="o">=</span> <span class="n">MatrixExp</span><span class="p">()</span>
<span class="n">exp2_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">()</span>
<span class="n">truncate_div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncateDiv</span><span class="p">()</span>
<span class="n">truncate_mod_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TruncateMod</span><span class="p">()</span>
<span class="n">sparse_segment_mean_</span> <span class="o">=</span> <span class="n">SparseSegmentMean</span><span class="p">()</span>
<span class="n">lu_unpack_</span> <span class="o">=</span> <span class="n">LuUnpack</span><span class="p">()</span>
<span class="n">xlogy_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Xlogy</span><span class="p">()</span>
<span class="n">square_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Square</span><span class="p">()</span>
<span class="n">sqrt_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sqrt</span><span class="p">()</span>
<span class="n">cumsum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">CumSum</span><span class="p">()</span>


<span class="c1">#####################################</span>
<span class="c1"># Element-wise Operation Functions.</span>
<span class="c1">#####################################</span>

<div class="viewcode-block" id="addn"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addn.html#mindspore.ops.addn">[docs]</a><span class="k">def</span> <span class="nf">addn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes addition of all input tensors element-wise.</span>

<span class="sd">    All input tensors must have the same shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union(tuple[Tensor], list[Tensor])): A tuple or list composed of Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as each Tensor of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is neither tuple nor list.</span>
<span class="sd">        ValueError: If there are Tensors with different shapes in `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addn([x, y, x, y])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10. 14. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">addn_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="abs"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.abs.html#mindspore.ops.abs">[docs]</a><span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns absolute value of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = |input_i|</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1.0, 1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.abs(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">absolute_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="absolute"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.absolute.html#mindspore.ops.absolute">[docs]</a><span class="k">def</span> <span class="nf">absolute</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.abs` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.add.html#mindspore.ops.add">[docs]</a><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds other value to input Tensor.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} + other_{i}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make</span>
<span class="sd">          the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `input` , `other` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, number.Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: x and y are both Tensor.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: x is a scalar and y is a Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # the data type of x is int32, the data type of y is float32,</span>
<span class="sd">        &gt;&gt;&gt; # and the output is the data format of higher precision float32.</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="addcdiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addcdiv.html#mindspore.ops.addcdiv">[docs]</a><span class="k">def</span> <span class="nf">addcdiv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise division of tensor tensor1 by tensor tensor2,</span>
<span class="sd">    multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[i] = input[i] + value[i] * (tensor1[i] / tensor2[i])</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The tensor to be added.</span>
<span class="sd">        tensor1 (Tensor): The numerator tensor.</span>
<span class="sd">        tensor2 (Tensor): The denominator tensor.</span>
<span class="sd">        value (Union[Tensor, Number]): The multiplier for tensor1/tensor2. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as tensor1/tensor2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `tensor1`, `tensor2`, `input` is not tensor.</span>
<span class="sd">        ValueError: If `tensor1` could not be broadcast to a tensor with shape of `tensor2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to tensors with shapes of `tensor1/tensor2`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to tensors with shapes of `value*(tensor1/tensor2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4, 3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.addcdiv(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.25      1.6666667 2.5       5.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Addcdiv</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">))</span></div>


<div class="viewcode-block" id="addcmul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addcmul.html#mindspore.ops.addcmul">[docs]</a><span class="k">def</span> <span class="nf">addcmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise product of tensor tensor1 and tensor tensor2,</span>
<span class="sd">    multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = input[i] + value[i] * (tensor1[i] * tensor2[i])</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The tensor to be added.</span>
<span class="sd">        tensor1 (Tensor): The tensor to be multiplied.</span>
<span class="sd">        tensor2 (Tensor): The tensor to be multiplied.</span>
<span class="sd">        value (Union[Tensor, Number]): The multiplier for tensor1*tensor2. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1*x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `tensor1`, `tensor2`, `input` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not one of: float32, float16, int32.</span>
<span class="sd">        TypeError: If dtype of `tensor1` or `tensor2` is not one of: float32, float16, int32.</span>
<span class="sd">        TypeError: If dtype of `value` is not one of: float32, float16, int32.</span>
<span class="sd">        ValueError: If `tensor1` could not be broadcast to a tensor with shape of `tensor2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to tensors with shapes of `tensor1` * `tensor2`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to tensors with shapes of `value*(tensor1*tensor2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.addcmul(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.  3.  4.]</span>
<span class="sd">         [ 3.  5.  7.]</span>
<span class="sd">         [ 4.  7. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Addcmul</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">value</span><span class="p">))</span></div>


<div class="viewcode-block" id="angle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.angle.html#mindspore.ops.angle">[docs]</a><span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise argument of a complex tensor.</span>
<span class="sd">    The elements in input are considered to be complex numbers of the form a+bj, where a is the real part and b</span>
<span class="sd">    is the imaginary part. The argument returned by this function is of the form :math:`atan2(b, a)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. types: complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the float32 or float64 type and the same shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of: complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([-1.5 + 7.8j, 3 + 5.75j], mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.angle(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.7607845 1.0899091]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">angle_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="bincount"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bincount.html#mindspore.ops.bincount">[docs]</a><span class="k">def</span> <span class="nf">bincount</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Counts the number of occurrences of each value in `input`.</span>

<span class="sd">    If you don&#39;t specify `minlength`, the length of output Tensor will be</span>
<span class="sd">    the maximum value of the input `input` plus one.</span>

<span class="sd">    If `minlength` is specified, the length of output Tensor is the value of maximum of `input` plus 1 and `minlength`.</span>

<span class="sd">    Each value in the output Tensor marks the number of occurrences of that index in `input`.</span>
<span class="sd">    If &#39;weights&#39; is specified, the output results are weighted, i.e ``out[n] += weight[i]`` instead of ``out[n] += 1``.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): 1-d input tensor.</span>
<span class="sd">        weights (Tensor, optional): Weights, a tensor of the same shape as `input`. Default: ``None`` .</span>
<span class="sd">        minlength (int, optional): A minimum number of bins for the output tensor. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor of shape [max(input)+1] if input is non-empty, otherwise, the shape is [0].</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `weights` is not a tensor.</span>
<span class="sd">        ValueError: If `input` is not one-dimensional, or if `input` and `weights` do not have the same shape.</span>
<span class="sd">        ValueError: If `input` contains negative value.</span>
<span class="sd">        ValueError: If `minlength` is a negative integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2, 4, 1, 0, 0], dtype=mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.bincount(x, minlength=7))</span>
<span class="sd">        [2. 1. 1. 0. 1. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; weights = Tensor([0, 0.25, 0.5, 0.75, 1], dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.bincount(x, weights=weights))</span>
<span class="sd">        [1.75 0.5  0.   0.   0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For math function &#39;bincount&#39;, &#39;input&#39; must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For math function &#39;bincount&#39;, &#39;weights&#39; must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">minlength</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">minlength</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For math function &#39;bincount&#39;, &#39;minlength&#39; must be int but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">minlength</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">rank_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">)()</span>
    <span class="k">if</span> <span class="n">rank_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For math function &#39;bincount&#39;, &#39;input&#39; should be one-dimensional tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">input</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bincount&#39;, elements of &#39;input&#39; should be non-negative.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">([])</span>
    <span class="k">if</span> <span class="n">minlength</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bincount&#39;, &#39;minlength&#39; should be &gt;= 0 but got </span><span class="si">{</span><span class="n">minlength</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">minlength</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">minlength</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length</span><span class="p">)</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">idx_mapping</span> <span class="o">=</span> <span class="n">equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;for bincount `input` and `weights` must have the same length&#39;</span><span class="p">)</span>
        <span class="n">idx_mapping</span> <span class="o">*=</span> <span class="n">weights</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()(</span><span class="n">idx_mapping</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span></div>


<div class="viewcode-block" id="bucketize"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bucketize.html#mindspore.ops.bucketize">[docs]</a><span class="k">def</span> <span class="nf">bucketize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">boundaries</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bucketizes `input` based on `boundaries`. If `right` is ``False``, the left boundary is closed. For each element x</span>
<span class="sd">    in `input`, the returned index satisfies the following rules:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{cases}</span>
<span class="sd">        boundaries[i-1] &lt; x &lt;= boundaries[i], &amp; \text{if right} = False\\</span>
<span class="sd">        boundaries[i-1] &lt;= x &lt; boundaries[i], &amp; \text{if right} = True</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A tensor containing the search value(s).</span>
<span class="sd">        boundaries (list): A sorted list of boundary values of the buckets.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        right (bool, optional): if ``False``, gets the lower bound index for each value in input from boundaries;</span>
<span class="sd">            If ``True``, gets the upper bound index instead. Default: ``False``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the indexes Tensor, with the same shape as the input, and data type is int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `boundaries` is not a list.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[3, 6, 9], [3, 6, 9]]))</span>
<span class="sd">        &gt;&gt;&gt; boundaries = list(np.array([1., 3., 5., 7., 9.]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bucketize(input, boundaries, right=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2 3 5]</span>
<span class="sd">         [2 3 5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">bucketize_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Bucketize</span><span class="p">)</span>
    <span class="n">epsilon_</span> <span class="o">=</span> <span class="mf">0.</span> <span class="k">if</span> <span class="n">right</span> <span class="k">else</span> <span class="mf">1.e-6</span>
    <span class="n">boundaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">boundary</span><span class="o">+</span><span class="n">epsilon_</span> <span class="k">for</span> <span class="n">boundary</span> <span class="ow">in</span> <span class="n">boundaries</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">bucketize_op</span><span class="p">(</span><span class="n">boundaries</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="exp2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.exp2.html#mindspore.ops.exp2">[docs]</a><span class="k">def</span> <span class="nf">exp2</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes base two exponential of Tensor `input` element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = 2^{input_i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.exp2(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4.  8. 16.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">tensor_2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">exp2_</span><span class="p">(</span><span class="n">tensor_2</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="argmin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.argmin.html#mindspore.ops.argmin">[docs]</a><span class="k">def</span> <span class="nf">argmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum value of a tensor across the axis.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        axis (Union[int, None], optional): Axis where the Argmin operation applies to. Default: ``None`` .</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor retains the specified</span>
<span class="sd">            dimension. Ignored if `axis` is None. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, indices of the min value of input tensor across the axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2.0, 3.1, 1.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = ops.argmin(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index)</span>
<span class="sd">        2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">is_axis_none</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">is_axis_none</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Argmin</span><span class="p">)(</span><span class="n">axis</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_axis_none</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="n">neg_tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()</span>


<div class="viewcode-block" id="neg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.neg.html#mindspore.ops.neg">[docs]</a><span class="k">def</span> <span class="nf">neg</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with negative values of the input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = - input_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, -1, 2, 0, -3.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.neg(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  -2.   1.  -2.   0.   3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="negative"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.negative.html#mindspore.ops.negative">[docs]</a><span class="k">def</span> <span class="nf">negative</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.neg` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="positive"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.positive.html#mindspore.ops.positive">[docs]</a><span class="k">def</span> <span class="nf">positive</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return self Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, self input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.positive(x))</span>
<span class="sd">        [ -5.    1.5   3.  100. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;positive&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span></div>


<div class="viewcode-block" id="numel"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.numel.html#mindspore.ops.numel">[docs]</a><span class="k">def</span> <span class="nf">numel</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Scalar of type int that represents the total number of elements in the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int. A scalar representing the total of elements in the Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.numel(input_x))</span>
<span class="sd">        4</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;numel&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span></div>


<div class="viewcode-block" id="permute"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.permute.html#mindspore.ops.permute">[docs]</a><span class="k">def</span> <span class="nf">permute</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input `axis` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor.</span>
<span class="sd">        axis (Union[tuple(int), int]): Permute will permute the tensor to the input `axis` order.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dimension as input tensor, with `axis` suitably permuted.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `axis` is None.</span>
<span class="sd">        ValueError: If the number of elements of `axis` is not equal to `input` ndim.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.permute(input_x, input_perm))</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">          [ 2.  5.]</span>
<span class="sd">          [ 3.  6.]]</span>
<span class="sd">         [[ 7. 10.]</span>
<span class="sd">          [ 8. 11.]</span>
<span class="sd">          [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">transpose_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="ceil"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ceil.html#mindspore.ops.ceil">[docs]</a><span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor up to the closest integer element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \lceil x_i \rceil = \lfloor x_i \rfloor + 1</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ceil(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  3. -1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_ceil</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.round.html#mindspore.ops.round">[docs]</a><span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns half to even of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i \approx input_i</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.round(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2.  2.  2. -4.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_round_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sub.html#mindspore.ops.sub">[docs]</a><span class="k">def</span> <span class="nf">sub</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts the second input tensor from the first input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} - other_{i}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` are not number.Number or bool or Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4, 5, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sub(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3 -3 -3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_sub</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="subtract"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.subtract.html#mindspore.ops.subtract">[docs]</a><span class="k">def</span> <span class="nf">subtract</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise subtract of input tensors.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = input[i] - alpha * other[i]</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number]): Tensor or Number involved in subtraction.</span>
<span class="sd">        other (Union[Tensor, number.Number]): Tensor or Number involved in subtraction.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        alpha (Number): The multiplier for :math:`other`. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: `input` or `other` is neither Tensor nor number.Number.</span>
<span class="sd">        TypeError: Both `input` and `other` are not Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; z = ops.subtract(input, y, alpha=1)</span>
<span class="sd">        &gt;&gt;&gt; print(z)</span>
<span class="sd">        [3. 3. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_sub</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="true_divide"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.true_divide.html#mindspore.ops.true_divide">[docs]</a><span class="k">def</span> <span class="nf">true_divide</span><span class="p">(</span><span class="n">dividend</span><span class="p">,</span> <span class="n">divisor</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.div` with :math:`rounding\_mode=None`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">div</span><span class="p">(</span><span class="n">dividend</span><span class="p">,</span> <span class="n">divisor</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></div>


<div class="viewcode-block" id="mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mul.html#mindspore.ops.mul">[docs]</a><span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} * other_{i}</span>
<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the</span>
<span class="sd">          data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, number.Number, bool.</span>
<span class="sd">        ValueError: If `input` and `other` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4. 10. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_mul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="multiply"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.multiply.html#mindspore.ops.multiply">[docs]</a><span class="k">def</span> <span class="nf">multiply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.asinh`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_mul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.div.html#mindspore.ops.div">[docs]</a><span class="k">def</span> <span class="nf">div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor in floating-point type element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time, and the shapes of them</span>
<span class="sd">          could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} / other_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        rounding_mode (str, optional): Type of rounding applied to the result. Default: ``None`` .</span>
<span class="sd">            Three types are defined as,</span>

<span class="sd">            - None: Default behavior, which is the same as true division in Python or `true_divide` in NumPy.</span>

<span class="sd">            - &quot;floor&quot;: Rounds the division of the inputs down, which is the same as floor division in Python</span>
<span class="sd">              or `floor_divide` in NumPy.</span>

<span class="sd">            - &quot;trunc&quot;: Rounds the division of the inputs towards zero, which is the same as C-style integer division.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `rounding_mode` value is not None, &quot;floor&quot; or &quot;trunc&quot;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.25 0.4 0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">rounding_mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">rounding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;floor&#39;</span><span class="p">,</span> <span class="s1">&#39;trunc&#39;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For ops.div, rounding_mode value should be None, &#39;floor&#39; or &#39;trunc&#39;.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rounding_mode</span> <span class="o">==</span> <span class="s1">&#39;floor&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">FloorDiv</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Div</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rounding_mode</span> <span class="o">==</span> <span class="s1">&#39;trunc&#39;</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Trunc</span><span class="p">)()(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="divide"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.divide.html#mindspore.ops.divide">[docs]</a><span class="k">def</span> <span class="nf">divide</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.div` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="n">rounding_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="float_power"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.float_power.html#mindspore.ops.float_power">[docs]</a><span class="k">def</span> <span class="nf">float_power</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes `input` to the power of the exponent.</span>
<span class="sd">    For the real number type, cast `input` and `exponent` to mindspore.float64 to calculate.</span>
<span class="sd">    Currently, complex type calculation is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a tensor or a number.</span>
<span class="sd">        exponent (Union[Tensor, Number]): The second input, if the first input is Tensor,</span>
<span class="sd">            the second input can be Number or Tensor. Otherwise, it must be a Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting. For the complex type,</span>
<span class="sd">        the return value type is the same as the input type. For the real number type,</span>
<span class="sd">        the return value type is mindspore.float64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `exponent` is a Tensor.</span>
<span class="sd">        TypeError: If the data type of `input` or `exponent` is not in Tensor and Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1.5, 0., 2.]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.float_power(input, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.25 0.   4.  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">exponent</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;At least one of the types of inputs must be tensor, &quot;</span> <span class="o">+</span> \
                        <span class="sa">f</span><span class="s2">&quot;but the type of &#39;input&#39; got is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span> <span class="o">+</span> \
                        <span class="sa">f</span><span class="s2">&quot;and the type of &#39;exponent&#39; is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">exponent</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The type of &#39;input&#39; must be Tensor or Number, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">exponent</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The type of &#39;exponent&#39; must be Tensor or Number, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">exponent</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_complex</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span> <span class="ow">or</span> \
            <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">exponent</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">is_complex</span><span class="p">(</span><span class="n">exponent</span><span class="p">))</span> <span class="ow">or</span> \
            <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">complex</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">exponent</span><span class="p">,</span> <span class="nb">complex</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
        <span class="n">exponent</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">exponent</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">exponent</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">exponent</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">pow</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor_div.html#mindspore.ops.floor_div">[docs]</a><span class="k">def</span> <span class="nf">floor_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and round down to the closest integer.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\text{floor}( \\frac{x_i}{y_i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1 -1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_floordiv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="fmod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fmod.html#mindspore.ops.fmod">[docs]</a><span class="k">def</span> <span class="nf">fmod</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the floating-point remainder of the division operation input/other.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out = input - n * other</span>

<span class="sd">    Where :math:`n` is :math:`input/other` with its fractional part truncated.</span>
<span class="sd">    The returned value has the same sign as `input` and is less than `other` in magnitude.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): the dividend.</span>
<span class="sd">        other (Union[Tensor, Number]): the divisor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-4., -3.5, 0, 3.5, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fmod(input, 2.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.5 -1.   0.   1.   1.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">))):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;At least one of the types of inputs must be tensor, &quot;</span> <span class="o">+</span> \
                        <span class="sa">f</span><span class="s2">&quot;but the type of &#39;input&#39; got is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span> <span class="o">+</span> \
                        <span class="sa">f</span><span class="s2">&quot;and the type of &#39;other&#39; is </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">rounding_mode</span><span class="o">=</span><span class="s2">&quot;trunc&quot;</span><span class="p">)</span> <span class="o">*</span> <span class="n">other</span></div>


<div class="viewcode-block" id="pow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pow.html#mindspore.ops.pow">[docs]</a><span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the `exponent` power of each element in `input`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} ^{ exponent_{i}}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `exponent` comply with the implicit type conversion rules to make the</span>
<span class="sd">          data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them can be broadcast.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        exponent (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `exponent` is not one of the following: Tensor, number.Number or bool.</span>
<span class="sd">        ValueError: If the shape of `input` and `exponent` are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 3.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  8. 64.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 64.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_pow</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_mod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor_mod.html#mindspore.ops.floor_mod">[docs]</a><span class="k">def</span> <span class="nf">floor_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of division element-wise. It&#39;s a flooring divide.</span>
<span class="sd">    E.g. :math:`floor(x / y) * y + mod(x, y) = x`.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\text{floor}(x_{i} // y_{i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Data of input `y` should not be 0, or the maximum value of its dtype will be returned.</span>
<span class="sd">        - When the elements of input exceeds 2048 , the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as :math:`(D1, D2 ..., Dn)`, then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision of the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.exp.html#mindspore.ops.exp">[docs]</a><span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = e^{x_i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.exp(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.718282  7.389056 54.598152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_exp</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="expm1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.expm1.html#mindspore.ops.expm1">[docs]</a><span class="k">def</span> <span class="nf">expm1</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential then minus 1 of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = e^{x_i} - 1</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.expm1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        1.718282  6.389056 53.598152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_expm1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log.html#mindspore.ops.log">[docs]</a><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of a tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \log_e(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator Log is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affacted.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64 on CPU.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32 on Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.6931472 1.3862944]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="logdet"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logdet.html#mindspore.ops.logdet">[docs]</a><span class="k">def</span> <span class="nf">logdet</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates log determinant of one or a batch of square matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of any dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the log determinant of `input`. If the matrix determinant is smaller than 0, nan will be returned. If</span>
<span class="sd">        the matrix determinant is 0, -inf will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float32, float64, Complex64 or Complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([[[8, 9], [1, 2]], [[5, 6], [3, 4]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logdet(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.9459091 0.6931454]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">det_x</span> <span class="o">=</span> <span class="n">det</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">log_</span><span class="p">(</span><span class="n">det_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor.html#mindspore.ops.floor">[docs]</a><span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor down to the closest integer element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \lfloor x_i \rfloor</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, its data type must be float16,</span>
<span class="sd">            float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not in [float16, float32, float64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2. -2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_floor</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Floor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_floor</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="i0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.i0.html#mindspore.ops.i0">[docs]</a><span class="k">def</span> <span class="nf">i0</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.bessel_i0` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i0</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="inplace_update"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_update.html#mindspore.ops.inplace_update">[docs]</a><span class="k">def</span> <span class="nf">inplace_update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates specified values in `x` to `v` according to `indices`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Note:</span>
<span class="sd">        `indices` can only be indexed along the highest dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor which to be inplace updated. It can be one of the following data types:</span>
<span class="sd">            float32, float16 and int32.</span>
<span class="sd">        v (Tensor): A tensor with the same type as `x` and the same dimension size as `x` except</span>
<span class="sd">            the first dimension, which must be the same as the size of `indices`.</span>
<span class="sd">        indices (Union[int, tuple[int], Tensor]): Determines which rows of `x` to update with `v`,</span>
<span class="sd">            should be several int. It is an int or tuple or tensor with one dimension,</span>
<span class="sd">            whose value is in [-x.shape[0], x.shape[0]).</span>
<span class="sd">            If it is a tuple or Tensor, the size of &#39;indices&#39; should be the same as the first dimension of &#39;v&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int nor tuple nor Tensor.</span>
<span class="sd">        TypeError: If `indices` is a tuple or Tensor, but its element is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inplace_update(x, v, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5 1. ]</span>
<span class="sd">         [1.  1.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inplace_update_inner</span> <span class="o">=</span> <span class="n">InplaceUpdateV2</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">inplace_update_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="inplace_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_add.html#mindspore.ops.inplace_add">[docs]</a><span class="k">def</span> <span class="nf">inplace_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds `v` into specified rows of `x`. Computes `y` = `x`; y[i,] += `v`.</span>

<span class="sd">    Note:</span>
<span class="sd">            `indices` refers to the left-most dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input is a tensor whose data type is float16, float32, float64 or int32.</span>
<span class="sd">        v (Tensor): The second input is a tensor that has the same dimension sizes as `x` except</span>
<span class="sd">            the first dimension, which must be the same as indices&#39; size. It has the same data type with `x`.</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of `x`</span>
<span class="sd">            to add with `v`. It is an integer or a tuple, whose value is in [0, the first dimension size of `x`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `indices` is a tuple whose elements are not all int.</span>
<span class="sd">        ValueError: If the rank of `x` is not equal to the rank of `v`.</span>
<span class="sd">        ValueError: If the length of `indices` is not equal to `v.shape[0]`.</span>
<span class="sd">        ValueError: If the values of `indices` are not in range of `[0, x.shape[0])`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inplace_add(x, input_v, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.5 3. ]</span>
<span class="sd">         [4.  5.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inplace_add_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InplaceAdd</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inplace_add_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="inplace_index_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_index_add.html#mindspore.ops.inplace_index_add">[docs]</a><span class="k">def</span> <span class="nf">inplace_index_add</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds Tensor `updates` to specified axis and indices of Tensor `var` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        var (Parameter): The input Parameter to add to, with data type uint8, int8, int16, int32,</span>
<span class="sd">            float16, float32, float64.</span>
<span class="sd">        indices (Tensor): The indies along `axis` to perform the addition. A 1D Tensor</span>
<span class="sd">            of shape :math:`(updates.shape[axis],)`, every value of it</span>
<span class="sd">            should be in range :math:`[0, var.shape[axis])` with data type int32.</span>
<span class="sd">        updates (Tensor): The input Tensor with the value to add. Must have same data type as `var`.</span>
<span class="sd">            The shape must be the same as `var` except the `axis` th dimension.</span>
<span class="sd">        axis (int): The dimension along which to index. It should be in range :math:`[0, len(var.dim))`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, updated result, has the same shape and dtype as `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var` is not a Parameter.</span>
<span class="sd">        TypeError: If neither `indices` nor `updates` is a Tensor.</span>
<span class="sd">        ValueError: If `axis` is out of valid range.</span>
<span class="sd">        ValueError: If `var` rank is not the same as `updates` rank.</span>
<span class="sd">        ValueError: If shape of `indices` is not :math:`(updates.shape[axis],)`.</span>
<span class="sd">        ValueError: If `updates`&#39;s shape is not the same as `var` except the `axis` th dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; var = Parameter(Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; var = ops.inplace_index_add(var, indices, updates, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(var)</span>
<span class="sd">        [[1.5 3. ]</span>
<span class="sd">         [4.  5.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">inplace_index_add_</span> <span class="o">=</span> <span class="n">InplaceIndexAdd</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inplace_index_add_</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="inplace_sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inplace_sub.html#mindspore.ops.inplace_sub">[docs]</a><span class="k">def</span> <span class="nf">inplace_sub</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Subtracts `v` into specified rows of `x`. Computes :math:`y = x`; :math:`y[i,] -= input\_v`.</span>

<span class="sd">    Note:</span>
<span class="sd">        `indices` refers to the left-most dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The first input is a tensor whose data type is float16, float32, float64 or int32.</span>
<span class="sd">            Tensors of arbitrary dimensions are supported.</span>
<span class="sd">        v (Tensor): The second input is a tensor who has the same dimension sizes as `x` except</span>
<span class="sd">            the first dimension, which must be the same as indices&#39; size. It has the same data type with `x`.</span>
<span class="sd">        indices (Union[int, tuple]): Indices into the left-most dimension of `x`, and determines which rows of `x`</span>
<span class="sd">            to subtract with `v`. It is an int or tuple, whose value is in [0, the first dimension size of `x`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `indices` is a tuple whose elements are not all int.</span>
<span class="sd">        ValueError: If the rank of `x` is not equal to the rank of `v`.</span>
<span class="sd">        ValueError: If the length of `indices` is not equal to `v.shape[0]`.</span>
<span class="sd">        ValueError: If the values of `indices` are not in range of `[0, x.shape[0])`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = (0, 1)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4], [5, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_v = Tensor(np.array([[0.5, 1.0], [1.0, 1.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inplace_sub(x, input_v, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5 1. ]</span>
<span class="sd">         [2.  2.5]</span>
<span class="sd">         [5.  6. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inplace_sub_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">InplaceSub</span><span class="p">)(</span><span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inplace_sub_inner</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_not"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_not.html#mindspore.ops.logical_not">[docs]</a><span class="k">def</span> <span class="nf">logical_not</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical NOT&quot; of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\neg input_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the `input`, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_not(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logical_not_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_or"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_or.html#mindspore.ops.logical_or">[docs]</a><span class="k">def</span> <span class="nf">logical_or</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical OR&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \\vee y_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        LogicalOr supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, bool]): The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">            converted to bool.</span>
<span class="sd">        other (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logical_or_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_and"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_and.html#mindspore.ops.logical_and">[docs]</a><span class="k">def</span> <span class="nf">logical_and</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical AND&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} \wedge other_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        LogicalAnd supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, bool]): The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">            converted to bool.</span>
<span class="sd">        other (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logical_and_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="sign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sign.html#mindspore.ops.sign">[docs]</a><span class="k">def</span> <span class="nf">sign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an element-wise indication of the sign of a number.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}_{i} = \begin{cases}</span>
<span class="sd">                          -1 &amp; \text{input}_{i} &lt; 0 \\</span>
<span class="sd">                           0 &amp; \text{input}_{i} = 0 \\</span>
<span class="sd">                           1 &amp; \text{input}_{i} &gt; 0</span>
<span class="sd">                         \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the sign of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[-1, 0, 2, 4, 6], [2, 3, 5, -6, 0]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sign(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1  0  1  1  1]</span>
<span class="sd">         [ 1  1  1 -1  0]]</span>
<span class="sd">        &gt;&gt;&gt; ms.set_context(device_target=&quot;CPU&quot;)</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([[-1, 0, float(&#39;inf&#39;), 4, float(&#39;nan&#39;)], [2, 3, float(&#39;-inf&#39;), -6, 0]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sign(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.  0.  1.  1.  0.]</span>
<span class="sd">         [ 1.  1. -1. -1.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For sign, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Sign</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="signbit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.signbit.html#mindspore.ops.signbit">[docs]</a><span class="k">def</span> <span class="nf">signbit</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine the symbol of each element. If the element value is less than 0,</span>
<span class="sd">    the corresponding output position is True; otherwise, it is False.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input value.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the signbit of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([0.3, 1.2, 0., -2.5])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.signbit(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For signbit, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="sgn"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sgn.html#mindspore.ops.sgn">[docs]</a><span class="k">def</span> <span class="nf">sgn</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extension of :func:`mindspore.ops.sign` in complex domain.</span>
<span class="sd">    For real number input, this function is the same as :func:`mindspore.ops.sign`.</span>
<span class="sd">    For complex input, this function is calculated according to the following formula.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}_{i} = \begin{cases}</span>
<span class="sd">                        0 &amp; |\text{input}_i| == 0 \\</span>
<span class="sd">                        \frac{{\text{input}_i}}{|{\text{input}_i}|} &amp; \text{otherwise}</span>
<span class="sd">                        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input value.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the sgn of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[3 + 4j, 7 - 24j, 0, 6 + 8j, 8], [15 + 20j, 7 - 24j, 0, 3 + 4j, 20]], dtype=ms.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sgn(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.6 +0.8j  0.28-0.96j 0.  +0.j   0.6 +0.8j  1.  +0.j  ]</span>
<span class="sd">         [0.6 +0.8j  0.28-0.96j 0.  +0.j   0.6 +0.8j  1.  +0.j  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For sgn, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">modulus</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ComplexAbs</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">zeros_mask</span> <span class="o">=</span> <span class="n">modulus</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">non_zero_modulus</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">modulus</span><span class="p">,</span> <span class="n">zeros_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">zeros_modulus</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">non_zero_modulus</span><span class="p">)</span>
    <span class="n">complex_modulus</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Complex</span><span class="p">()(</span><span class="n">non_zero_modulus</span><span class="p">,</span> <span class="n">zeros_modulus</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">/</span> <span class="n">complex_modulus</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sin.html#mindspore.ops.sin">[docs]</a><span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sin(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sin(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5810352 0.27635565 0.41687083 0.5810352]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sin_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinc.html#mindspore.ops.sinc">[docs]</a><span class="k">def</span> <span class="nf">sinc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the normalized sinc of input.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases} \frac{sin(\pi input_i)}{\pi input_i} &amp; input_i\neq 0\\</span>
<span class="sd">        1 &amp; input_i=0 \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`. The dtype of output is float32 when dtype of `input` is in</span>
<span class="sd">        [int, bool]. Otherwise output has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinc(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.47735003 0.8759357  0.7224278  0.47735003]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sinc_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cos.html#mindspore.ops.cos">[docs]</a><span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cosine of input element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \cos(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Supported dtypes are float16 and float32, and using float64 may</span>
<span class="sd">        cause a problem of missing precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.971338 0.6748758 0.95233357 0.9959527]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cos_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosine_similarity"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cosine_similarity.html#mindspore.ops.cosine_similarity">[docs]</a><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate cosine similarity between `x1` and `x2` along the axis, `dim`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, broadcast of input is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): The first input Tensor.</span>
<span class="sd">        x2 (Tensor): The second input Tensor.</span>
<span class="sd">        dim (int, optional): Axis for calculating cosine similarity. Default: ``1`` .</span>
<span class="sd">        eps (float, optional): Minimal value to avoid division by zero. Default: ``1e-08`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, cosine similarity between x1 and x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of x1 or x2 is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = ms.Tensor([[-0.0256, 0.0127, -0.2475, 0.2316, 0.8037],</span>
<span class="sd">        ...                 [0.5809, -1.2712, -0.7038, -0.2558, 0.7494]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = ms.Tensor([[-0.6115, -0.1965, -0.8484, 0.2389, 0.2409],</span>
<span class="sd">        ...                 [1.8940, -2.1997, 0.1915, 0.0856, 0.7542]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosine_similarity(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.4843164  0.81647635]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">molecule</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">molecule</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">_check_cov_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weights_name</span><span class="p">,</span> <span class="n">num_observations</span><span class="p">,</span> <span class="n">valid_type</span><span class="p">,</span> <span class="n">valid_type_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check cov weights valid&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For cov, the </span><span class="si">{</span><span class="n">weights_name</span><span class="si">}</span><span class="s2"> must have one or fewer dimensions, but got </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> dimensions.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For cov, the dtype of </span><span class="si">{</span><span class="n">weights_name</span><span class="si">}</span><span class="s2"> must be </span><span class="si">{</span><span class="n">valid_type_name</span><span class="si">}</span><span class="s2"> type, but got type </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">!=</span> <span class="n">num_observations</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For cov, the numel of </span><span class="si">{</span><span class="n">weights_name</span><span class="si">}</span><span class="s2"> must equal the number of columns of input, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;but got numel:</span><span class="si">{</span><span class="n">ops</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span><span class="si">}</span><span class="s2">, number of columns of input:</span><span class="si">{</span><span class="n">num_observations</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">_get_default_div_type</span><span class="p">(</span><span class="n">param</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;get the default type when div&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">param</span>
    <span class="k">return</span> <span class="n">param</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<div class="viewcode-block" id="cov"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cov.html#mindspore.ops.cov">[docs]</a><span class="k">def</span> <span class="nf">cov</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">correction</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fweights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">aweights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given the input and weights, returns the covariance matrix (the square matrix of the covariance of each pair of</span>
<span class="sd">    variables) of input, where the input row is the variable and the column is the observation value.</span>

<span class="sd">    The diagonal contains each variable and its own covariance. If input is a scalar or 1D vector of a single variable,</span>
<span class="sd">    its variance will be returned.</span>

<span class="sd">    The unbiased sample covariance of the variables :math:`a` and :math:`b` is given by the following formula:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{cov}_w(a,b) = \frac{\sum^{N}_{i = 1}(a_{i} - \bar{a})(b_{i} - \bar{b})}{N~-~1}</span>

<span class="sd">    where :math:`\bar{a}` and :math:`\bar{b}` are the simple means of the :math:`a` and :math:`b` respectively.</span>

<span class="sd">    If `fweights` and/or `aweights` are provided, the unbiased weighted covariance</span>
<span class="sd">    is calculated, which is given by:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{cov}_w(a,b) = \frac{\sum^{N}_{i = 1}w_i(a_{i} - \mu_a^*)(b_{i} - \mu_b^*)}{\sum^{N}_{i = 1}w_i~-~1}</span>

<span class="sd">    where :math:`w` denotes `fweights` or `aweights` based on whichever is provided, or</span>
<span class="sd">    :math:`w = fweights \times aweights` if both are provided, and</span>
<span class="sd">    :math:`\mu_x^* = \frac{\sum^{N}_{i = 1}w_ix_{i} }{\sum^{N}_{i = 1}w_i}` is the weighted mean of the variable.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The values of `fweights` and `aweights` cannot be negative, and the negative weight scene result is undefined.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Currently, complex number is not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A 2D matrix, or a scalar or 1D vector of a single variable</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        correction (int, optional): The difference between sample size and sample degrees of freedom.</span>
<span class="sd">            Defaults to Bessel&#39;s correction, `correction = 1` which returns the unbiased estimate,</span>
<span class="sd">            even if both `fweights` and `aweights` are specified. `correction = 0`</span>
<span class="sd">            will return the simple average. Default: ``1`` .</span>
<span class="sd">        fweights (Tensor, optional): Scalar or one-dimensional Tensor containing integer frequency weight, indicating</span>
<span class="sd">            the number of repetition of each observation vector. Its numel must equal the number of columns of `input`.</span>
<span class="sd">            Ignored if `None`. Default: ``None`` .</span>
<span class="sd">        aweights (Tensor, optional): A scalar or 1D Tensor containing float observation weights represents</span>
<span class="sd">            the importance of each observation vector. The higher the importance, the greater the corresponding value.</span>
<span class="sd">            Its numel must equal the number of columns of `input`. Must have floating point dtype. Ignored if `None`.</span>
<span class="sd">            Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The covariance matrix Tensor of `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the dimensions of input is greater than 2.</span>
<span class="sd">        ValueError: If the dimensions of fweights is greater than 1.</span>
<span class="sd">        ValueError: If the numel of fweights not equal the number of columns of input.</span>
<span class="sd">        ValueError: If the numel of aweights not equal the number of columns of input.</span>
<span class="sd">        ValueError: If the dimensions of aweights is greater than 1.</span>
<span class="sd">        TypeError: If the dtype of input is bool.</span>
<span class="sd">        TypeError: If the dtype of fweights is not an integer type.</span>
<span class="sd">        TypeError: If the dtype of aweights is not a floating point type.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([[0., 3.], [5., 5.], [7., 0.]]).T</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[0. 5. 7.]</span>
<span class="sd">         [3. 5. 0.]]</span>
<span class="sd">        &gt;&gt;&gt; print(ops.cov(x))</span>
<span class="sd">        [[13.        -3.5      ]</span>
<span class="sd">         [-3.5        6.3333335]]</span>
<span class="sd">        &gt;&gt;&gt; print(ops.cov(x, correction=0))</span>
<span class="sd">        [[ 8.666667  -2.3333333]</span>
<span class="sd">         [-2.3333333  4.2222223]]</span>
<span class="sd">        &gt;&gt;&gt; fw = ms.Tensor([5, 2, 4], dtype=ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; aw = ms.Tensor([0.4588, 0.9083, 0.7616], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.cov(x, fweights=fw, aweights=aw))</span>
<span class="sd">        [[10.146146 -3.47241 ]</span>
<span class="sd">         [-3.47241   4.716825]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For cov, the input must have two or fewer dimensions, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2"> dimensions.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For cov, the input dtype can not be bool.&quot;</span><span class="p">)</span>

    <span class="c1"># View input tensor as 2D</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="nb">input</span>
    <span class="n">num_observations</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">fweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_cov_weights</span><span class="p">(</span><span class="n">fweights</span><span class="p">,</span> <span class="s2">&quot;fweights&quot;</span><span class="p">,</span> <span class="n">num_observations</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="s2">&quot;an integer&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_cov_weights</span><span class="p">(</span><span class="n">aweights</span><span class="p">,</span> <span class="s2">&quot;aweights&quot;</span><span class="p">,</span> <span class="n">num_observations</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">,</span> <span class="s2">&quot;a floating point&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">fweights</span>
    <span class="k">elif</span> <span class="n">fweights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">aweights</span>
    <span class="k">elif</span> <span class="n">fweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">fweights</span> <span class="o">*</span> <span class="n">aweights</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w_sum</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_x</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">_get_default_div_type</span><span class="p">(</span><span class="n">w_sum</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w_sum</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">num_observations</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">input_x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">_get_default_div_type</span><span class="p">(</span><span class="n">w_sum</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">aweights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">correction</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">norm_factor</span> <span class="o">=</span> <span class="n">w_sum</span> <span class="o">-</span> <span class="n">correction</span> <span class="o">*</span> <span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">aweights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">w_sum</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">norm_factor</span> <span class="o">=</span> <span class="n">w_sum</span> <span class="o">-</span> <span class="n">correction</span>

    <span class="n">norm_factor</span> <span class="o">=</span> <span class="n">norm_factor</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">input_x</span> <span class="o">=</span> <span class="n">input_x</span> <span class="o">-</span> <span class="n">avg</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">input_x</span> <span class="o">*</span> <span class="n">w</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">input_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">norm_factor</span> <span class="o">=</span> <span class="n">norm_factor</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">true_divide</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">_get_default_div_type</span><span class="p">(</span><span class="n">norm_factor</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span></div>


<div class="viewcode-block" id="t"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.t.html#mindspore.ops.t">[docs]</a><span class="k">def</span> <span class="nf">t</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transposes a 2-D Tensor. 1-D Tensor are returned as it is.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the transpose of `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the dimension of `input` is larger than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [2, 3, 4]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.t(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2.]</span>
<span class="sd">         [2. 3.]</span>
<span class="sd">         [3. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For t(), the dimension of tensor should be less than 3, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">input</span></div>


<div class="viewcode-block" id="tan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tan.html#mindspore.ops.tan">[docs]</a><span class="k">def</span> <span class="nf">tan</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes tangent of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tan(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor, valid for any dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1.0, 0.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tan(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.5574081 0. 1.5574081]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tan_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="xlogy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.xlogy.html#mindspore.ops.xlogy">[docs]</a><span class="k">def</span> <span class="nf">xlogy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the first input tensor multiplied by the logarithm of second input tensor element-wise.</span>
<span class="sd">    Returns zero when `input` is zero.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = input_{i}\ln{other_{i}}</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - On Ascend, the data type of `input` and `other` must be float16 or float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input is a number.Number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not a number.Number or a bool or a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` and `other` is not in [float16, float32, float64, complex64, complex128].</span>
<span class="sd">        ValueError: If `input` could not be broadcast to a tensor with shape of `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-5, 0, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.xlogy(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-3.465736   0.        2.7725887]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span> \
            <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">xlogy_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="arccosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arccosh.html#mindspore.ops.arccosh">[docs]</a><span class="k">def</span> <span class="nf">arccosh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.acosh`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acosh_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="arcsin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arcsin.html#mindspore.ops.arcsin">[docs]</a><span class="k">def</span> <span class="nf">arcsin</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.asin`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asin_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arctan.html#mindspore.ops.arctan">[docs]</a><span class="k">def</span> <span class="nf">arctan</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.atan`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atan_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arctan2.html#mindspore.ops.arctan2">[docs]</a><span class="k">def</span> <span class="nf">arctan2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For details, please refer to :func:`mindspore.ops.atan2`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_atan2</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Atan2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_atan2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="polar"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.polar.html#mindspore.ops.polar">[docs]</a><span class="k">def</span> <span class="nf">polar</span><span class="p">(</span><span class="nb">abs</span><span class="p">,</span> <span class="n">angle</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts polar coordinates to Cartesian coordinates.</span>

<span class="sd">    Returns a complex tensor, its elements are Cartesian coordinates constructed with the polar</span>
<span class="sd">    coordinates which is specified by radial distance `abs` and polar angle `angle`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_{i} =  abs_{i} * \cos(angle_{i}) + abs_{i} * \sin(angle_{i}) * j</span>

<span class="sd">    Args:</span>
<span class="sd">        abs (Tensor): Radial distance. The shape of tensor is</span>
<span class="sd">          :math:`(N,*)` where :math:`N` means the batchsize of the input tensor,</span>
<span class="sd">          :math:`*` means, any number of additional dimensions.</span>
<span class="sd">          Must be one of the following types: float32, float64.</span>
<span class="sd">        angle (Tensor):  Polar angle. It has the same shape and dtype as `abs`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `abs`.</span>

<span class="sd">        - If the inputs are float32, data type must be complex64.</span>
<span class="sd">        - If the inputs are float64, data type must be complex128.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `abs` nor `angle` is a Tensor.</span>
<span class="sd">        TypeError: If the dtype of input is not one of: float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `abs` and `angle` are not the same.</span>
<span class="sd">        ValueError: If `abs`&#39;s shape is not the same as `angle`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; abs = Tensor(np.array([1, 2]), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; angle = Tensor(np.array([np.pi / 2, 5 * np.pi / 4]), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.polar(abs, angle)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 6.12323400e-17+1.j         -1.41421356e+00-1.41421356j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">polar_</span><span class="p">(</span><span class="nb">abs</span><span class="p">,</span> <span class="n">angle</span><span class="p">)</span></div>


<div class="viewcode-block" id="asin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.asin.html#mindspore.ops.asin">[docs]</a><span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arcsine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sin^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.8330704  0.04001067  0.30469266  0.5943858 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asin_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="acos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.acos.html#mindspore.ops.acos">[docs]</a><span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arccosine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cos^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acos(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.737726  1.5307857 1.2661036 0.9764105]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acos_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="arccos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arccos.html#mindspore.ops.arccos">[docs]</a><span class="k">def</span> <span class="nf">arccos</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.acos` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acos</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="atan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atan.html#mindspore.ops.atan">[docs]</a><span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tan^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7853982 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atan_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinh.html#mindspore.ops.sinh">[docs]</a><span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sinh(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of hyperbolic sine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6604918  0.28367308 0.44337422 0.6604918 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sinh_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cosh.html#mindspore.ops.cosh">[docs]</a><span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic cosine of input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of hyperbolic cosine function, its data type</span>
<span class="sd">            must be float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of the following types:</span>
<span class="sd">                   float16, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.0289385 1.364684 1.048436 1.0040528]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cosh_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tanh.html#mindspore.ops.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic tangent of input element-wise. The Tanh function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        tanh(x_i) = \frac{\exp(x_i) - \exp(-x_i)}{\exp(x_i) + \exp(-x_i)} = \frac{\exp(2x_i) - 1}{\exp(2x_i) + 1},</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input of Tanh, with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tanh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7615941 0.9640276 0.9950547 0.9993293 0.9999092]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tanh_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="asinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.asinh.html#mindspore.ops.asinh">[docs]</a><span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sinh^{-1}(x_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor of inverse hyperbolic sine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asinh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.3124382  1.1947632  1.8184465  5.298342 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asinh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="arcsinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arcsinh.html#mindspore.ops.arcsinh">[docs]</a><span class="k">def</span> <span class="nf">arcsinh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.asinh`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asinh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="arctanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arctanh.html#mindspore.ops.arctanh">[docs]</a><span class="k">def</span> <span class="nf">arctanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.atanh`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atanh</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="acosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.acosh.html#mindspore.ops.acosh">[docs]</a><span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic cosine of the inputs element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh^{-1}(input_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Given an input tensor input, the function computes inverse hyperbolic cosine of every element.</span>
<span class="sd">        Input range is [1, inf].</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of inverse hyperbolic cosine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.9624237 1.7627472 5.298292 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acosh_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="atanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atanh.html#mindspore.ops.atanh">[docs]</a><span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tanh^{-1}(x_{i})</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, -0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atanh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.         -0.54930615]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atanh_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="atan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atan2.html#mindspore.ops.atan2">[docs]</a><span class="k">def</span> <span class="nf">atan2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns arctangent of input/other element-wise.</span>

<span class="sd">    It returns :math:`\theta\ \in\ [-\pi, \pi]`</span>
<span class="sd">    such that :math:`input = r*\sin(\theta), other = r*\cos(\theta)`, where :math:`r = \sqrt{input^2 + other^2}`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Arg `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">          If they have different data types, the lower precision data type will be converted to relatively the</span>
<span class="sd">          highest precision data type.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor, Number.number): The input tensor or scalar.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32, float64</span>
<span class="sd">        other (Tensor, Number.number): The input tensor or scalar. It has the same shape with `input`.</span>

<span class="sd">    Note:</span>
<span class="sd">        At least one of the input args should be Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the shape is the same as the one after broadcasting,and the data type is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor or scalar.</span>
<span class="sd">        RuntimeError: If the data type of `input` and `other` conversion of Parameter is required</span>
<span class="sd">                      when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan2(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.7853982]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_atan2</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Atan2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_atan2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_and"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_and.html#mindspore.ops.bitwise_and">[docs]</a><span class="k">def</span> <span class="nf">bitwise_and</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `and` of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = input_{i} \wedge other_{i}</span>

<span class="sd">    Args of `input` and `other` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor with shape :math:`(N, *)` where :math:`*` means</span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        other (Tensor): The second input tensor with the same dtype as `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_and(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  0  1 -1  1  0  1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bitwise_and_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_or"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_or.html#mindspore.ops.bitwise_or">[docs]</a><span class="k">def</span> <span class="nf">bitwise_or</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `or` of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = input_{i} \mid other_{i}</span>

<span class="sd">    Args of `input` and `other` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor with shape :math:`(N, *)` where :math:`*` means</span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        other (Tensor): The second input tensor with the same dtype as `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_or(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  1 -1 -1  3  3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bitwise_or_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_xor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_xor.html#mindspore.ops.bitwise_xor">[docs]</a><span class="k">def</span> <span class="nf">bitwise_xor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bitwise `xor` of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = input_{i} \oplus other_{i}</span>

<span class="sd">    Args of `input` and `other` comply with the implicit type conversion rules to</span>
<span class="sd">    make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor with shape :math:`(N, *)` where :math:`*` means</span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        other (Tensor): The second input tensor with the same dtype as `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 0, 1, -1, 1, 1, 1]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([0, 1, 1, -1, -1, 2, 3]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_xor(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1  0  0 -2  3  2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bitwise_xor_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_left_shift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_left_shift.html#mindspore.ops.bitwise_left_shift">[docs]</a><span class="k">def</span> <span class="nf">bitwise_left_shift</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform a left bitwise shift operation on the `input` element-wise, where the number of bits to shift is</span>
<span class="sd">    specified by `other`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;out_{i} =input_{i} &lt;&lt; other_{i}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, int, bool]): The input to be left shifted.</span>
<span class="sd">        other (Union[Tensor, int, bool]): The number of bit to be applied on left arithmetic shift.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the result after bitwise left shift.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a tensor.</span>
<span class="sd">        TypeError: If either `input` or `other` is not a bool, int or a tensor of dtype: int or uint.</span>
<span class="sd">        TypeError: If `input` and `other` do not have the same dtype.</span>
<span class="sd">        ValueError: If `input` and `other` could not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1024, 2]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([2]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_left_shift(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4096    8]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bitwise_left_shift&#39;, at least one of the inputs should be a Tensor.&quot;</span><span class="p">)</span>

    <span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bitwise_left_shift&#39;, &#39;input&#39; must be an integer, but got input:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bitwise_left_shift&#39;, &#39;other&#39; must be an integer, but got other:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">ls</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">LeftShift</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ls</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="bitwise_right_shift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bitwise_right_shift.html#mindspore.ops.bitwise_right_shift">[docs]</a><span class="k">def</span> <span class="nf">bitwise_right_shift</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform a right bitwise shift operation on the `input` element-wise, where the number of bits to shift is</span>
<span class="sd">    specified by `other`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;out_{i} =input_{i} &gt;&gt; other_{i}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, int, bool]): The input to be right shifted.</span>
<span class="sd">        other (Union[Tensor, int, bool]): The number of bit to be applied on right arithmetic shift.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the result after bitwise right shift.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a tensor.</span>
<span class="sd">        TypeError: If either `input` or `other` is not a bool, int or a tensor of dtype: int or uint.</span>
<span class="sd">        TypeError: If `input` and `other` do not have the same dtype.</span>
<span class="sd">        ValueError: If `input` and `other` could not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1024, 2]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([2]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bitwise_right_shift(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [256   0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bitwise_left_shift&#39;, at least one of the inputs should be a Tensor.&quot;</span><span class="p">)</span>
    <span class="n">cast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bitwise_left_shift&#39;, &#39;input&#39; must be an integer, but got input:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;bitwise_left_shift&#39;, &#39;other&#39; must be an integer, but got other:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">rs</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">RightShift</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">rs</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="nextafter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nextafter.html#mindspore.ops.nextafter">[docs]</a><span class="k">def</span> <span class="nf">nextafter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next representable floating-point value after `input` towards `other` element-wise.</span>

<span class="sd">    Say there are two float32 numbers :math:`a`, :math:`b`, and let the</span>
<span class="sd">    representable delta of float32 datatype is :math:`eps`. If :math:`a &lt; b`,</span>
<span class="sd">    then the next representable of :math:`a` towards :math:`b` is :math:`a+eps`,</span>
<span class="sd">    the next representable of :math:`b` towards :math:`a` is :math:`b-eps`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  nextafter({input_{i}, other_{i}})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. The shape of tensor is :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. Must be one of the following types: float32, float64.</span>

<span class="sd">        other (Tensor): The second input tensor. The shape of tensor is :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. Must be one of the following types: float32, float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` and `other` is not one of: float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `input` and `other` are not same.</span>
<span class="sd">        ValueError: If `input`&#39;s shape is not the same as `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_ = Tensor(np.asarray([0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other_ = Tensor(np.asarray([0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_ = ops.nextafter(input_, other_)</span>
<span class="sd">        &gt;&gt;&gt; print(output_)</span>
<span class="sd">        [1.e-45]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nextafter_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">NextAfter</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">nextafter_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="inv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inv.html#mindspore.ops.inv">[docs]</a><span class="k">def</span> <span class="nf">inv</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Reciprocal of input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \frac{1}{x_{i} }</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of any dimension. Must be one of the following types: float16, float32 or int32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of float16, float32, int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.25, 0.4, 0.31, 0.52]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4.        2.5       3.2258065 1.923077 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">inv_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="inverse"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inverse.html#mindspore.ops.inverse">[docs]</a><span class="k">def</span> <span class="nf">inverse</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the inverse of the input matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated. Input `input` must be at least two dimensions, and the size of</span>
<span class="sd">            the last two dimensions must be the same size.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If the size of the last two dimensions of `input` is not the same.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1., 2.], [3., 4.]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.inverse(x))</span>
<span class="sd">        [[-2.   1. ]</span>
<span class="sd">         [ 1.5 -0.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;inverse&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">:</span>
        <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatrixInverse</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="invert"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.invert.html#mindspore.ops.invert">[docs]</a><span class="k">def</span> <span class="nf">invert</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flips all bits of input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \sim x_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">            The data type should be one of the following types: int16, uint16.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither int16 nor uint16.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([25, 4, 13, 9]), mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.invert(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-26 -5 -14 -10]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">invert_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="erf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erf.html#mindspore.ops.erf">[docs]</a><span class="k">def</span> <span class="nf">erf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Gauss error function of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erf(x)=\frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of Gaussian error function. Its data type</span>
<span class="sd">            must be float16 float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.8427168   0.          0.8427168   0.99530876  0.99997765]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erf_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="erfc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erfc.html#mindspore.ops.erfc">[docs]</a><span class="k">def</span> <span class="nf">erfc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the complementary error function of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfc(x) = 1 - \frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.8427168e+00 1.0000000e+00 1.5728319e-01 4.6912432e-03 2.2351742e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erfc_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_j0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_j0.html#mindspore.ops.bessel_j0">[docs]</a><span class="k">def</span> <span class="nf">bessel_j0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel j0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_j0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.93846981  0.76519769  0.22389078  -0.39714981]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_j0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_j1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_j1.html#mindspore.ops.bessel_j1">[docs]</a><span class="k">def</span> <span class="nf">bessel_j1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel j1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_j1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.24226846  0.44005059  0.57672481 -0.06604333]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_j1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i0.html#mindspore.ops.bessel_i0">[docs]</a><span class="k">def</span> <span class="nf">bessel_i0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.266066  1.0634835 1.0634835 1.266066]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i0e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i0e.html#mindspore.ops.bessel_i0e">[docs]</a><span class="k">def</span> <span class="nf">bessel_i0e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i0e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i0e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.46575961  0.64503527  0.64503527  0.46575961]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i0e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k0.html#mindspore.ops.bessel_k0">[docs]</a><span class="k">def</span> <span class="nf">bessel_k0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.92441907  0.42102444  0.11389387  0.01115968]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k0e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k0e.html#mindspore.ops.bessel_k0e">[docs]</a><span class="k">def</span> <span class="nf">bessel_k0e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k0e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k0e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.52410939  1.14446308  0.84156822  0.60929767]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k0e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_y0"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_y0.html#mindspore.ops.bessel_y0">[docs]</a><span class="k">def</span> <span class="nf">bessel_y0</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel y0 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_y0(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.44451874  0.08825696  0.51037567  -0.01694074]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_y0_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_y1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_y1.html#mindspore.ops.bessel_y1">[docs]</a><span class="k">def</span> <span class="nf">bessel_y1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel y1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_y1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.47147239  -0.78121282  -0.10703243  0.39792571]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_y1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="linspace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.linspace.html#mindspore.ops.linspace">[docs]</a><span class="k">def</span> <span class="nf">linspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">steps</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor whose value is `steps` evenly spaced in the interval `start` and `end` (including `start` and</span>
<span class="sd">    `end`), and the length of the output Tensor is `steps`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;step = (end - start)/(steps - 1)\\</span>
<span class="sd">        &amp;output = [start, start+step, start+2*step, ... , end]</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Union[Tensor, int, float]): Start value of interval. The tensor data type must be float32 or float64</span>
<span class="sd">            and with shape of 0-D.</span>
<span class="sd">        end (Union[Tensor, int, float]): Last value of interval. The tensor data type must be float32 or float64</span>
<span class="sd">            and with shape of 0-D.</span>
<span class="sd">        steps (Union[Tensor, int]): Number of ticks in the interval, inclusive of start and end.</span>
<span class="sd">            Must be positive int number or 0D int32/int64 Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `start`, and the shape of :math:`(steps)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start` or `end` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `start` or dtype of `end` is not float32 or float64.</span>
<span class="sd">        ValueError: If shape of `start` or shape of `end` is not 0-D.</span>
<span class="sd">        TypeError: If `steps` is not int or 0D int32/int64 Tensor.</span>
<span class="sd">        ValueError: If `steps` is not positive int number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; steps = 5</span>
<span class="sd">        &gt;&gt;&gt; output = ops.linspace(start, end, steps)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.    3.25  5.5   7.75 10.  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">linspace_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">det</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the determinant of one or more square matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated, its shape should be :math:`[..., M, M]` who must</span>
<span class="sd">          have at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The shape is :math:`input.shape[:-2]`, and the dtype is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `input` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.det(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-16.5 21. ]</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatrixDeterminant</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `matrix_determinant` is deprecated, please use `det` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;matrix_determinant is deprecated, please use `det` instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatrixDeterminant</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `log_matrix_determinant` is deprecated, please use `matrix_solve` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;`log_matrix_determinant` is deprecated, please use `matrix_solve` instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogMatrixDeterminant</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">matrix_exp</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the exponential of a single or a batch of square matrices.</span>

<span class="sd">    .. math::</span>

<span class="sd">        matrix\_exp(x) = \sum_{k=0}^{\infty} \frac{1}{k !} x^{k} \in \mathbb{K}^{n \times n}</span>

<span class="sd">    where :math:`x` corresponds to `input` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(*, n, n)` where * is zero or more batch dimensions.</span>
<span class="sd">            Must be one of the following types: float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of the following dtype:</span>
<span class="sd">                   float16, float32, float64, complex64, complex128.</span>
<span class="sd">        ValueError: If the rank of `input` is less than 2.</span>
<span class="sd">        ValueError: If the size of last two dimensions of `input` are not equal.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1, 2], [0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_exp(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.7182817 5.436563 ]</span>
<span class="sd">        [0.        2.7182817]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">matrix_exp_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<div class="viewcode-block" id="lu_solve"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lu_solve.html#mindspore.ops.lu_solve">[docs]</a><span class="k">def</span> <span class="nf">lu_solve</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the solution y to the system of linear equations :math:`Ay = b` ,</span>
<span class="sd">    given LU decomposition :math:`A` and column vector :math:`b`.</span>

<span class="sd">    LU decomposition of a matrix can be generated from :func:`mindspore.scipy.linalg.lu_factor` .</span>

<span class="sd">    Args:</span>
<span class="sd">        b (Tensor): Column vector `b` in the above equation. It has shape :math:`(*, m, k)`,</span>
<span class="sd">            where :math:`*` is batch dimensions, with data type float32, float16.</span>
<span class="sd">        LU_data (Tensor): LU decomposition. It has shape :math:`(*, m, m)`, where :math:`*` is batch</span>
<span class="sd">            dimensions, that can be decomposed into an upper triangular matrix U and a lower triangular</span>
<span class="sd">            matrix L, with data type float32, float16.</span>
<span class="sd">        LU_pivots (Tensor): Permutation matrix P of LU decomposition. It has</span>
<span class="sd">            shape :math:`(*, m)`, where :math:`*` is batch dimensions, that can be converted</span>
<span class="sd">            to a permutation matrix P, with data type int32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same data type as the `b` and `LU_data`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `b` or `LU_data` is not one of: float32, float16.</span>
<span class="sd">        TypeError: If dtype of `LU_pivots` is not: int32.</span>
<span class="sd">        TypeError: If `b`, `LU_data` or `LU_pivots` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `b` is not same as dtype of `LU_data`.</span>
<span class="sd">        ValueError: If the batch dimensions of LU_pivots does not match the batch dimensions of LU_data.</span>
<span class="sd">        ValueError: If `b` dimension less than 2, `LU_data` dimension less than 2 or `LU_pivots` dimension less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.array([[1], [3], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; LU_data = Tensor(np.array([[2, 1, 1], [0.5, 1, 1.5], [0.5, 0, 2.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; LU_pivots = Tensor(np.array([2, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.lu_solve(b, LU_data, LU_pivots)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 1.9000002]</span>
<span class="sd">         [-1.4000001]</span>
<span class="sd">         [ 0.6      ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lu_solve_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LuSolve</span><span class="p">)()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">lu_solve_</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="matrix_solve"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_solve.html#mindspore.ops.matrix_solve">[docs]</a><span class="k">def</span> <span class="nf">matrix_solve</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">adjoint</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves systems of linear equations.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;matrix[..., M, M] * x[..., M, K] = rhs[..., M, K]\\</span>
<span class="sd">        &amp;adjoint(matrix[..., M, M]) * x[..., M, K] = rhs[..., M, K]</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        On GPU, if the matrix is irreversible, an error may be reported or an unknown result may be returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        matrix (Tensor): The shape of tensor is :math:`(..., M, M)` .</span>
<span class="sd">        rhs (Tensor): The shape of tensor is :math:`(..., M, K)` . `rhs` must have the same dtype as `matrix`.</span>
<span class="sd">        adjoint(bool): Indicating whether to solve with matrix or its (block-wise) adjoint. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        x (Tensor), The dtype and shape is the same as &#39;rhs&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If adjoint is not the type of bool.</span>
<span class="sd">        TypeError: If the type of matrix is not one of the following dtype:</span>
<span class="sd">                   mstype.float16, mstype.float32, mstype.float64, mstype.complex64, mstype.complex128.</span>
<span class="sd">        TypeError: If the type of `matrix` is not the same as that of `rhs`.</span>
<span class="sd">        ValueError: If the rank of `matrix` less than 2.</span>
<span class="sd">        ValueError: If the dimension of `matrix` is not the same as `rhs`.</span>
<span class="sd">        ValueError: If the inner-most 2 dimension of `matrix` is not the same.</span>
<span class="sd">        ValueError: If the inner-most 2 dimension of `rhs` does not match `matrix`.</span>
<span class="sd">        ValueError: If the `matrix` is irreversible.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; matrix = Tensor([[5, 4], [3, 1]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rhs = Tensor([[7], [2]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.matrix_solve(matrix, rhs)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0.14285707]</span>
<span class="sd">         [1.5714287 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_solve_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixSolve</span><span class="p">)(</span><span class="n">adjoint</span><span class="o">=</span><span class="n">adjoint</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_solve_</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span></div>


<div class="viewcode-block" id="slogdet"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.slogdet.html#mindspore.ops.slogdet">[docs]</a><span class="k">def</span> <span class="nf">slogdet</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sign and the log of the absolute value of the determinant of one or more square matrices.</span>

<span class="sd">    Note:</span>
<span class="sd">        The type of output always be real-value, even `input` is complex.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated, its shape is :math:`(..., M, M)`.</span>
<span class="sd">          The matrix must be at least two dimensions, and the last two</span>
<span class="sd">          dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The signs of the log determinants. The shape is :math:`input.shape[:-2]`.</span>

<span class="sd">        Tensor. The absolute values of the log determinants. The shape is :math:`input.shape[:-2]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `input` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sign, output = ops.slogdet(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(sign)</span>
<span class="sd">        [-1.   1.]</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.80336046e+00    3.04452229e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogMatrixDeterminant</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="trace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.trace.html#mindspore.ops.trace">[docs]</a><span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor that is the sum of the input trace.</span>

<span class="sd">    Note:</span>
<span class="sd">        Input must be matrix, and complex number is not supported at present.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated. The matrix must be two dimensional.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same data type as input `input`, and size equals to 1.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimension of `input` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trace(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        42.0</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(1, 13).reshape(3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trace(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        18.0</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(12, 0, -1).reshape(4, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trace(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        24.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">trace_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Trace</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">trace_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="truncate_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.truncate_div.html#mindspore.ops.truncate_div">[docs]</a><span class="k">def</span> <span class="nf">truncate_div</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and rounds the results</span>
<span class="sd">    of division towards zero. Equivalent to C-style integer division.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    Note:</span>
<span class="sd">        Broadcasting is supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        x(Union[Tensor, Number, bool]): The first input is a number, or a bool,</span>
<span class="sd">            or a tensor whose data type is number or bool.</span>
<span class="sd">        y(Union[Tensor, Number, bool]): The second input is a number, or a bool when the first input</span>
<span class="sd">            is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.truncate_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">truncate_div_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="truncate_mod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.truncate_mod.html#mindspore.ops.truncate_mod">[docs]</a><span class="k">def</span> <span class="nf">truncate_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the remainder of division element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The input data does not support 0.</span>
<span class="sd">        - When the elements of input exceed 2048 , the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as (D1,D2... ,Dn), then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, numbers.Number, bool]): The first input is a number, or a bool,</span>
<span class="sd">            or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, numbers.Number, bool]): The second input is a number, or a bool when the first input</span>
<span class="sd">            is a tensor, or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is one of the following: Tensor, number, bool.</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>
<span class="sd">        ValueError: If the shape `x` and `y` cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.truncate_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2  1 -1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">truncate_mod_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="trunc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.trunc.html#mindspore.ops.trunc">[docs]</a><span class="k">def</span> <span class="nf">trunc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor with the truncated integer values of the elements of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same shape and data type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([3.4742, 0.5466, -0.8008, -3.9079]),mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trunc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 0. 0. -3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Trunc</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="ldexp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ldexp.html#mindspore.ops.ldexp">[docs]</a><span class="k">def</span> <span class="nf">ldexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies input Tensor by :math:`2^{other}` element-wise.</span>

<span class="sd">    It takes two arguments, a mantissa `x` and an exponent `other`,</span>
<span class="sd">    and returns their product as a floating-point number:</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} * ( 2 ^{other_{i}} )</span>

<span class="sd">    Note:</span>
<span class="sd">        This function is commonly used to construct</span>
<span class="sd">        floating-point numbers from their component parts, or to scale a</span>
<span class="sd">        floating-point number by a power of two.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor.</span>
<span class="sd">        other (Tensor): A Tensor of integers that represent exponents.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `other` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `x` and `other` can not broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 2, 3, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.ldexp(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [ 2.  4.  8. 16.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.], [2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([[1.], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.ldexp(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[2.]</span>
<span class="sd">         [8.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">pow_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()</span>
    <span class="n">mul_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">)()</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">mul_ops</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pow_ops</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">other</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="logit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logit.html#mindspore.ops.logit">[docs]</a><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the logit of a tensor element-wise. When eps is not None, element in `input` is clamped to [eps, 1-eps].</span>
<span class="sd">    When eps is None, input `input` is not clamped.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        y_{i} &amp; = \ln(\frac{z_{i}}{1 - z_{i}}) \\</span>
<span class="sd">        z_{i} &amp; = \begin{cases}</span>
<span class="sd">        input_{i} &amp; \text{if eps is None} \\</span>
<span class="sd">        \text{eps} &amp; \text{if } input_{i} \lt \text{eps} \\</span>
<span class="sd">        input_{i} &amp; \text{if } \text{eps} \leq input_{i} \leq 1 - \text{eps} \\</span>
<span class="sd">        1 - \text{eps} &amp; \text{if } input_{i} \gt 1 - \text{eps}</span>
<span class="sd">        \end{cases}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        eps (float, optional): The epsilon. If eps is not None, the input clamp bound is defined as [eps, 1-eps],</span>
<span class="sd">            otherwise, the input `input` is not clamped. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logit(x, eps=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
    <span class="n">logit_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Logit</span><span class="p">)(</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logit_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="c1">#####################################</span>
<span class="c1"># Comparison Operation Functions.</span>
<span class="c1">#####################################</span>


<div class="viewcode-block" id="less"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.less.html#mindspore.ops.less">[docs]</a><span class="k">def</span> <span class="nf">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt; y` element-wise.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&lt;y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&gt;=y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor, or it can be a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_lt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="lt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lt.html#mindspore.ops.lt">[docs]</a><span class="k">def</span> <span class="nf">lt</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.less` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">less</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="le"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.le.html#mindspore.ops.le">[docs]</a><span class="k">def</span> <span class="nf">le</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt;= y` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&lt;=y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&gt;y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.le(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_le</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="gt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gt.html#mindspore.ops.gt">[docs]</a><span class="k">def</span> <span class="nf">gt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the value of the input parameters :math:`x,y` element-wise, and the output result is a bool value.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&gt;y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&lt;=y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time,</span>
<span class="sd">          and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If the input Tensor can be broadcast, the low dimension will be extended to the corresponding high dimension</span>
<span class="sd">          in another input by copying the value of the dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        y (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gt(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_greater</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_greater</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="ge"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ge.html#mindspore.ops.ge">[docs]</a><span class="k">def</span> <span class="nf">ge</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &gt;= y` element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time,</span>
<span class="sd">          and the shapes of them can be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If the input Tensor can be broadcast, the low dimension will be extended to the corresponding high dimension</span>
<span class="sd">          in another input by copying the value of the dimension.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } x_{i}&gt;=y_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } x_{i}&lt;y_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ge(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_greater_equal</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.equal.html#mindspore.ops.equal">[docs]</a><span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the equivalence between two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } input_{i} = other_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } input_{i} \ne other_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        - `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a number or</span>
<span class="sd">            a tensor whose data type is number.</span>
<span class="sd">        other (Union[Tensor, Number]): The second input is a number</span>
<span class="sd">            when the first input is a tensor or a tensor whose data type is number.</span>
<span class="sd">            The data type is the same as the first input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: The shape of two inputs are different</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.equal(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: The shape of two inputs are the same</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">equal_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="ne"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ne.html#mindspore.ops.ne">[docs]</a><span class="k">def</span> <span class="nf">ne</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the non-equivalence of two tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">        &amp; \text{True,    if } x_{i} \ne y_{i} \\</span>
<span class="sd">        &amp; \text{False,   if } x_{i} = y_{i}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ne(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ne(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">not_equal_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="not_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.not_equal.html#mindspore.ops.not_equal">[docs]</a><span class="k">def</span> <span class="nf">not_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.ne` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ne</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="approximate_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.approximate_equal.html#mindspore.ops.approximate_equal">[docs]</a><span class="k">def</span> <span class="nf">approximate_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tolerance</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns True if abs(x-y) is smaller than tolerance element-wise, otherwise False.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        &amp; \text{ if } \left | x_{i} - y_{i} \right | &lt; \text{tolerance},\ \ True  \\</span>
<span class="sd">        &amp; \text{ if } \left | x_{i} - y_{i} \right | \ge \text{tolerance},\ \  False</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where `tolerance` indicates Acceptable maximum tolerance.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower precision data type will be converted to</span>
<span class="sd">    the relatively highest precision data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A tensor. Must be one of the following types: float32, float16.</span>
<span class="sd">          :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        y (Tensor): A tensor of the same type and shape as `x`.</span>
<span class="sd">        tolerance (float): The maximum deviation that two elements can be considered equal. Default: 1e-5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the shape of `x`, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tolerance` is not a float.</span>
<span class="sd">        RuntimeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                      but data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tol = 1.5</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 4, 6]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.approximate_equal(Tensor(x), Tensor(y), tol)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ApproximateEqual</span><span class="p">(</span><span class="n">tolerance</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="isfinite"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isfinite.html#mindspore.ops.isfinite">[docs]</a><span class="k">def</span> <span class="nf">isfinite</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are finite for each position.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">          &amp; \text{ if } x_{i} = \text{Finite},\ \ True \\</span>
<span class="sd">          &amp; \text{ if } x_{i} \ne \text{Finite},\ \ False</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isfinite(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">isfinite_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="isnan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isnan.html#mindspore.ops.isnan">[docs]</a><span class="k">def</span> <span class="nf">isnan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are NaN for each position.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">          &amp; \ True,\ \text{ if } x_{i} = \text{Nan} \\</span>
<span class="sd">          &amp; \ False,\ \text{ if } x_{i} \ne  \text{Nan}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`Nan` means not a number.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isnan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">isnan_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="isclose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isclose.html#mindspore.ops.isclose">[docs]</a><span class="k">def</span> <span class="nf">isclose</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor with boolean elements representing if each element of `x1`</span>
<span class="sd">    is “close” to the corresponding element of `x2`. Closeness is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        ∣x1−x2∣  ≤  atol + rtol × ∣x2∣</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): First Tensor to compare, with data type belongs to float32, float16, int32.</span>
<span class="sd">        x2 (Tensor): Second Tensor to compare, with data type belongs to float32, float16, int32.</span>
<span class="sd">        rtol (float, optional): Relative tolerance. Default: 1e-05.</span>
<span class="sd">        atol (float, optional): Absolute tolerance. Default: 1e-08.</span>
<span class="sd">        equal_nan (bool, optional): If True, then two NaNs will be considered equal. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A bool Tensor, with the shape as broadcasted result of the input `x1` and `x2`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If either of `x1` and `x2` is not Tensor.</span>
<span class="sd">        TypeError: If either of `x1` and `x2` is not float16, float32 or int32.</span>
<span class="sd">        TypeError: If either of `atol` and `rtol` is not float.</span>
<span class="sd">        TypeError: If `equal_nan` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `x1` is not same as the `x2`.</span>
<span class="sd">        ValueError: If `x1` and `x2` can not be broadcast.</span>
<span class="sd">        ValueError: If either of `atol` and `rtol` is less than zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.3, 2.1, 3.2, 4.1, 5.1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1.3, 3.3, 2.3, 3.1, 5.1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isclose(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">is_close</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IsClose</span><span class="p">)(</span><span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">equal_nan</span><span class="o">=</span><span class="n">equal_nan</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">is_close</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<div class="viewcode-block" id="isreal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isreal.html#mindspore.ops.isreal">[docs]</a><span class="k">def</span> <span class="nf">isreal</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tests element-wise for real number.</span>
<span class="sd">    A complex value is considered real when its imaginary part is 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">       Tensor, true where `input` is real number, false otherwise.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 1+1j, 2+0j], mstype.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isreal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;isreal&quot;</span><span class="p">)</span>

    <span class="c1"># Note: Integral and Floating tensor values are always real</span>
    <span class="n">fillv2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">FillV2</span><span class="p">)()</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="n">real_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">real_dtype</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">fillv2_op</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="n">imag_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Imag</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">imag_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span></div>


<div class="viewcode-block" id="is_complex"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.is_complex.html#mindspore.ops.is_complex">[docs]</a><span class="k">def</span> <span class="nf">is_complex</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Return True if the data type of the tensor is complex, otherwise return False.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Bool, return whether the data type of the tensor is complex.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([1, 1+1j, 2+2j], mstype.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.is_complex(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex_type</span></div>


<div class="viewcode-block" id="nan_to_num"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nan_to_num.html#mindspore.ops.nan_to_num">[docs]</a><span class="k">def</span> <span class="nf">nan_to_num</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace the `NaN`, positive infinity and negative infinity values in &#39;input&#39; with the</span>
<span class="sd">    specified values in `nan`, `posinf` and `neginf` respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(input_1, input_2, ..., input_R)`.</span>
<span class="sd">            With float32 or float16 data type.</span>
<span class="sd">        nan (float): The replace value of &#39;NaN&#39;. Default value is 0.0.</span>
<span class="sd">        posinf (float): the value to replace positive infinity values with. Default: None,</span>
<span class="sd">            replacing positive infinity with the maximum value supported by the data type of `input`.</span>
<span class="sd">        neginf (float): the value to replace negative infinity values with. Default: None,</span>
<span class="sd">            replacing negative infinity with the minimum value supported by the data type of `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([float(&#39;nan&#39;), float(&#39;inf&#39;), -float(&#39;inf&#39;), 5.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nan_to_num(input, 1.0, 2.0, 3.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.  2.  3.  5.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">nan</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nan</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the parameter nan&#39;s dtype must be float.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">nan</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">if</span> <span class="n">posinf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">posinf</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the parameter posinf&#39;s dtype must be float.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">posinf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">posinf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">neginf</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">neginf</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the parameter neginf&#39;s dtype must be float.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="n">neginf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="n">neginf</span> <span class="o">=</span> <span class="p">(</span><span class="nb">float</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
    <span class="n">_nan_to_num</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NanToNum</span><span class="p">)(</span><span class="n">nan</span><span class="o">=</span><span class="n">nan</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="n">posinf</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="n">neginf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_nan_to_num</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="fmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fmax.html#mindspore.ops.fmax">[docs]</a><span class="k">def</span> <span class="nf">fmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = \max(x1_i, x2_i)</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - Shapes of `input` and `other` should be able to broadcast.</span>
<span class="sd">        - If one of the elements to be compared is NaN, another element is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first tensor. The supported dtypes are: float16, float32, float64, int32, int64.</span>
<span class="sd">        other (Tensor): The second tensor. The supported dtypes are: float16, float32, float64, int32, int64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `other` is not one of: float16, float32, float64, int32, int64.</span>
<span class="sd">        ValueError: If the shape of  `input` and `other` can not broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fmax(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fmax_</span> <span class="o">=</span> <span class="n">Fmax</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fmax_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="maximum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.maximum.html#mindspore.ops.maximum">[docs]</a><span class="k">def</span> <span class="nf">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar,</span>
<span class="sd">          the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = max(x_i, y_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `x` and `y` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">maximum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">fmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = min(input_i, other_i)</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - Shapes of `input` and `other` should be able to broadcast.</span>
<span class="sd">        - If one of the elements to be compared is NaN, another element is returned.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first tensor. The supported dtypes are: float16, float32, float64, int32, int64.</span>
<span class="sd">        other (Tensor): The second tensor. The supported dtypes are: float16, float32, float64, int32, int64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `other` is not one of: float16, float32, float64, int32, int64.</span>
<span class="sd">        ValueError: If the shape of  `input` and `other` can not broadcast.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 5.0, 3.0]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4.0, 2.0, 6.0]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fmin(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fmin_</span> <span class="o">=</span> <span class="n">Fmin</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fmin_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<div class="viewcode-block" id="minimum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.minimum.html#mindspore.ops.minimum">[docs]</a><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Shapes of them are supposed to be broadcast.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = min(x_i, y_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `x` and `y` are not the same shape after broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">minimum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="median"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.median.html#mindspore.ops.median">[docs]</a><span class="k">def</span> <span class="nf">median</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the median and indices of input tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - `indices` does not necessarily contain the first occurrence of each median value found in the `input`,</span>
<span class="sd">          unless it is unique. The specific implementation of this API is device-specific.</span>
<span class="sd">          The results may be different on CPU and GPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor of any dimension whose data type is int16, int32, int64, float32 or float64.</span>
<span class="sd">        axis (int, optional): The dimension need to reduce. Default: ``-1`` .</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor need to retain `axis` dimension or not.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        y (Tensor), has the same dtype as the `input`. If `keepdims` is true,</span>
<span class="sd">        the `y` has the same shape as the `input` except the shape of `y` in dimension `axis` is size 1.</span>
<span class="sd">        Otherwise, the `y` lacks `axis` dimension than input.</span>

<span class="sd">        indices (Tensor), has the same shape as the `y`, but dtype is int64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not one of the following: int16, int32, int64, float32, float64.</span>
<span class="sd">        TypeError: If input `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not a int.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is not in range of [-x.dim, x.dim-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.57, 0.11, 0.21],[0.38, 0.50, 0.57], [0.36, 0.16, 0.44]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.median(x, axis=0, keepdims=False)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value= [ 3.79999995e-01,  1.59999996e-01,  4.39999998e-01]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Int64, value= [1, 2, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">median_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Median</span><span class="p">)(</span><span class="n">global_median</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">median_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">nanmedian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the median and indices of input tensor, ignoring NaN.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor of any dimension whose data type is int16, int32, int64, float32 or float64.</span>
<span class="sd">        axis (int, optional): The dimension need to reduce. Default: -1.</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor needs to retain `axis` dimension or not. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `x`. If `keepdims` is true,</span>
<span class="sd">        has the same shape as `x` with dimension `axis` being 1.</span>
<span class="sd">        Otherwise, dimension `axis` is reduced.</span>

<span class="sd">        indices (Tensor), has the same shape as the `y`, but dtype is int64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not one of the following: int16, int32, int64, float32, float64.</span>
<span class="sd">        TypeError: If input `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not a int.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is not in range of [-x.dim, x.dim-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.5, -1.1, float(&#39;nan&#39;)], [3.4, float(&#39;nan&#39;), 0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y, indices = ops.nanmedian(x, axis=0, keepdims=False)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [ 0.5 -1.1  0.7]</span>
<span class="sd">        &gt;&gt;&gt; print(indices)</span>
<span class="sd">        [0 0 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nanmedian_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Median</span><span class="p">)(</span><span class="n">global_median</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nanmedian_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="orgqr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.orgqr.html#mindspore.ops.orgqr">[docs]</a><span class="k">def</span> <span class="nf">orgqr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the explicit representation of the orthogonal matrix :math:`Q`</span>
<span class="sd">    returned by :class:`mindspore.ops.Geqrf`.</span>

<span class="sd">    Take the case of input without batch dimension as an example,</span>
<span class="sd">    computes the first :math:`N` columns of a product of</span>
<span class="sd">    `Householder &lt;https://en.wikipedia.org/wiki/Householder_transformation#Householder_matrix&gt;`_</span>
<span class="sd">    matrices. Suppose input `input` is a matrix of size :math:`(M, N)` after householder transformation.</span>
<span class="sd">    When the diagonal of `input` is set to 1, every colunm of lower triangular in `input` is</span>
<span class="sd">    denoted as :math:`w_j` for :math:`j` for</span>
<span class="sd">    :math:`j=1, \ldots, M`, this function returns the first :math:`N` columns of the matrix</span>

<span class="sd">    .. math::</span>
<span class="sd">        H_{1} H_{2} \ldots H_{k} \quad \text { with } \quad H_{j}=\mathrm{I}_{M}-\tau_{j} w_{j} w_{j}^{\mathrm{H}}</span>

<span class="sd">    where :math:`\mathrm{I}_{M}` is the :math:`M`-dimensional identity matrix. And when :math:`w` is complex,</span>
<span class="sd">    :math:`w^{\mathrm{H}}` is the conjugate transpose, otherwise the transpose.</span>
<span class="sd">    The output matrix is the same size as the input matrix `input`.</span>
<span class="sd">    :math:`tau` is corresponding to `input2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(*, M, N)`, indicating 2D or 3D matrices,</span>
<span class="sd">            with float32, float64, complex64 and complex128 data type.</span>
<span class="sd">        input2 (Tensor): Tensor of shape :math:`(*, K)`, where `K` is less than or equal to `N`, indicating the</span>
<span class="sd">            reflecting coefficient in Householder transformation, which have the same type as `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `input2` are not Tensors.</span>
<span class="sd">        TypeError: If dtype of `input` and `input2` is not one of: float64, float32, complex64, complex128.</span>
<span class="sd">        ValueError: If `input` and `input2` have different batch size.</span>
<span class="sd">        ValueError: If input.shape[-2] &lt; input.shape[-1].</span>
<span class="sd">        ValueError: If input.shape[-1] &lt; input2.shape[-1].</span>
<span class="sd">        ValueError: If rank(input) - rank(input2) != 1.</span>
<span class="sd">        ValueError: If rank(input) != 2 or 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[-114.6, 10.9, 1.1], [-0.304, 38.07, 69.38], [-0.45, -0.17, 62.]]),</span>
<span class="sd">        ... mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input2 = Tensor(np.array([1.55, 1.94, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.orgqr(input, input2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[-0.54999995 -0.2128925   0.8137956 ]</span>
<span class="sd">         [ 0.47119996 -0.8752807   0.08240613]</span>
<span class="sd">         [ 0.69749993  0.42560163  0.57772595]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">orgqr_</span> <span class="o">=</span> <span class="n">Orgqr</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">orgqr_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span></div>


<div class="viewcode-block" id="ormqr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ormqr.html#mindspore.ops.ormqr">[docs]</a><span class="k">def</span> <span class="nf">ormqr</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transpose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates two matrices multiplication of a product of a general matrix with Householder matrices.</span>
<span class="sd">    Calculates the product of a matrix C(given by `other`) with dimensions (m, n) and a matrix Q which is represented</span>
<span class="sd">    using Householder reflectors (`input`, `tau`). Returns a Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(*, mn, k)`, when `left` is True, mn equals to m,</span>
<span class="sd">            otherwise, mn equals to n. And `*` is zero or more batch dimensions.</span>
<span class="sd">        tau (Tensor): Tensor of shape :math:`(*, min(mn, k))` where `*` is zero or more batch dimensions,</span>
<span class="sd">            and its type is the same as `input`.</span>
<span class="sd">        other (Tensor): Tensor of shape :math:`(*, m, n)` where `*` is zero or more batch dimensions,</span>
<span class="sd">            and its type is the same as `input`.</span>
<span class="sd">        left (bool, optional): determines the order of multiplication. If True, computes op(Q) \* `other` ,</span>
<span class="sd">            otherwise, compute `other` \* op(Q). Default: True.</span>
<span class="sd">        transpose (bool, optional): If True, the matrix Q is conjugate transposed,</span>
<span class="sd">            otherwise, not conjugate transposing matrix Q. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `other`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `tau` or `other` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `tau` or `other` is not one of: float64, float32, complex64, complex128.</span>
<span class="sd">        ValueError: If the dimension of `input` or `other` is less than 2D.</span>
<span class="sd">        ValueError: If rank(`input`) - rank(`tau`) != 1.</span>
<span class="sd">        ValueError: If tau.shape[:-2] != input.shape[:-2]</span>
<span class="sd">        ValueError: If other.shape[:-2] != input.shape[:-2]</span>
<span class="sd">        ValueError: If left == true, other.shape[-2] &lt; tau.shape[-1].</span>
<span class="sd">        ValueError: If left == true, other.shape[-2] != input.shape[-2].</span>
<span class="sd">        ValueError: If left == false, other.shape[-1] &lt; tau.shape[-1].</span>
<span class="sd">        ValueError: If left == false, other.shape[-1] != input.shape[-2].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[-114.6, 10.9, 1.1], [-0.304, 38.07, 69.38], [-0.45, -0.17, 62]]),</span>
<span class="sd">        &gt;&gt;&gt;                mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tau = Tensor(np.array([1.55, 1.94, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([[-114.6, 10.9, 1.1],</span>
<span class="sd">        &gt;&gt;&gt;                          [-0.304, 38.07, 69.38],</span>
<span class="sd">        &gt;&gt;&gt;                          [-0.45, -0.17, 62]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ormqr(input, tau, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  63.82713   -13.823125 -116.28614 ]</span>
<span class="sd">         [ -53.659264  -28.157839  -70.42702 ]</span>
<span class="sd">         [ -79.54292    24.00183   -41.34253 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ormqr_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Ormqr</span><span class="p">)(</span><span class="n">left</span><span class="p">,</span> <span class="n">transpose</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ormqr_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="hypot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hypot.html#mindspore.ops.hypot">[docs]</a><span class="k">def</span> <span class="nf">hypot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hypotenuse of input tensors element-wise as legs of a right triangle.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: float32, float64</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \sqrt{input_i^2 + other_i^2}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor.</span>
<span class="sd">        other (Tensor): The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher precision in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `input` or `other` is not float32 or float64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([3., 5., 7.]))</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4., 12., 24.]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.hypot(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [ 5. 13. 25.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">hypot_</span> <span class="o">=</span> <span class="n">Hypot</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">hypot_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="heaviside"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.heaviside.html#mindspore.ops.heaviside">[docs]</a><span class="k">def</span> <span class="nf">heaviside</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Heaviside step function for each element in input.</span>

<span class="sd">    .. math::</span>
<span class="sd">            \text { heaviside }(\text { input, values })=\left\{\begin{array}{ll}</span>
<span class="sd">            0, &amp; \text { if input }&lt;0 \\</span>
<span class="sd">            \text { values, } &amp; \text { if input }=0 \\</span>
<span class="sd">            1, &amp; \text { if input }&gt;0</span>
<span class="sd">            \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. With real number data type.</span>
<span class="sd">        values (Tensor): The values to use where `input` is zero. Values can be broadcast with `input` .</span>
<span class="sd">            `input` should have the same dtype with `values` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as `input` and `values`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `values` is not Tensor.</span>
<span class="sd">        TypeError: If data type `input` and `values` is different.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-5., 1., 0., 2., 0.]))</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([3.]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.heaviside(input, values)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0. 1. 3. 1. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">heaviside_</span> <span class="o">=</span> <span class="n">Heaviside</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">heaviside_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span></div>


<div class="viewcode-block" id="histc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.histc.html#mindspore.ops.histc">[docs]</a><span class="k">def</span> <span class="nf">histc</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the histogram of a tensor.</span>

<span class="sd">    The elements are sorted into equal width bins between `min` and `max`.</span>
<span class="sd">    If `min` and `max` are both zero, the minimum and maximum values of the data are used.</span>

<span class="sd">    Elements lower than min or higher than max are ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor, type support list :math:`[float16, float32, int32]`.</span>
<span class="sd">        bins (int, optional): Number of histogram bins, optional. If specified, must be positive. Default: ``100`` .</span>
<span class="sd">        min (int, float, optional): An optional float of the lower end of the range (inclusive). Default: ``0.0`` .</span>
<span class="sd">        max (int, float, optional): An optional float of the upper end of the range (inclusive). Default: ``0.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, 1-D Tensor with type int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `input` datetype not in support list.</span>
<span class="sd">        TypeError: If attr `min` or `max` is not float or int.</span>
<span class="sd">        TypeError: If attr `bins` is not int.</span>
<span class="sd">        ValueError: If attr value `min` &gt; `max`.</span>
<span class="sd">        ValueError: If attr `bins` &lt;= 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1., 2, 1])</span>
<span class="sd">        &gt;&gt;&gt; y = ops.histc(x, bins=4, min=0.0, max=3.0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0 2 1 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">min</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;histc&#39;, parameter &#39;min&#39; must be an int or float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">min</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">max</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;histc&#39;, parameter &#39;max&#39; must be an int or float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">max</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">histogram_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Histogram</span><span class="p">)(</span><span class="n">bins</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="nb">min</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="nb">max</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">histogram_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="logspace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logspace.html#mindspore.ops.logspace">[docs]</a><span class="k">def</span> <span class="nf">logspace</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a 1-D Tensor with size `steps` whose value is from :math:`base^{start}` to :math:`base^{end}`,</span>
<span class="sd">    and use `base` as the base number.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;step = (end - start)/(steps - 1)\\</span>
<span class="sd">        &amp;output = [base^{start}, base^{start + 1 * step}, ... , base^{start + (steps-2) * step}, base^{end}]</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    Note:</span>
<span class="sd">        - Input `base` must be integer.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Union[float, Tensor]): Start value of interval.</span>
<span class="sd">        end (Union[float, Tensor]): End value of interval.</span>
<span class="sd">        steps (int): The steps must be a non-negative integer.</span>
<span class="sd">        base (int, optional): The base must be a non-negative integer. Default: 10.</span>
<span class="sd">        dtype (mindspore.dtype, optional): The dtype of output. Default: mstype.float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor has the shape as :math:`(step, )`. Its datatype is set by the attr &#39;dtype&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start` is not a float or a Tensor.</span>
<span class="sd">        TypeError: If `end` is not a float or a Tensor.</span>
<span class="sd">        TypeError: If `steps` is not an int.</span>
<span class="sd">        TypeError: If `base` is not an int.</span>
<span class="sd">        ValueError: If `steps` is not a non-negative integer.</span>
<span class="sd">        ValueError: If `base` is not a non-negative integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logspace(start, end, steps = 10, base = 10, dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.e+01 1.e+02 1.e+03 1.e+04 1.e+05 1.e+06 1.e+07 1.e+08 1.e+09 1.e+10]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">logspace_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogSpace</span><span class="p">)(</span><span class="n">steps</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logspace_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span></div>


<div class="viewcode-block" id="logaddexp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logaddexp.html#mindspore.ops.logaddexp">[docs]</a><span class="k">def</span> <span class="nf">logaddexp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the logarithm of the sum of exponentiations of the inputs.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \log(exp(input_i) + \exp(other_i))</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor. The dtype of `input` must be float.</span>
<span class="sd">        other (Tensor): Input Tensor. The dtype of `input` must be float.</span>
<span class="sd">            If the shape of `input` is not equal to the shape of `other`,</span>
<span class="sd">            they must be broadcastable to a common shape (which becomes the shape of the output).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input`, `other` is not a Tensor.</span>
<span class="sd">        TypeError: The dtype of `input` or `other` is not float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array(2).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logaddexp(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.312 2.693 3.312]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For logaddexp, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For logaddexp, the other must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For logaddexp2, the dtype of &#39;input&#39; and &#39;other&#39; must be float,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">maximum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">abs_val</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">exp_val</span> <span class="o">=</span> <span class="n">tensor_exp</span><span class="p">(</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">abs_val</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="n">log1p</span><span class="p">(</span><span class="n">exp_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="logaddexp2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logaddexp2.html#mindspore.ops.logaddexp2">[docs]</a><span class="k">def</span> <span class="nf">logaddexp2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the logarithm of the sum of exponentiations in base of 2 of the inputs.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \log_2(2^{input_i} + 2^{other_i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor. The dtype of `input` must be float.</span>
<span class="sd">        other (Tensor): Input tensor. The dtype of `other` must be float.</span>
<span class="sd">            If ``input.shape != other.shape``, they must be broadcastable to</span>
<span class="sd">            a common shape (which becomes the shape of the output).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input`, `other` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input`, `other` is not a float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([2, 4, 8]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([2]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logaddexp2(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4.32 8.02]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;logaddexp2&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;other&quot;</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="s2">&quot;logaddexp2&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="n">other</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For logaddexp2, the dtype of &#39;input&#39; and &#39;other&#39; must be float,&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">maximum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">abs_val</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">exp2_val</span> <span class="o">=</span> <span class="n">pows</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="n">abs_val</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="n">log2</span><span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">exp2_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_and_canonicalize_axes</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check whether the types and values of input axes are valid.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_and_canonicalize_axes</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_var_std_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;ddof&quot;</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">cls_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_and_canonicalize_axes</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axis</span>


<div class="viewcode-block" id="var"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.var.html#mindspore.ops.var">[docs]</a><span class="k">def</span> <span class="nf">var</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the variance of each row of the input Tensor by default, or it can calculate them</span>
<span class="sd">    in specified dimension `axis`. If `axis` is a list of dimensions, reduce over all of them.</span>

<span class="sd">    Note:</span>
<span class="sd">        If ddof is 0, 1, True or False, the supported device is only Ascend and CPU. In other cases,</span>
<span class="sd">        the supported device is Ascend, GPU and CPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): Input Tensor with a dtype of number.Number, its shape should be :math:`(N, *)`</span>
<span class="sd">            where :math:`*` means any number of additional dims.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): The dimensions to reduce. Only constant value is allowed.</span>
<span class="sd">            Must be in the range [-rank(`input`), rank(`input`)). Default: None, reduce all dimensions.</span>
<span class="sd">        ddof (Union[int, bool], optional): Means Delta Degrees of Freedom.</span>
<span class="sd">            If ddof is an integer, the divisor used in calculations is :math:`N - ddof`,</span>
<span class="sd">            where :math:`N` represents the number of elements.</span>
<span class="sd">            If ddof is True, will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ddof is False, will through the biased estimation to calculate variance.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        keepdims (bool, optional): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the variance.</span>
<span class="sd">        Suppose the shape of `input` is :math:`(x_0, x_1, ..., x_R)`:</span>

<span class="sd">        - If `axis` is () and `keepdims` is set to False, returns a 0-D Tensor, indicating</span>
<span class="sd">          the standard deviation of all elements in `input`.</span>
<span class="sd">        - If `axis` is int 1 and `keepdims` is set to False, then the returned Tensor</span>
<span class="sd">          has shape :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), e.g. (1, 2) and `keepdims` is set to False,</span>
<span class="sd">          then the returned Tensor has shape :math:`(x_0, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: None, int, tuple.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[1, 2, 3, 4], [-1, 1, 4, -10]], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ms.ops.var(input, 1, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.5]</span>
<span class="sd">         [54.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_var_std_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;var&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">var_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="var_mean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.var_mean.html#mindspore.ops.var_mean">[docs]</a><span class="k">def</span> <span class="nf">var_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the variance and mean of each row of the input Tensor by default,</span>
<span class="sd">    or it can calculate them in specified dimension `axis`.</span>
<span class="sd">    If `axis` is a list of dimensions, reduce over all of them.</span>

<span class="sd">    Note:</span>
<span class="sd">        If ddof is 0, 1, True or False, the supported device is only Ascend and CPU. In other cases,</span>
<span class="sd">        the supported device is Ascend, GPU and CPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): Input Tensor with a dtype of number.Number, its shape should be :math:`(N, *)`</span>
<span class="sd">            where :math:`*` means any number of additional dims.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): The dimensions to reduce. Only constant value is allowed.</span>
<span class="sd">            Must be in the range [-rank(`input`), rank(`input`)). Default: None, reduce all dimensions.</span>
<span class="sd">        ddof (Union[int, bool], optional): Means Delta Degrees of Freedom.</span>
<span class="sd">            If ddof is an integer, the divisor used in calculations is :math:`N - ddof`,</span>
<span class="sd">            where :math:`N` represents the number of elements.</span>
<span class="sd">            If ddof is True, will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ddof is False, will through the biased estimation to calculate the variance.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        keepdims (bool, optional): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing the variance and mean.</span>
<span class="sd">        Suppose the shape of `input` is :math:`(x_0, x_1, ..., x_R)`:</span>

<span class="sd">        - If `axis` is () and `keepdims` is set to False, returns a 0-D Tensor, indicating</span>
<span class="sd">          the standard deviation of all elements in `input`.</span>
<span class="sd">        - If `axis` is int 1 and `keepdims` is set to False, then the returned Tensor</span>
<span class="sd">          has shape :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), e.g. (1, 2) and `keepdims` is set to False,</span>
<span class="sd">          then the returned Tensor has shape :math:`(x_0, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: None, int, tuple.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[1, 2, 3, 4], [-1, 1, 4, -10]], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_var, output_mean = ms.ops.var_mean(input, 1, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_var)</span>
<span class="sd">        [[ 2.5]</span>
<span class="sd">         [54.5]]</span>
<span class="sd">        &gt;&gt;&gt; print(output_mean)</span>
<span class="sd">        [[ 2.5]</span>
<span class="sd">         [-1.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_var_std_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;var_mean&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ddof</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceStd</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="n">ddof</span><span class="p">),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x_mean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">x_sub</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">x_mean</span><span class="p">)</span>
    <span class="n">x_pow</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()(</span><span class="n">x_sub</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x_sum</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">x_pow</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">res_mean</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">nums</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="p">():</span>
        <span class="n">nums</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
            <span class="n">nums</span> <span class="o">*=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">ax</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">true_divide</span><span class="p">(</span><span class="n">x_sum</span><span class="p">,</span> <span class="n">nums</span> <span class="o">-</span> <span class="n">ddof</span><span class="p">),</span> <span class="n">res_mean</span></div>


<div class="viewcode-block" id="std"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.std.html#mindspore.ops.std">[docs]</a><span class="k">def</span> <span class="nf">std</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the standard-deviation of each row of the input Tensor by default, or it can calculate them</span>
<span class="sd">    in specified dimension `axis`. If `axis` is a list of dimensions, reduce over all of them.</span>

<span class="sd">    Note:</span>
<span class="sd">        If ddof is 0, 1, True or False, the supported device is only Ascend and CPU. In other cases,</span>
<span class="sd">        the supported device is Ascend, GPU and CPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): Input Tensor with a dtype of number.Number, its shape should be :math:`(N, *)`</span>
<span class="sd">            where :math:`*` means any number of additional dims.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): The dimensions to reduce. Only constant value is allowed.</span>
<span class="sd">            Must be in the range [-rank(`input`), rank(`input`)). Default: None, reduce all dimensions.</span>
<span class="sd">        ddof (Union[int, bool], optional): Means Delta Degrees of Freedom.</span>
<span class="sd">            If ddof is an integer, the divisor used in calculations is :math:`N - ddof`,</span>
<span class="sd">            where :math:`N` represents the number of elements.</span>
<span class="sd">            If ddof is True, will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ddof is False, will through the biased estimation to calculate the standard deviation.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        keepdims (bool, optional): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the standard deviation.</span>
<span class="sd">        Suppose the shape of `input` is :math:`(x_0, x_1, ..., x_R)`:</span>

<span class="sd">        - If `axis` is () and `keepdims` is set to False, returns a 0-D Tensor, indicating</span>
<span class="sd">          the standard deviation of all elements in `input`.</span>
<span class="sd">        - If `axis` is int 1 and `keepdims` is set to False, then the returned Tensor</span>
<span class="sd">          has shape :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), e.g. (1, 2) and `keepdims` is set to False,</span>
<span class="sd">          then the returned Tensor has shape :math:`(x_0, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: None, int, tuple.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[1, 2, 3, 4], [-1, 1, 4, -10]], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ms.ops.std(input, 1, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.5811388]</span>
<span class="sd">         [7.3824115]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_var_std_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">std_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></div>


<div class="viewcode-block" id="std_mean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.std_mean.html#mindspore.ops.std_mean">[docs]</a><span class="k">def</span> <span class="nf">std_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the standard-deviation and mean of each row of the input Tensor by default,</span>
<span class="sd">    or it can calculate them in specified dimension `axis`.</span>
<span class="sd">    If `axis` is a list of dimensions, reduce over all of them.</span>

<span class="sd">    Note:</span>
<span class="sd">        If ddof is 0, 1, True or False, the supported device is only Ascend and CPU. In other cases,</span>
<span class="sd">        the supported device is Ascend, GPU and CPU.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): Input Tensor with a dtype of number.Number, its shape should be :math:`(N, *)`</span>
<span class="sd">            where :math:`*` means any number of additional dims.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): Specifies the dimensions from which to calculate the standard</span>
<span class="sd">            deviation and mean. Only constant value is allowed. Must be in the range [-rank(`input`), rank(`input`)).</span>
<span class="sd">            Default: None, reduce all dimensions.</span>
<span class="sd">        ddof (Union[int, bool], optional): Means Delta Degrees of Freedom.</span>
<span class="sd">            If ddof is an integer, the divisor used in calculations is :math:`N - ddof`,</span>
<span class="sd">            where :math:`N` represents the number of elements.</span>
<span class="sd">            If ddof is True, will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ddof is False, will through the biased estimation to calculate the standard deviation.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        keepdims (bool, optional): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple containing the standard deviation and mean.</span>
<span class="sd">        Suppose the shape of `input` is :math:`(x_0, x_1, ..., x_R)`:</span>

<span class="sd">        - If `axis` is () and `keepdims` is set to False, returns a 0-D Tensor, indicating</span>
<span class="sd">          the standard deviation of all elements in `input`.</span>
<span class="sd">        - If `axis` is int 1 and `keepdims` is set to False, then the returned Tensor</span>
<span class="sd">          has shape :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), e.g. (1, 2) and `keepdims` is set to False,</span>
<span class="sd">          then the returned Tensor has shape :math:`(x_0, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: None, int, tuple.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[1, 2, 3, 4], [-1, 1, 4, -10]], ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_std, output_mean = ms.ops.std_mean(input, 1, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output_std)</span>
<span class="sd">        [[1.5811388]</span>
<span class="sd">         [7.3824115]]</span>
<span class="sd">        &gt;&gt;&gt; print(output_mean)</span>
<span class="sd">        [[ 2.5]</span>
<span class="sd">         [-1.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_var_std_input</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;std_mean&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ddof</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceStd</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="n">ddof</span><span class="p">),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">var_mean</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Pow</span><span class="p">)()(</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></div>


<div class="viewcode-block" id="real"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.real.html#mindspore.ops.real">[docs]</a><span class="k">def</span> <span class="nf">real</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor that is the real part of the input.</span>
<span class="sd">    If input is real, it is returned unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.asarray(np.complex(1.3+0.4j)), ms.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.real(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Real</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="reciprocal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reciprocal.html#mindspore.ops.reciprocal">[docs]</a><span class="k">def</span> <span class="nf">reciprocal</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns reciprocal of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{x_{i}}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.array([1.0, 2.0, 4.0]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reciprocal(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.   0.5  0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For reciprocal, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_complex</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Reciprocal</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="rsqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rsqrt.html#mindspore.ops.rsqrt">[docs]</a><span class="k">def</span> <span class="nf">rsqrt</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes reciprocal of square root of input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{\sqrt{input_{i}}}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of rsqrt. Its each element must be a non-negative</span>
<span class="sd">            number, if an element is negative, the calculation result is nan.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([-0.0370,  0.2970,  1.5420, -0.9105])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rsqrt(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [       nan 1.8349396  0.80530024        nan]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Rsqrt</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sqrt.html#mindspore.ops.sqrt">[docs]</a><span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sqrt of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\sqrt{x_{i}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of number.Number.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sqrt(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sqrt_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="square"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.square.html#mindspore.ops.square">[docs]</a><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = input_i ^ 2</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.square(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 4. 9.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">square_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="outer"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.outer.html#mindspore.ops.outer">[docs]</a><span class="k">def</span> <span class="nf">outer</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vec2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return outer product of `input` and `vec2`. If `input` is a vector of size :math:`n`</span>
<span class="sd">    and `vec2` is a vector of size :math:`m` , then output must be a matrix of shape :math:`(n, m)` .</span>

<span class="sd">    Note:</span>
<span class="sd">        This function does not broadcast.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): 1-D input vector.</span>
<span class="sd">        vec2 (Tensor): 1-D input vector.</span>

<span class="sd">    Returns:</span>
<span class="sd">        out (Tensor, optional), 2-D matrix, the outer product of two vectors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `vec2` is not a Tensor.</span>
<span class="sd">        ValueError: If `input` or `vec2` is not an 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([7, 8, 9]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; vec2 = Tensor(np.array([7, 10, 11]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.outer(input, vec2)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[49 70 77]</span>
<span class="sd">         [56 80 88]</span>
<span class="sd">         [63 90 99]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input input must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec2</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input vec2 must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the input input must be a 1-D vector!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vec2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;the input vec2 must be a 1-D vector!&quot;</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">mul_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">)()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">mul_ops</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="mv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mv.html#mindspore.ops.mv">[docs]</a><span class="k">def</span> <span class="nf">mv</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `mat` and vector `vec`.</span>

<span class="sd">    If `mat` is a Tensor with :math:`(N, M)`, `vec` is a 1-D Tensor of size :math:`M`,</span>
<span class="sd">    out will be 1-D of size :math:`N`.</span>

<span class="sd">    Args:</span>
<span class="sd">        mat (Tensor): Input matrix of shape :math:`(N, M)`.</span>
<span class="sd">        vec (Tensor): Input vector of shape :math:`(M,)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output Tensor is :math:`(N,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `mat` or `vec` is not a Tensor.</span>
<span class="sd">        ValueError: If `mat` is not a 2-D Tensor or `vec` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; mat = Tensor(np.array([[3., 4.], [1., 6.], [1., 3.]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec = Tensor(np.array([1., 2.]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mv(mat, vec)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [11. 13. 7.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">matmul_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">)()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">)()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input mat must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input vec must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The input mat must be 2-D Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The input vec must be 1-D Tensor.&quot;</span><span class="p">)</span>

    <span class="n">length_vec</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">vec</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="p">(</span><span class="n">length_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">matmul_op</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">T</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="addbmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addbmm.html#mindspore.ops.addbmm">[docs]</a><span class="k">def</span> <span class="nf">addbmm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies batch matrix multiplication to `batch1` and `batch2`, with a reduced add step and add `input` to the result.</span>

<span class="sd">    The optional values `alpha` and `beta` are the matrix-matrix product between `batch1` and `batch2` and the scale</span>
<span class="sd">    factor for the added tensor `input` respectively. If `beta` is 0, then `input` will be ignored.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = \beta input + \alpha (\sum_{i=0}^{b-1} {batch1_i @ batch2_i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor to be added.</span>
<span class="sd">        batch1 (Tensor): The first batch of tensor to be multiplied.</span>
<span class="sd">        batch2 (Tensor): The second batch of tensor to be multiplied.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        beta (Union[int, float], optional): Multiplier for `input`. Default: 1.</span>
<span class="sd">        alpha (Union[int, float], optional): Multiplier for `batch1` @ `batch2`. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` or beta is not an int or float.</span>
<span class="sd">        ValueError: If `batch1`, `batch2` cannot apply batch matrix multiplication.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; m = np.ones((3, 3)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.arange(24).astype(np.float32).reshape((2, 3, 4))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.arange(24).astype(np.float32).reshape((2, 4, 3))</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(arr1)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(arr2)</span>
<span class="sd">        &gt;&gt;&gt; c = Tensor(m)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addbmm(c, a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 949. 1009. 1069.]</span>
<span class="sd">         [1285. 1377. 1469.]</span>
<span class="sd">         [1621. 1745. 1869.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dim1</span> <span class="o">=</span> <span class="n">batch1</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">dim2</span> <span class="o">=</span> <span class="n">batch2</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">dim1</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="n">dim2</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;addbmm&#39;, &#39;batch1&#39; and &#39;batch2&#39; must be 3D, but got </span><span class="si">{</span><span class="n">dim1</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="n">dim2</span><span class="si">}</span><span class="s2"> respectively.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;addbmm&#39;, parameter &#39;alpha&#39; must be an int or float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;addbmm&#39;, parameter &#39;beta&#39; must be an int or float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">bmm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)()</span>
    <span class="n">bmm_res</span> <span class="o">=</span> <span class="n">bmm_op</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">beta</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">bmm_res</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span></div>


<div class="viewcode-block" id="addmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addmm.html#mindspore.ops.addmm">[docs]</a><span class="k">def</span> <span class="nf">addmm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `mat1` and matrix `mat2`. The matrix `input` is added to the final result.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = \beta input + \alpha (mat1 @ mat2)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor to be added.</span>
<span class="sd">        mat1 (Tensor): The first tensor to be multiplied.</span>
<span class="sd">        mat2 (Tensor): The second tensor to be multiplied.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        beta (Union[int, float], optional): Multiplier for `input`. Default: 1.</span>
<span class="sd">        alpha (Union[int, float], optional): Multiplier for `mat1` @ `mat2`. Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `mat1`, `mat2` cannot apply matrix multiplication.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; m = np.ones((3, 3)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; arr1 = np.arange(12).astype(np.float32).reshape((3, 4))</span>
<span class="sd">        &gt;&gt;&gt; arr2 = np.arange(12).astype(np.float32).reshape((4, 3))</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(arr1)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(arr2)</span>
<span class="sd">        &gt;&gt;&gt; c = Tensor(m)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addmm(c, a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 43.  49.  55.]</span>
<span class="sd">         [115. 137. 159.]</span>
<span class="sd">         [187. 225. 263.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;addmm&#39;, parameter &#39;alpha&#39; must be an int or float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;addmm&#39;, parameter &#39;beta&#39; must be an int or float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">matmul_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">beta</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">matmul_op</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">mat2</span><span class="p">))</span></div>


<div class="viewcode-block" id="addmv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addmv.html#mindspore.ops.addmv">[docs]</a><span class="k">def</span> <span class="nf">addmv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies matrix `mat` and vector `vec`. The vector `x` is added to the final result.</span>

<span class="sd">    If mat is a :math:`(N, M)` tensor, vec is a 1-D tensor of size :math:`M`, then `x` must be broadcastable</span>
<span class="sd">    with a 1-D tensor of size :math:`N`.In this case `out` will be 1-D tensor of size :math:`N`.</span>

<span class="sd">    The optional values `beta` and `alpha` are the matrix-vector product between `mat` and `vec` and the scale</span>
<span class="sd">    factor for the added Tensor `x` respectively. If `beta` is 0, then `x` will be ignored.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = β x + α (mat @ vec)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Vector to be added. The shape of the tensor is :math:`(N,)`.</span>
<span class="sd">        mat (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(N, M)`.</span>
<span class="sd">        vec (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(M,)`.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        beta (scalar[int, float, bool], optional): Multiplier for `x` (β). The `beta` must be int or</span>
<span class="sd">            float or bool. Default: ``1`` .</span>
<span class="sd">        alpha (scalar[int, float, bool], optional): Multiplier for `mat` @ `vec` (α). The `alpha` must</span>
<span class="sd">            be int or float or bool. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N,)`, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `mat`, `vec`, `x` is not a Tensor.</span>
<span class="sd">        TypeError: If inputs `mat`, &#39;vec&#39; are not the same dtype.</span>
<span class="sd">        ValueError: If `mat` is not a 2-D Tensor.</span>
<span class="sd">        ValueError: If `vec` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2., 3.]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mat = Tensor(np.array([[2., 5., 3.], [4., 2., 2.]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec = Tensor(np.array([3., 2., 4.]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addmv(x, mat, vec)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [30. 27.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addmv, inputs must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">mat</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addmv, the mat and vec should be the same dtype.&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;vec&quot;</span><span class="p">,</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_input_2d</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">,</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                        <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addmv&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="n">scalar_cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">mv</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">vec</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="adjoint"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.adjoint.html#mindspore.ops.adjoint">[docs]</a><span class="k">def</span> <span class="nf">adjoint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the conjugation of Tensor element by element, and transposes the last two dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the calculated result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([[0. + 0.j, 1. + 1.j], [2. + 2.j, 3. + 3.j]]), mindspore.complex128)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.adjoint(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.-0.j 2.-2.j]</span>
<span class="sd">         [1.-1.j 3.-3.j]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span></div>


<div class="viewcode-block" id="addr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.addr.html#mindspore.ops.addr">[docs]</a><span class="k">def</span> <span class="nf">addr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the outer product of two vector `vec1` and `vec2`, and adds the resulting matrix to `x`.</span>

<span class="sd">    Given `vec1` and `vec2` of sizes :math:`N` and :math:`M`,</span>
<span class="sd">    `x` must be able to broadcast to a matrix of shape :math:`(N, M)`.</span>

<span class="sd">    `beta` and `alpha` are optional scaling factors for the outer product of `vec1` and `vec2`,</span>
<span class="sd">    and the matrix `x` respectively. Setting `beta` to 0 will exclude `x` from the computation.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = β x + α (vec1 ⊗ vec2)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Vector to be added. The shape of the tensor is :math:`(N, M)`.</span>
<span class="sd">        vec1 (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(N,)`.</span>
<span class="sd">        vec2 (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(M,)`.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        beta (scalar[int, float, bool], optional): Multiplier for `x` (β). The `beta` must be int or</span>
<span class="sd">            float or bool, Default: 1.</span>
<span class="sd">        alpha (scalar[int, float, bool], optional): Multiplier for `vec1` ⊗ `vec2` (α). The `alpha` must</span>
<span class="sd">            be int or float or bool, Default: 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, M)`, has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `vec1`, `vec2` is not a Tensor.</span>
<span class="sd">        TypeError: If inputs `vec1`, `vec2` are not the same dtype.</span>
<span class="sd">        ValueError: If `vec1`, `vec2` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2., 2.], [3., 2.], [3., 4.]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec1 = Tensor(np.array([2., 3., 2.], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; vec2 = Tensor(np.array([3, 4], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.addr(x, vec1, vec2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 8. 10.]</span>
<span class="sd">         [12. 14.]</span>
<span class="sd">         [ 9. 12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">vec2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addr, inputs must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">vec1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">vec2</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Addr, the vec1 and vec2 should be the same dtype.&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">vec1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;vec1&quot;</span><span class="p">,</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_input_1d</span><span class="p">(</span><span class="n">vec2</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;vec2&quot;</span><span class="p">,</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_input_2d</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span>
                       <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                        <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;Addr&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="n">scalar_cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">scalar_cast</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">matmul_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="n">length_vec1</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">vec1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">vec1</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="p">(</span><span class="n">length_vec1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">length_vec2</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">vec2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">vec2</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">vec2</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">length_vec2</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">matmul_op</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="lcm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lcm.html#mindspore.ops.lcm">[docs]</a><span class="k">def</span> <span class="nf">lcm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes least common multiplier of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor.</span>
<span class="sd">        other (Tensor): The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher digits in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `input` or `other` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.lcm(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [14 24 36]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lcm_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Lcm</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">lcm_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="cdist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cdist.html#mindspore.ops.cdist">[docs]</a><span class="k">def</span> <span class="nf">cdist</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes p-norm distance between each pair of row vectors of two input Tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        x1 (Tensor): Input tensor of shape :math:`(B, P, M)`.</span>
<span class="sd">          Letter :math:`B` represents 0 or positive int number.</span>
<span class="sd">          When :math:`B` is equal to 0, it means this dimension can be ignored,</span>
<span class="sd">          i.e. shape of the tensor is :math:`(P, M)`. The supported dtype is</span>
<span class="sd">          [float32, float64] on GPU, or [float32] on CPU.</span>
<span class="sd">        x2 (Tensor): Input tensor of shape :math:`(B, R, M)`, has the same dtype as `x1`.</span>
<span class="sd">        p (float, optional): P value for the p-norm distance to calculate between each</span>
<span class="sd">          vector pair, P ∈ [0,∞]. Default: ``2.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, p-norm distance, has the same dtype as `x1`, its shape is :math:`(B, P, R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1` or `x2` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `x1` or `x2` is not in [float32, float64] on GPU, or is not in [float32] on CPU.</span>
<span class="sd">        TypeError: If `p` is not float32.</span>
<span class="sd">        ValueError: If `p` is negative.</span>
<span class="sd">        ValueError: If dimension of `x1` is not the same as `x2`.</span>
<span class="sd">        ValueError: If dimension of `x1` or `x2` is neither 2 nor 3.</span>
<span class="sd">        ValueError: If the batch shape of `x1` is not the same as the shape of `x2`.</span>
<span class="sd">        ValueError: If the number of columns of `x1` is not the same as the number of `x2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1.0, 1.0], [2.0, 2.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[[3.0, 3.0], [3.0, 3.0]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cdist(x, y, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2.8284273 2.8284273]</span>
<span class="sd">          [1.4142137 1.4142137]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cdist_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cdist</span><span class="p">)(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cdist_</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span></div>


<div class="viewcode-block" id="gcd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gcd.html#mindspore.ops.gcd">[docs]</a><span class="k">def</span> <span class="nf">gcd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes greatest common divisor of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor.</span>
<span class="sd">        other (Tensor): The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher digits in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `input` or `other` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.gcd(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [7 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">gcd_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Gcd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gcd_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="lerp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lerp.html#mindspore.ops.lerp">[docs]</a><span class="k">def</span> <span class="nf">lerp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Does a linear interpolation of two tensors input and end based on a float or tensor weight.</span>

<span class="sd">    If `weight` is a tensor, the shapes of three inputs need to be broadcast;</span>
<span class="sd">    If `weight` is a float, the shapes of `input` and `end` need to be broadcast.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_{i} = input_{i} + weight_{i} * (end_{i} - input_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The tensor with the starting points. Data type must be float16 or float32.</span>
<span class="sd">        end (Tensor): The tensor with the ending points. Data type must be the same as `input`.</span>
<span class="sd">        weight (Union[float, Tensor]): The weight for the interpolation formula. Must be a float</span>
<span class="sd">            or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `end` is not a tensor.</span>
<span class="sd">        TypeError: If `weight` is neither scalar(float) nor tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `end` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `weight` is neither float16 nor float32 when it is a tensor.</span>
<span class="sd">        TypeError: If `input` and `end` have different data types.</span>
<span class="sd">        TypeError: If `input`, `end` and `weight` have different data types when `weight` is a tensor.</span>
<span class="sd">        ValueError: If `end` could not be broadcast to a tensor with shape of `input`.</span>
<span class="sd">        ValueError: If `weight` could not be broadcast to tensors with shapes of `input` and `end` when it is a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(np.array([10., 10., 10., 10.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lerp(input, end, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5.5 6. 6.5 7. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">lerp_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="bernoulli"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bernoulli.html#mindspore.ops.bernoulli">[docs]</a><span class="k">def</span> <span class="nf">bernoulli</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly set the elements of output to 0 or 1 with the probability of `p`</span>
<span class="sd">    which follows the Bernoulli distribution.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_{i} \sim Bernoulli(p_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor. Data</span>
<span class="sd">                        type must be int8, uint8, int16, int32, int64, bool, float32 or float64.</span>
<span class="sd">        p (Union[Tensor, float], optional): Success probability, representing the probability of setting 1 for the</span>
<span class="sd">            corresponding position of the current Tensor. It has the same shape as `input`, the value of `p`</span>
<span class="sd">            must be in the range `[0, 1]`. Default: 0.5.</span>
<span class="sd">        seed (Union[int, None], optional): The seed value for random generating. The value of `seed` must be -1 or a</span>
<span class="sd">            positive integer, and -1 means using the current timestamp. Default: None,</span>
<span class="sd">            which will be treated as 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor), with the same shape and type as `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not one of: int8, uint8, int16, int32, int64, bool, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `p` is not one of: float32, float64.</span>
<span class="sd">        TypeError: If dtype of `seed` is not int or None.</span>
<span class="sd">        ValueError: If `p` is not in range [0, 1].</span>
<span class="sd">        ValueError: If `seed` is less than 0 and not -1.</span>
<span class="sd">        ValueError: If `p` is a Tensor but has different shape than `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.int8)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bernoulli(input_x, p=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 1 1]</span>
<span class="sd">        &gt;&gt;&gt; input_p = Tensor(np.array([0.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bernoulli(input_x, input_p)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="s1">&#39;bernoulli&#39;</span><span class="p">)</span>
    <span class="n">bernoulli_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Bernoulli</span><span class="p">)(</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="n">p</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">bernoulli_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i1.html#mindspore.ops.bessel_i1">[docs]</a><span class="k">def</span> <span class="nf">bessel_i1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.5651591  -0.25789431  0.25789431  0.5651591]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_i1e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_i1e.html#mindspore.ops.bessel_i1e">[docs]</a><span class="k">def</span> <span class="nf">bessel_i1e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel i1e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, -0.5, 0.5, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_i1e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.20791042  -0.15642083  0.15642083  0.20791042]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_i1e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k1.html#mindspore.ops.bessel_k1">[docs]</a><span class="k">def</span> <span class="nf">bessel_k1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k1 function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k1(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.65644112  0.60190723  0.13986588  0.0124835]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k1_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="bessel_k1e"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bessel_k1e.html#mindspore.ops.bessel_k1e">[docs]</a><span class="k">def</span> <span class="nf">bessel_k1e</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Bessel k1e function of x element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor. The data type must be float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 1., 2., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bessel_k1e(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.73100971  1.63615349  1.03347685  0.68157595]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">bessel_k1e_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_input_dtype</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<div class="viewcode-block" id="deg2rad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deg2rad.html#mindspore.ops.deg2rad">[docs]</a><span class="k">def</span> <span class="nf">deg2rad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts angles in degrees to angles in radians element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>
<span class="sd">            With float16, float32 or float64 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` isn&#39;t float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[90.0, -90.0], [180.0, -180.0], [270.0, -270.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deg2rad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.5707964 -1.5707964]</span>
<span class="sd">         [ 3.1415927 -3.1415927]</span>
<span class="sd">         [ 4.712389  -4.712389 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x must be tensor&quot;</span><span class="p">)</span>
    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">180.0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">180.0</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="rad2deg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rad2deg.html#mindspore.ops.rad2deg">[docs]</a><span class="k">def</span> <span class="nf">rad2deg</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts angles in radians to angles in degrees element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` isn&#39;t float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[6.283, -3.142],[1.570, -6.283],[3.142, -1.570]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rad2deg(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 359.98935 -180.02333]</span>
<span class="sd">         [  89.95438 -359.98935]</span>
<span class="sd">         [ 180.02333  -89.95438]]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input x must be tensor&quot;</span><span class="p">)</span>
    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">180.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">180.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="frac"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.frac.html#mindspore.ops.frac">[docs]</a><span class="k">def</span> <span class="nf">frac</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the fractional part of each element in the input</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): x is a tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2, 4.2, -2.5], mstype.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.frac(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.      0.1992 -0.5   ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">frac_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mod</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">frac_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<span class="c1">#####################################</span>
<span class="c1"># Reduction Operation Functions.</span>
<span class="c1">#####################################</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_create_cummin_perm</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Insure axis is in [-len(x_shape),len(s_shape)-1]&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">len_axis</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The date type of &#39;axis&#39; must be Int, but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">len_axis</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;</span> <span class="n">len_axis</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The value of axis must be in [</span><span class="si">{</span><span class="o">-</span><span class="n">len_axis</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">len_axis</span><span class="si">}</span><span class="s2">], but got </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">len_axis</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="n">_check</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">len_axis</span><span class="p">)</span>
    <span class="n">prem</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">len_axis</span><span class="p">)]</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">len_axis</span>
    <span class="n">prem</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prem</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis</span><span class="p">,</span> <span class="mi">0</span>
    <span class="n">prem</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">prem</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prem</span>


<div class="viewcode-block" id="cummin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cummin.html#mindspore.ops.cummin">[docs]</a><span class="k">def</span> <span class="nf">cummin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tuple (values,indices) where &#39;values&#39; is the cumulative minimum value of input Tensor `input`</span>
<span class="sd">    along the dimension `axis`, and `indices` is the index location of each minimum value.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            y_{i} = \min(x_{1}, x_{2}, ... , x_{i})</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor, rank of `input` &gt; 0.</span>
<span class="sd">        axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">            `[-input.ndim, input.ndim - 1]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple [Tensor], tuple of 2 Tensors, containing the cumulative minimum of elements and the index.</span>
<span class="sd">        The shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is out the range of `[-input.ndim, input.ndim - 1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cummin(a, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [0 1 1 1 4 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cummin_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cummin</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span> <span class="o">=</span> <span class="n">cummin_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">transpose</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)()</span>
        <span class="n">_shape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">)()</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">_shape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">prem</span> <span class="o">=</span> <span class="n">_create_cummin_perm</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">prem</span><span class="p">)</span>
        <span class="n">out1</span><span class="p">,</span> <span class="n">out2</span> <span class="o">=</span> <span class="n">cummin_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">out1</span><span class="p">,</span> <span class="n">prem</span><span class="p">)</span>
        <span class="n">out2</span> <span class="o">=</span> <span class="n">transpose</span><span class="p">(</span><span class="n">out2</span><span class="p">,</span> <span class="n">prem</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span><span class="p">]</span></div>


<div class="viewcode-block" id="cummax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cummax.html#mindspore.ops.cummax">[docs]</a><span class="k">def</span> <span class="nf">cummax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tuple (values,indices) where &#39;values&#39; is the cumulative maximum value of input Tensor `input`</span>
<span class="sd">    along the dimension `axis`, and `indices` is the index location of each maximum value.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            y_{i} = \max(x_{1}, x_{2}, ... , x_{i})</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor, rank of `input` &gt; 0.</span>
<span class="sd">        axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">            `[-input.ndim, input.ndim - 1]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple [Tensor], tuple of 2 Tensors, containing the cumulative maximum of elements and the index.</span>
<span class="sd">        The shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is out the range of `[-input.ndim, input.ndim - 1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cummax(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 3.  6.  7. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [[0 0 0 0]</span>
<span class="sd">         [0 1 1 0]</span>
<span class="sd">         [2 1 2 0]</span>
<span class="sd">         [2 1 2 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_cummax</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Cummax</span><span class="p">)(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_cummax</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cumsum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cumsum.html#mindspore.ops.cumsum">[docs]</a><span class="k">def</span> <span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum of input Tensor along `axis`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 + x_2 + x_3 + ... + x_i</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, the dtype of `x` only support :int8, uint8, int32, float16 or float32 in case of static shape.</span>
<span class="sd">        For the case of dynamic shape, the dtype of `x` only support int32, float16 or float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor to accumulate.</span>
<span class="sd">        axis (int): Axis along which the cumulative sum is computed.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The desired dtype of returned Tensor. If specified,</span>
<span class="sd">            the input Tensor will be cast to `dtype` before the computation. This is useful for preventing overflows.</span>
<span class="sd">            If not specified, stay the same as original Tensor. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output Tensor is consistent with the input Tensor&#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the axis is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; y = ops.cumsum(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 4. 10. 13. 19.]</span>
<span class="sd">         [ 8. 13. 21. 26.]</span>
<span class="sd">         [ 9. 16. 28. 35.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; y = ops.cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  7. 13. 23.]</span>
<span class="sd">         [ 1.  7. 14. 23.]</span>
<span class="sd">         [ 4.  7. 15. 22.]</span>
<span class="sd">         [ 1.  4. 11. 20.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cumsum_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="sparse_segment_mean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sparse_segment_mean.html#mindspore.ops.sparse_segment_mean">[docs]</a><span class="k">def</span> <span class="nf">sparse_segment_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a Tensor such that :math:`output_i = \frac{\sum_j x_{indices[j]}}{N}` where mean is over :math:`j` such</span>
<span class="sd">    that :math:`segment\_ids[j] == i` and :math:`N` is the total number of values summed. If the mean is empty for</span>
<span class="sd">    a given segment ID :math:`i`, :math:`output[i] = 0`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - On CPU, values in `segment_ids` are always validated to be sorted, and an error is thrown for indices that</span>
<span class="sd">          are not increasing. Moreover, values in `indices` are validated to be bounded, and an error is thrown when</span>
<span class="sd">          `indices` are out of range[0, x.shape[0]).</span>
<span class="sd">        - On GPU, this does not throw an error for unsorted `segment_ids` and out-of-bound `indices`. Out-of-order</span>
<span class="sd">          `segment_ids` result in safe but unspecified behavior, while out-of-range `indices` will be ignored.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): A Tensor, and its rank must be greater than or equal to 1.</span>
<span class="sd">        indices (Tensor): A 1-D Tensor, with int32 or int64 data type.</span>
<span class="sd">        segment_ids (Tensor): A 1-D Tensor, must have the same dtype as `indices`.</span>
<span class="sd">            Values should be sorted and can be repeated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, whose dtype and rank is the same as `x`. The first dimension is equal to the value of the last element</span>
<span class="sd">        of `segment_ids` plus one, and the other dimensions are the same as those of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `indices` or `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `x` is not one of the following dtype: float16, float32, float64.</span>
<span class="sd">        TypeError: If the dtype of `indices` and `segment_ids` are not one of the following dtype: int32, int64.</span>
<span class="sd">        TypeError: If the dtype of `indices` and `segment_ids` are not the same.</span>
<span class="sd">        ValueError: If the shape of `x`, &#39;indices&#39; or `segment_ids` don&#39;t meet the parameter description.</span>
<span class="sd">        ValueError: If the size of &#39;indices&#39; and `segment_ids` are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 1, 2], [1, 2, 3], [3, 6, 7]], dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1, 2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([1,2,2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.sparse_segment_mean(x, indices, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 1. 2.]</span>
<span class="sd">         [2. 4. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sparse_segment_mean_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">)</span></div>


<div class="viewcode-block" id="block_diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.block_diag.html#mindspore.ops.block_diag">[docs]</a><span class="k">def</span> <span class="nf">block_diag</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a block diagonal matrix from the provided Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Tensor): One or more tensors, the dimension of Tensor should be 0, 1 or 2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, two-dimensional with all input tensors arranged in</span>
<span class="sd">        order so that their top left and bottom right corners are</span>
<span class="sd">        diagonally adjacent. All other elements are set to 0.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a Tensor.</span>
<span class="sd">        ValueError: If the dimension of Tensor is not 0, 1 or 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([[4], [3], [2]], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([7, 6, 5], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x3 = Tensor(1, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x4 = Tensor([[5, 4, 3], [2, 1, 0]], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x5 = Tensor([[8, 7], [7, 8]], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.block_diag(x1, x2, x3, x4, x5)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        [[4 0 0 0 0 0 0 0 0 0]</span>
<span class="sd">         [3 0 0 0 0 0 0 0 0 0]</span>
<span class="sd">         [2 0 0 0 0 0 0 0 0 0]</span>
<span class="sd">         [0 7 6 5 0 0 0 0 0 0]</span>
<span class="sd">         [0 0 0 0 1 0 0 0 0 0]</span>
<span class="sd">         [0 0 0 0 0 5 4 3 0 0]</span>
<span class="sd">         [0 0 0 0 0 2 1 0 0 0]</span>
<span class="sd">         [0 0 0 0 0 0 0 0 8 7]</span>
<span class="sd">         [0 0 0 0 0 0 0 0 7 8]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">to_col_block</span><span class="p">(</span><span class="n">arys</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span>
            <span class="n">a</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="n">i</span> <span class="k">else</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ary</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">ary</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ary</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">arys</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="nf">to_2d</span><span class="p">(</span><span class="n">ary</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ary</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For &#39;block_diag&#39;, each element of &#39;inputs&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">ary</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">ary</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ary</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">ary</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ary</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">ary</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;For &#39;block_diag&#39;, the dimension of each elements in &#39;inputs&#39; must be 0, 1, or 2, but got &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ary</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">arys</span> <span class="o">=</span> <span class="p">[</span><span class="n">to_2d</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span> <span class="k">for</span> <span class="n">ary</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
    <span class="n">matrix</span> <span class="o">=</span> <span class="p">[</span><span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">to_col_block</span><span class="p">(</span><span class="n">arys</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ary</span><span class="p">))</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ary</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">arys</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">matrix</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="atleast_1d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atleast_1d.html#mindspore.ops.atleast_1d">[docs]</a><span class="k">def</span> <span class="nf">atleast_1d</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes Tensor in `inputs`, every Tensor has at least one dimension after this operation.</span>

<span class="sd">    Scalar is converted to a 1-D Tensor, input tensor with one or more dimensions will be returned as it is.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union[Tensor, list[Tensor]]): One or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or list[Tensor]. If returned a list, every element `a` in that list satisfies `a.ndim &gt;= 1`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `input` is not a tensor or a list of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.ones((2, 3)))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.ones(()))</span>
<span class="sd">        &gt;&gt;&gt; x3 = Tensor(np.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; out = ops.atleast_1d([x1, x2, x3])</span>
<span class="sd">        &gt;&gt;&gt; print(out[0].asnumpy())</span>
<span class="sd">        [[1. 1. 1.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; print(out[1].asnumpy())</span>
<span class="sd">        [1.]</span>
<span class="sd">        &gt;&gt;&gt; print(out[2].asnumpy())</span>
<span class="sd">        [1. 1. 1. 1. 1.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_expand</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;atleast_1d&#39;, each element of &#39;inputs&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">_expand</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span></div>


<div class="viewcode-block" id="dstack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dstack.html#mindspore.ops.dstack">[docs]</a><span class="k">def</span> <span class="nf">dstack</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors along the third axis.</span>

<span class="sd">    1-D tensors :math:`(N,)` should be reshaped to :math:`(1,N,1)`.</span>
<span class="sd">    2-D tensors :math:`(M,N)` should be reshaped to :math:`(M,N,1)` before concatenation.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union(List[Tensor], Tuple[Tensor])): A sequence of tensors.</span>
<span class="sd">            The tensors must have the same shape along all but the third axis.</span>
<span class="sd">            1-D or 2-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, will be at least 3-D.</span>
<span class="sd">        The output shape is similar to the output of `numpy.dstack()` function.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `inputs` is not tuple or list.</span>
<span class="sd">        ValueError: If `inputs` is empty.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.arange(1, 7).reshape(2, 3))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.arange(7, 13).reshape(2, 3))</span>
<span class="sd">        &gt;&gt;&gt; out = ops.dstack([x1, x2])</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        [[[ 1.  7.]</span>
<span class="sd">          [ 2.  8.]</span>
<span class="sd">          [ 3.  9.]]</span>
<span class="sd">         [[ 4. 10.]</span>
<span class="sd">          [ 5. 11.]</span>
<span class="sd">          [ 6. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;dstack&#39;, &#39;inputs&#39; must be list or tuple of tensors, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;dstack&#39;, &#39;inputs&#39; can not be empty.&quot;</span><span class="p">)</span>
    <span class="n">trans_inputs</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;dstack&#39;, each elements of &#39;inputs&#39; must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;dstack&#39;, each elements of &#39;inputs&#39; can not be empty.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">trans_inputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_inputs</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;dstack&#39;, at least one tensor is needed to concatenate.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">2</span><span class="p">)(</span><span class="n">trans_inputs</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_is_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<div class="viewcode-block" id="diff"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diff.html#mindspore.ops.diff">[docs]</a><span class="k">def</span> <span class="nf">diff</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">prepend</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the n-th discrete difference along a specified axis of a given input `x`.</span>

<span class="sd">    The first difference is calculated as :math:`out[i] = x[i+1] - x[i]` along the specified `axis`.</span>
<span class="sd">    To compute higher differences, the function is called recursively</span>
<span class="sd">    using the output from the previous iteration as input.</span>

<span class="sd">    Note:</span>
<span class="sd">        Zero-shaped Tensor is not supported, a value error is raised if</span>
<span class="sd">        an empty Tensor is encountered. Any dimension of an Tensor is 0 is considered</span>
<span class="sd">        an empty Tensor. Tensor with shape of :math:`(0,)`, :math:`(1, 2, 0, 4)` are all</span>
<span class="sd">        empty Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor.</span>
<span class="sd">            Full support for signed integers, partial support for floats and complex numbers</span>
<span class="sd">        n (int, optional): The number of times values are differenced. If zero,</span>
<span class="sd">            the input is returned as-is. Currently only 1 is supported. Default: ``1`` .</span>
<span class="sd">        axis (int, optional): The axis along which the difference is taken, default</span>
<span class="sd">            is the last axis. Default: ``-1`` .</span>
<span class="sd">        prepend (Tensor, optional): Values to prepend to `x` along</span>
<span class="sd">            `axis` prior to performing the difference. Scalar values are expanded to</span>
<span class="sd">            arrays with length 1 in the direction of `axis` and the shape of the input</span>
<span class="sd">            array along all other axis. Otherwise the dimension and shape must</span>
<span class="sd">            match `x` except along `axis`. Default: ``None`` .</span>
<span class="sd">        append (Tensor, optional): Values to append to `x` along</span>
<span class="sd">            `axis` prior to performing the difference. Scalar values are expanded to</span>
<span class="sd">            arrays with length 1 in the direction of `axis` and the shape of the input</span>
<span class="sd">            array along all other axis. Otherwise the dimension and shape must</span>
<span class="sd">            match `x` except along `axis`. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the n-th differences of input. The shape of the output is the same as `x`</span>
<span class="sd">        except along `axis` where the size is reduced by `n`. The type of the output</span>
<span class="sd">        is the same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of the elementes in `x` is uint16, uint32 or uint64.</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        ValueError: If `x` is an empty Tensor.</span>
<span class="sd">        ValueError: If the dim of `x` is less than 1.</span>
<span class="sd">        RuntimeError: If `n` is not 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 3, -1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.diff(x)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        [ 2 -4  1  4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;diff&#39;, &#39;x&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;diff&#39;, the dimension &#39;x&#39; must be at least 1, but got </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;diff&#39;, &#39;x&#39; can not be an empty Tensor.&quot;</span><span class="p">)</span>
    <span class="n">_check_is_int</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;diff&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;diff&#39;, &#39;n&#39; must be 1, but got </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;diff&#39;, the data type of the elements in &#39;x&#39; cannot be uint16, uint32, uint64, but got </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">prepend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">append</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)((</span><span class="n">prepend</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">append</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">append</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)((</span><span class="n">x</span><span class="p">,</span> <span class="n">append</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">prepend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="p">)((</span><span class="n">prepend</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">])</span>
    <span class="n">a1</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">TupleToTensor</span><span class="p">()(</span><span class="n">a</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">a2</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">TupleToTensor</span><span class="p">()(</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a2</span> <span class="o">-</span> <span class="n">a1</span></div>


<div class="viewcode-block" id="tril_indices"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tril_indices.html#mindspore.ops.tril_indices">[docs]</a><span class="k">def</span> <span class="nf">tril_indices</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the indices of the lower triangular elements in a `row` * `col` matrix</span>
<span class="sd">    and returns them as a 2-by-N Tensor. The first row of the Tensor contains</span>
<span class="sd">    row coordinates, and the second row contains column coordinates. The coordinates are</span>
<span class="sd">    sorted by row and then by column.</span>

<span class="sd">    The lower triangular part of the matrix consists of all elements on and below the diagonal.</span>

<span class="sd">    Note:</span>
<span class="sd">        When running on CUDA, row * col must be less than 2^59 to prevent overflow during calculation.</span>

<span class="sd">    Args:</span>
<span class="sd">        row (int): number of rows in the 2-D matrix.</span>
<span class="sd">        col (int): number of columns in the 2-D matrix.</span>
<span class="sd">        offset (int, optional): diagonal offset from the main diagonal. Default: ``0`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor.</span>
<span class="sd">            An optional data type of `mindspore.int32` and `mindspore.int64`. Default: ``mstype.int64`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - indices of the elements in lower triangular part of matrix. The type is specified by `dtype`.</span>
<span class="sd">          The shape of output is :math:`(2, tril\_size)`, where :math:`tril\_size` is the number of elements in the</span>
<span class="sd">          lower triangular matrix.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `row`, `col` or `offset` is not an int.</span>
<span class="sd">        TypeError: If `dtype` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `row` or `col` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tril_indices(4, 3, -1, dtype=mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2 2 3 3 3]</span>
<span class="sd">         [0 0 1 0 1 2]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tril_indices_</span> <span class="o">=</span> <span class="n">TrilIndices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tril_indices_</span><span class="p">()</span></div>


<div class="viewcode-block" id="triu_indices"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.triu_indices.html#mindspore.ops.triu_indices">[docs]</a><span class="k">def</span> <span class="nf">triu_indices</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the indices of the upper triangular elements in a `row` * `col` matrix</span>
<span class="sd">    and returns them as a 2-by-N Tensor. The first row of the Tensor contains</span>
<span class="sd">    row coordinates, and the second row contains column coordinates. The coordinates are</span>
<span class="sd">    sorted by row and then by column.</span>

<span class="sd">    The upper triangular part of the matrix consists of all elements on and above the diagonal.</span>

<span class="sd">    Note:</span>
<span class="sd">        When running on CUDA, row * col must be less than 2^59 to prevent overflow during calculation.</span>

<span class="sd">    Args:</span>
<span class="sd">        row (int): number of rows in the 2-D matrix.</span>
<span class="sd">        col (int): number of columns in the 2-D matrix.</span>
<span class="sd">        offset (int, optional): diagonal offset from the main diagonal. Default: ``0`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor.</span>
<span class="sd">            An optional data type of `mindspore.int32` and `mindspore.int64`. Default: ``mstype.int64``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - indices of the elements in upper triangular part of matrix. The type is specified by `dtype`.</span>
<span class="sd">          The shape of output is :math:`(2, triu\_size)`, where :math:`triu\_size` is the number of elements in the</span>
<span class="sd">          upper triangular matrix.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `row`, `col` or `offset` is not an int.</span>
<span class="sd">        TypeError: If `dtype` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `row` or `col` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.triu_indices(4, 4, 2, dtype=mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 1]</span>
<span class="sd">         [2 3 3]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">triu_indices_</span> <span class="o">=</span> <span class="n">TriuIndices</span><span class="p">(</span><span class="n">row</span><span class="o">=</span><span class="n">row</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">triu_indices_</span><span class="p">()</span></div>


<div class="viewcode-block" id="atleast_2d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atleast_2d.html#mindspore.ops.atleast_2d">[docs]</a><span class="k">def</span> <span class="nf">atleast_2d</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes Tensor in `inputs`, every Tensor has at least 2 dimension after this operation.</span>

<span class="sd">    Scalar or 1-D Tensor is converted to 2-D Tensor, tensor with higher dimensions will be returned as it is.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union[Tensor, list[Tensor]]): One or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or list[Tensor]. If returned a list, every element `a` in that list satisfies `a.ndim &gt;= 2` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `input` is not a tensor or a list of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.ones(())</span>
<span class="sd">        &gt;&gt;&gt; x3 = np.ones(5)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.atleast_2d([x1, x2, x3])</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        (Tensor(shape=[2, 3], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00, 1.00000000e+00, 1.00000000e+00],</span>
<span class="sd">        [ 1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00]]), Tensor(shape=[1, 5], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_expand</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;expect Tensor or list of tensors, but got &quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">_expand</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span></div>


<span class="k">def</span> <span class="nf">cartesian_prod</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a Cartesian product for a given tensor sequence.</span>
<span class="sd">    The behavior is similar to Python&#39;s `itertools.product`.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (List[Tensor]): Tensor sequence.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a Cartesian product for a given tensor sequence.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1, 2])</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([5])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.cartesian_prod(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        [[1 5]</span>
<span class="sd">         [2 5]]</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1, 2, 3, 4])</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([5, 6, 7])</span>
<span class="sd">        &gt;&gt;&gt; x3 = Tensor([8, 9, 0, 1, 2])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.cartesian_prod(x1, x2, x3)</span>
<span class="sd">        &gt;&gt;&gt; print(len(out))</span>
<span class="sd">        60</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">meshgrid</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Meshgrid</span><span class="p">(</span><span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;ij&quot;</span><span class="p">)</span>
    <span class="n">meshgrid_output</span> <span class="o">=</span> <span class="n">meshgrid</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">stack</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Stack</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">stack_output</span> <span class="o">=</span> <span class="n">stack</span><span class="p">(</span><span class="n">meshgrid_output</span><span class="p">)</span>
    <span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">reshape</span><span class="p">(</span><span class="n">stack_output</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>


<div class="viewcode-block" id="atleast_3d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atleast_3d.html#mindspore.ops.atleast_3d">[docs]</a><span class="k">def</span> <span class="nf">atleast_3d</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshapes Tensor in `inputs`, every Tensor has at least 3 dimension after this operation.</span>

<span class="sd">    Scalar, 1-D or 2-D Tensor is converted to 3-D Tensor,</span>
<span class="sd">    tensor with higher dimensions will be returned as it is.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union[Tensor, list[Tensor]]): One or more input tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or list[Tensor]. If returned a list, every element `a` in that list satisfies `a.ndim &gt;= 3`.</span>
<span class="sd">        For example, a 1-D Tensor of shape :math:`(N,)` becomes a Tensor of shape :math:`(1, N, 1)`, and</span>
<span class="sd">        a 2-D Tensor of shape :math:`(M, N)` becomes a tensor of shape :math:`(M, N, 1)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `input` is not a tensor or a list of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.ones((2, 3)))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.ones(()))</span>
<span class="sd">        &gt;&gt;&gt; x3 = Tensor(np.ones(5))</span>
<span class="sd">        &gt;&gt;&gt; out = ops.atleast_3d([x1, x2, x3])</span>
<span class="sd">        &gt;&gt;&gt; print(out[0].asnumpy())</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [1.]</span>
<span class="sd">          [1.]]</span>
<span class="sd">        &lt;BLANKLINE&gt;</span>
<span class="sd">         [[1.]</span>
<span class="sd">          [1.]</span>
<span class="sd">          [1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(out[1].asnumpy())</span>
<span class="sd">        [[[1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(out[2].asnumpy())</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [1.]</span>
<span class="sd">          [1.]</span>
<span class="sd">          [1.]</span>
<span class="sd">          [1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_expand3</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">()(</span><span class="n">arr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">arr</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Size</span><span class="p">()(</span><span class="n">arr</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">arr</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">arr</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="k">return</span> <span class="n">arr</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">_expand3</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;atleast_3d&#39;, each element of &#39;inputs&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">_expand3</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="k">for</span> <span class="n">arr</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">])</span></div>


<div class="viewcode-block" id="view_as_real"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.view_as_real.html#mindspore.ops.view_as_real">[docs]</a><span class="k">def</span> <span class="nf">view_as_real</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    View a complex Tensor as a real Tensor.</span>
<span class="sd">    The size of last dimension of the returned real Tensor is 2, and the last dimension is composed of</span>
<span class="sd">    the real and imaginary components of complex numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input must be a complex Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A real Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input Tensor is not a complex Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2+1j,2+3j,2-1j,2], mstype.complex64)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.view_as_real(x))</span>
<span class="sd">        [[ 2.  1.]</span>
<span class="sd">         [ 2.  3.]</span>
<span class="sd">         [ 2. -1.]</span>
<span class="sd">         [ 2.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_complex</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For view_as_real, the dtype of input Tensor must be complex.&quot;</span><span class="p">)</span>
    <span class="n">real_part</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">real</span><span class="p">()</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">imag_part</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">imag</span><span class="p">()</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">con</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">con</span><span class="p">((</span><span class="n">real_part</span><span class="p">,</span> <span class="n">imag_part</span><span class="p">))</span></div>


<div class="viewcode-block" id="vstack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.vstack.html#mindspore.ops.vstack">[docs]</a><span class="k">def</span> <span class="nf">vstack</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence vertically.</span>

<span class="sd">    This is equivalent to concatenation along the first axis.</span>
<span class="sd">    1-D tensors :math:`(N,)` should firstly be reshaped to :math:`(1, N)`,</span>
<span class="sd">    and then be concatenated along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (Union(List[tensor], Tuple[tensor])): A sequence of 1-D or 2-D tensors.</span>
<span class="sd">            The tensors must have the same shape along all but the first axis.</span>
<span class="sd">            1-D tensors must have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, formed by stacking the given tensors, will be at least 3-D.</span>
<span class="sd">        The output shape is similar to the output of `numpy.vstack()` function.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `inputs` is not list or tuple.</span>
<span class="sd">        ValueError: If `inputs` is empty.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x1 = np.array([3, 1, 4])</span>
<span class="sd">        &gt;&gt;&gt; x2 = np.array([1, 5, 9])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.vstack([x1, x2])</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[3 1 4]</span>
<span class="sd">         [1 5 9]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;vstack&#39;, list or tuple of tensors are required, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;For &#39;vstack&#39;, inputs can not be empty&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">trans_tup</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;vstack&#39;, Tensor is required, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">shape</span><span class="p">,)</span>
            <span class="n">ndim_diff</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">ndim_diff</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim_diff</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">]</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">tensor</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">trans_tup</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_tup</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;vstack&#39;, need at least one tensor to concatenate.&quot;</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">trans_tup</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="combinations"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.combinations.html#mindspore.ops.combinations">[docs]</a><span class="k">def</span> <span class="nf">combinations</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">with_replacement</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns all r-length subsequences of input Tensor.</span>

<span class="sd">    When `with_replacement` is set to `False`, it works similar to Python&#39;s</span>
<span class="sd">    `itertools.combinations`, and when `with_replacement` is set to `True`,</span>
<span class="sd">    it behaves like `itertools.combinations_with_replacement`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): One-dimensional tensors.</span>
<span class="sd">        r (int, optional): Number of elements to perform combination. Default: ``2`` .</span>
<span class="sd">        with_replacement (bool, optional): Allow duplication or not. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, contains all possible combinations of elements sampled from input Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        TypeError: If `x` is not an int.</span>
<span class="sd">        TypeError: If `with_replacement` is not bool.</span>
<span class="sd">        ValueError: If `x` is not one-dimensional.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 3, -1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.combinations(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[ 1  3]</span>
<span class="sd">         [ 1 -1]</span>
<span class="sd">         [ 1  0]</span>
<span class="sd">         [ 1  4]</span>
<span class="sd">         [ 3 -1]</span>
<span class="sd">         [ 3  0]</span>
<span class="sd">         [ 3  4]</span>
<span class="sd">         [-1  0]</span>
<span class="sd">         [-1  4]</span>
<span class="sd">         [ 0  4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_combinations</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()(</span><span class="n">Tensor</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">r</span><span class="p">)]),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">pool</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">r</span> <span class="o">&gt;</span> <span class="n">n</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lst</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pool</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">stop</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">!=</span> <span class="n">index</span> <span class="o">+</span> <span class="n">n</span> <span class="o">-</span> <span class="n">r</span><span class="p">:</span>
                    <span class="n">stop</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">i</span> <span class="o">=</span> <span class="n">index</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">lst</span>
            <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
                <span class="n">indices</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pool</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
            <span class="n">lst</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">lst</span><span class="p">,</span> <span class="n">item</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">_combinations_with_replacement</span><span class="p">(</span><span class="n">iterable</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([])</span>
        <span class="n">pool</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">iterable</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pool</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">n</span> <span class="ow">and</span> <span class="n">r</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lst</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">r</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pool</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">stop</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">r</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">stop</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="n">i</span> <span class="o">=</span> <span class="n">index</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">stop</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">lst</span>
            <span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">pool</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
            <span class="n">lst</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">lst</span><span class="p">,</span> <span class="n">item</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;combinations&#39;, &#39;x&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;combinations&#39;, the dimension &#39;x&#39; must be 1, but got </span><span class="si">{</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">comb_func</span> <span class="o">=</span> <span class="n">_combinations_with_replacement</span> <span class="k">if</span> <span class="n">with_replacement</span> <span class="k">else</span> <span class="n">_combinations</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">comb_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ret</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span></div>


<div class="viewcode-block" id="dist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dist.html#mindspore.ops.dist">[docs]</a><span class="k">def</span> <span class="nf">dist</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes batched the :math:`p`-norm distance between each pair of the two collections of row vectors.</span>

<span class="sd">    Note:</span>
<span class="sd">        Since only normalization for integer :math:`p`-normal form is supported in MindSpore,</span>
<span class="sd">        a type error will be raised if :math:`p` is not an integer.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. The dtype must be float16 or float32.</span>
<span class="sd">        other (Tensor): The second input tensor. The dtype must be float16 or float32.</span>
<span class="sd">        p (int, optional): The order of norm. `p` is greater than or equal to 0. Default: ``2`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`, which shape is :math:`(1)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `other` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `p` is not a non-negative integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[[1.0, 1.0], [2.0, 2.0]]])</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor([[[3.0, 3.0], [3.0, 3.0]]])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.dist(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        3.1622777</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;dist&#39;, &#39;input&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;dist&#39;, &#39;other&#39; must be a tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">other</span>
    <span class="k">if</span> <span class="n">z</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="c1"># the types of p will expend once ops.LpNorm supports float</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">LpNorm</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)(</span><span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)))</span></div>


<div class="viewcode-block" id="copysign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.copysign.html#mindspore.ops.copysign">[docs]</a><span class="k">def</span> <span class="nf">copysign</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a new floating-point tensor with the magnitude of `x` and the sign of `other`, element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor]): Values to change the sign of.</span>
<span class="sd">        other (Union[int, float, Tensor]): The sign of `other` is copied to `x`. If `x.shape != other.shape`,</span>
<span class="sd">            `other` must be broadcastable to the shape of `x` (which is also the shape of the output).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The dtype of the tensor is float.</span>
<span class="sd">        The values of `x` with the sign of `other`, the shape is the same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of the input is not in the given types or</span>
<span class="sd">            the input can not be converted to tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([[0.3, -0.7], [0.5, 0.5]])</span>
<span class="sd">        &gt;&gt;&gt; other = np.array([[-0.4, 0.6], [0.4, -0.6]])</span>
<span class="sd">        &gt;&gt;&gt; out = ops.copysign(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[-0.3  0.7]</span>
<span class="sd">         [ 0.5 -0.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_broadcast_to_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Broadcasts x from current shape to shape&quot;&quot;&quot;</span>
        <span class="n">ndim_to</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">shape</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Tensor is expected, but got &quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="s2">&quot;integer, float or Tensor is expected, but got &quot;</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">_type_convert</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="n">other</span> <span class="o">=</span> <span class="n">_broadcast_to_shape</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype bool.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex64.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">other</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex64.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex128.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">other</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;copysign does not accept dtype complex128.&quot;</span><span class="p">)</span>

    <span class="n">x_float</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">x</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pos_tensor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Abs</span><span class="p">()(</span><span class="n">x_float</span><span class="p">)</span>
    <span class="n">less_zero</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()(</span><span class="n">other</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()(</span><span class="n">less_zero</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Neg</span><span class="p">()(</span><span class="n">pos_tensor</span><span class="p">),</span> <span class="n">pos_tensor</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_non_negative_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<div class="viewcode-block" id="hann_window"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hann_window.html#mindspore.ops.hann_window">[docs]</a><span class="k">def</span> <span class="nf">hann_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a Hann Window.</span>

<span class="sd">    The Hann window is defined as</span>

<span class="sd">    .. math::</span>
<span class="sd">        w(n) = \frac{1}{2} - \frac{1}{2} \cos\left(\frac{2\pi{n}}{M-1}\right),\qquad 0 \leq n \leq M-1</span>

<span class="sd">    Args:</span>
<span class="sd">        window_length (int): Length of window.</span>
<span class="sd">        periodic (bool, optional): When set to True, generates a periodic window for spectral analysis.</span>
<span class="sd">            When set to False, generates a symmetric window for filter design.Default: True.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (mindspore.dtype, optional): The output window data type, it must be float. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a Hann window.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `window_length` is not an integer.</span>
<span class="sd">        TypeError: If `periodic` is not a variable of Boolean type.</span>
<span class="sd">        ValueError: If `window_length` is negative.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; window_length = 5</span>
<span class="sd">        &gt;&gt;&gt; out = ops.hann_window(window_length)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        [0.        0.3454915 0.9045085 0.9045085 0.3454915]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_non_negative_int</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="s1">&#39;window_length&#39;</span><span class="p">,</span> <span class="s1">&#39;hann_window&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window_length</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">periodic</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">bool_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;hann_window&#39;, &#39;periodic&#39; must be a variable of Boolean type, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">periodic</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;hann_window&#39;, &#39;dtype&#39; must be floating point dtypes, but got </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">periodic</span><span class="p">:</span>
        <span class="n">window_length</span> <span class="o">=</span> <span class="n">window_length</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">window_length</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="p">(</span><span class="n">window_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">w</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">periodic</span> <span class="k">else</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">w</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_type_convert</span><span class="p">(</span><span class="n">force</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert type of `obj` to `force`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">force</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>


<div class="viewcode-block" id="logsumexp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logsumexp.html#mindspore.ops.logsumexp">[docs]</a><span class="k">def</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by calculating exponential for all elements in the dimension,</span>
<span class="sd">    then calculate logarithm of the sum.</span>

<span class="sd">    .. math::</span>

<span class="sd">        logsumexp(input) = \log(\sum(e^{input-input_{max}})) + input_{max}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. With float16 or float32 data type.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Only constant value is allowed.</span>
<span class="sd">        keep_dims (bool): If True, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions.</span>
<span class="sd">            Default : ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the sum of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_3, ..., input_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_4, ..., input_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logsumexp(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_exp</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Exp</span><span class="p">)()</span>
    <span class="n">_reduce_sum</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="n">_log</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">)()</span>

    <span class="n">input_max</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">input_exp</span> <span class="o">=</span> <span class="n">_exp</span><span class="p">(</span><span class="nb">input</span> <span class="o">-</span> <span class="n">input_max</span><span class="p">)</span>
    <span class="n">input_sumexp</span> <span class="o">=</span> <span class="n">_reduce_sum</span><span class="p">(</span><span class="n">input_exp</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">input_logsumexp</span> <span class="o">=</span> <span class="n">_log</span><span class="p">(</span><span class="n">input_sumexp</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">keep_dims</span><span class="p">:</span>
        <span class="n">input_max</span> <span class="o">=</span> <span class="n">input_max</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">input_logsumexp</span> <span class="o">+</span> <span class="n">input_max</span></div>


<div class="viewcode-block" id="amin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.amin.html#mindspore.ops.amin">[docs]</a><span class="k">def</span> <span class="nf">amin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces all dimensions of a tensor by returning the minimum value in `input`, by default. And also can</span>
<span class="sd">    reduce a dimension of `input` along specified `axis`. `keepdims` determines whether the dimensions of</span>
<span class="sd">    output and input are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: ``None`` , reduce all dimensions.</span>
<span class="sd">            Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r).</span>
<span class="sd">        keepdims (bool): If true, keep these reduced dimensions and the length is 1. If false, don&#39;t keep</span>
<span class="sd">            these dimensions. Default: ``False`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        initial (scalar, optional): The minimum value of an output element. Must be present to allow computation</span>
<span class="sd">            on empty slice. Default: ``None`` .</span>
<span class="sd">        where (Tensor[bool], optional): A Tensor indicating whether to replace the primitive value in `input`</span>
<span class="sd">            with the value in `initial`. If True, do not replace, otherwise replace. For the index of True in `where`,</span>
<span class="sd">            the corresponding value in `initial` must be assigned. Default: ``None`` , which indicates True by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is None, and `keepdims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keepdims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keepdims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 1, keepdims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the minimum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3. 3. 3.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]]</span>
<span class="sd">         [[4. 4. 4. 4. 4. 4.]]</span>
<span class="sd">         [[7. 7. 7. 7. 7. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amin(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">_init_and_select_elem</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">)(</span><span class="n">keepdims</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_init_and_select_elem</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">cmp_fn</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the input according to Initial, and select the element according to where.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">initial</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">cmp_fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;initial value must be provided for where masks&#39;</span><span class="p">)</span>
        <span class="n">where</span> <span class="o">=</span> <span class="n">where</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">initial</span> <span class="o">=</span> <span class="n">initial</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span>


<div class="viewcode-block" id="amax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.amax.html#mindspore.ops.amax">[docs]</a><span class="k">def</span> <span class="nf">amax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces all dimensions of a tensor by returning the maximum value in `input`, by default. And also can</span>
<span class="sd">    reduce a dimension of `input` along specified `axis`.  `keepdims` determines whether the dimensions of</span>
<span class="sd">    output and input are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: ``None`` , reduce all dimensions.</span>
<span class="sd">            Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r).</span>
<span class="sd">        keepdims (bool): If true, keep these reduced dimensions and the length is 1. If false, don&#39;t keep these</span>
<span class="sd">            dimensions. Default: ``False`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        initial (scalar, optional): The minimum value of an output element. Must be present to allow computation</span>
<span class="sd">            on empty slice. Default: ``None`` .</span>
<span class="sd">        where (Tensor[bool], optional): A Tensor indicating whether to replace the primitive value in `input`</span>
<span class="sd">            with the value in `initial`. If True, do not replace, otherwise replace. For the index of True in `where`,</span>
<span class="sd">            the corresponding value in `initial` must be assigned. Default: ``None`` , which indicates True by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is None, and `keepdims` is False, the output is a 0-D tensor representing the product of all</span>
<span class="sd">          elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keepdims` is False, the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keepdims` is False, the shape of output is</span>
<span class="sd">          :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 1, keepdims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the maximum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        9.0</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7. 7. 7. 7. 7. 7.]</span>
<span class="sd">          [8. 8. 8. 8. 8. 8.]</span>
<span class="sd">          [9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">         [[6. 6. 6. 6. 6. 6.]]</span>
<span class="sd">         [[9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.amax(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">_init_and_select_elem</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">)(</span><span class="n">keepdims</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="mean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mean.html#mindspore.ops.mean">[docs]</a><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces all dimension of a tensor by averaging all elements in the dimension, by default.</span>
<span class="sd">    And reduce a dimension of `x` along the specified `axis`. `keep_dims`</span>
<span class="sd">    determines whether the dimensions of the output and input are the same.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: ``None`` , reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Assume the rank of `x` is r, and the value range is [-r,r).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is None, and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by averaging all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],</span>
<span class="sd">        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),</span>
<span class="sd">        ... mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        5.0</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 4. 4. 4. 4. 4.]</span>
<span class="sd">          [5. 5. 5. 5. 5. 5.]</span>
<span class="sd">          [6. 6. 6. 6. 6. 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2. 2. 2. 2. 2.]]</span>
<span class="sd">         [[5. 5. 5. 5. 5. 5.]]</span>
<span class="sd">         [[8. 8. 8. 8. 8. 8.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along the axis 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mean(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.]</span>
<span class="sd">          [ 2.]</span>
<span class="sd">          [ 2.]]</span>
<span class="sd">         [[ 4.]</span>
<span class="sd">          [ 5.]</span>
<span class="sd">          [ 6.]]</span>
<span class="sd">         [[ 6.]</span>
<span class="sd">          [ 8.]</span>
<span class="sd">          [10.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceMean</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="prod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.prod.html#mindspore.ops.prod">[docs]</a><span class="k">def</span> <span class="nf">prod</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `input` along the axis. Determine whether the dimensions of the output and input are the same</span>
<span class="sd">    by controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): The input tensor. The dtype of the tensor to be reduced is number.</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): The dimensions to reduce. Default: None, reduce all dimensions.</span>
<span class="sd">          Only constant value is allowed. Assume the rank of `input` is r, and the value range is [-r,r).</span>
<span class="sd">        keep_dims (bool): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If false, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as input tensor.</span>

<span class="sd">        - If `axis` is None, and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(input_0, input_2, ..., input_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(input_0, input_3, ..., input_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by multiplying all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.2833798e+33</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        ()</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 0, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 28.  28.  28.  28.  28.  28.]</span>
<span class="sd">          [ 80.  80.  80.  80.  80.  80.]</span>
<span class="sd">          [162. 162. 162. 162. 162. 162.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 1, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  6.   6.   6.   6.   6.   6.]]</span>
<span class="sd">         [[120. 120. 120. 120. 120. 120.]]</span>
<span class="sd">         [[504. 504. 504. 504. 504. 504.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prod(x, 2, True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.00000e+00]</span>
<span class="sd">          [6.40000e+01]</span>
<span class="sd">          [7.29000e+02]]</span>
<span class="sd">         [[4.09600e+03]</span>
<span class="sd">          [1.56250e+04]</span>
<span class="sd">          [4.66560e+04]]</span>
<span class="sd">         [[1.17649e+05]</span>
<span class="sd">          [2.62144e+05]</span>
<span class="sd">          [5.31441e+05]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceProd</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_multi_svd_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;_multi_svd_norm for norm.&quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">_moveaxis</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">svd_res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;amax&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">svd_res</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;amin&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">svd_res</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">op</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">svd_res</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For svd_norm, the op input must be one of [&#39;amax&#39;, &#39;amin&#39;, &#39;sum&#39;], but got f</span><span class="si">{</span><span class="n">op</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_normalize_axis_index</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;normalize_axis_index for norm.&quot;&quot;&quot;</span>
    <span class="c1"># pylint: disable=chained-comparison</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">axis</span>
    <span class="c1"># pylint: disable=chained-comparison</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">ndim</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ndim</span> <span class="o">+</span> <span class="n">axis</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;For norm, the dim is out of range.&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_moveaxis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
    <span class="n">destination</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">_normalize_axis_index</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">destination</span><span class="p">])</span>
    <span class="n">source</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">_normalize_axis_index</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">source</span><span class="p">])</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">source</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">dest</span><span class="p">,</span> <span class="n">src</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">source</span><span class="p">)):</span>
        <span class="n">perm</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dest</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">ord</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;axis check&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">))</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">ord</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">ord</span> <span class="o">==</span> <span class="s1">&#39;fro&#39;</span> <span class="ow">and</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">ord</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="n">axis</span><span class="p">,)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For norm, the dimensions is out of range.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For norm, the dim should be int or tuple of int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axis</span><span class="p">,</span> <span class="kc">False</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_ord</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For norm, ord mode can not be str for vectors, but got </span><span class="si">{</span><span class="nb">ord</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="s1">&#39;fro&#39;</span><span class="p">,</span> <span class="s1">&#39;nuc&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For norm, the ord mode must be in &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;[2, -2, 1, -1, float(&#39;inf&#39;), -float(&#39;inf&#39;), &#39;fro&#39;, &#39;nuc&#39;, None] for matrices, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">ord</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_dtype</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="ow">in</span> <span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">if</span> <span class="n">d1</span> <span class="o">==</span> <span class="n">d2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">d1</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;the dtype is not supported.&#39;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_last_dim_shape_eq</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;shapes are not aligned&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_complex_square</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;calculate square with complex or not&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">ops</span><span class="o">.</span><span class="n">is_complex</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">*</span> <span class="n">A</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>


<div class="viewcode-block" id="norm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.norm.html#mindspore.ops.norm">[docs]</a><span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix norm or vector norm of a given tensor.</span>

<span class="sd">    `ord` is the calculation mode of norm. The following norm modes are supported.</span>

<span class="sd">    ====================== ================================ ==========================================</span>
<span class="sd">    `ord`                   norm for matrices               norm for vectors</span>
<span class="sd">    ====================== ================================ ==========================================</span>
<span class="sd">    `None` (default)        Frobenius norm                   `2`-norm (see below)</span>
<span class="sd">    `&#39;fro&#39;`                 Frobenius norm                   -- not supported --</span>
<span class="sd">    `&#39;nuc&#39;`                 nuclear norm                     -- not supported --</span>
<span class="sd">    `inf`                   :math:`max(sum(abs(x), dim=1))`  :math:`max(abs(x))`</span>
<span class="sd">    `-inf`                  :math:`min(sum(abs(x), dim=1))`  :math:`min(abs(x))`</span>
<span class="sd">    `0`                     -- not supported --              :math:`sum(x != 0)`</span>
<span class="sd">    `1`                     :math:`max(sum(abs(x), dim=0))`  as below</span>
<span class="sd">    `-1`                    :math:`min(sum(abs(x), dim=0))`  as below</span>
<span class="sd">    `2`                     largest singular value           as below</span>
<span class="sd">    `-2`                    smallest singular value          as below</span>
<span class="sd">    other `int` or `float`  -- not supported --              :math:`sum(abs(x)^{ord})^{(1 / ord)}`</span>
<span class="sd">    ====================== ================================ ==========================================</span>

<span class="sd">    .. note::</span>
<span class="sd">        Currently, complex numbers are not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        A (Tensor): Tensor of shape :math:`(*, n)` or :math:`(*, m, n)` where * is zero or more batch dimensions.</span>
<span class="sd">        ord (Union[int, float, inf, -inf, &#39;fro&#39;, &#39;nuc&#39;], optional): norm&#39;s mode. refer to the table above for</span>
<span class="sd">            behavior. Default: None.</span>
<span class="sd">        dim (Union[int, Tuple(int)], optional): calculate the dimension of vector norm or matrix norm. Default: None.</span>

<span class="sd">            - When `dim` is int, it will be calculated by vector norm.</span>

<span class="sd">            - When `dim` is a 2-tuple, it will be calculated by matrix norm.</span>

<span class="sd">            - If `dim` is None and `ord` is None, `A` will be flattened to 1D and the 2-norm</span>
<span class="sd">              of the vector will be calculated.</span>

<span class="sd">            - If `dim` is None and `ord` is not None, `A` must be 1D or 2D.</span>

<span class="sd">        keepdim (bool): whether the output Tensor retains the original dimension. Default: False.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): When set, `A` will be converted to the specified type,</span>
<span class="sd">            `dtype`, before execution, and dtype of returned Tensor will also be `dtype`. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the result of norm calculation on the specified dimension, `dim`, has the same dtype as `A`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `dim` is out of range.</span>
<span class="sd">        TypeError: If `dim` is neither an int nor a tuple of int.</span>
<span class="sd">        TypeError: If `A` is a vector and `ord` is a str.</span>
<span class="sd">        ValueError: If `A` is a matrices and `ord` is not in valid mode.</span>
<span class="sd">        ValueError: If `A` is a matrices and `ord` is an integer but not in [1, -1, 2, -2].</span>
<span class="sd">        ValueError: If two elements of `dim` is same after normalize.</span>
<span class="sd">        ValueError: If any elements of `dim` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = ops.arange(-12, 13, dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = x.reshape(5, 5)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x))</span>
<span class="sd">        36.05551</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, float(&#39;inf&#39;)))</span>
<span class="sd">        12.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, float(&#39;-inf&#39;)))</span>
<span class="sd">        0.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, 0))</span>
<span class="sd">        24.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, 1))</span>
<span class="sd">        156.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, -1))</span>
<span class="sd">        0.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, 2))</span>
<span class="sd">        36.05551</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, -2))</span>
<span class="sd">        0.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, 3))</span>
<span class="sd">        23.000631</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(x, -3))</span>
<span class="sd">        0.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y))</span>
<span class="sd">        36.05551</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, &#39;fro&#39;))</span>
<span class="sd">        36.05551</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, &#39;nuc&#39;))</span>
<span class="sd">        42.42641</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, float(&#39;inf&#39;)))</span>
<span class="sd">        50.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, float(&#39;-inf&#39;)))</span>
<span class="sd">        6.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, 1))</span>
<span class="sd">        32.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, -1))</span>
<span class="sd">        30.0</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(y, 2))</span>
<span class="sd">        35.355343</span>
<span class="sd">        &gt;&gt;&gt; m = ms.Tensor([[1., -1., 2.], [-2., 3., -4.]])</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(m, dim=0))</span>
<span class="sd">        [2.236068  3.1622777 4.472136 ]</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(m, dim=1))</span>
<span class="sd">        [2.4494898 5.3851647]</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(m, ord=1, dim=1))</span>
<span class="sd">        [4. 9.]</span>
<span class="sd">        &gt;&gt;&gt; n = ops.arange(27, dtype=ms.float32).reshape(3, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(n, dim=(1, 2)))</span>
<span class="sd">        [14.282857 39.76179  66.45299 ]</span>
<span class="sd">        &gt;&gt;&gt; print(ops.norm(n[0, :, :]), ops.norm(n[1, :, :]), ops.norm(n[2, :, :]))</span>
<span class="sd">        14.282857 39.76179 66.45299</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">dim</span><span class="p">,</span> <span class="n">immediate</span> <span class="o">=</span> <span class="n">_check_axis</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">_check_ord</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Immediately handle some default, simple, fast, and common cases.</span>
    <span class="k">if</span> <span class="n">immediate</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">_complex_square</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">dim</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ndim</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span> <span class="o">=</span> <span class="n">dim</span>
            <span class="n">row_axis</span> <span class="o">=</span> <span class="n">_normalize_axis_index</span><span class="p">(</span><span class="n">row_axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
            <span class="n">col_axis</span> <span class="o">=</span> <span class="n">_normalize_axis_index</span><span class="p">(</span><span class="n">col_axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">col_axis</span> <span class="o">&gt;</span> <span class="n">row_axis</span><span class="p">:</span>
                    <span class="n">col_axis</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">row_axis</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">col_axis</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">col_axis</span> <span class="o">&gt;</span> <span class="n">row_axis</span><span class="p">:</span>
                    <span class="n">col_axis</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">row_axis</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">col_axis</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">_multi_svd_norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span><span class="p">,</span> <span class="s1">&#39;amax&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">_multi_svd_norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span><span class="p">,</span> <span class="s1">&#39;amin&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For norm, the ord </span><span class="si">{</span><span class="nb">ord</span><span class="si">}</span><span class="s2"> are not support for matrices.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
                <span class="n">ret_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="n">ret_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">ret_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ret_shape</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ret</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="p">(</span><span class="n">A</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">ord</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">_lp_norm</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">LpNorm</span><span class="p">)(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">ord</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">_lp_norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="nb">ord</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="nb">ord</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Zero norm</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">A</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">ord</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># special case for speedup</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">_complex_square</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
            <span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)(</span><span class="n">keepdim</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
        <span class="c1"># None of the str-type keywords for ord (&#39;fro&#39;, &#39;nuc&#39;)</span>
        <span class="c1"># are valid for vectors</span>
        <span class="n">absx</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
        <span class="n">absx</span> <span class="o">**=</span> <span class="nb">ord</span>
        <span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)(</span><span class="n">keepdim</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">absx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">ord</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">**=</span> <span class="n">ops</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">(</span><span class="nb">ord</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">**=</span> <span class="mi">1</span> <span class="o">/</span> <span class="nb">ord</span>
        <span class="k">return</span> <span class="n">ret</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">row_axis</span> <span class="o">=</span> <span class="n">_normalize_axis_index</span><span class="p">(</span><span class="n">row_axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="n">col_axis</span> <span class="o">=</span> <span class="n">_normalize_axis_index</span><span class="p">(</span><span class="n">col_axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">row_axis</span> <span class="o">==</span> <span class="n">col_axis</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;For norm, the elements of dim can not be duplicate.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">ord</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">row_axis</span> <span class="o">&gt;</span> <span class="n">col_axis</span><span class="p">:</span>
                <span class="n">row_axis</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">col_axis</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">row_axis</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">row_axis</span> <span class="o">&gt;</span> <span class="n">col_axis</span><span class="p">:</span>
                <span class="n">row_axis</span> <span class="o">-=</span> <span class="mi">1</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">col_axis</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">row_axis</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="s1">&#39;fro&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">_complex_square</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">dim</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">ord</span> <span class="o">==</span> <span class="s1">&#39;nuc&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">_multi_svd_norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">row_axis</span><span class="p">,</span> <span class="n">col_axis</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">_complex_square</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">dim</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">keepdim</span><span class="p">:</span>
            <span class="n">ret_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">ret_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">ret_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ret_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>
    <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="lu_unpack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.lu_unpack.html#mindspore.ops.lu_unpack">[docs]</a><span class="k">def</span> <span class="nf">lu_unpack</span><span class="p">(</span><span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">,</span> <span class="n">unpack_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unpack_pivots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts `LU_data` and `LU_pivots` back into P, L and U matrices, where</span>
<span class="sd">    P is a permutation matrix, L is a lower triangular matrix, and U is an</span>
<span class="sd">    upper triangular matrix. Typically, `LU_data` and `LU_pivots` are generated</span>
<span class="sd">    from the LU decomposition of a matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        LU_data (Tensor): The packed LU factorization data. A Tensor of shape :math:`(*, M, N)`, where :math:`*` is</span>
<span class="sd">            batch dimensions. The dim of `LU_data` must be equal to or greater than 2.</span>
<span class="sd">        LU_pivots (Tensor): The packed LU factorization pivots. A Tensor of shape :math:`(*, min(M, N))`,</span>
<span class="sd">            where :math:`*` is</span>
<span class="sd">            batch dimensions, with data type int8, uint8, int16, int32, int64.</span>
<span class="sd">        unpack_data (bool, optional): A flag indicating if the `LU_data` should be unpacked. If False,</span>
<span class="sd">            then the returned L and U are None. Default: True.</span>
<span class="sd">        unpack_pivots (bool, optional): A flag indicating if the `LU_pivots` should be unpacked into</span>
<span class="sd">            a permutation matrix P. If False, then the returned P is None. Default: True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - pivots(Tensor) - The permutation matrix of LU factorization.</span>
<span class="sd">          The shape is :math:`(*, M, M)`, the dtype is same as `LU_data`.</span>
<span class="sd">        - L (Tensor) - The L matrix  of LU factorization. The dtype is same as `LU_data`.</span>
<span class="sd">        - U (Tensor) - The U matrix  of LU factorization. The dtype is same as `LU_data`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `LU_data` is int, uint or float.</span>
<span class="sd">        TypeError: If the dtype of `LU_pivots` is not one of the following: int8, uint8, int16, int32, int64.</span>
<span class="sd">        ValueError: If the dimension of `LU_data` is less than 2.</span>
<span class="sd">        ValueError: If the dimension of `LU_pivots` is less than 1.</span>
<span class="sd">        ValueError: If the size of the last dimension of LU_pivots is not equal to the minimum of the sizes of the last</span>
<span class="sd">                    two dimensions of LU_data.</span>
<span class="sd">        ValueError: If the batch dimensions of LU_data&#39;s does not match LU_pivots&#39;s batch dimensions.</span>
<span class="sd">        ValueError: On the CPU platform, if the value of `LU_pivots` are out of range :math:`[1, LU\_data.shape[-2])`.</span>
<span class="sd">        RuntimeError: On the Ascend platform, if the value of `LU_pivots` are</span>
<span class="sd">                    out of range :math:`[1, LU\_data.shape[-2])`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; LU_data = Tensor(np.array([[[-0.3806, -0.4872,  0.5536],</span>
<span class="sd">        ...                             [-0.1287,  0.6508, -0.2396],</span>
<span class="sd">        ...                             [ 0.2583,  0.5239,  0.6902]],</span>
<span class="sd">        ...                            [[ 0.6706, -1.1782,  0.4574],</span>
<span class="sd">        ...                             [-0.6401, -0.4779,  0.6701],</span>
<span class="sd">        ...                             [ 0.1015, -0.5363,  0.6165]]]), mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; LU_pivots = Tensor(np.array([[1, 3, 3],</span>
<span class="sd">        ...                              [2, 3, 3]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; pivots, L, U = ops.lu_unpack(LU_data, LU_pivots)</span>
<span class="sd">        &gt;&gt;&gt; print(pivots)</span>
<span class="sd">        [[[1. 0. 0.]</span>
<span class="sd">          [0. 0. 1.]</span>
<span class="sd">          [0. 1. 0.]]</span>
<span class="sd">         [[0. 0. 1.]</span>
<span class="sd">          [1. 0. 0.]</span>
<span class="sd">          [0. 1. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(L)</span>
<span class="sd">        [[[ 1.       0.       0.]</span>
<span class="sd">          [-0.1287   1.       0.]</span>
<span class="sd">          [ 0.2583   0.5239   1.]]</span>
<span class="sd">         [[ 1.0000   0.       0.]</span>
<span class="sd">          [-0.6401   1.       0.]</span>
<span class="sd">          [ 0.1015  -0.5363   1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(U)</span>
<span class="sd">        [[[-0.3806  -0.4872   0.5536]</span>
<span class="sd">          [ 0.       0.6508  -0.2396]</span>
<span class="sd">          [ 0.       0.       0.6902]]</span>
<span class="sd">         [[ 0.6706  -1.1782   0.4574]</span>
<span class="sd">          [ 0.      -0.4779   0.6701]</span>
<span class="sd">          [ 0.       0.       0.6165]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pivots</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">lu_unpack_</span><span class="p">(</span><span class="n">LU_data</span><span class="p">,</span> <span class="n">LU_pivots</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">unpack_data</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">unpack_pivots</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span>
    <span class="k">if</span> <span class="n">unpack_pivots</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pivots</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="renorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.renorm.html#mindspore.ops.renorm">[docs]</a><span class="k">def</span> <span class="nf">renorm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Renormalizes the sub-tensors along dimension `axis`, and each sub-tensor&#39;s p-norm should not exceed the</span>
<span class="sd">    &#39;maxnorm&#39;. The values of current sub-tensor don&#39;t need change if the p-norm of the sub-tensor is less than</span>
<span class="sd">    `maxnorm`. Otherwise the sub-tensor needs to be modified to the original value of the corresponding position</span>
<span class="sd">    divided by the p-norm of the substensor and then multiplied by `maxnorm`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor, types: float32 or float16.</span>
<span class="sd">        p (int): Power of norm calculation.</span>
<span class="sd">        axis (int): The dimension that expected to get the slice-tensor.</span>
<span class="sd">        maxnorm (float32): Max norm.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `p` is not int.</span>
<span class="sd">        TypeError: If dtype of `axis` is not int.</span>
<span class="sd">        TypeError: If dtype of `maxnorm` is not float32.</span>
<span class="sd">        ValueError: If the value of `p` less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.renorm(x, p=1, axis=0, maxnorm=5.)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[1.       1.        1.        ]</span>
<span class="sd">        [1.6666666 1.6666666 1.6666666 ]</span>
<span class="sd">        [1.6666667 1.6666667 1.6666667 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">renorm_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Renorm</span><span class="p">)(</span><span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">maxnorm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">renorm_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_attr_dtype</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_positive_float</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_int_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span> <span class="n">upper_limit</span><span class="p">,</span> <span class="n">arg_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">lower_limit</span><span class="p">,</span>
                              <span class="n">upper_limit</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_logits_tensor</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input logits must be tensor&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_logits_shape</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For gumbel_softmax, the 0-D input is not supported.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="gumbel_softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gumbel_softmax.html#mindspore.ops.gumbel_softmax">[docs]</a><span class="k">def</span> <span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the samples from the Gumbel-Softmax distribution and optionally discretizes. If `hard = True`, the returned</span>
<span class="sd">    samples will be one-hot, otherwise it will be probability distributions that sum to 1 across `dim`.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Unnormalized log probabilities. The data type must be float16 or float32.</span>
<span class="sd">        tau (float): The scalar temperature, which is a positive number. Default: ``1.0`` .</span>
<span class="sd">        hard (bool): if `True`, the returned samples will be discretized as one-hot vectors, but will be differentiated</span>
<span class="sd">          as if it is the soft sample in autograd. Default: ``False`` .</span>
<span class="sd">        dim (int): Dim for softmax to compute. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` is not one of: float16, float32.</span>
<span class="sd">        TypeError: If `tau` is not an float.</span>
<span class="sd">        TypeError: If `hard` is not a bool.</span>
<span class="sd">        TypeError: If `dim` is not a int.</span>
<span class="sd">        ValueError: If If `tau` is not positive.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gumbel_softmax(input_x, 1.0, True, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_logits_tensor</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">_check_logits_shape</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">logits_dtype</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">_check_input_dtype</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;hard&quot;</span><span class="p">,</span> <span class="n">hard</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="n">_check_positive_float</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="n">_check_int_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_check_int_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                         <span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="s2">&quot;gumbel_softmax&quot;</span><span class="p">)</span>

    <span class="n">log_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log</span><span class="p">)()</span>
    <span class="n">const_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">)()</span>

    <span class="n">sample_shape</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">)()(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">uniform</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">,</span> <span class="n">const_op</span><span class="p">(</span>
        <span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">const_op</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">uniform</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">uniform</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">)</span>
    <span class="n">gumbel</span> <span class="o">=</span> <span class="n">neg_tensor</span><span class="p">(</span><span class="n">log_op</span><span class="p">(</span><span class="n">neg_tensor</span><span class="p">(</span><span class="n">log_op</span><span class="p">(</span><span class="n">uniform</span><span class="p">))))</span>
    <span class="n">gumbel</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span> <span class="o">+</span> <span class="n">gumbel</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span>
    <span class="n">y_soft</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Softmax</span><span class="p">)(</span><span class="n">dim</span><span class="p">)(</span><span class="n">gumbel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">y_hard</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">)(</span><span class="n">dim</span><span class="p">)(</span><span class="n">index</span><span class="p">,</span> <span class="n">sample_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">),</span>
                                                <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">logits_dtype</span><span class="p">))</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">y_hard</span> <span class="o">-</span> <span class="n">ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">y_soft</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_soft</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">y_soft</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="kaiser_window"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.kaiser_window.html#mindspore.ops.kaiser_window">[docs]</a><span class="k">def</span> <span class="nf">kaiser_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">12.0</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a Kaiser window, which is also known as the Kaiser-Bessel window.</span>

<span class="sd">    The Kaiser window is defined as</span>

<span class="sd">    .. math::</span>
<span class="sd">        w(n) = \frac{I_{0}\left( \beta\sqrt{1 - \frac{4n^{2}}{(M - 1)^{2}}} \right)}{I_{0}(\beta)}</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        - \frac{M - 1}{2} \leq n \leq \frac{M - 1}{2}</span>

<span class="sd">    where :math:`I_0` is the modified zeroth-order Bessel function.</span>

<span class="sd">    Args:</span>
<span class="sd">        window_length (int): Length of window.</span>
<span class="sd">        periodic (bool, optional): When set to True, generates a periodic window for spectral analysis.</span>
<span class="sd">            When set to False, generates a symmetric window for filter design. Default: True.</span>
<span class="sd">        beta (float, optional): Shape parameter, when `beta` gets large, the window narrows. Default: 12.0.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (mindspore.dtype, optional): The output window data type, it must be float. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a Kaiser window.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `window_length` or `beta` is not an integer.</span>
<span class="sd">        TypeError: If `periodic` is not a variable of Boolean type.</span>
<span class="sd">        ValueError: If `window_length` is negative.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; window_length = 5</span>
<span class="sd">        &gt;&gt;&gt; out = ops.kaiser_window(window_length)</span>
<span class="sd">        &gt;&gt;&gt; print(out.asnumpy())</span>
<span class="sd">        [5.27734413e-05 1.01719688e-01 7.92939834e-01 7.92939834e-01</span>
<span class="sd">         1.01719688e-01]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_non_negative_int</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="s1">&#39;window_length&#39;</span><span class="p">,</span> <span class="s1">&#39;kaiser_window&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window_length</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">periodic</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For &#39;kaiser_window&#39;, &#39;periodic&#39; must be a variable of Boolean type, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">periodic</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;kaiser_window&#39;, &#39;dtype&#39; must be floating point dtypes, but got </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">periodic</span><span class="p">:</span>
        <span class="n">window_length</span> <span class="o">=</span> <span class="n">window_length</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">window_length</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="p">(</span><span class="n">window_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">i0</span><span class="p">(</span>
        <span class="n">beta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">((</span><span class="n">n</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">i0</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">beta</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">w</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">w</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">periodic</span> <span class="k">else</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="k">def</span> <span class="nf">stft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">win_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
         <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;REFLECT&quot;</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_complex</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    STFT segments the signal into narrow time intervals and takes the Fourier transform</span>
<span class="sd">    of each segment to quantify the change of a nonstationary signal&#39;s frequency</span>
<span class="sd">    and phase content over time.</span>

<span class="sd">    Ignoring the optional batch dimension, this operation computes the following expression:</span>

<span class="sd">    .. math::</span>

<span class="sd">        X[\omega, m]=\sum_{k=0}^{\text {win_length-1 }}</span>
<span class="sd">        \text { window }[k] \text { input }[m \times \text { hop_length }+</span>
<span class="sd">        k] \exp \left(-j \frac{2 \pi \cdot \omega k}{\text { win_length }}\right)</span>

<span class="sd">    where :math:`m` is the index of the sliding window, and</span>
<span class="sd">    :math:`ω` is the frequency in range :math:`0 \leq \omega &lt; \text{n\_fft}0≤ω&lt;n_fft`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Time sequences of stft, must be either a 1-D time tensor or a 2-D tensor.</span>
<span class="sd">        n_fft (int): The size of Fourier transform.</span>
<span class="sd">        hop_length (int, optional): The distance between neighboring sliding window</span>
<span class="sd">            frames. Default: None(treated as equal to :math:`floor(n_fft / 4)`).</span>
<span class="sd">        win_length (int, optional): the size of window frame and STFT filter.</span>
<span class="sd">            Default: None(treated as equal to `n_fft`).</span>
<span class="sd">        window (Tensor, optional): the optional window function, 1-D tensor of size `win_length`.</span>
<span class="sd">            Default: None(treated as window of all :math:`1` s). If `win_length` &lt; `n_fft`,</span>
<span class="sd">            `window` will be padded on both sides with ones to length `n_fft` before it takes effect.</span>
<span class="sd">        center (bool, optional): whether to pad `x` on both sides. Default: True.</span>
<span class="sd">        pad_mode (str, optional): controls the padding method used when</span>
<span class="sd">            `center` is True. Default: &#39;REFLECT&#39;.</span>
<span class="sd">        normalized (bool, optional): controls whether to return the normalized STFT results</span>
<span class="sd">             Default: False.</span>
<span class="sd">        onesided (bool, optional): controls whether to return half of results to</span>
<span class="sd">            avoid redundancy for real inputs.</span>
<span class="sd">            Default: None. True for real `x` and `window`, False otherwise.</span>
<span class="sd">        return_complex (bool, optional): whether to return a complex tensor, or</span>
<span class="sd">            a real tensor with an extra last dimension for the real and</span>
<span class="sd">            imaginary components.</span>
<span class="sd">            Default: None. True for complex `x` or `window`, False otherwise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **output** (Tensor) - A tensor containing the STFT result.</span>
<span class="sd">            If `return_complex` is True, it returns a complex Tensor with shape :math:`(*, N, T)`.</span>
<span class="sd">            If `return_complex` is False, it returns a real Tensor with shape :math:`(*, N, T, 2)`.</span>

<span class="sd">            `N` is size of Fourier transform, it depends on parameter `onesided`:</span>
<span class="sd">            - If `onesided` is False, :math:`N = n_fft`.</span>
<span class="sd">            - If `onesided` is True, :math:`N = n_fft // 2 + 1`.</span>

<span class="sd">            `T` is the total number of frames used, calculated by this formula:</span>
<span class="sd">            :math:`T = 1 + (len - n_fft) / hop_length`, where `len` depends on parameter `center`:</span>
<span class="sd">            - If `center` is False, :math:`len = signal_length`.</span>
<span class="sd">            - If `center` is True, :math:`len = signal_length + (n_fft // 2) * 2`.</span>
<span class="sd">            where :math:`signal_length` is the signal length, it equals to :math:`x.shape[-1]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a 1-D or 2-D tensor.</span>
<span class="sd">        TypeError: If `window` is not a 1-D tensor.</span>
<span class="sd">        TypeError: If any one of `center` , `normalized` , `onesided`</span>
<span class="sd">            and `return_complex` is assigned a nonboolean value.</span>
<span class="sd">        TypeError: If `pad_mode` is is assigned a value that is not string.</span>
<span class="sd">        TypeError: If `n_fft` , `hop_length` or `hop_length` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor(np.random.rand(2,7192), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.stft(n_fft=64, x=x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 33, 450, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">hop_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">hop_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">n_fft</span> <span class="o">/</span> <span class="mi">4</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">win_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">win_length</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">n_fft</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">window</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">window</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()((</span><span class="n">win_length</span><span class="p">,),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">complex64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex128</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">onesided</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">onesided</span> <span class="o">=</span> <span class="p">(</span><span class="ow">not</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">window</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">return_complex</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">return_complex</span> <span class="o">=</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_is_complex</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">center</span><span class="p">:</span>
        <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;stft&quot;</span><span class="p">)</span>
        <span class="n">signal_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">pad</span> <span class="o">=</span> <span class="n">n_fft</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="k">if</span> <span class="n">signal_dim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),),</span> <span class="n">pad_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">signal_dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">Pad</span><span class="p">(((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="n">pad_mode</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Expected a 1-D tensor or a 2-D tensor, but got </span><span class="si">{</span><span class="n">signal_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">stft_</span> <span class="o">=</span> <span class="n">STFT</span><span class="p">(</span><span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span>
                 <span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="n">return_complex</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">stft_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dtype1</span> <span class="o">==</span> <span class="n">dtype2</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_max</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the maximum value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_min</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the minimum value.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_infer_shape_rem</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">,</span> <span class="n">ndim1</span><span class="p">,</span> <span class="n">ndim2</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Infers the shape of the last two dimensions after performing matmul.&quot;&quot;&quot;</span>
    <span class="n">shape_rem</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">shape_rem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">transpose_b</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ndim2</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">shape_rem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">shape_rem</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_rem</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_value</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">max_size</span><span class="p">,</span> <span class="n">msg_prefix</span><span class="p">,</span> <span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">item</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> operands could not be broadcast together with shape1 </span><span class="si">{</span><span class="n">shape1</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;shape2 </span><span class="si">{</span><span class="n">shape2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_matmul_shapes</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks shape1 and shape2 are valid to perform matmul, and returns output shape after broadcasting.&quot;&quot;&quot;</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>

    <span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
        <span class="n">ndim1</span><span class="p">,</span> <span class="n">ndim2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ndim1</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">ndim2</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimension of input operands must be at least 1, but got &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the length of shape1: </span><span class="si">{</span><span class="n">ndim1</span><span class="si">}</span><span class="s2">, the length of shape2: </span><span class="si">{</span><span class="n">ndim2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ndim2</span> <span class="o">&gt;=</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> shape1[-1] must be equal to shape2[-2] when the length of shape2 &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is greater than or equal to 2, but got shape1[-1]: </span><span class="si">{</span><span class="n">shape1</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;shape2[-2]: </span><span class="si">{</span><span class="n">shape2</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">_check</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">)</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="n">r_shape1</span> <span class="o">=</span> <span class="n">shape1</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">r_shape2</span> <span class="o">=</span> <span class="n">shape2</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">r_shape1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">r_shape2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
        <span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="n">it</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">max_len</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">it</span><span class="p">)]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">-</span> <span class="n">max_len</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">it</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="p">(</span><span class="n">r_shape1</span><span class="p">,</span> <span class="n">r_shape2</span><span class="p">)]</span>
        <span class="n">max_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
        <span class="n">_check_value</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">max_size</span><span class="p">,</span> <span class="n">msg_prefix</span><span class="p">,</span> <span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">)</span>
        <span class="n">shape_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_out</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_need_broadcast</span><span class="p">(</span><span class="n">shape1</span><span class="p">,</span> <span class="n">shape2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns True if broadcast is necessary for batchmatmul.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">shape1</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="n">shape2</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_input_1d</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> should be 1d, but got shape </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_input_2d</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">func_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">func_name</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> should be 2d, but got shape </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_expand</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expand x to ndim from axis, which can be 0 or -1.&quot;&quot;&quot;</span>
    <span class="n">rank_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">)()</span>
    <span class="n">expand_dims_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="k">while</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">expand_dims_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">_broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape_cur</span><span class="p">,</span> <span class="n">shape_to</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Broadcasts x from shape_cur to shape_to.&quot;&quot;&quot;</span>
    <span class="n">tile_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">)()</span>
    <span class="n">tile_size_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TileSize</span><span class="p">)()</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">tile_size_op</span><span class="p">(</span><span class="n">shape_cur</span><span class="p">,</span> <span class="n">shape_to</span><span class="p">,</span> <span class="n">ndim_to</span><span class="p">)</span>
    <span class="n">F</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tile_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<div class="viewcode-block" id="matmul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matmul.html#mindspore.ops.matmul">[docs]</a><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix product of two tensors.</span>

<span class="sd">    Note:</span>
<span class="sd">        Numpy arguments `out`, `casting`, `order`, `subok`, `signature`, and `extobj` are</span>
<span class="sd">        not supported.</span>
<span class="sd">        On GPU, the supported dtypes are np.float16 and np.float32.</span>
<span class="sd">        On CPU, the supported dtypes are np.float16 and np.float32.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor, scalar not allowed.</span>
<span class="sd">          The last dimension of `input` must be the same size as the second last dimension of `other`.</span>
<span class="sd">          And the shape of input and other could be broadcast.</span>
<span class="sd">        other (Tensor): Input tensor, scalar not allowed.</span>
<span class="sd">          The last dimension of `input` must be the same size as the second last dimension of `other`.</span>
<span class="sd">          And the shape of input and other could be broadcast.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or scalar, the matrix product of the inputs. This is a scalar only</span>
<span class="sd">        when both `input`, `other` are 1-d vectors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the last dimension of `input` is not the same size as the</span>
<span class="sd">            second-to-last dimension of `other`, or if a scalar value is passed in.</span>
<span class="sd">        ValueError: If the shape of `input` and `other` could not broadcast together.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : Reasonable application of broadcast mechanism</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(2*3*4).reshape(2, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.arange(4*5).reshape(4, 5), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matmul(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  70.   76.   82.   88.   94.]</span>
<span class="sd">        [ 190.  212.  234.  256.  278.]</span>
<span class="sd">        [ 310.  348.  386.  424.  462.]]</span>
<span class="sd">        [[ 430.  484.  538.  592.  646.]</span>
<span class="sd">        [ 550.  620.  690.  760.  830.]</span>
<span class="sd">        [ 670.  756.  842.  928. 1014.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 5)</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : the rank of `input` is 1</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.ones([2,]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matmul(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For matmul op, inputs must be all tensors.&quot;</span><span class="p">)</span>

    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">rank_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">)()</span>
    <span class="n">shape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">)()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">)()</span>

    <span class="n">dtype1</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">dtype2</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_same_type</span><span class="p">(</span><span class="n">dtype1</span><span class="p">,</span> <span class="n">dtype2</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="n">ndim1_orig</span><span class="p">,</span> <span class="n">ndim2_orig</span> <span class="o">=</span> <span class="n">rank_op</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    <span class="n">shape1_orig</span><span class="p">,</span> <span class="n">shape2_orig</span> <span class="o">=</span> <span class="n">shape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
    <span class="n">transpose_b</span> <span class="o">=</span> <span class="n">ndim2_orig</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="n">shape_backbone</span> <span class="o">=</span> <span class="n">_check_matmul_shapes</span><span class="p">(</span><span class="n">shape1_orig</span><span class="p">,</span> <span class="n">shape2_orig</span><span class="p">,</span> <span class="s1">&#39;matmul&#39;</span><span class="p">)</span>
    <span class="c1"># infers the shape of the output</span>
    <span class="n">shape_out</span> <span class="o">=</span> <span class="n">shape_backbone</span> <span class="o">+</span> <span class="n">_infer_shape_rem</span><span class="p">(</span><span class="n">shape1_orig</span><span class="p">,</span> <span class="n">shape2_orig</span><span class="p">,</span>
                                                  <span class="n">ndim1_orig</span><span class="p">,</span> <span class="n">ndim2_orig</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)</span>

    <span class="n">_matmul</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatMul</span><span class="p">)(</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)</span>
    <span class="n">_batch_matmul</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)(</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_b</span><span class="p">)</span>

    <span class="nb">input</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">other</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">other</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">rank_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape1_orig</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># broadcasts input.shape[:-2] with other.shape[:-2]</span>
        <span class="n">ndim_aligned</span> <span class="o">=</span> <span class="n">_max</span><span class="p">(</span><span class="n">ndim1_orig</span><span class="p">,</span> <span class="n">ndim2_orig</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">_expand</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">shape1_aligned</span><span class="p">,</span> <span class="n">shape2_aligned</span> <span class="o">=</span> <span class="n">shape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">other</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape1_aligned</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">shape_backbone</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">shape2_aligned</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">shape_backbone</span><span class="p">,</span> <span class="n">ndim_aligned</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_batch_matmul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">shape_out</span><span class="p">)</span></div>


<div class="viewcode-block" id="inner"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.inner.html#mindspore.ops.inner">[docs]</a><span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inner product of two tensors.</span>

<span class="sd">    For 1-D tensors (without complex conjugation), returns the ordinary inner product of vectors.</span>

<span class="sd">    For higher dimensions, returns a sum product over the last axis.</span>

<span class="sd">    Note:</span>
<span class="sd">         If `input` or `other` is a Tensor scalar, :func:`mindspore.ops.inner` will be the same as</span>
<span class="sd">         :func:`mindspore.ops.mul` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): First input.</span>
<span class="sd">        other (Tensor): Second input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the result of the inner product.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If neither `input` nor `other` is scalar, and the last dimension of the two input tensors do not</span>
<span class="sd">            match.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case1: 2 1D tensors</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([1, 2, 3], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ms.Tensor([4, 5, 6], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inner(input, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        32</span>
<span class="sd">        &gt;&gt;&gt; # case2: Tensor scalar and tensor</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[[1, 2, 3], [3, 2, 1]], [[4, 5, 6], [4, 5, 6]]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ms.Tensor(2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inner(input, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.  4.  6.]</span>
<span class="sd">          [ 6.  4.  2.]]</span>
<span class="sd">         [[ 8. 10. 12.]</span>
<span class="sd">          [ 8. 10. 12.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case3: Two tensors</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([[[1, 2, 3], [3, 2, 1]], [[4, 5, 6], [4, 5, 6]]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ms.Tensor([[2, 3, 4], [4, 3, 2]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.inner(input, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[20. 16.]</span>
<span class="sd">          [16. 20.]]</span>
<span class="sd">         [[47. 43.]</span>
<span class="sd">          [47. 43.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">other_dim</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">x_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>

    <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">other_dim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">other</span>
        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x_dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">other_shape</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">other_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;inner&#39;, the last dimension of &#39;input&#39; and &#39;other&#39; must be the same, </span><span class="se">\</span>
<span class="s2">                         but got input.shape: </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2"> and other.shape: </span><span class="si">{</span><span class="n">other_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">C</span><span class="o">.</span><span class="n">tensor_dot</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span></div>


<div class="viewcode-block" id="bmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bmm.html#mindspore.ops.bmm">[docs]</a><span class="k">def</span> <span class="nf">bmm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mat2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes matrix multiplication between two tensors by batch.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output}[..., :, :] = \text{matrix}(input_x[..., :, :]) * \text{matrix}(mat2[..., :, :])</span>

<span class="sd">    The dim of `input_x` can not be less than `3` and the dim of `mat2` can not be less than `2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The first tensor to be multiplied. The shape of the tensor is :math:`(*B, N, C)`,</span>
<span class="sd">            where :math:`*B` represents the batch size which can be multidimensional, :math:`N` and :math:`C` are the</span>
<span class="sd">            size of the last two dimensions.</span>
<span class="sd">        mat2 (Tensor): The second tensor to be multiplied. The shape of the tensor is :math:`(*B, C, M)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(*B, N, M)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If dim of `input_x` is less than `3` or dim of `mat2` is less than `2`.</span>
<span class="sd">        ValueError: If the length of the third dim of `input_x` is not equal to</span>
<span class="sd">            the length of the second dim of `mat2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(24).reshape((2, 4, 1, 3)), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; mat2 = Tensor(np.arange(72).reshape((2, 4, 3, 3)), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bmm(input_x, mat2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[  15.   18.   21.]]</span>
<span class="sd">          [[ 150.  162.  174.]]</span>
<span class="sd">          [[ 447.  468.  489.]]</span>
<span class="sd">          [[ 906.  936.  966.]]]</span>
<span class="sd">         [[[1527. 1566. 1605.]]</span>
<span class="sd">          [[2310. 2358. 2406.]]</span>
<span class="sd">          [[3255. 3312. 3369.]]</span>
<span class="sd">          [[4362. 4428. 4494.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mat2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For bmm op, inputs input_x and mat2 must be all tensors.&quot;</span><span class="p">)</span>

    <span class="n">bmm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">bmm_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mat2</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">quantile</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the q-th quantiles of all elements in `input`, when the</span>
<span class="sd">    q-th quantile lies between two data points, a linear interpolation is implemented between them.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">            Supported dtypes: float32, float64.</span>
<span class="sd">        q (Union[float, Tensor]): A scalar or 1D tensor of quantile values in the range [0, 1].</span>
<span class="sd">            Supported dtypes: float32, float64.</span>
<span class="sd">        axis (int, optional): The dimension to reduce. By default, `axis` is None resulting in the</span>
<span class="sd">            input tensor being flattened before computation. Default: None.</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor has dim retained or not. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">        Suppose the shape of `input` is :math:`(m, x_0, x_1, ..., x_i, ..., X_R)`, `axis` = :math:`i` and m is</span>
<span class="sd">        the element count of input `q`.</span>

<span class="sd">        - If `q` is scalar and `keepdims` is True, the shape of output is :math:`(x_0, x_1, ..., 1, ..., X_R)`.</span>
<span class="sd">        - If `q` is scalar and `keepdims` is False, the shape of output is :math:`(x_0, x_1, ..., X_R)`.</span>
<span class="sd">        - If `q` is 1D Tensor and `keepdims` is True, the shape of output is :math:`(m, x_0, x_1, ..., 1, ..., X_R)`.</span>
<span class="sd">        - If `q` is 1D Tensor and `keepdims` is False, the shape of output is :math:`(m, x_0, x_1, ..., X_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `q` is not a Tensor or float.</span>
<span class="sd">        TypeError: If dtype of `input` is not float32 or float64.</span>
<span class="sd">        TypeError: If dtype of `q` is not float32 or float64.</span>
<span class="sd">        TypeError: If dtype of `input` and the dtype of `q` is different.</span>
<span class="sd">        ValueError: If the `q` values not in the range [0, 1].</span>
<span class="sd">        ValueError: If the `axis` values out of range.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-0.7832, 0.8003, 0.8111]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; q = Tensor(np.array([0.1, 0.7, 0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.quantile(x, q)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [-0.4665 0.80462 0.80894]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;quantile&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;quantile&quot;</span><span class="p">)</span>

    <span class="n">quantile_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Quantile</span><span class="p">)(</span><span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantile_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">nanquantile</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operator is derived from mindspore.ops.quantile() that &#39;ignores&#39; NaN values.</span>
<span class="sd">    It computes quantiles as though the input has no NaN values. If all values in a</span>
<span class="sd">    reduced dimension are NaN then the quantiles for that reduction will be NaN.</span>

<span class="sd">    Refer to :func:`mindspore.ops.quantile` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">            Supported dtypes: float32, float64.</span>
<span class="sd">        q (Union[float, Tensor]): A scalar or 1D tensor of quantile values in the range [0, 1].</span>
<span class="sd">            Supported dtypes: float32, float64.</span>
<span class="sd">        axis (int, optional): The dimension to reduce. By default, `axis` is None resulting in the</span>
<span class="sd">            input tensor being flattened before computation. Default: None.</span>
<span class="sd">        keepdims (bool, optional): Whether the output tensor has dim retained or not. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">        Suppose the shape of `input` is :math:`(m, x_0, x_1, ..., x_i, ..., X_R)`, `axis` = :math:`i` and m is</span>
<span class="sd">        the element count of input `q`.</span>

<span class="sd">        - If `q` is scalar and `keepdims` is True, the shape of output is :math:`(x_0, x_1, ..., 1, ..., X_R)`.</span>
<span class="sd">        - If `q` is scalar and `keepdims` is False, the shape of output is :math:`(x_0, x_1, ..., X_R)`.</span>
<span class="sd">        - If `q` is 1D Tensor and `keepdims` is True, the shape of output is :math:`(m, x_0, x_1, ..., 1, ..., X_R)`.</span>
<span class="sd">        - If `q` is 1D Tensor and `keepdims` is False, the shape of output is :math:`(m, x_0, x_1, ..., X_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `q` is not a Tensor or float.</span>
<span class="sd">        TypeError: If dtype of `input` is not float32 or float64.</span>
<span class="sd">        TypeError: If dtype of `q` is not float32 or float64.</span>
<span class="sd">        TypeError: If dtype of `input` and the dtype of `q` is different.</span>
<span class="sd">        ValueError: If the `q` values not in the range [0, 1].</span>
<span class="sd">        ValueError: If the `axis` values out of range.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([float(&#39;nan&#39;), 0.8003, 0.8111]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; q = Tensor(np.array([0.1, 0.7, 0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nanquantile(x, q)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [0.80138 0.80786 0.81002]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;nanquantile&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;keepdims&quot;</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;nanquantile&quot;</span><span class="p">)</span>

    <span class="n">quantile_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Quantile</span><span class="p">)(</span><span class="n">dim</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">,</span> <span class="n">ignore_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">quantile_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>


<div class="viewcode-block" id="baddbmm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.baddbmm.html#mindspore.ops.baddbmm">[docs]</a><span class="k">def</span> <span class="nf">baddbmm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The result is the sum of the input and a batch matrix-matrix product of matrices in batch1 and batch2.</span>
<span class="sd">    The formula is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}_{i} = \beta \text{input}_{i} + \alpha (\text{batch1}_{i} \mathbin{@} \text{batch2}_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor. When batch1 is a :math:`(C, W, T)` Tensor and batch2 is a</span>
<span class="sd">            :math:`(C, T, H)` Tensor, input must be broadcastable with :math:`(C, W, H)` Tensor.</span>
<span class="sd">        batch1 (Tensor): :math:`batch1` in the above formula. Must be 3-D Tensor, dtype is same as input.</span>
<span class="sd">        batch2 (Tensor): :math:`batch2` in the above formula. Must be 3-D Tensor, dtype is same as input.</span>
<span class="sd">        beta (Union[float, int], optional): multiplier for input. The default is 1.</span>
<span class="sd">        alpha (Union[float, int], optional): multiplier for :math:`batch1 @ batch2`. The default is 1.</span>
<span class="sd">            Arguments beta and alpha must be integers when inputs of type not FloatTensor, otherwise they should</span>
<span class="sd">            be a real number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as input, shape will be :math:`(C, W, H)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: The type of `input`, `batch1`, `batch2` is not Tensor.</span>
<span class="sd">        TypeError: The types of `input`, `batch1`, `batch2` are different.</span>
<span class="sd">        TypeError: For inputs of type FloatTensor or DoubleTensor, \</span>
<span class="sd">                    arguments beta and alpha not be real numbers, otherwise not be integers.</span>
<span class="sd">        TypeError: For Baddbmm, attributes alpha and beta are not real numbers</span>
<span class="sd">        ValueError: If `batch1` and `batch2` are not 3-D tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([1, 3, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; batch1 = Tensor(np.ones([1, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; batch2 = Tensor(np.ones([1, 4, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.baddbmm(input, batch1, batch2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5. 5. 5.]</span>
<span class="sd">          [5. 5. 5.]</span>
<span class="sd">          [5. 5. 5.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>
    <span class="n">bmmop</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">)(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch2</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Baddbmm, inputs must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For batch1 and batch2 must be 3-D tensors each containing the same number of matrices, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got length of batch1:&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">batch1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;, length of batch2:&#39;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">batch2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">batch1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">input_dtype</span> <span class="o">==</span> <span class="n">dtypeop</span><span class="p">(</span><span class="n">batch2</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Baddbmm, the inputs should be the same dtype.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For attributes alpha and beta should be real numbers.&quot;</span><span class="p">)</span>
        <span class="n">check_is_number</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
        <span class="n">check_is_number</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For inputs of type not FloatTensor or DoubleTensor, &quot;</span>
                            <span class="s2">&quot;arguments beta and alpha must be integers.&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">bmmop</span><span class="p">(</span><span class="n">batch1</span><span class="p">,</span> <span class="n">batch2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">y</span></div>


<div class="viewcode-block" id="log2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log2.html#mindspore.ops.log2">[docs]</a><span class="k">def</span> <span class="nf">log2</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor by taking the base 2 logarithm of the elements in the input Tensor.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \log_2(input_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator log2 is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affected.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32 or float64 on CPU and GPU, if dtype of `input`</span>
<span class="sd">            is not float16 or float32 on Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, 8]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log2(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">)()</span>

    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">_make_tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">))</span>
    <span class="n">frac_log</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">frac_log</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">arrange</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">lists</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">lists</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lists</span>


<div class="viewcode-block" id="rot90"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rot90.html#mindspore.ops.rot90">[docs]</a><span class="k">def</span> <span class="nf">rot90</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.</span>
<span class="sd">    Rotation direction is from the first towards the second axis if k &gt; 0,</span>
<span class="sd">    and from the second towards the first for k &lt; 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        k (int): Number of times to rotate.</span>
<span class="sd">        dims (Union[list(int), tuple(int)]): Axis to rotate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `k` is not integer.</span>
<span class="sd">        TypeError: If `dims` is not tuple of integers or list of ints.</span>
<span class="sd">        ValueError: If the length of `dims` is not `2`.</span>
<span class="sd">        ValueError: If any dims is out of Tensor&#39;s range [-input.ndim, input.ndim).</span>
<span class="sd">        RuntimeError: If rotation dims are not different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1], [2, 3]])).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; k = 1</span>
<span class="sd">        &gt;&gt;&gt; dims = [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rot90(x, k, dims)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 3.]</span>
<span class="sd">        [0. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, the `input` must be Tensor!, but get </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, the `k` must be int!, but get </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, the `dims` must be list or tuple!, but get </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">total_dims</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">total_rot_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">total_rot_dims</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, total rotation dims must be 2, but get </span><span class="si">{</span><span class="n">total_rot_dims</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">or</span> <span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">total_dims</span> <span class="ow">or</span> <span class="p">(</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">total_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, rotation dims must be different, but get dim0=</span><span class="si">{</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, dim1=</span><span class="si">{</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">total_dims</span> <span class="ow">or</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">total_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, rotation dim0 is out of range, dim0=</span><span class="si">{</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">total_dims</span> <span class="ow">or</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">total_dims</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `rot90`, rotation dim1 is out of range, dim1=</span><span class="si">{</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">k</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span> <span class="o">+</span> <span class="p">(</span><span class="n">k</span> <span class="o">%</span> <span class="mi">4</span><span class="p">))</span> <span class="o">%</span> <span class="mi">4</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="k">return</span> <span class="n">out</span>
    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">op1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">op1</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">op2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">op2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">axes_list</span> <span class="o">=</span> <span class="n">arrange</span><span class="p">(</span><span class="n">total_dims</span><span class="p">)</span>
    <span class="p">(</span><span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span> <span class="o">=</span> <span class="p">(</span><span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
                                                <span class="n">axes_list</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>

    <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axes_list</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">axes_list</span><span class="p">)</span>
        <span class="n">op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="roll"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.roll.html#mindspore.ops.roll">[docs]</a><span class="k">def</span> <span class="nf">roll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shifts</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the elements of a tensor along an axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        shifts (Union[list(int), tuple(int), int]): Specifies the number of places by which elements are shifted</span>
<span class="sd">            positively (towards larger indices) along the specified dimension. Negative shifts will roll the elements</span>
<span class="sd">            in the opposite direction.</span>
<span class="sd">        dims (Union[list(int), tuple(int), int], optional): Specifies the dimension indexes of shape to be rolled.</span>
<span class="sd">            Default: None. If dims is None, the Tensor will be flattened before rolling and then restored to the</span>
<span class="sd">            original shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shifts` is not an int, a tuple or a list.</span>
<span class="sd">        TypeError: If `dims` is not an int, a tuple or a list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, 1, 2, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.roll(input_x, shifts=2, dims=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4. 0. 1. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">flatten_x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Roll</span><span class="p">(</span><span class="n">shifts</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">flatten_x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Roll</span><span class="p">(</span><span class="n">shifts</span><span class="p">,</span> <span class="n">dims</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="xdivy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.xdivy.html#mindspore.ops.xdivy">[docs]</a><span class="k">def</span> <span class="nf">xdivy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise. Returns zero when `x` is zero.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. note::</span>
<span class="sd">        When `x` and `y` are both of datatype complex, they should be both complex64 or complex128 at the same time.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]):  Tensor of datatype number.Number or bool, or it can be a bool or number.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): Tensor of datatype number.Number or bool, or it can be a bool or number.</span>
<span class="sd">            `x` and `y` can not be both bool at the same time.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` and `y` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        TypeError: If dtype of `x` and &#39;y&#39; is not in [float16, float32, float64, complex64, complex128, bool].</span>
<span class="sd">        ValueError: If `x` could not be broadcast to a tensor with shape of `y`.</span>
<span class="sd">        RuntimeError: If the data type of `x`, `y` conversion of Parameter is given</span>
<span class="sd">                      but data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.xdivy(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.   2.  -0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">xdivy_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="log10"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log10.html#mindspore.ops.log10">[docs]</a><span class="k">def</span> <span class="nf">log10</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new Tensor by taking the base 10 logarithm of the elements in the input Tensor.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \log_{10}(input_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator log10 is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affected.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of any dimension. The each element in Tensor must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32 or float64 on CPU and GPU, if dtype of `input`</span>
<span class="sd">            is not float16 or float32 on Ascend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, 10]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log10(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.301 0.602 1.   ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">dtype_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>

    <span class="n">x_dtype</span> <span class="o">=</span> <span class="n">dtype_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="n">_make_tensor</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">))</span>
    <span class="n">frac_log</span> <span class="o">=</span> <span class="n">log_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">frac_log</span> <span class="o">/</span> <span class="n">denominator</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="log1p"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log1p.html#mindspore.ops.log1p">[docs]</a><span class="k">def</span> <span class="nf">log1p</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = {log_e}(input_i + 1)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. With float16 or float32 data type.</span>
<span class="sd">            The value must be greater than -1.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log1p(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6931472 1.0986123 1.609438 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_log1p</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Log1p</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_log1p</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="kron"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.kron.html#mindspore.ops.kron">[docs]</a><span class="k">def</span> <span class="nf">kron</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kronecker product :math:`x ⊗ y`, denoted by ⊗, of `x` and `y`.</span>

<span class="sd">    If `x` is a :math:`(a_{0}` x :math:`a_{1}` x ... x :math:`a_{n})` Tensor</span>
<span class="sd">    and `y` is a :math:`(b_{0}` x :math:`b_{1}` x ... x :math:`b_{n})` Tensor,</span>
<span class="sd">    the result will be a :math:`(a_{0}*b_{0}` x :math:`a_{1}*b_{1}` x ... x :math:`a_{n}*b_{n})`</span>
<span class="sd">    Tensor with the following entries:</span>

<span class="sd">    .. math::</span>
<span class="sd">        (x ⊗ y)_{k_{0},k_{1},...k_{n}} =</span>
<span class="sd">        x_{i_{0},i_{1},...i_{n}} * y_{j_{0},j_{1},...j_{n}},</span>

<span class="sd">    where :math:`k_{t} = i_{t} * b_{t} + j_{t}` for 0 ≤ `t` ≤ `n`. If one</span>
<span class="sd">    Tensor has fewer dimensions than the other it is unsqueezed</span>
<span class="sd">    until it has the same number of dimensions.</span>

<span class="sd">    Note:</span>
<span class="sd">        Supports real-valued and complex-valued inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor, has the shape :math:`(r0, r1, ... , rN)`.</span>
<span class="sd">        y (Tensor): Input Tensor, has the shape :math:`(s0, s1, ... , sN)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the shape :math:`(r0 * s0, r1 * s1, ... , rN * sN)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `y` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1, 2], [3, 4, 5]])).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[-1, -2, -3], [-4, -6, -8]])).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.kron(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  0.   0.   0.  -1.  -2.  -3.  -2.  -4.  -6.]</span>
<span class="sd">         [  0.   0.   0.  -4.  -6.  -8.  -8. -12. -16.]</span>
<span class="sd">         [ -3.  -6.  -9.  -4.  -8. -12.  -5. -10. -15.]</span>
<span class="sd">         [-12. -18. -24. -16. -24. -32. -20. -30. -40.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input y must be Tensor!&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;=</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
        <span class="n">maxdim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">maxdim</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">pad_x</span> <span class="o">=</span> <span class="n">maxdim</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">pad_y</span> <span class="o">=</span> <span class="n">maxdim</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">x_reshape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">maxdim</span><span class="p">)]</span>
    <span class="n">y_reshape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">maxdim</span><span class="p">)]</span>
    <span class="n">result_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxdim</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxdim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">pad_x</span><span class="p">:</span>
            <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">pad_x</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">pad_y</span><span class="p">:</span>
            <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">pad_y</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">result_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">y_reshape</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x_reshape</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_reshape</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">result_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="k">def</span> <span class="nf">_check_is_tensor</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns True if input is Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="all"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.all.html#mindspore.ops.all">[docs]</a><span class="k">def</span> <span class="nf">all</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of `input` by the &quot;logical AND&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `input` along the axis. Determine whether the dimensions of the output and input are the same</span>
<span class="sd">    by controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor, has the shape :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)], optional): The dimensions to reduce. Suppose the rank of `input` is</span>
<span class="sd">            r, axis must be in the range [-rank(input), rank(input)). Default: ``None`` , all dimensions are reduced.</span>
<span class="sd">        keep_dims (bool, optional): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If false, don&#39;t keep these dimensions. Default : ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If `axis` is None, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D Tensor representing the &quot;logical AND&quot; of all elements in the input Tensor.</span>
<span class="sd">        - If `axis` is int, such as 2, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_3, ..., input_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), such as (2, 3), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_4, ..., input_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logicalAND&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.all(x, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.all(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.all(x, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;all&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceAll</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="any"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.any.html#mindspore.ops.any">[docs]</a><span class="k">def</span> <span class="nf">any</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of `input` by the &quot;logical OR&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `input` along the axis. Determine whether the dimensions of the output and input are the same</span>
<span class="sd">    by controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor, has the shape :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)], optional): The dimensions to reduce. Suppose the rank of `input` is r,</span>
<span class="sd">            axis must be in the range [-rank(input), rank(input)). Default: ``None`` , all dimensions are reduced.</span>
<span class="sd">        keep_dims (bool, optional): If true, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If false, don&#39;t keep these dimensions. Default : ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If `axis` is None, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D Tensor representing the &quot;logical OR&quot; of all elements in the input Tensor.</span>
<span class="sd">        - If `axis` is int, such as 2, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_3, ..., input_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), such as (2, 3), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_4, ..., input_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logical OR&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.any(x, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.any(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True True]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.any(x, axis=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;any&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceAny</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="remainder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.remainder.html#mindspore.ops.remainder">[docs]</a><span class="k">def</span> <span class="nf">remainder</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar. When the inputs are two tensors,</span>
<span class="sd">    both dtypes cannot be bool, and the shapes of them could be broadcast. When the inputs are one tensor</span>
<span class="sd">    and one scalar, the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        remainder(input, other) = input - input.div(other, rounding\_mode=&quot;floor&quot;) * other</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - When the elements of input exceed 2048, there might be accuracy problems.</span>
<span class="sd">        - The calculation results of this operator on Ascend and CPU might be inconsistent.</span>
<span class="sd">        - If shape is expressed as (D1,D2... ,Dn), then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, numbers.Number, bool]): The first input is a number, a bool</span>
<span class="sd">            or a tensor whose data type is number.</span>
<span class="sd">        other (Union[Tensor, numbers.Number, bool]): When the first input is a tensor, The second input</span>
<span class="sd">            could be a number, a bool or a tensor whose data type is number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is one of the following: Tensor, number, bool.</span>
<span class="sd">        ValueError: If the shape `input` and `other` cannot be broadcasted to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3.0, 2.0, 3.0]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.remainder(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.  1.  0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">out</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">tensor_floordiv</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span> <span class="o">*</span> <span class="n">other</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="accumulate_n"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.accumulate_n.html#mindspore.ops.accumulate_n">[docs]</a><span class="k">def</span> <span class="nf">accumulate_n</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes accumulation of all input tensors element-wise.</span>

<span class="sd">    :func:`mindspore.ops.accumulate_n` is similar to :func:`mindspore.ops.addn`,</span>
<span class="sd">    but there is a significant difference between them: accumulate_n will not wait</span>
<span class="sd">    for all of its inputs to be ready before summing. That is to say, accumulate_n is able to save memory when inputs</span>
<span class="sd">    are ready at different time since the minimum temporary storage is proportional to the output size rather than the</span>
<span class="sd">    input size.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union(tuple[Tensor], list[Tensor])): The input tuple or list is made up of multiple tensors whose dtype is</span>
<span class="sd">            number to be added together. Each element of tuple or list should have the same shape.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as each entry of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is neither tuple nor list.</span>
<span class="sd">        ValueError: If there is an input element with a different shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.accumulate_n([x, y, x, y])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10. 14. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">accumulate_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">AccumulateNV2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">accumulate_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="iou"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.iou.html#mindspore.ops.iou">[docs]</a><span class="k">def</span> <span class="nf">iou</span><span class="p">(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_boxes</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;iou&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates intersection over union for boxes.</span>

<span class="sd">    Computes the intersection over union (IOU) or the intersection over foreground (IOF) based on the ground-truth and</span>
<span class="sd">    predicted regions.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{IOU} = \frac{\text{Area of Overlap}}{\text{Area of Union}}</span>

<span class="sd">        \text{IOF} = \frac{\text{Area of Overlap}}{\text{Area of Ground Truth}}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In Ascend, only computation of float16 data is supported. To avoid overflow, the input length</span>
<span class="sd">        and width are scaled by 0.2 internally.</span>

<span class="sd">    Args:</span>
<span class="sd">        anchor_boxes (Tensor): Anchor boxes, tensor of shape :math:`(N, 4)` . &quot;N&quot; indicates the number of anchor boxes,</span>
<span class="sd">            and the value &quot;4&quot; refers to &quot;x0&quot;, &quot;y0&quot;, &quot;x1&quot;, and &quot;y1&quot;.</span>
<span class="sd">            Data type must be either float16， float32 or float64.</span>
<span class="sd">        gt_boxes (Tensor): Ground truth boxes, tensor of shape :math:`(M, 4)` . &quot;M&quot; indicates the number of ground</span>
<span class="sd">            truth boxes, and the value &quot;4&quot; refers to &quot;x0&quot;, &quot;y0&quot;, &quot;x1&quot;, and &quot;y1&quot;.</span>
<span class="sd">            Data type must be either float16, float32 or float64.</span>
<span class="sd">        mode (string): The mode is used to specify the calculation method,</span>
<span class="sd">            now supporting &#39;iou&#39; (intersection over union) or &#39;iof&#39; (intersection over foreground) mode.</span>
<span class="sd">            Default: ``&#39;iou&#39;`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the &#39;iou&#39; values, tensor of shape :math:`(M, N)` , with the same data type as `anchor_boxes`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        KeyError: When `mode` is not &#39;iou&#39; or &#39;iof&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; anchor_boxes = Tensor(np.random.randint(1.0, 5.0, [3, 4]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; gt_boxes = Tensor(np.random.randint(1.0, 5.0, [3, 4]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; mode = &#39;iou&#39;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.iou(anchor_boxes, gt_boxes, mode)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IOU</span><span class="p">)(</span><span class="n">mode</span><span class="p">)(</span><span class="n">anchor_boxes</span><span class="p">,</span> <span class="n">gt_boxes</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_is_float</span><span class="p">(</span><span class="n">dtype</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_list_comprehensions</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">item</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">obj</span><span class="p">)])</span>


<span class="k">def</span> <span class="nf">_tuple_setitem</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>
    <span class="n">tup</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_dim_in_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">_check</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;axes should be integers, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="o">-</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="n">dim</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">&gt;=</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dim </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s1"> is out of bounds for array of dimension </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">_check</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dim</span> <span class="o">%</span> <span class="n">ndim</span>


<span class="k">def</span> <span class="nf">dotrapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">y_left</span> <span class="o">=</span> <span class="n">select_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">y_right</span> <span class="o">=</span> <span class="n">select_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">y_sum</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_sum</span> <span class="o">-</span> <span class="p">(</span><span class="n">y_left</span> <span class="o">+</span> <span class="n">y_right</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>


<span class="k">def</span> <span class="nf">dotrapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">y_start_dim_left</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="n">y_start_dim_left</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">y_start_dim_left</span><span class="p">)</span>
    <span class="n">y_start_dim_right</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">y_start_dim_right</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">y_start_dim_right</span><span class="p">)</span>
    <span class="n">y_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">y</span><span class="p">)[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_slice_left</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y_start_dim_right</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="n">y_slice_right</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y_start_dim_right</span><span class="p">,</span> <span class="n">y_slice_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()(</span><span class="n">y_slice_left</span><span class="p">,</span> <span class="n">y_slice_right</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.</span>


<span class="k">def</span> <span class="nf">add_padding_to_shape</span><span class="p">(</span><span class="n">curr_shape</span><span class="p">,</span> <span class="n">target_n_dim</span><span class="p">):</span>
    <span class="n">curr_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">curr_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">curr_size</span> <span class="o">&gt;=</span> <span class="n">target_n_dim</span><span class="p">:</span>
        <span class="n">target_n_dim</span> <span class="o">=</span> <span class="n">curr_size</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">target_n_dim</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">curr_size</span><span class="p">):</span>
        <span class="n">new_shape</span><span class="p">[</span><span class="n">target_n_dim</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">curr_shape</span><span class="p">[</span><span class="n">curr_size</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">new_shape</span>


<span class="k">def</span> <span class="nf">zeros_like_except</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="n">_check_dim_in_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">zeros</span>


<span class="k">def</span> <span class="nf">trapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    add trapezoid implementation when x is not None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros_like_except</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_left</span><span class="p">)</span>
        <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_right</span><span class="p">)</span>
        <span class="n">x_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">x_left</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
        <span class="n">x_right</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">x_right</span> <span class="o">-</span> <span class="n">x_left</span>
        <span class="n">new_sizes</span> <span class="o">=</span> <span class="n">add_padding_to_shape</span><span class="p">(</span><span class="n">dx</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sizes</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">dotrapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;There must be one `x` value for each sample point&quot;</span><span class="p">)</span>
        <span class="n">new_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)]</span>
        <span class="n">new_sizes</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">x_viewed</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">new_sizes</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x_viewed</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="n">x_start_dim_left</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_left</span><span class="p">)</span>
    <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_viewed</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">x_start_dim_right</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_start_dim_right</span><span class="p">)</span>
    <span class="n">x_slice_size</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">x_viewed</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">x_viewed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">x_left</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x_viewed</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
    <span class="n">x_right</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="n">x_viewed</span><span class="p">,</span> <span class="n">x_start_dim_left</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_start_dim_right</span><span class="p">,</span> <span class="n">x_slice_size</span><span class="p">)</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">x_right</span> <span class="o">-</span> <span class="n">x_left</span>
    <span class="k">return</span> <span class="n">dotrapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">trapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros_like_except</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dotrapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">depth</span> <span class="o">==</span> <span class="n">dim</span><span class="p">:</span>
        <span class="n">r</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ts</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ts</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">get</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">select_</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
    <span class="n">select_shape</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">select_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">select_shape</span><span class="p">)</span>
    <span class="n">select_shape</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="n">dim</span><span class="p">]</span> <span class="o">+</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">indexes</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">select_shape</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feat</span><span class="o">.</span><span class="n">gather_elements</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">indexes</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>


<div class="viewcode-block" id="trapz"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.trapz.html#mindspore.ops.trapz">[docs]</a><span class="k">def</span> <span class="nf">trapz</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Integrates `y(x)` along given dim using trapezoidal rule.</span>
<span class="sd">    By default x-dim distances between points will be 1.0,</span>
<span class="sd">    alternatively they can be provided with `x` array or with `dx` scalar.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \mathop{ \int }\nolimits_{{}}^{{}}{y}{ \left( {x} \right) } \text{d} x</span>

<span class="sd">    Args:</span>
<span class="sd">        y (Tensor): Input tensor to integrate.</span>
<span class="sd">        x (Tensor, optional): The sample points corresponding to the `y` values. If `x` is None,</span>
<span class="sd">            the sample points are assumed to be evenly spaced `dx` apart. Default: ``None`` .</span>
<span class="sd">            If `x` is not None, after subtracting 1 from the axis specified by `dim`, the shape of `x`</span>
<span class="sd">            should be same as `y` or can broadcast to `y`.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dx (float, optional): The spacing between sample points when `x` is None. If `x` is specified,</span>
<span class="sd">            `dx` does not take effect. Default: ``1.0`` .</span>
<span class="sd">        dim (int, optional): The dim along which to integrate. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of float, definite integral as approximated by trapezoidal rule.</span>
<span class="sd">        If `y` is a one-dimensional array, the result is a floating-point number. If `y` is</span>
<span class="sd">        an n-dimensional array, the result is an N-1 dimensional array.</span>

<span class="sd">    Raises:</span>
<span class="sd">        RuntimeError: If dim of `x` is 1, and x.shape[0] is not equal to y.shape[dim].</span>
<span class="sd">        ValueError: If `dim` is out of range of :math:`[-y.ndim, y.ndim)`.</span>
<span class="sd">        TypeError: If `y` is not a Tensor.</span>
<span class="sd">        TypeError: If `x` is not None and is not a Tensor.</span>
<span class="sd">        TypeError: If `dx` is not a float number.</span>
<span class="sd">        TypeError: If `dim` is not a Integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [1, 3, 5], [1, 4, 7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trapz(y, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 4. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `trapz`, the input `y` must be Tensor, but get </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `trapz`, the input `dx` must be float, but get f</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `trapz`, the input `dim` must be int, but get </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_is_float</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">_check_dim_in_range</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">trapezoid</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For `trapz`, the input `x` must be Tensor, but get </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trapezoid_tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="cholesky"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cholesky.html#mindspore.ops.cholesky">[docs]</a><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the Cholesky decomposition of zero or more batch dimensions consisting of symmetric positive-definite</span>
<span class="sd">    matrices.</span>

<span class="sd">    If `upper` is `True`, returns an upper-triangular matrix, :math:`U`, and the decomposition has the form:</span>

<span class="sd">    .. math::</span>
<span class="sd">        A = U^TU</span>

<span class="sd">    If `upper` is `False`, returns a lower-triangular matrix, :math:`L`, and the decomposition has the form:</span>

<span class="sd">    .. math::</span>
<span class="sd">        A = LL^T</span>

<span class="sd">    where `A` is the symmetric positive-definite matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of shape :math:`(*, N, N)`, where :math:`*` is zero or more batch dimensions</span>
<span class="sd">            consisting of symmetric positive-definite matrices, with float32 or float64 data type.</span>
<span class="sd">        upper (bool): If `upper` is `True`, returns an upper-triangular matrix. If `upper` is `False`, returns</span>
<span class="sd">            a lower-triangular matrix. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `upper` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of: float64, float32.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If `input_x` is not a or a batch of square matrix.</span>
<span class="sd">        ValueError: If `input_x` is not symmetric positive definite.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 1.0], [1.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cholesky(input_x, upper=False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cholesky</span><span class="p">)(</span><span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cholesky_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">cholesky_inverse</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inverse of the positive definite matrix using cholesky matrix factorization by its Cholesky factor.</span>

<span class="sd">    If `upper` is `True`, :math:`U` is an upper triangular such that the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        inv = (U^{T}U)^{-1}</span>

<span class="sd">    If `upper` is `False`, :math:`U` is a lower triangular such that the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        inv = (UU^{T})^{-1}</span>

<span class="sd">    Note:</span>
<span class="sd">        The input must be either an upper-triangular matrix or a lower-triangular matrix from Cholesky decomposition.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor with a rank of 2. Supported dtypes: float32, float64.</span>
<span class="sd">        upper (bool): If `upper` is `True`, return an upper triangular matrix. If `upper` is `False`, return</span>
<span class="sd">            a lower-triangular matrix. Default: False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of: float32, float64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2,0,0], [4,1,0], [-1,1,2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cholesky_inverse(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 5.8125 -2.625   0.625 ]</span>
<span class="sd">         [-2.625   1.25   -0.25  ]</span>
<span class="sd">         [ 0.625  -0.25    0.25  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_inv_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CholeskyInverse</span><span class="p">)(</span><span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cholesky_inv_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<div class="viewcode-block" id="cholesky_solve"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cholesky_solve.html#mindspore.ops.cholesky_solve">[docs]</a><span class="k">def</span> <span class="nf">cholesky_solve</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the solution of a set of linear equations with a positive definite matrix,</span>
<span class="sd">    according to its Cholesky decomposition factor `input2` .</span>

<span class="sd">    If `upper` is set to ``True`` and `input2` is upper triangular, the output tensor is that:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = (input2^{T} * input2)^{{-1}}input</span>

<span class="sd">    If `upper` is set to ``False`` and `input2` is lower triangular, the output is that:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = (input2 * input2^{T})^{{-1}}input</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(*, N, M)`, indicating 2D or 3D matrices,</span>
<span class="sd">            with float32 or float64 data type.</span>
<span class="sd">        input2 (Tensor): Tensor of shape :math:`(*, N, N)`, indicating 2D or 3D square matrices composed of</span>
<span class="sd">            upper or lower triangular Cholesky factor, with float32 or float64 data type.</span>
<span class="sd">            `input` and `input2` must have the same type.</span>
<span class="sd">        upper (bool, optional): A flag indicates whether to treat the Cholesky factor</span>
<span class="sd">            as an upper or a lower triangular matrix. Default: ``False``, treating the Cholesky factor</span>
<span class="sd">            as a lower triangular matrix.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `upper` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input` and `input2` is not float64 or float32.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `input2` is not a Tensor.</span>
<span class="sd">        ValueError: If `input` and `input2` have different batch size.</span>
<span class="sd">        ValueError: If `input` and `input2` have different row numbers.</span>
<span class="sd">        ValueError: If `input` is not 2D or 3D matrices.</span>
<span class="sd">        ValueError: If `input2` is not 2D or 3D square matrices.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input1 = Tensor(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input2 = Tensor(np.array([[2, 0, 0], [4, 1, 0], [-1, 1, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.cholesky_solve(input1, input2, upper=False)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[ 5.8125 -2.625   0.625 ]</span>
<span class="sd">         [-2.625   1.25   -0.25  ]</span>
<span class="sd">         [ 0.625  -0.25    0.25  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CholeskySolve</span><span class="p">)(</span><span class="n">upper</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span></div>


<div class="viewcode-block" id="conj"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conj.html#mindspore.ops.conj">[docs]</a><span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of complex numbers that are the complex conjugate of each element in input.</span>
<span class="sd">    The complex numbers in input must be of the form a + bj, where a is the real part and b is the imaginary part.</span>

<span class="sd">    The complex conjugate returned by this operation is of the form a - bj.</span>

<span class="sd">    If `input` is real, it is returned unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to. Must have numeric type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` is not a numeric type.</span>
<span class="sd">        TypeError: If the `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conj(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1.3-0.4j)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For conj op, input must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Conj</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cross"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cross.html#mindspore.ops.cross">[docs]</a><span class="k">def</span> <span class="nf">cross</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cross product of `input` and `other` in dimension `dim`.</span>
<span class="sd">    `input` and `other` must have the same shape, and the size of their `dim` dimension should be `3`.</span>
<span class="sd">    If `dim` is not specified, it is set to be the first dimension found with the size `3`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input is a tensor.</span>
<span class="sd">        other (Tensor):  The other Tensor, `other` must have the same shape and type as input `input`, and</span>
<span class="sd">            the size of their `dim` dimension should be `3`.</span>
<span class="sd">        dim (int, optional): dimension to apply cross product in. if `dim` is None, it is set to be the first dimension</span>
<span class="sd">            found with the size `3`. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `other` is not a Tensor.</span>
<span class="sd">        TypeError: If the type of `input` is not the same as that of `other`.</span>
<span class="sd">        ValueError: If `input` and `other` not have the same size, and the size of their `dim` dimension not be `3`.</span>
<span class="sd">        ValueError: If `input` and `other` not have the same shape.</span>
<span class="sd">        ValueError: If `dim` is out of range, `dim` should be [-len(input.shape), len(input.shape)-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: dim=None.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [1, 2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor([[4, 5, 6], [4, 5, 6]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3  6 -3]</span>
<span class="sd">         [-3  6 -3]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: dim=1.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [1, 2, 3]])</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor([[4, 5, 6], [4, 5, 6]])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cross(x, other, dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3  6 -3]</span>
<span class="sd">         [-3  6 -3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">65530</span>
    <span class="n">cross_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cross</span><span class="p">)(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cross_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_einsum_convert_num_to_char</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;For einsum, convert number into char.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="p">[</span><span class="n">num</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="bp">Ellipsis</span><span class="p">]:</span>
        <span class="k">return</span> <span class="s1">&#39;...&#39;</span>
    <span class="c1"># pylint: disable=chained-comparison</span>
    <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">num</span> <span class="o">&lt;</span> <span class="mi">26</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">num</span> <span class="o">+</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;A&#39;</span><span class="p">))</span>
    <span class="c1"># pylint: disable=chained-comparison</span>
    <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;=</span> <span class="mi">26</span> <span class="ow">and</span> <span class="n">num</span> <span class="o">&lt;</span> <span class="mi">52</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">chr</span><span class="p">(</span><span class="n">num</span> <span class="o">-</span> <span class="mi">26</span> <span class="o">+</span> <span class="nb">ord</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">))</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For Einsum, the number in sublist should be in range [0， 52), but got </span><span class="si">{</span><span class="n">num</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="einsum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.einsum.html#mindspore.ops.einsum">[docs]</a><span class="k">def</span> <span class="nf">einsum</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="o">*</span><span class="n">operands</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    According to the Einstein summation Convention (Einsum),</span>
<span class="sd">    the product of the input tensor elements is summed along the specified dimension.</span>
<span class="sd">    You can use this operator to perform diagonal, reducesum, transpose, matmul, mul, inner product operations, etc.</span>

<span class="sd">    Note:</span>
<span class="sd">        The sublist format is also supported. For example, ops.einsum(op1, sublist1, op2, sublist2, ..., sublist_out).</span>
<span class="sd">        In this format, equation can be derived by the sublists which are made up of Python&#39;s Ellipsis and list of</span>
<span class="sd">        integers in [0, 52). Each operand is followed by a sublist and an output sublist is at the end.</span>

<span class="sd">    Args:</span>
<span class="sd">        equation (str): Notation based on the Einstein summation convention, represent the operation you want to do.</span>
<span class="sd">            the value can contain only letters, commas, ellipsis and arrow.</span>
<span class="sd">            The letters represent input tensor dimension, commas represent separate tensors, ellipsis indicates</span>
<span class="sd">            the tensor dimension that you do not care about, the left of the arrow indicates the input tensors,</span>
<span class="sd">            and the right of it indicates the desired output dimension.</span>
<span class="sd">        operands (Tensor): Input tensor used for calculation. The dtype of the tensor must be the same.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of it can be obtained from the `equation` , and the dtype is the same as input tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `equation` is invalid, or the `equation` does not match the input tensor.</span>
<span class="sd">        ValueError: If the number in sublist is not in [0, 52) in sublist format.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;i-&gt;&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [7.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;i,i-&gt;i&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2. 8. 12.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([[2.0, 3.0], [1.0, 2.0], [4.0, 5.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;ij,jk-&gt;ik&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[16. 22.]</span>
<span class="sd">         [37. 52.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;ij-&gt;ji&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 4.]</span>
<span class="sd">         [2. 5.]</span>
<span class="sd">         [3. 6.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;ij-&gt;j&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;...-&gt;&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [21.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; equation = &quot;j,i-&gt;ji&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(equation, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2. 4. 1.]</span>
<span class="sd">         [ 4. 8. 2.]</span>
<span class="sd">         [ 6. 12. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; x = mindspore.Tensor([1, 2, 3, 4], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = mindspore.Tensor([1, 2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.einsum(x, [..., 1], y, [..., 2], [..., 1, 2])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2.]</span>
<span class="sd">         [2. 4.]</span>
<span class="sd">         [3. 6.]</span>
<span class="sd">         [4. 8.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">equation</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">equ_tmp</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lst</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">operands</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lst</span><span class="p">):</span>
                    <span class="n">equ_tmp</span> <span class="o">+=</span> <span class="n">_einsum_convert_num_to_char</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="n">equ_tmp</span> <span class="o">+=</span> <span class="s1">&#39;,&#39;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">operands</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">equ_tmp</span> <span class="o">+=</span> <span class="s1">&#39;-&gt;&#39;</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">num</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">equ_tmp</span> <span class="o">+=</span> <span class="n">_einsum_convert_num_to_char</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
            <span class="n">operands_tmp</span> <span class="o">=</span> <span class="nb">list</span><span class="p">([</span><span class="n">equation</span><span class="p">])</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">operands_tmp</span> <span class="o">=</span> <span class="nb">list</span><span class="p">([</span><span class="n">equation</span><span class="p">])</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">operands</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">equation</span> <span class="o">=</span> <span class="n">equ_tmp</span>
        <span class="n">operands</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">operands_tmp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Einsum</span><span class="p">)(</span><span class="n">equation</span><span class="p">)(</span><span class="n">operands</span><span class="p">)</span></div>


<div class="viewcode-block" id="erfinv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erfinv.html#mindspore.ops.erfinv">[docs]</a><span class="k">def</span> <span class="nf">erfinv</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the result of the inverse error function with `input`, which is defined in the</span>
<span class="sd">    range `(-1, 1)` as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfinv(erf(x)) = x</span>

<span class="sd">    where :math:`x` is the `input`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to, with data type float32, float16 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0.5, -0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfinv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.          0.47695306 -1.1630805 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Erfinv</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="less_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.less_equal.html#mindspore.ops.less_equal">[docs]</a><span class="k">def</span> <span class="nf">less_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input &lt;= other` element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } input_{i}&lt;=other_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } input_{i}&gt;other_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less_equal(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="cumprod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cumprod.html#mindspore.ops.cumprod">[docs]</a><span class="k">def</span> <span class="nf">cumprod</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative product of the `input` tensor along dimension `dim`.</span>
<span class="sd">    For example, if `input` is a vector of size `N`, the result will also be a vector of size `N`, with elements.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 * x_2 * x_3 * ... * x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor[Number]): The input tensor.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        dim (int): The dimensions to compute the cumulative product. Only constant value is allowed.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The desired data type of output.</span>
<span class="sd">            If not specified, remains the same as the original Tensor. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input` unless `dtype` is specified.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dim` is not an int.</span>
<span class="sd">        TypeError: If `dtype` conversion is not acceptable.</span>
<span class="sd">        ValueError: If `dim` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cumprod(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 6.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cumprod_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">CumProd</span><span class="p">)()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">cumprod_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">output</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="greater"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.greater.html#mindspore.ops.greater">[docs]</a><span class="k">def</span> <span class="nf">greater</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input &gt; other` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.greater(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">greater_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Greater</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">greater_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="greater_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.greater_equal.html#mindspore.ops.greater_equal">[docs]</a><span class="k">def</span> <span class="nf">greater_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input \geq other` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool\_.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool\_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.greater_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">greater_equal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">GreaterEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">greater_equal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="igamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.igamma.html#mindspore.ops.igamma">[docs]</a><span class="k">def</span> <span class="nf">igamma</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates lower regularized incomplete Gamma function.</span>

<span class="sd">    If we define `input` as `a` and `other` as `x`, the lower regularized incomplete Gamma function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(a, x) = Gamma(a, x) / Gamma(a) = 1 - Q(a, x)</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        Gamma(a, x) = \int_0^x t^{a-1} \exp^{-t} dt</span>

<span class="sd">    is the lower incomplete Gamma function.</span>

<span class="sd">    Above :math:`Q(a, x)` is the upper regularized complete Gamma function.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. With type of float32 or float64.</span>
<span class="sd">        other (Tensor): The second input tensor. With float32 or float64 type. `other` should have</span>
<span class="sd">          the same dtype with `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input` and `other`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input `other` and a is not float32 nor float64.</span>
<span class="sd">        TypeError: If `other` has different dtype with `input`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to a tensor with shape of `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([2.0, 4.0, 6.0, 8.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2.0, 3.0, 4.0, 5.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.igamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.593994 0.35276785 0.21486944 0.13337152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">igamma_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Igamma</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">igamma_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="igammac"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.igammac.html#mindspore.ops.igammac">[docs]</a><span class="k">def</span> <span class="nf">igammac</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates upper regularized incomplete Gamma function.</span>

<span class="sd">    If we define `input` as `a` and `other` as `x`, the upper regularized incomplete Gamma function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        Q(a, x) = Gamma(a, x) / Gamma(a) = 1 - P(a, x)</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        Gamma(a, x) = \int_{x}^{\infty} t^{a-1} exp(-t) dt</span>

<span class="sd">    is the upper incomplete Gama function.</span>

<span class="sd">    Above :math:`P(a, x)` is the lower regularized complete Gamma function.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. With type of float32 or float64.</span>
<span class="sd">        other (Tensor): The second input tensor. With float32 or float64 type. `other` should have</span>
<span class="sd">            the same dtype with `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input` and `other`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of input `other` and a is not float32 nor float64.</span>
<span class="sd">        TypeError: If `other` has different dtype with `input`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to a tensor with shape of `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([2.0, 4.0, 6.0, 8.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2.0, 3.0, 4.0, 5.0]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.igammac(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print (output)</span>
<span class="sd">        [0.40600586 0.6472318 0.7851304 0.8666283]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">igammac_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Igammac</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">igammac_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">lgamma</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the natural logarithm of the absolute value of the gamma function on input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{out}_{i} = \ln \Gamma(|\text{input}_{i}|)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. With type of float16 or float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.5, 3.2, 8.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lgamma(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5723649 0.8854049 9.549267 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lgamma_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Lgamma</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">lgamma_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<div class="viewcode-block" id="digamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.digamma.html#mindspore.ops.digamma">[docs]</a><span class="k">def</span> <span class="nf">digamma</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the grad of the lgamma function on input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        P(input) = grad(\ln \Gamma(input))</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. With type of float16 or float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.5, 0.5, 9]).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.digamma(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.0365 -1.964   2.14  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">digamma_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Digamma</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">digamma_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="polygamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.polygamma.html#mindspore.ops.polygamma">[docs]</a><span class="k">def</span> <span class="nf">polygamma</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the :math:`n`-th derivative of the polygamma function on `input`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \psi^{(a)}(x) = \frac{d^{(a)}}{dx^{(a)}} \psi(x)</span>

<span class="sd">    where :math:`\psi(x)` is the digamma function.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (Tensor): The order of the polygamma function.</span>
<span class="sd">            Supported dtypes: int32, int64. The shape of `n` is :math:`()`.</span>
<span class="sd">        input (Tensor): The tensor to compute the :math:`n`-th derivative of the polygamma function with.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not one of: float16, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `n` is not one of: int32, int64.</span>
<span class="sd">        TypeError: If shape of `n` is not :math:`()`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([3.14, -2.71]), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array(1), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.polygamma(a, x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.37446456 15.49884838]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">polygamma_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Polygamma</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">polygamma_op</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="isinf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isinf.html#mindspore.ops.isinf">[docs]</a><span class="k">def</span> <span class="nf">isinf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines which elements are inf or -inf for each position.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">          &amp; \ True,\ \text{ if } x_{i} = \text{Inf} \\</span>
<span class="sd">          &amp; \ False,\ \text{ if } x_{i} \ne  \text{Inf}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`Inf` means not a number.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isinf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">isinf_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IsInf</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">isinf_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_is_sign_inf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests element-wise for infinity with sign.&quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Zeros</span><span class="p">)()(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">)()(</span><span class="n">shape</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">is_inf</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IsInf</span><span class="p">)()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">is_sign</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">zeros_tensor</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">is_inf</span><span class="p">,</span> <span class="n">ones_tensor</span><span class="p">,</span> <span class="n">zeros_tensor</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">is_sign</span><span class="p">,</span> <span class="n">res</span><span class="p">,</span> <span class="n">zeros_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">res</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>


<div class="viewcode-block" id="isposinf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isposinf.html#mindspore.ops.isposinf">[docs]</a><span class="k">def</span> <span class="nf">isposinf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tests element-wise for positive infinity.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input values.</span>

<span class="sd">    Returns:</span>
<span class="sd">       Tensor, true where `input` is positive infinity, false otherwise.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isposinf(Tensor([[-float(&quot;inf&quot;), float(&quot;inf&quot;)], [1, float(&quot;inf&quot;)]], mstype.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False  True]</span>
<span class="sd">         [False  True]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;isposinf&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_is_sign_inf</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor_gt</span><span class="p">)</span></div>


<div class="viewcode-block" id="isneginf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.isneginf.html#mindspore.ops.isneginf">[docs]</a><span class="k">def</span> <span class="nf">isneginf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tests element-wise for negative infinity.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">       Tensor, true where `input` is negative infinity, false otherwise.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; output = ops.isneginf(Tensor([[-float(&quot;inf&quot;), float(&quot;inf&quot;)], [1, -float(&quot;inf&quot;)]], mstype.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True False]</span>
<span class="sd">         [False  True]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;isneginf&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_is_sign_inf</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor_lt</span><span class="p">)</span></div>


<div class="viewcode-block" id="logical_xor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logical_xor.html#mindspore.ops.logical_xor">[docs]</a><span class="k">def</span> <span class="nf">logical_xor</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical XOR&quot; of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \oplus y_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input is a tensor whose data type can be implicitly converted to bool.</span>
<span class="sd">        other (Tensor): The second input is a tensor whose data type can be implicitly converted to bool</span>
<span class="sd">            to compute XOR with the first input.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor whose data type is bool.</span>
<span class="sd">        ValueError: If the shape of two inputs cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
    <span class="n">logical_xor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">LogicalXor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">logical_xor_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="imag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.imag.html#mindspore.ops.imag">[docs]</a><span class="k">def</span> <span class="nf">imag</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor containing imaginary value of the `input`.</span>
<span class="sd">    If `input` is real, it will return zeros.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3 + 0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.imag(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.4</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Imag</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_repeat_in_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_ndim</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;check repeat dim in axis&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="n">axis_deal</span> <span class="o">=</span> <span class="p">[</span><span class="n">dim</span> <span class="o">+</span> <span class="n">x_ndim</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">dim</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">axis_deal</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">axis_deal</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">, dim </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2"> appears multiple times in axis.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="nansum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nansum.html#mindspore.ops.nansum">[docs]</a><span class="k">def</span> <span class="nf">nansum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sum of `input` over a given dimension, treating NaNs as zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>
<span class="sd">        axis (Union[int, tuple(int)], optional): The dimensions to reduce. Supposed the rank of `input` is r,</span>
<span class="sd">            axis must be in the range [-rank(input), rank(input)). Default: None, all dimensions are reduced.</span>
<span class="sd">        keepdims (bool, optional): Whether the output Tensor keeps dimensions or not. Default: False.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The dtype of output Tensor. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the sum of input `input` in the given dimension dim, treating NaNs as zero.</span>

<span class="sd">        - If axis is None, keepdims is False,</span>
<span class="sd">          the output is a 0-D Tensor representing the sum of all elements in the input Tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keepdims is False,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_3, ..., input_R)`.</span>
<span class="sd">        - If axis is tuple(int) or list(int), set as (2, 3), and keepdims is False,</span>
<span class="sd">          the shape of output is :math:`(input_1, input_4, ..., input_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        TypeError: If the dtype of `input` or `dtype` is complex type.</span>
<span class="sd">        ValueError: If &#39;axis&#39; not in [-rank(`input`), rank(`input`)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[float(&quot;nan&quot;), 2, 3], [1, 2, float(&quot;nan&quot;)]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output1 = ops.nansum(x, axis=0, keepdims=False, dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.nansum(x, axis=0, keepdims=True, dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [1. 4. 3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        [[1. 4. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;nansum&quot;</span><span class="p">)</span>
    <span class="n">_check_repeat_in_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="s2">&quot;nansum&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_complex</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For nansum, input are not supported complex type, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">complex_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For nansum, dtype not supported complex type, but got </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">is_nan</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">IsNan</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">is_nan</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)(</span><span class="n">keepdims</span><span class="p">)(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">dtype</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span></div>


<div class="viewcode-block" id="diag_embed"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diag_embed.html#mindspore.ops.diag_embed">[docs]</a><span class="k">def</span> <span class="nf">diag_embed</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with diagonals filled by `input`. The remaining elements are filled by 0.</span>
<span class="sd">    If the shape of `input` is :math:`[x_{0}, x_{1}, ..., x_{n-1}, x_{n}]`, the output shape is: the vector obtained</span>
<span class="sd">    by inserting :math:`x_{n}+|offset|` into the vector :math:`[x_{0}, x_{1}, ..., x_{n-1}]`</span>
<span class="sd">    at position `dim1` and `dim2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Values to fill diagonal.</span>
<span class="sd">        offset (int, optional): Offset of the diagonal. :math:`offset=0` refers to the main diagonal. Default: ``0`` .</span>

<span class="sd">            - If :math:`offset&gt;0`, fill the diagonals that are `offset` units upward from the main diagonal.</span>
<span class="sd">            - If :math:`offset&lt;0`, fill the diagonals that are `|offset|` units downward from the main diagonal.</span>

<span class="sd">        dim1 (int, optional): The first dimension in `input` with respect to which to fill diagonal. Default: ``-2`` .</span>
<span class="sd">        dim2 (int, optional): The second dimension in `input` with respect to which to fill diagonal. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as `input`, but the shape of output is one dimension higher than the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not supported.</span>
<span class="sd">        TypeError: If `offset` is not an int.</span>
<span class="sd">        TypeError: If `dim1` or `dim2` is not an int.</span>
<span class="sd">        ValueError: If the dimension of `input` is not 1D-6D.</span>
<span class="sd">        ValueError: If `dim1` is not in range of [-len(input.shape) - 1, len(input.shape)].</span>
<span class="sd">        ValueError: If `dim2` is not in range of [-len(input.shape) - 1, len(input.shape)].</span>
<span class="sd">        ValueError: If `dim1` and `dim2` are identical.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2,3,4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diag_embed(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 0. 0.]</span>
<span class="sd">         [0. 3. 0.]</span>
<span class="sd">         [0. 0. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">transpose_op</span> <span class="o">=</span> <span class="n">Transpose</span><span class="p">()</span>
    <span class="n">matrix_set_diag_op</span> <span class="o">=</span> <span class="n">MatrixSetDiagV3</span><span class="p">(</span><span class="n">align</span><span class="o">=</span><span class="s2">&quot;LEFT_RIGHT&quot;</span><span class="p">)</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Zeros</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;diag_embed&#39;, &#39;input&#39; must be Tensor.&quot;</span><span class="p">)</span>
    <span class="n">dtypeop</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()</span>
    <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">dtypeop</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">input_dtype</span> <span class="ow">in</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span>
                            <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;diag_embed&#39;, the dtype of &#39;input&#39; must be int8, int16, int32, int64, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;uint8, uint16, uint32, uint64, float16, float32 or float64, but got &#39;</span><span class="si">{</span><span class="n">input_dtype</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;diag_embed&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;dim1&quot;</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;diag_embed&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;dim2&quot;</span><span class="p">,</span> <span class="n">dim2</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;diag_embed&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">6</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;diag_embed&#39;, the dimension of &#39;input&#39; must be 1-6D.&quot;</span><span class="p">)</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">output_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">dim1</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">output_dim</span> <span class="ow">or</span> <span class="n">dim1</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">output_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;diag_embed&#39;, &#39;dim1&#39; must be in range of [</span><span class="si">{</span><span class="o">-</span><span class="n">output_dim</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">output_dim</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">], &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">dim1</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim2</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">output_dim</span> <span class="ow">or</span> <span class="n">dim2</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">output_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;diag_embed&#39;, &#39;dim2&#39; must be in range of [</span><span class="si">{</span><span class="o">-</span><span class="n">output_dim</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">output_dim</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">], &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">dim2</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim1</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim1_</span> <span class="o">=</span> <span class="n">dim1</span> <span class="o">+</span> <span class="n">output_dim</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim1_</span> <span class="o">=</span> <span class="n">dim1</span>
    <span class="k">if</span> <span class="n">dim2</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dim2_</span> <span class="o">=</span> <span class="n">dim2</span> <span class="o">+</span> <span class="n">output_dim</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dim2_</span> <span class="o">=</span> <span class="n">dim2</span>
    <span class="k">if</span> <span class="n">dim1_</span> <span class="o">==</span> <span class="n">dim2_</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;diag_embed&#39;, &#39;dim1&#39; must not be identical to &#39;dim2&#39;.&quot;</span><span class="p">)</span>
    <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dsize</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">offset</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dsize</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">offset</span>
    <span class="n">diag_plane</span> <span class="o">=</span> <span class="p">(</span><span class="n">dsize</span><span class="p">,</span> <span class="n">dsize</span><span class="p">)</span>
    <span class="n">output_shape_trans</span> <span class="o">=</span> <span class="n">batch_shape</span> <span class="o">+</span> <span class="n">diag_plane</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">output_shape_trans</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">offset</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">matrix_set_diag_op</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">dim1_</span><span class="p">:</span>
            <span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span> <span class="o">+</span> <span class="p">(</span><span class="n">output_dim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,)</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">==</span> <span class="n">dim2_</span><span class="p">:</span>
            <span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span> <span class="o">+</span> <span class="p">(</span><span class="n">output_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span> <span class="o">+</span> <span class="p">(</span><span class="n">dim</span><span class="p">,)</span>
            <span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">transpose_op</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="sum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sum.html#mindspore.ops.sum">[docs]</a><span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate sum of Tensor elements over a given dim.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        dim (Union[None, int, tuple(int), list(int)]): Dimensions along which a sum is performed.</span>
<span class="sd">            If None, sum all the elements of the input tensor.</span>
<span class="sd">            If the `dim` is a tuple or list of ints, a sum is performed on all the dimensions specified in the tuple.</span>
<span class="sd">            Must be in the range :math:`[-input.ndim, input.ndim)` . Default: None.</span>
<span class="sd">        keepdim (bool): Whether the output tensor has dim retained or not.</span>
<span class="sd">            If True, keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If False, don&#39;t keep these dimensions. Default: False.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The desired data type of returned Tensor. Default: None.</span>

<span class="sd">    Returns:</span>
<span class="sd">       A Tensor, sum of elements over a given dim in `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `dim` is not an int, tulpe(int), list(int) or None.</span>
<span class="sd">        ValueError: If `dim` is not in the range :math:`[-input.ndim, input.ndim)` .</span>
<span class="sd">        TypeError: If `keepdim` is not a bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.sum(x)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        270.0</span>
<span class="sd">        &gt;&gt;&gt; out = ops.sum(x, dim=2)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[ 6. 12. 18.]</span>
<span class="sd">         [24. 30. 36.]</span>
<span class="sd">         [42. 48. 54.]]</span>
<span class="sd">        &gt;&gt;&gt; out = ops.sum(x, dim=2, keepdim=True)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[ 6.]</span>
<span class="sd">         [12.]</span>
<span class="sd">         [18.]]</span>
<span class="sd">        [[24.]</span>
<span class="sd">         [30.]</span>
<span class="sd">         [36.]]</span>
<span class="sd">        [[42.]</span>
<span class="sd">         [48.]</span>
<span class="sd">         [54.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;sum&#39;, &#39;input&#39; must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;sum&#39;, &#39;dim&#39; must be int, tuple(int), list(int) or None.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">keepdim</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;sum&#39;, &#39;keepdim&#39; must be bool.&quot;</span><span class="p">)</span>

    <span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tanhshrink.html#mindspore.ops.tanhshrink">[docs]</a><span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Tanhshrink Activation, :math:`Tanhshrink(x)=x-Tanh(x)` , where :math:`x` corresponds to `input` .</span>
<span class="sd">    See :class:`mindspore.nn.Tanhshrink` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3, 2, 1]), ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tanhshrink(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.2383 1.036  2.004  1.036  0.2383]</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For tanhshrink, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint_type</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="n">tanh_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Tanh</span><span class="p">)()</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="n">tanh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="zeta"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.zeta.html#mindspore.ops.zeta">[docs]</a><span class="k">def</span> <span class="nf">zeta</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Elemental-wise compute the Hurwitz zeta output.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \zeta(x, q) = \sum_{k=0}^{\infty} \frac{1}{(k + q)^x}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, int, float]): Input Tensor. Represented as :math:`x` in the formula. If it&#39;s a Tensor, its</span>
<span class="sd">            dtype must be either float32 or float64.</span>
<span class="sd">        other (Union[Tensor, int, float]): Input Tensor must have the same dtype as `input`.</span>
<span class="sd">            Represented as :math:`q` in the formula.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The result of Hurwitz zeta function.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float32 nor float64.</span>
<span class="sd">        TypeError: If dtype of `other` is neither float32 nor float64.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([10.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; q = Tensor(np.array([1.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; z = ops.zeta(x, q)</span>
<span class="sd">        &gt;&gt;&gt; print(z)</span>
<span class="sd">        [1.0009946]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;zeta&#39;, at least one of the inputs should be Tensor.&quot;</span><span class="p">)</span>
        <span class="n">_dtype</span> <span class="o">=</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;zeta&#39;, at least one of the inputs should be Tensor.&quot;</span><span class="p">)</span>
        <span class="n">_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">)()(</span><span class="n">other</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span> <span class="o">&lt;</span> <span class="n">other</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">)(</span><span class="n">other</span><span class="o">.</span><span class="n">shape</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span> <span class="o">&gt;</span> <span class="n">other</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
        <span class="n">other</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">)(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)(</span><span class="n">other</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Zeta</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<span class="k">def</span> <span class="nf">matrix_power</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Raises a square matrix to the (integer) power `n` .</span>

<span class="sd">    - When :math:`n=0` , returns the identity matrix, which has the same shape as `input` .</span>
<span class="sd">    - When :math:`n&lt;0` and `input` is invertible, returns the inverse of `input` to the power of :math:`-n` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A 3-D Tensor. Supported data types are float16 and float32.</span>
<span class="sd">            The shape is :math:`(b, m, m)` , represents b m-D square matrices.</span>
<span class="sd">        n (int): The exponent, a required int.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 3-D Tensor. Data type and shape are the same as `input` &#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `n` is not int.</span>
<span class="sd">        TypeError: If the data type of `input` is neither float32 nor float16.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If `input` is not a 3-D tensor.</span>
<span class="sd">        ValueError: If shape[1] and shape[2] of `input` are not the same.</span>
<span class="sd">        ValueError: If `n` is negative but got input `input` has singular matrices.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[[0, 1], [-1, 0]], [[1, 0], [0, -1]]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.matrix_power(input, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[-1.  0.]</span>
<span class="sd">          [-0. -1.]]</span>
<span class="sd">         [[ 1.  0.]</span>
<span class="sd">          [ 0.  1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_power_ops</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MatrixPower</span><span class="p">)(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_power_ops</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;addn&#39;</span><span class="p">,</span>
    <span class="s1">&#39;absolute&#39;</span><span class="p">,</span>
    <span class="s1">&#39;abs&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bucketize&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addbmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addcdiv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addcmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;angle&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argmin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arccosh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arcsin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arctan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arctan2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bincount&#39;</span><span class="p">,</span>
    <span class="s1">&#39;neg_tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;neg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;negative&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_lt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;less&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logaddexp2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_le&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lcm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;le&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lerp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;norm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_gt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logaddexp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addmv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;adjoint&#39;</span><span class="p">,</span>
    <span class="s1">&#39;outer&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_ge&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ge&#39;</span><span class="p">,</span>
    <span class="s1">&#39;addr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;subtract&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;multiply&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nan_to_num&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nansum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;digamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lgamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;divide&#39;</span><span class="p">,</span>
    <span class="s1">&#39;true_divide&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_floordiv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floordiv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;float_power&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fmod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xdivy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_pow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pows&#39;</span><span class="p">,</span>
    <span class="s1">&#39;renorm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_mod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor_mod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floormod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_exp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;exp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_expm1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;expm1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;not_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ne&#39;</span><span class="p">,</span>
    <span class="s1">&#39;numel&#39;</span><span class="p">,</span>
    <span class="s1">&#39;permute&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_update&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isfinite&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isnan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isclose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isreal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isneginf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isposinf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_complex&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logdet&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log_matrix_determinant&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_determinant&#39;</span><span class="p">,</span>
    <span class="s1">&#39;det&#39;</span><span class="p">,</span>
    <span class="s1">&#39;linspace&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logspace&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lu_solve&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_solve&#39;</span><span class="p">,</span>
    <span class="s1">&#39;std&#39;</span><span class="p">,</span>
    <span class="s1">&#39;maximum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;minimum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;median&#39;</span><span class="p">,</span>
    <span class="s1">&#39;positive&#39;</span><span class="p">,</span>
    <span class="s1">&#39;floor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_not&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_or&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_and&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logit&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gcd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logsumexp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ldexp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rsqrt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reciprocal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;real&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sqrt&#39;</span><span class="p">,</span>
    <span class="s1">&#39;square&#39;</span><span class="p">,</span>
    <span class="s1">&#39;t&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;acos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arccos&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atan&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sinc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sinh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cosh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tanhshrink&#39;</span><span class="p">,</span>
    <span class="s1">&#39;asinh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arcsinh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;acosh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atanh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arctanh&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atan2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;round&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_and&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_or&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_xor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_left_shift&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bitwise_right_shift&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inverse&#39;</span><span class="p">,</span>
    <span class="s1">&#39;invert&#39;</span><span class="p">,</span>
    <span class="s1">&#39;erf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;erfc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cdist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ceil&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bernoulli&#39;</span><span class="p">,</span>
    <span class="s1">&#39;heaviside&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hypot&#39;</span><span class="p">,</span>
    <span class="s1">&#39;i0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_j0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_j1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i0e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k0e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_y0&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_y1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_i1e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bessel_k1e&#39;</span><span class="p">,</span>
    <span class="s1">&#39;exp2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;deg2rad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stft&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rad2deg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;truncate_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;truncate_mod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;trunc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gumbel_softmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kaiser_window&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matmul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inner&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cummin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cummax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cumsum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;amin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;amax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;prod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;all&#39;</span><span class="p">,</span>
    <span class="s1">&#39;any&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sparse_segment_mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;block_diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atleast_1d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dstack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diff&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atleast_2d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cartesian_prod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;atleast_3d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;view_as_real&#39;</span><span class="p">,</span>
    <span class="s1">&#39;vstack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;var&#39;</span><span class="p">,</span>
    <span class="s1">&#39;var_mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;std_mean&#39;</span><span class="p">,</span>
    <span class="s1">&#39;combinations&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dist&#39;</span><span class="p">,</span>
    <span class="s1">&#39;copysign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hann_window&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;slogdet&#39;</span><span class="p">,</span>
    <span class="s1">&#39;trace&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xlogy&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log10&#39;</span><span class="p">,</span>
    <span class="s1">&#39;log1p&#39;</span><span class="p">,</span>
    <span class="s1">&#39;approximate_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frac&#39;</span><span class="p">,</span>
    <span class="s1">&#39;kron&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rot90&#39;</span><span class="p">,</span>
    <span class="s1">&#39;remainder&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sgn&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sign&#39;</span><span class="p">,</span>
    <span class="s1">&#39;signbit&#39;</span><span class="p">,</span>
    <span class="s1">&#39;accumulate_n&#39;</span><span class="p">,</span>
    <span class="s1">&#39;iou&#39;</span><span class="p">,</span>
    <span class="s1">&#39;baddbmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
    <span class="s1">&#39;trapz&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cholesky&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cholesky_inverse&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cholesky_solve&#39;</span><span class="p">,</span>
    <span class="s1">&#39;conj&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cosine_similarity&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cov&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cross&#39;</span><span class="p">,</span>
    <span class="s1">&#39;einsum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;erfinv&#39;</span><span class="p">,</span>
    <span class="s1">&#39;less_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cumprod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;greater&#39;</span><span class="p">,</span>
    <span class="s1">&#39;greater_equal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;igamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;igammac&#39;</span><span class="p">,</span>
    <span class="s1">&#39;isinf&#39;</span><span class="p">,</span>
    <span class="s1">&#39;logical_xor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;imag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;roll&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_exp&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_power&#39;</span><span class="p">,</span>
    <span class="s1">&#39;orgqr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ormqr&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diag_embed&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fmin&#39;</span><span class="p">,</span>
    <span class="s1">&#39;inplace_index_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lu_unpack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nanquantile&#39;</span><span class="p">,</span>
    <span class="s1">&#39;polar&#39;</span><span class="p">,</span>
    <span class="s1">&#39;polygamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;quantile&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tril_indices&#39;</span><span class="p">,</span>
    <span class="s1">&#39;histc&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nextafter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;triu_indices&#39;</span><span class="p">,</span>
    <span class="s1">&#39;zeta&#39;</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>