

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.operations.array_ops &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/language_data.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_cpp/mindspore.html">C++ APIâ†—</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.ops.operations.array_ops</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.ops.operations.array_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for array.&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">numbers</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">Zero</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._utils</span> <span class="kn">import</span> <span class="n">get_broadcast_shape</span>
<span class="kn">from</span> <span class="nn">mindspore.common._utils</span> <span class="kn">import</span> <span class="n">is_shape_unknown</span><span class="p">,</span> <span class="n">is_dim_unknown</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">prim_attr_register</span><span class="p">,</span> <span class="n">_run_op</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">,</span> <span class="n">is_pack_tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">CSRTensor</span><span class="p">,</span> <span class="n">COOTensor</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">CSRTensor</span> <span class="k">as</span> <span class="n">CSRTensor_</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">COOTensor</span> <span class="k">as</span> <span class="n">COOTensor_</span>


<span class="k">class</span> <span class="nc">_ScatterOp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines Scatter operators</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_scatter_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">indices_shape</span> <span class="o">!=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">updates_shape</span> <span class="ow">and</span> <span class="n">updates_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;updates_shape = indices_shape + input_x_shape[1:], but got input_x_shape: </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, updates_shape: </span><span class="si">{</span><span class="n">updates_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _ScatterOp&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_scatter_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">updates_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="n">updates_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<span class="k">class</span> <span class="nc">UnravelIndex</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transforms an array consisting of flattened indices into a tuple that contains coordinate arrays.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - The input Tensor, containing indices that will be transformed</span>
<span class="sd">          into the flattened form of an array with dimensions specified by `dims`.</span>
<span class="sd">          The dimension of `indices` must be 0-D or 1-D.</span>
<span class="sd">          Must be one of the following types: int32, int64.</span>
<span class="sd">        - **dims** (Tensor) - The shape of the array to use for unraveling indices.</span>
<span class="sd">          The dimension of `dims` must be 1-D. Must have the same type as `indices`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Tensor, it should be 2-D or 1-D(if `indices` is 0D)</span>
<span class="sd">          and has the same type as `indices`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `indices` and `dims` are different.</span>
<span class="sd">        TypeError: If the data type of `indices` and `dims` is not int32 or int64.</span>
<span class="sd">        ValueError: If the dimension of `dims` is not 1 or dimension of `indices` is not 1 or 0.</span>
<span class="sd">        ValueError: If `indices` contains negative elements.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([2, 5]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dims = Tensor(np.array([3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.UnravelIndex()(indices, dims)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 2]</span>
<span class="sd">         [1 2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Shape&quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">_ScatterOpDynamic</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines Scatter operators with dynamic shape</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_scatter_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
        <span class="c1"># x_shape cannot be dynamic</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the &#39;input_x&#39; does not support dynamic shape, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the shape of &#39;input_x&#39; is </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="c1"># support indices and updates dynamic</span>
        <span class="k">if</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">)</span> <span class="ow">or</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">updates_shape</span><span class="p">):</span>
            <span class="k">pass</span>
        <span class="k">elif</span> <span class="n">indices_shape</span> <span class="o">!=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">updates_shape</span> <span class="ow">and</span> <span class="n">updates_shape</span> <span class="o">!=</span> <span class="n">indices_shape</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;updates_shape = indices_shape + input_x_shape[1:], but got input_x_shape: </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, updates_shape: </span><span class="si">{</span><span class="n">updates_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _ScatterOpDynamic&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_scatter_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">updates_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="n">updates_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ScatterNdOp</span><span class="p">(</span><span class="n">_ScatterOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines _ScatterNd operators</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_check_scatter_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;the dimension of x&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span>
                        <span class="s1">&#39;the dimension of indices&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">indices_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">indices_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:]</span> <span class="o">!=</span> <span class="n">updates_shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, updates_shape = &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape[:-1] + x_shape[indices_shape[-1]:], but got x_shape: </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2">, updates_shape: </span><span class="si">{</span><span class="n">updates_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_check_infer_attr_reduce</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_dims&#39;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Expand</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expands the Tensor along singleton dimensions(dim with size 1) to match given desired shape.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.expand` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">        - **shape** (Tensor) - The new shape of x.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor after expansion, it shape is determined by `shape`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = Tensor(np.array([3,4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; expand = ops.Expand()</span>
<span class="sd">        &gt;&gt;&gt; y = expand(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[1. 1. 1. 1.]</span>
<span class="sd">         [2. 2. 2. 2.]</span>
<span class="sd">         [3. 3. 3. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Expand.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="ExpandDims"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ExpandDims.html#mindspore.ops.ExpandDims">[docs]</a><span class="k">class</span> <span class="nc">ExpandDims</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds an additional dimension to `input_x` at the given axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.expand_dims` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **axis** (int) - Specifies the dimension index at which to expand</span>
<span class="sd">          the shape of `input_x`. The value of axis must be in the range</span>
<span class="sd">          `[-input_x.ndim-1, input_x.ndim]`. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(1, x_1, x_2, ..., x_R)` if the</span>
<span class="sd">        value of `axis` is 0. It has the same data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; expand_dims = ops.ExpandDims()</span>
<span class="sd">        &gt;&gt;&gt; output = expand_dims(input_tensor, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2.]</span>
<span class="sd">          [2. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ExpandDims&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="DType"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DType.html#mindspore.ops.DType">[docs]</a><span class="k">class</span> <span class="nc">DType</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the data type of the input tensor as mindspore.dtype.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        mindspore.dtype, the data type of a tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.DType()(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DType&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">CSRTensor</span><span class="p">,</span> <span class="n">COOTensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">,</span> <span class="n">CSRTensor_</span><span class="p">,</span> <span class="n">COOTensor_</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For Primitive[Dtype], the input argument[input_x] &quot;</span>
                            <span class="s2">&quot;must be a Tensor, CSRTensor or COOTensor, but got &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span></div>


<span class="k">class</span> <span class="nc">CheckNumerics</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks a tensor for NaN and Inf values. A runtime error is raised if input has NaN or Inf values.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor of any dimension. The data type is float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `x` if `x` has no NaN or Inf values.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` data type is not float16, float32, float64.</span>
<span class="sd">        RuntimeError: If `x` has NaN or Inf values.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 3], [2, 4]], dtype=np.float32))</span>
<span class="sd">        &gt;&gt;&gt; checknumerics = ops.CheckNumerics()</span>
<span class="sd">        &gt;&gt;&gt; output = checknumerics(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 3.]</span>
<span class="sd">         [2. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init CheckNumerics&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Cast"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cast.html#mindspore.ops.Cast">[docs]</a><span class="k">class</span> <span class="nc">Cast</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with the new specified data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number]) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          The tensor to be cast.</span>
<span class="sd">        - **type** (dtype.Number) - The valid data type of the output tensor. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is the same as `input_x`, :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither Tensor nor Number.</span>
<span class="sd">        TypeError: If `type` is not a Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_np)</span>
<span class="sd">        &gt;&gt;&gt; type_dst = mindspore.int32</span>
<span class="sd">        &gt;&gt;&gt; cast = ops.Cast()</span>
<span class="sd">        &gt;&gt;&gt; output = cast(input_x, type_dst)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 4, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cast&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;dst_type&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_elim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">):</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtype</span><span class="p">:</span>
                    <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">dtype</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_pack_tensor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">x</span><span class="o">.</span><span class="n">set_cast_dtype</span><span class="p">()</span>
                <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
                <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">src_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">get_py_obj_dtype</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">src_type</span><span class="p">,</span>
                                 <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">,</span> <span class="n">dst_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">src_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">)):</span>
            <span class="n">src_type</span> <span class="o">=</span> <span class="n">src_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dst_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">)):</span>
            <span class="n">dst_type</span> <span class="o">=</span> <span class="n">dst_type</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span>

        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">np_dst_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="n">dst_type</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np_dst_type</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np_dst_type</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">value</span></div>


<div class="viewcode-block" id="Im2Col"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Im2Col.html#mindspore.ops.Im2Col">[docs]</a><span class="k">class</span> <span class="nc">Im2Col</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts sliding local blocks from a batched input tensor.</span>

<span class="sd">    Consider a batched input tensor of shape :math:`(N, C, *)`,</span>
<span class="sd">    where :math:`N` is the batch dimension, :math:`C` is the channel dimension,</span>
<span class="sd">    and :math:`*` represent arbitrary spatial dimensions. This operation flattens</span>
<span class="sd">    each sliding `ksizes`- sized block within the spatial dimensions</span>
<span class="sd">    of input `x` into a column (i.e., last dimension) of a 4-D output</span>
<span class="sd">    tensor of shape :math:`(N, C, \prod(\text{kernel_size}), L)`, where</span>
<span class="sd">    :math:`C \times \prod(\text{kernel_size})` is the total number of values</span>
<span class="sd">    within each block (a block has :math:`\prod(\text{kernel_size})` spatial</span>
<span class="sd">    locations each containing a `C`-channeled vector), and :math:`L` is</span>
<span class="sd">    the total number of such blocks:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \prod_d \left\lfloor\frac{\text{spatial_size}[d] + 2 \times \text{pads}[d] %</span>
<span class="sd">            - \text{dilations}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{strides}[d]} + 1\right\rfloor,</span>

<span class="sd">    where :math:`\text{spatial_size}` is formed by the spatial dimensions</span>
<span class="sd">    of input `x` (:math:`*` above), and :math:`d` is over all spatial</span>
<span class="sd">    dimensions.</span>

<span class="sd">    Therefore, indexing `output` at the last dimension (column dimension)</span>
<span class="sd">    gives all values within a certain block.</span>

<span class="sd">    The `pads`, `strides` and `dilations` arguments specify</span>
<span class="sd">    how the sliding blocks are retrieved.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, only 4-D input tensors (batched image-like tensors) are supported.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksizes (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">        strides (Union[int, tuple[int], list[int]], optional): The stride of the window, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>
<span class="sd">        dilations (Union[int, tuple[int], list[int]], optional): The dilation of the window, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>

<span class="sd">        pads (Union[int, tuple[int], list[int]], optional): The pad of the window, that must be a tuple of</span>
<span class="sd">            one or two `int` for height and width. Default: ``0`` .</span>

<span class="sd">            - If one int, :math:`pad\_height = pad\_width`.</span>
<span class="sd">            - If two int, :math:`pad\_height = pads[0]`, :math:`pad\_width = pads[1]`.</span>
<span class="sd">            - If four int, :math:`pads = [pad\_height\_top, pad\_height\_bottom, pad\_width\_left, pad\_width\_right]`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - input tensor, only 4-D input tensors (batched image-like tensors) are supported.</span>
<span class="sd">          support all real number data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a 4-D Tensor with same type of input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `ksizes` data type is not in Union[int, tuple[int], list[int]].</span>
<span class="sd">        TypeError: If `strides` data type is not in Union[int, tuple[int], list[int]].</span>
<span class="sd">        TypeError: If `dilations` data type is not in Union[int, tuple[int], list[int]].</span>
<span class="sd">        TypeError: If `pads` data type isnot in Union[int, tuple[int], list[int]].</span>
<span class="sd">        ValueError: If `ksizes` value is not greater than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If `strides` value is not greater than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If `dilations` value is not greater than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If `pads` value is not greater than zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(input_data=np.random.rand(4, 4, 32, 32), dtype=mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; im2col = ops.Im2Col(ksizes=3, strides=1, dilations=1)</span>
<span class="sd">        &gt;&gt;&gt; y = im2col(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">        (4, 4, 9, 900)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilations</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Im2Col.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ksizes&#39;</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="n">dilations</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ksizes</span> <span class="o">=</span> <span class="p">(</span><span class="n">ksizes</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ksizes</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">ksizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">strides</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilations</span> <span class="o">=</span> <span class="p">(</span><span class="n">dilations</span><span class="p">,</span> <span class="n">dilations</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilations</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">dilations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="p">(</span><span class="n">pads</span><span class="p">,</span> <span class="n">pads</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pads</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">pads</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;ksizes size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksizes</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksizes</span><span class="p">,</span> <span class="s2">&quot;ksizes&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;strides size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;dilations size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilations</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilations</span><span class="p">,</span> <span class="s2">&quot;dilations&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;pads size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">,</span> <span class="s2">&quot;pads&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksizes&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilations</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="s2">&quot;CALCULATED&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="Col2Im"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Col2Im.html#mindspore.ops.Col2Im">[docs]</a><span class="k">class</span> <span class="nc">Col2Im</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combines an array of sliding local blocks into a large containing tensor. It is</span>
<span class="sd">    usually used to reconstruct an image from a set of image patches(or sliding local blocks).</span>

<span class="sd">    Consider a batched :attr:`input` tensor containing sliding local blocks,</span>
<span class="sd">    e.g., patches of images, of shape :math:`(N, C, \prod(\text{kernel_size}), L)`,</span>
<span class="sd">    where :math:`N` is batch dimension, :math:`C` is channel dimension,</span>
<span class="sd">    :math:`\prod(\text{kernel_size})` is the block size, and</span>
<span class="sd">    :math:`L` is the total number of blocks. This operation combines these</span>
<span class="sd">    local blocks into the large :attr:`output` tensor of</span>
<span class="sd">    shape :math:`(N, C, \text{output_size}[0], \text{output_size}[1], \dots)`</span>
<span class="sd">    by summing the overlapping values.</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \prod_d \left\lfloor\frac{\text{output_size}[d] + 2 \times \text{padding}[d] %</span>
<span class="sd">            - \text{dilation}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor</span>

<span class="sd">    where :math:`d` is over all spatial dimensions. The `padding`, `stride`</span>
<span class="sd">    and `dilation` arguments specify how the sliding blocks are retrieved.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two positive int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">        dilation (Union[int, tuple[int], list[int]], optional): The size of the dilation, should be two positive int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>
<span class="sd">        padding (Union[int, tuple[int], list[int]], optional): The size of the padding, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``0`` .</span>
<span class="sd">        stride (Union[int, tuple[int], list[int]], optional): The size of the stride, should be two positive int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - 4D tensor with data type float16 or float32.</span>
<span class="sd">        - **output_size** (Tensor) - 1D tensor with 2 elements of data type int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a 4-D Tensor with same type of input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `kernel_size` , `dilation` , `padding` or `stride` is not in</span>
<span class="sd">                   Union[int, tuple[int], list[int]].</span>
<span class="sd">        ValueError: If values in `kernel_size` , `dilation` , `padding` or `stride` are not greater than zero or any</span>
<span class="sd">                    one of them has more than 2 elements.</span>
<span class="sd">        ValueError: If x.shape[2] != kernel_size[0] * kernel_size[1].</span>
<span class="sd">        ValueError: If x.shape[3] does not match the calculated number of sliding blocks.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(input_data=np.random.rand(16, 16, 4, 25), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = Tensor(input_data=[8, 8], dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; col2im = ops.Col2Im(kernel_size=[2, 2], dilation=[2, 2], padding=[2, 2], stride=[2, 2])</span>
<span class="sd">        &gt;&gt;&gt; y = col2im(x, output_size)</span>
<span class="sd">        &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">        (16, 16, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Col2Im.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">stride</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;kernel_size size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;dilation size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;padding size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;stride size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span></div>


<div class="viewcode-block" id="Reshape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Reshape.html#mindspore.ops.Reshape">[docs]</a><span class="k">class</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearranges the input Tensor based on the given shape.</span>

<span class="sd">    Refer to :func:`mindspore.ops.reshape` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **input_shape** (tuple[int]) - The input tuple is constructed by multiple</span>
<span class="sd">          integers, i.e., :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; reshape = ops.Reshape()</span>
<span class="sd">        &gt;&gt;&gt; output = reshape(input_x, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Reshape&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;infer value&quot;&quot;&quot;</span>
        <span class="c1"># for shape is not constant</span>
        <span class="k">if</span> <span class="n">shape</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">none_in_tuple_or_list</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="ow">or</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">shape</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">neg_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">dim_prod</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">shp_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">shp_i</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">shp_i</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">neg_index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, there can be at most one &#39;-1&#39; in &#39;input_shape&#39;, &quot;</span>
                                     <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                <span class="n">neg_index</span> <span class="o">=</span> <span class="n">i</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dim_prod</span> <span class="o">*=</span> <span class="n">shp_i</span>
        <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span>
            <span class="n">x_shp</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">if</span> <span class="n">dim_prod</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of &#39;input_x&#39; is </span><span class="si">{</span><span class="n">x_shp</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;the value of &#39;input_shape&#39; is </span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="s2">. &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;The product of &#39;input_shape&#39; should &gt; 0, but got </span><span class="si">{</span><span class="n">dim_prod</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">arr_prod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_shp</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">neg_index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">shape</span><span class="p">[</span><span class="n">neg_index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">arr_prod</span> <span class="o">//</span> <span class="n">dim_prod</span><span class="p">)</span>
                <span class="n">dim_prod</span> <span class="o">*=</span> <span class="n">shape</span><span class="p">[</span><span class="n">neg_index</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">dim_prod</span> <span class="o">!=</span> <span class="n">arr_prod</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the product of the &#39;input_x&#39; shape &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;should be equal to product of &#39;input_shape&#39;, but got product of the&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot; shape of &#39;input_x&#39;: </span><span class="si">{</span><span class="n">arr_prod</span><span class="si">}</span><span class="s2">, product of &#39;input_shape&#39;: </span><span class="si">{</span><span class="n">dim_prod</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">none_in_tuple_or_list</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">in</span> <span class="n">x</span></div>


<div class="viewcode-block" id="Shape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Shape.html#mindspore.ops.Shape">[docs]</a><span class="k">class</span> <span class="nc">Shape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.shape` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[int], the output tuple is constructed by multiple integers,</span>
<span class="sd">        :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = ops.Shape()</span>
<span class="sd">        &gt;&gt;&gt; output = shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (3, 2, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Shape&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">COOTensor</span><span class="p">,</span> <span class="n">CSRTensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For primitive[</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">], the input argument must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="TensorShape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorShape.html#mindspore.ops.TensorShape">[docs]</a><span class="k">class</span> <span class="nc">TensorShape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = ops.TensorShape()</span>
<span class="sd">        &gt;&gt;&gt; output = shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init Shape&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">Unsqueeze</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Unsqueeze&quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>


<span class="k">class</span> <span class="nc">DynamicShape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator TensorShape. DynamicShape will be deprecated in the future.</span>
<span class="sd">    Please use TensorShape instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.7&quot;</span><span class="p">,</span> <span class="s2">&quot;TensorShape&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init Shape&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;tensor&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;is_dynamic_shape&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="Squeeze"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Squeeze.html#mindspore.ops.Squeeze">[docs]</a><span class="k">class</span> <span class="nc">Squeeze</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the Tensor after deleting the dimension of size 1 in the specified `axis`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.squeeze` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple(int)]): Specifies the dimension indexes of shape to be removed, which will remove</span>
<span class="sd">            all the dimensions of size 1 in the given axis parameter. If specified, it must be int32 or int64.</span>
<span class="sd">            Default: ``()`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(x_1, x_2, ..., x_S)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; squeeze = ops.Squeeze(2)</span>
<span class="sd">        &gt;&gt;&gt; output = squeeze(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Squeeze&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">idx</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="p">(</span><span class="n">axis</span><span class="p">,)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span></div>


<div class="viewcode-block" id="Transpose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Transpose.html#mindspore.ops.Transpose">[docs]</a><span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input permutation.</span>

<span class="sd">    Refer to :func:`mindspore.ops.transpose` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **input_perm** (tuple[int]) - The permutation to be converted. The elements in `input_perm` are composed of</span>
<span class="sd">          the indexes of each dimension of `input_x`. The length of `input_perm` and the shape of `input_x` must be</span>
<span class="sd">          the same. Only constant value is allowed. Must be in the range [0, rank(input_x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the type of output tensor is the same as `input_x` and the shape of output tensor is decided by the</span>
<span class="sd">        shape of `input_x` and the value of `input_perm`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; transpose = ops.Transpose()</span>
<span class="sd">        &gt;&gt;&gt; output = transpose(input_x, input_perm)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">          [ 2.  5.]</span>
<span class="sd">          [ 3.  6.]]</span>
<span class="sd">         [[ 7. 10.]</span>
<span class="sd">          [ 8. 11.]</span>
<span class="sd">          [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Transpose&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;perm&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">ConjugateTranspose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the conjugate matrix of input x which has been transposed according to input perm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[i,j,k,...,s,t,u] == conj(x[perm[i], perm[j], perm[k],...,perm[s], perm[t], perm[u]])</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **perm** (tuple[int]) - The permutation to be converted. The elements in `perm` are composed of</span>
<span class="sd">          the indexes of each dimension of `x`. The length of `perm` and the shape of `x` must be</span>
<span class="sd">          the same. Only constant value is allowed. Must be in the range [0, rank(x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the type of output tensor is the same as `x` and the shape of output tensor is decided by the</span>
<span class="sd">        shape of `x` and the value of `Conj(perm)`:</span>

<span class="sd">        .. math::</span>
<span class="sd">            y.shape[i] = x.shape[perm[i]]</span>

<span class="sd">        where i is in range [0, rank(x) - 1].</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `perm` is not a tuple.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to length of shape of `perm`.</span>
<span class="sd">        ValueError: If the same element exists in `perm`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1 + 1j,2 + 2j], [3 + 3j, 4 + 4j]]), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; perm = (1, 0)</span>
<span class="sd">        &gt;&gt;&gt; conjugate_transpose = ops.ConjugateTranspose()</span>
<span class="sd">        &gt;&gt;&gt; output = conjugate_transpose(x, perm)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[1.-1.j 3.-3.j]</span>
<span class="sd">            [2.-2.j 4.-4.j]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ConjugateTranspose&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;perm&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Unique"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Unique.html#mindspore.ops.Unique">[docs]</a><span class="k">class</span> <span class="nc">Unique</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the unique elements of input tensor and also return a tensor containing the index of each value of input</span>
<span class="sd">    tensor corresponding to the output unique tensor.</span>

<span class="sd">    The output contains Tensor `y` and Tensor `idx`, the format is probably similar to (`y`, `idx`).</span>
<span class="sd">    The shape of Tensor `y` and Tensor `idx` is different in most cases, because Tensor `y` will be duplicated,</span>
<span class="sd">    and the shape of Tensor `idx` is consistent with the input.</span>

<span class="sd">    To get the same shape between `idx` and `y`, please refer to :class:`mindspore.ops.UniqueWithPad`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, containing Tensor objects (`y`, `idx`), `y` is a tensor with the</span>
<span class="sd">        same type as `input_x`, and contains the unique elements in `x`.</span>
<span class="sd">        `idx` is a tensor containing indices of elements in</span>
<span class="sd">        the input corresponding to the output tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 5, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Unique()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int32, value= [1, 2, 5]), Tensor(shape=[4], dtype=Int32, value= [0, 1, 2, 1]))</span>
<span class="sd">        &gt;&gt;&gt; y = output[0]</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1 2 5]</span>
<span class="sd">        &gt;&gt;&gt; idx = output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 1 2 1]</span>
<span class="sd">        &gt;&gt;&gt; # As can be seen from the above, y and idx shape</span>
<span class="sd">        &gt;&gt;&gt; # note that for GPU, this operator must be wrapped inside a model, and executed in graph mode.</span>
<span class="sd">        &gt;&gt;&gt; class UniqueNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(UniqueNet, self).__init__()</span>
<span class="sd">        ...         self.unique_op = ops.Unique()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         output, indices = self.unique_op(x)</span>
<span class="sd">        ...         return output, indices</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 5, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = UniqueNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int32, value= [1, 2, 5]), Tensor(shape=[4], dtype=Int32, value= [0, 1, 2, 1]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="UniqueConsecutive"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UniqueConsecutive.html#mindspore.ops.UniqueConsecutive">[docs]</a><span class="k">class</span> <span class="nc">UniqueConsecutive</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the elements that are unique in each consecutive group of equivalent elements in the input tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.unique_consecutive` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        return_idx (bool, optional): Whether to return the index of where the element in the original input</span>
<span class="sd">            maps to the position in the output. Default: ``False`` .</span>
<span class="sd">        return_counts (bool, optional): Whether to return the counts of each unique element. Default: ``False`` .</span>
<span class="sd">        axis (int, optional): The dimension to apply unique. If ``None`` , the unique of the flattened input is</span>
<span class="sd">            returned. If specified, it must be int32 or int64. Default: ``None`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tensor or a tuple of tensors containing tensor objects (`output`, `idx`, `counts`).</span>

<span class="sd">        - `output` has the same type as `x` and is used to represent the output list of unique scalar elements.</span>
<span class="sd">        - If `return_idx` is True, there will be an additional returned tensor, `idx`,</span>
<span class="sd">          which has the same shape as `x` and represents</span>
<span class="sd">          the index of where the element in the original input maps to the position in the output.</span>
<span class="sd">        - If `return_counts` is True, there will be an additional returned tensor, `counts`,</span>
<span class="sd">          which represents the number of occurrences for each unique value or tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 1, 2, 2, 3, 1, 1, 2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; unique_consecutive = ops.UniqueConsecutive(True, True, None)</span>
<span class="sd">        &gt;&gt;&gt; output, idx, counts = unique_consecutive(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3 1 2]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 0 1 1 2 3 3 4]</span>
<span class="sd">        &gt;&gt;&gt; print(counts)</span>
<span class="sd">        [2 2 1 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_idx</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UniqueConsecutive&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;return_idx&quot;</span><span class="p">,</span> <span class="n">return_idx</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;return_counts&quot;</span><span class="p">,</span> <span class="n">return_counts</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;return_idx&quot;</span><span class="p">,</span> <span class="n">return_idx</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;return_counts&quot;</span><span class="p">,</span> <span class="n">return_counts</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="Gather"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Gather.html#mindspore.ops.Gather">[docs]</a><span class="k">class</span> <span class="nc">Gather</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_dims (int, optional): Specifies the number of batch dimensions.</span>
<span class="sd">            It must be less than or euqal to the rank of `input_indices`. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_params** (Tensor) - The original Tensor. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **input_indices** (Tensor) - Index tensor to be sliced, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">          Specifies the indices of elements of the original Tensor. The data type can be int32 or int64.</span>
<span class="sd">        - **axis** (int) - Specifies the dimension index to gather indices.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is</span>
<span class="sd">        :math:`input\_params.shape[:axis] + input\_indices.shape + input\_params.shape[axis + 1:]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case1: input_indices is a Tensor with shape (5, ).</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([1, 2, 3, 4, 5, 6, 7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 4, 2, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Gather()(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 3. 5. 3. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,</span>
<span class="sd">        the output shape is equal to the input_indices shape.</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([[0, 2], [2, 6]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Gather()(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1. 3.]</span>
<span class="sd">         [ 3. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; # case3: input_indices is a Tensor with shape (2, ). input_params is a Tensor with shape (3, 4) and axis is 0.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Gather()(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  2.  3.  4.]</span>
<span class="sd">         [9. 10. 11. 12.]]</span>
<span class="sd">        &gt;&gt;&gt; # case4: input_indices is a Tensor with shape (2, ).</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 1, batch_dims is 1.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; batch_dims = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Gather(batch_dims)(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  7. 10.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Gather&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;batch_dims&quot;</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;batch_dims&quot;</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">GatherV2</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator Gather. GatherV2 will be deprecated in the future.</span>
<span class="sd">    Please use Gather instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;Gather&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GatherV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;batch_dims&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__check__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">axis_v</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">axis_v</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="SparseGatherV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseGatherV2.html#mindspore.ops.SparseGatherV2">[docs]</a><span class="k">class</span> <span class="nc">SparseGatherV2</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a slice of input tensor based on the specified indices and axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_params** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **input_indices** (Tensor) - The shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">          Specifies the indices of elements of the original Tensor, must be in the range</span>
<span class="sd">          `[0, input_params.shape[axis])`.</span>
<span class="sd">        - **axis** (int) - Specifies the dimension index to gather indices.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(z_1, z_2, ..., z_N)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 7, 42], [3, 4, 54, 22], [2, 2, 55, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; out = ops.SparseGatherV2()(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[2. 7.]</span>
<span class="sd">         [4. 54.]</span>
<span class="sd">         [2. 55.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseGatherV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;bprop_return_sparse&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__check__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">axis_v</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">axis_v</span><span class="p">,</span> <span class="o">-</span><span class="n">rank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Padding"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Padding.html#mindspore.ops.Padding">[docs]</a><span class="k">class</span> <span class="nc">Padding</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extends the last dimension of the input tensor from 1 to pad_dim_size, by filling with 0.</span>

<span class="sd">    Refer to :func:`mindspore.ops.padding` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_dim_size (int, optional): The value of the last dimension of `x` to be</span>
<span class="sd">            extended, which must be positive. Default: ``8`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor of 2D or higher-dimensional.</span>
<span class="sd">          The last dimension of `x` must be 1. The data type is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the padded Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8], [10]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_dim_size = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Padding(pad_dim_size)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 8.  0.  0.  0.]</span>
<span class="sd">         [10.  0.  0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_dim_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize padding&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pad_dim_size&quot;</span><span class="p">,</span> <span class="n">pad_dim_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">pad_dim_size</span><span class="p">,</span> <span class="s2">&quot;pad_dim_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_dim_size</span> <span class="o">=</span> <span class="n">pad_dim_size</span></div>


<div class="viewcode-block" id="UniqueWithPad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UniqueWithPad.html#mindspore.ops.UniqueWithPad">[docs]</a><span class="k">class</span> <span class="nc">UniqueWithPad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns unique elements and relative indexes in 1-D tensor, filled with padding num.</span>

<span class="sd">    The basic function is the same as the Unique operator, but the UniqueWithPad operator adds a Pad function.</span>
<span class="sd">    The returned tuple(`y`, `idx`) after the input Tensor `x` is processed by the unique operator,</span>
<span class="sd">    in which the shapes of `y` and `idx` are mostly not equal. Therefore, in order to solve the above situation,</span>
<span class="sd">    the UniqueWithPad operator will fill the `y` Tensor with the `pad_num` specified by the user</span>
<span class="sd">    to make it have the same shape as the Tensor `idx`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.unique_with_pad` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The tensor need to be unique. Must be 1-D vector with types: int32, int64.</span>
<span class="sd">        - **pad_num** (int) - Pad num. The data type is an int.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple(Tensor), tuple of 2 tensors, `y` and `idx`.</span>

<span class="sd">        - y (Tensor) - The unique elements filled with pad_num, the shape and data type same as `x`.</span>
<span class="sd">        - idx (Tensor) - The index of each value of `x` in the unique output `y`, the shape and data type same as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 1, 2, 2, 3, 3, 4, 5]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; pad_num = 8</span>
<span class="sd">        &gt;&gt;&gt; output = ops.UniqueWithPad()(x, pad_num)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[8], dtype=Int32, value= [1, 2, 3, 4, 5, 8, 8, 8]),</span>
<span class="sd">         Tensor(shape=[8], dtype=Int32, value= [0, 0, 1, 1, 2, 2, 3, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;init UniqueWithPad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;pad_num&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;idx&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Split"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Split.html#mindspore.ops.Split">[docs]</a><span class="k">class</span> <span class="nc">Split</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor into output_num of tensors along the given axis and output numbers.</span>

<span class="sd">    Refer to :func:`mindspore.ops.split` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Index of the split position. Default: ``0`` .</span>
<span class="sd">        output_num (int): The number of output tensors. Must be positive int. Default: ``1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the shape of each output tensor is the same, which is</span>
<span class="sd">        :math:`(y_1, y_2, ..., y_S)`. And the data type is the same with `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; split = ops.Split(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[1 1 1 1]</span>
<span class="sd">         [2 2 2 2]]</span>
<span class="sd">        &gt;&gt;&gt; output = split(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">         [2, 2]]), Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">         [2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; split = ops.Split(1, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = split(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Split&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">output_num</span><span class="p">,</span> <span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_num</span> <span class="o">=</span> <span class="n">output_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;num_split&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_num</span><span class="p">)</span></div>


<div class="viewcode-block" id="Rank"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Rank.html#mindspore.ops.Rank">[docs]</a><span class="k">class</span> <span class="nc">Rank</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the rank of a tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.rank` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rank = ops.Rank()</span>
<span class="sd">        &gt;&gt;&gt; output = rank(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;int&#39;&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Rank&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;the input x must be Tensor!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="Size"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Size.html#mindspore.ops.Size">[docs]</a><span class="k">class</span> <span class="nc">Size</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Scalar of type int that represents the size of the input Tensor and the total number of elements in the</span>
<span class="sd">    Tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.size` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input parameters, the shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/master/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        int. A scalar representing the elements&#39; size of `input_x`, tensor is the number of elements</span>
<span class="sd">        in a tensor, :math:`size=x_1*x_2*...x_R`. The data type is an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = ops.Size()</span>
<span class="sd">        &gt;&gt;&gt; output = size(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        4</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Size&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="MatrixDiagV3"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatrixDiagV3.html#mindspore.ops.MatrixDiagV3">[docs]</a><span class="k">class</span> <span class="nc">MatrixDiagV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a diagonal matrix or a batch of diagonal matrices from a given input Tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.matrix_diag` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        align (str, optional): specifies how superdiagonals and subdiagonals should be aligned.</span>
<span class="sd">            Supported values: ``&quot;RIGHT_LEFT&quot;`` , ``&quot;LEFT_RIGHT&quot;`` , ``&quot;LEFT_LEFT&quot;`` , ``&quot;RIGHT_RIGHT&quot;`` .</span>
<span class="sd">            Default: ``&quot;RIGHT_LEFT&quot;`` .</span>

<span class="sd">            - When set to ``&quot;RIGHT_LEFT&quot;`` , the alignment of superdiagonals will be towards the right side</span>
<span class="sd">              (padding the row on the left), while subdiagonals will be towards the left side</span>
<span class="sd">              (padding the row on the right)</span>
<span class="sd">            - When set to ``&quot;LEFT_RIGHT&quot;`` , the alignment of superdiagonals will be towards the left side</span>
<span class="sd">              (padding the row on the right), while subdiagonals will be towards the right side</span>
<span class="sd">              (padding the row on the left)</span>
<span class="sd">            - When set to ``&quot;LEFT_LEFT&quot;`` , the alignment of  both superdiagonals and subdiagonals will be towards</span>
<span class="sd">              the left side(padding the row on the right).</span>
<span class="sd">            - When set to ``&quot;RIGHT_RIGHT&quot;`` , the alignment of both superdiagonals and subdiagonals will be towards</span>
<span class="sd">              the right side(padding the row on the left).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The diagonal Tensor.</span>
<span class="sd">        - **k** (Union[int, Tensor], optional) - Diagonal offsets.</span>
<span class="sd">          A Tensor of type int32. Positive value means superdiagonal,</span>
<span class="sd">          0 refers to the main diagonal, and negative value means subdiagonals. `k` can be a single integer</span>
<span class="sd">          (for a single diagonal) or a pair of integers specifying the low and high ends of a matrix band.</span>
<span class="sd">          k[0] must not be larger than k[1]. The value must be in the range of given or derivated `num_rows`</span>
<span class="sd">          and `num_cols`, meaning value of k must be in (-num_rows, num_cols). Default: ``0`` .</span>
<span class="sd">        - **num_rows** (Union[int, Tensor], optional) - The number of rows of the output Tensor.</span>
<span class="sd">          A Tensor of type int32 with only one value. If `num_rows` is -1, indicating that the innermost</span>
<span class="sd">          matrix of the output Tensor is a square</span>
<span class="sd">          matrix, and the real number of rows will be derivated by other inputs. That is</span>
<span class="sd">          :math:`num\_rows = x.shape[-1] - min(k[1], 0)`. Otherwise, the value must be equal or greater than</span>
<span class="sd">          :math:`x.shape[-1] - min(k[1], 0)`. Default: -1.</span>
<span class="sd">        - **num_cols** (Union[int, Tensor], optional) - The number of columns of</span>
<span class="sd">          the output Tensor. A Tensor of type int32 with only one value.</span>
<span class="sd">          If `num_cols` is -1, indicating that the innermost matrix of the output</span>
<span class="sd">          Tensor is a square matrix, and the real number of columns will be derivated by other inputs.</span>
<span class="sd">          That is :math:`num\_cols = x.shape[-1] + max(k[0], 0)`. Otherwise, the value must be equal or</span>
<span class="sd">          greater than :math:`x.shape[-1] - min(k[1], 0)`.  Default: -1.</span>
<span class="sd">        - **padding_value** (Union[int, float, Tensor], optional) - The number to fill the area outside the specified</span>
<span class="sd">          diagonal band. A Tensor with only one value. Have the same dtype as x. Default: ``0`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor. Has the same type as `x`.</span>
<span class="sd">        Suppose `x` has r dimensions with shape :math:`(I, J, ..., M, N)` . The output Tensor has rank r + 1 with shape</span>
<span class="sd">        :math:`(I, J, ..., M, num\_rows, num\_cols)` when only one diagonal is given (k is an integer or k[0] == k[1]).</span>
<span class="sd">        Otherwise, it has rank r with shape :math:`(I, J, ..., num\_rows, num\_cols)` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8, 9, 0],</span>
<span class="sd">        ...                      [1, 2, 3],</span>
<span class="sd">        ...                      [0, 4, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([-1, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_rows = Tensor(np.array(3), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_cols = Tensor(np.array(3), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; padding_value = Tensor(np.array(11), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_diag_v3 = ops.MatrixDiagV3(align=&#39;LEFT_RIGHT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_diag_v3(x, k, num_rows, num_cols, padding_value)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  8. 11.]</span>
<span class="sd">         [ 4.  2.  9.]</span>
<span class="sd">         [11.  5.  3.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Initialize MatrixDiagV3&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align&quot;</span><span class="p">,</span> <span class="n">align</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">align</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LEFT_RIGHT&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT_LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;LEFT_LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT_RIGHT&#39;</span><span class="p">],</span> <span class="s1">&#39;align&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="s1">&#39;num_rows&#39;</span><span class="p">,</span> <span class="s1">&#39;num_cols&#39;</span><span class="p">,</span> <span class="s1">&#39;padding_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MatrixDiagPartV3"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatrixDiagPartV3.html#mindspore.ops.MatrixDiagPartV3">[docs]</a><span class="k">class</span> <span class="nc">MatrixDiagPartV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the diagonal part of a tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.matrix_diag_part` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        align (str, optional): An optional string from: &quot;RIGHT_LEFT&quot;(default),</span>
<span class="sd">            &quot;LEFT_RIGHT&quot;, &quot;LEFT_LEFT&quot;, &quot;RIGHT_RIGHT&quot;.</span>
<span class="sd">            It specifies how superdiagonals and subdiagonals should be aligned, respectively. &quot;RIGHT_LEFT&quot;</span>
<span class="sd">            aligns superdiagonals to the right (left-pads the row) and subdiagonals to the left (right-pads the row).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Rank r, where r &gt;= 2.</span>
<span class="sd">        - **k** (Tensor) - A Tensor of type int32. Diagonal offset(s). Positive value means superdiagonal, 0 refers to</span>
<span class="sd">          the main diagonal, and negative value means subdiagonals. k can be a single integer (for a single diagonal) or</span>
<span class="sd">          a pair of integers specifying the low and high ends of a matrix band. k[0] must not be larger than k[1]. The</span>
<span class="sd">          value of k has restructions, meaning value of k must be in (-x.shape[-2], x.shape[-1]).</span>
<span class="sd">        - **padding_value** (Tensor) - A Tensor. Have the same dtype as x. The number to fill the area outside the</span>
<span class="sd">          specified diagonal band with. There must be only one value.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor. Has the same type as `x`.</span>
<span class="sd">        Assume `x` has r dimensions :math:`(I, J, ..., M, N)` . Let `max_diag_len` be the maximum length among all</span>
<span class="sd">        diagonals to be extracted, :math:`max\_diag\_len = min(M + min(k[1], 0), N + min(-k[0], 0))`</span>
<span class="sd">        Let `num_diags` be the number of diagonals to extract, :math:`num\_diags = k[1] - k[0] + 1`.</span>
<span class="sd">        If :math:`num\_diags == 1`, the output tensor is of rank r - 1 with shape :math:`(I, J, ..., L, max\_diag\_len)`</span>
<span class="sd">        Otherwise, the output tensor has rank r with dimensions :math:`(I, J, ..., L, num\_diags, max\_diag\_len)` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3, 4],</span>
<span class="sd">        ...                      [5, 6, 7, 8],</span>
<span class="sd">        ...                      [9, 8, 7, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; padding_value = Tensor(np.array(9), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_diag_part_v3 = ops.MatrixDiagPartV3(align=&#39;RIGHT_LEFT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_diag_part_v3(x, k, padding_value)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[9. 9. 4.]</span>
<span class="sd">         [9. 3. 8.]</span>
<span class="sd">         [2. 7. 6.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Initialize MatrixDiagPartV3&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">200000000</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align&quot;</span><span class="p">,</span> <span class="n">align</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">align</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LEFT_RIGHT&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT_LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;LEFT_LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT_RIGHT&#39;</span><span class="p">],</span> <span class="s1">&#39;align&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="s1">&#39;padding_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MatrixSetDiagV3"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatrixSetDiagV3.html#mindspore.ops.MatrixSetDiagV3">[docs]</a><span class="k">class</span> <span class="nc">MatrixSetDiagV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the diagonal part of a batched tensor.</span>
<span class="sd">    It takes an Tensor `x` and `diagonal` as input and returns a Tensor in which</span>
<span class="sd">    the specified diagonal values in the innermost matrices will be replaced</span>
<span class="sd">    by the values in the `diagonal`.</span>

<span class="sd">    Diagonals shorter than `max_diag_len` need to be padded, where `max_diag_len` is the</span>
<span class="sd">    longest diagonal value.</span>
<span class="sd">    The dimension of `diagonal` is :math:`shape[-2]` must be equal to num_diags calculated by</span>
<span class="sd">    :math:`num\_diags = k[1] - k[0] + 1`.</span>
<span class="sd">    The dimension of `diagonal` is :math:`shape[-1]` must be equal to the longest diagonal value `max_diag_len`</span>
<span class="sd">    calculated by :math:`max\_diag\_len = min(x.shape[-2] + min(k[1], 0), x.shape[-1] + min(-k[0], 0))`.</span>

<span class="sd">    Assume `x` is an n-D Tensor with shape :math:`(d_1, d_2, ..., d_{n-2}, d_{n-1}, d_n)`.</span>
<span class="sd">    If `k` is an integer or :math:`k[0] == k[1]`, `diagonal` is an (n-1)-D Tensor with</span>
<span class="sd">    shape :math:`(d_1, d_2, ..., d_{n-2}, max\_diag\_len)`</span>
<span class="sd">    Otherwise, it has the same rank as `x`</span>
<span class="sd">    with shape :math:`(d_1, d_2, ..., d_{n-2}, num\_diags, max\_diag\_len)`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        align (str, optional): specifies how superdiagonals and subdiagonals should be aligned.</span>
<span class="sd">            Supported values: ``&quot;RIGHT_LEFT&quot;`` , ``&quot;LEFT_RIGHT&quot;``, ``&quot;LEFT_LEFT&quot;`` , ``&quot;RIGHT_RIGHT&quot;`` .</span>
<span class="sd">            Default: ``&quot;RIGHT_LEFT&quot;`` .</span>

<span class="sd">            - When set to ``&quot;RIGHT_LEFT&quot;`` , the alignment of superdiagonals will be towards the right side</span>
<span class="sd">              (padding the row on the left), while subdiagonals will be towards the left side</span>
<span class="sd">              (padding the row on the right)</span>
<span class="sd">            - When set to ``&quot;LEFT_RIGHT&quot;`` , the alignment of superdiagonals will be towards the left side</span>
<span class="sd">              (padding the row on the right), while subdiagonals will be towards the right side</span>
<span class="sd">              (padding the row on the left)</span>
<span class="sd">            - When set to ``&quot;LEFT_LEFT&quot;`` , the alignment of  both superdiagonals and subdiagonals will be towards</span>
<span class="sd">              the left side(padding the row on the right).</span>
<span class="sd">            - When set to ``&quot;RIGHT_RIGHT&quot;`` , the alignment of both superdiagonals and subdiagonals will be towards</span>
<span class="sd">              the right side(padding the row on the left).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A n-D Tensor, where :math:`n &gt;= 2`.</span>
<span class="sd">        - **diagonal** (Tensor) - A Tensor with the same dtype as `x`. Its rank depends on `k`.</span>
<span class="sd">          If `k` is an integer or :math:`k[0] == k[1]`, its dimension is :math:`n-1`.</span>
<span class="sd">          Otherwise, it has dimension :math:`n`.</span>
<span class="sd">        - **k** (Tensor) - Diagonal offset(s), Tensor of type int32.</span>
<span class="sd">          `k` can either be a single integer, which represents a single diagonal,</span>
<span class="sd">          or a pair of integers that specify the low and high ends of a matrix band.</span>
<span class="sd">          In this case, `k[0]` should not be greater than `k[1]`.</span>
<span class="sd">          The value of `k` has restructions, which means that value of `k` must be in range</span>
<span class="sd">          :math:`(-x.shape[-2], x.shape[-1])`.</span>
<span class="sd">          Input `k` must be const Tensor when taking Graph mode.</span>

<span class="sd">          - `k &gt; 0` refers to a superdiagonal.</span>
<span class="sd">          - `k = 0` refers to the main diagonal.</span>
<span class="sd">          - `k &lt; 0` refers to subdiagonals.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor. The same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If any input is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `diagonal` are not the same dtype.</span>
<span class="sd">        TypeError: If `k` is not int32 dtype.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid range.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `x` is not greater equal to 2.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If `k[1]` is not greater equal to `k[0]` in case the size of `k` is 2.</span>
<span class="sd">        ValueError: If the `diagonal` rank size don&#39;t match with input `x` rank size.</span>
<span class="sd">        ValueError: If the `diagonal` shape value don&#39;t match with input `x` shape value.</span>
<span class="sd">        ValueError: If the diagonal :math:`shape[-2]` is not equal to num_diags calculated by</span>
<span class="sd">            :math:`num\_diags = k[1] - k[0] + 1` .</span>
<span class="sd">        ValueError: If the value of `k` is not in :math:`(-x.shape[-2], x.shape[-1])`.</span>
<span class="sd">        ValueError: If the diagonal :math:`shape[-1]` is not equal to the max_diag_len calculated by</span>
<span class="sd">            :math:`max\_diag\_len = min(x.shape[-2] + min(k[1], 0), x.shape[-1] + min(-k[0], 0))` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[7, 7, 7, 7],</span>
<span class="sd">        ...                      [7, 7, 7, 7],</span>
<span class="sd">        ...                      [7, 7, 7, 7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; diagonal = Tensor(np.array([[0, 9, 1],</span>
<span class="sd">        ...                             [6, 5, 8],</span>
<span class="sd">        ...                             [1, 2, 3],</span>
<span class="sd">        ...                             [4, 5, 0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([-1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; matrix_set_diag_v3 = ops.MatrixSetDiagV3(align=&#39;RIGHT_LEFT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_set_diag_v3(x, diagonal, k)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 6. 9. 7.]</span>
<span class="sd">         [4. 2. 5. 1.]</span>
<span class="sd">         [7. 5. 3. 8.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;diagonal&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Initialize MatrixSetDiagV3&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">200000000</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align&quot;</span><span class="p">,</span> <span class="n">align</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">align</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LEFT_RIGHT&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT_LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;LEFT_LEFT&#39;</span><span class="p">,</span> <span class="s1">&#39;RIGHT_RIGHT&#39;</span><span class="p">],</span> <span class="s1">&#39;align&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;diagonal&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MatrixBandPart"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MatrixBandPart.html#mindspore.ops.MatrixBandPart">[docs]</a><span class="k">class</span> <span class="nc">MatrixBandPart</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts the central diagonal band of each matrix in a tensor, with all values outside</span>
<span class="sd">    the central band set to zero.</span>

<span class="sd">    Refer to :func:`mindspore.ops.matrix_band_part` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor. :math:`(*, m, n)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">          The data type must be float16, float32, float64, int32 or int64.</span>
<span class="sd">        - **lower** (Union[int, Tensor]) - Number of subdiagonals to keep. The data type must be int32 or int64.</span>
<span class="sd">          If negative, keep entire lower triangle.</span>
<span class="sd">        - **upper** (Union[int, Tensor]) - Number of superdiagonals to keep. The data type must be int32 or int64.</span>
<span class="sd">          If negative, keep entire upper triangle.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; matrix_band_part = ops.MatrixBandPart()</span>
<span class="sd">        &gt;&gt;&gt; x = np.ones([2, 4, 4]).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = matrix_band_part(Tensor(x), 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 0. 0.]</span>
<span class="sd">          [1. 1. 1. 0.]</span>
<span class="sd">          [1. 1. 1. 1.]</span>
<span class="sd">          [0. 1. 1. 1.]]</span>
<span class="sd">         [[1. 1. 0. 0.]</span>
<span class="sd">          [1. 1. 1. 0.]</span>
<span class="sd">          [1. 1. 1. 1.]</span>
<span class="sd">          [0. 1. 1. 1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;MatrixBandPart&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="s1">&#39;upper&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Fill.html#mindspore.ops.Fill">[docs]</a><span class="k">class</span> <span class="nc">Fill</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Fill interface is deprecated, please use the :class:`mindspore.ops.FillV2` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Fill&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">all_types</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, the supported data type is [&#39;bool&#39;, &#39;int8&#39;, &#39;int16&#39;, &#39;int32&#39;, &#39;int64&#39;, &#39;uint8&#39;, &quot;</span>
                <span class="s2">&quot;&#39;uint16&#39;, &#39;uint32&#39;, &#39;uint64&#39;,&#39;float16&#39;, &#39;float32&#39;, &#39;float64&#39;], but got an invalid dtype!.&quot;</span><span class="p">)</span>
        <span class="n">x_nptype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, input[1] must be tensor.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, the value input only takes scalar or scalar within a tensor!.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">dims</span> <span class="o">=</span> <span class="n">dims</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_nptype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_nptype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dims</span> <span class="ow">and</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">dims</span> <span class="o">=</span> <span class="n">dims</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_nptype</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<span class="k">class</span> <span class="nc">Fills</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The `Fills` primitive  is deprecated.</span>
<span class="sd">    Please use :func:`mindspore.ops.fill` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2,2)).astype(&#39;float32&#39;))</span>
<span class="sd">        &gt;&gt;&gt; fills = ops.Fills()</span>
<span class="sd">        &gt;&gt;&gt; output = fills(a, float(1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Fills.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="FillV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FillV2.html#mindspore.ops.FillV2">[docs]</a><span class="k">class</span> <span class="nc">FillV2</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with shape described by `shape` and fills it with values in `value` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (Union[Tuple[int], Tensor[int]]) - 1-D Tensor or Tuple, specify the shape</span>
<span class="sd">          of output tensor. Its dtype must be int32 or int64.</span>
<span class="sd">        - **value** (Tensor) - A 0-D Tensor, the value to fill the output tensor `y` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor, its shape and value are described above.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a 1-D tensor or tuple.</span>
<span class="sd">        TypeError: If the data type of `shape` is not int32 or int64.</span>
<span class="sd">        ValueError: If `value` is not a 0-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; fillV2 = ops.FillV2()</span>
<span class="sd">        &gt;&gt;&gt; output = fillV2(Tensor([2, 3], mindspore.int32), Tensor(1, mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1. 1.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = fillV2(Tensor([3, 3], mindspore.int64), Tensor(0, mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">         [0 0 0]</span>
<span class="sd">         [0 0 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FillV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">dims</span> <span class="o">=</span> <span class="n">dims</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">dims</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dims</span> <span class="ow">and</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Ones"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Ones.html#mindspore.ops.Ones">[docs]</a><span class="k">class</span> <span class="nc">Ones</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor filled with value ones.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ones` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (Union[tuple[int], int]) - The specified shape of output tensor.</span>
<span class="sd">        - **type** (:class:`mindspore.dtype`) - The specified type of output tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; ones = ops.Ones()</span>
<span class="sd">        &gt;&gt;&gt; output = ones((2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ones((3, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1. 1.]</span>
<span class="sd">         [1. 1. 1.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Ones&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="Zeros"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Zeros.html#mindspore.ops.Zeros">[docs]</a><span class="k">class</span> <span class="nc">Zeros</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Zeros will be deprecated in the future. Please use class `mindspore.ops.zeros` instead.</span>

<span class="sd">    Creates a tensor filled with value zeros.</span>

<span class="sd">    Creates a tensor with shape described by the first argument and</span>
<span class="sd">    fills it with value zeros in type of the second argument.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **shape** (Union[tuple[int], int]) - The specified shape of output tensor.</span>
<span class="sd">        - **type** (mindspore.dtype) - The specified type of output tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is neither int nor tuple.</span>
<span class="sd">        TypeError: If `shape` is a tuple whose elements are not all int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; zeros = ops.Zeros()</span>
<span class="sd">        &gt;&gt;&gt; output = zeros((2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0.]</span>
<span class="sd">         [0. 0.]]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Zeros&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="OnesLike"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.OnesLike.html#mindspore.ops.OnesLike">[docs]</a><span class="k">class</span> <span class="nc">OnesLike</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 1 and its shape and data type is the same as the input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ones_like` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x` but filled with ones.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; oneslike = ops.OnesLike()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = oneslike(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [1 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize OnesLike&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ZerosLike"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ZerosLike.html#mindspore.ops.ZerosLike">[docs]</a><span class="k">class</span> <span class="nc">ZerosLike</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 0 and its shape and data type is the same as the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input Tensor of any dimension. The data type is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x` but filled with zeros.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; zeroslike = ops.ZerosLike()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = zeroslike(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0.]</span>
<span class="sd">         [0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ZerosLike&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TupleToArray"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TupleToArray.html#mindspore.ops.TupleToArray">[docs]</a><span class="k">class</span> <span class="nc">TupleToArray</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a tuple to a tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tuple_to_array` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (tuple) - A tuple of numbers. These numbers have the same type.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means,any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if the input tuple contains `N` numbers, then the shape of the output tensor is :math:`(N,)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = (1,2,3)</span>
<span class="sd">        &gt;&gt;&gt; print(type(input_x))</span>
<span class="sd">        &lt;class &#39;tuple&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.TupleToArray()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TupleToArray&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;size of x&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;x[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">x</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, all elements of &#39;input_x&#39; must be have same type.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">range</span><span class="p">):</span>
            <span class="n">args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ScalarToArray</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The `ScalarToArray` primitive  is deprecated. Please use the :class:`mindspore.ops.ScalarToTensor` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2.0&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.scalar_to_tensor&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>


<div class="viewcode-block" id="ScalarToTensor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScalarToTensor.html#mindspore.ops.ScalarToTensor">[docs]</a><span class="k">class</span> <span class="nc">ScalarToTensor</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a scalar to a `Tensor`, and converts the data type to the specified type.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scalar_to_tensor` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[int, float]) - The input is a scalar. Only constant value is allowed.</span>
<span class="sd">        - **dtype** (mindspore.dtype) - The target data type. Default: ``mindspore.float32`` . Only</span>
<span class="sd">          constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor. 0-D Tensor and the content is the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ScalarToTensor()</span>
<span class="sd">        &gt;&gt;&gt; data = 1</span>
<span class="sd">        &gt;&gt;&gt; output = op(data, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_scalar&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output_data&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">data_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">dtype_to_nptype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">data_type</span><span class="p">))</span></div>


<div class="viewcode-block" id="InvertPermutation"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InvertPermutation.html#mindspore.ops.InvertPermutation">[docs]</a><span class="k">class</span> <span class="nc">InvertPermutation</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse of an index permutation.</span>

<span class="sd">    This operator is mainly used to calculate the inverse of index permutation.</span>
<span class="sd">    It requires a 1-dimensional integer tensor x, which represents the index of a zero-based array,</span>
<span class="sd">    and exchanges each value with its index position. In other words, For output tensor y and input tensor x,</span>
<span class="sd">    this operation calculates the following values:</span>

<span class="sd">    :math:`y[x[i]] = i, \quad i \in [0, 1, \ldots, \text{len}(x)-1]`.</span>

<span class="sd">    Note:</span>
<span class="sd">        These values must include 0. There must be no duplicate values and the</span>
<span class="sd">        values can not be negative.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union(tuple[int], list[int])) - The input is constructed by multiple</span>
<span class="sd">          integers, i.e., :math:`(y_1, y_2, ..., y_S)` representing the indices.</span>
<span class="sd">          The values must include 0. There can be no duplicate values or negative values.</span>
<span class="sd">          Only constant value is allowed. The maximum value must be equal to length of input_x.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[int]. It has the same length as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither tuple nor list.</span>
<span class="sd">        TypeError: If element of `input_x` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; invert = ops.InvertPermutation()</span>
<span class="sd">        &gt;&gt;&gt; input_data = (3, 4, 0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; output = invert(input_data)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (2, 4, 3, 0, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InvertPermutation&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x_shp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">x_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mstype</span><span class="o">.</span><span class="n">_issubclass_</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">):</span>  <span class="c1"># pylint: disable=W0212</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, the value of &#39;input_x&#39; must be non-Tensor, but got </span><span class="si">{</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the value of &#39;input_x&#39; can not be None, but got </span><span class="si">{</span><span class="n">x_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">x_shp</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">shp</span> <span class="ow">in</span> <span class="n">x_shp</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shp</span><span class="p">:</span>
                <span class="n">x_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_value</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, the dimension of &#39;input_x&#39; must be 1, but got </span><span class="si">{</span><span class="n">x_rank</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_value</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;input[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_value</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_value</span><span class="p">))]</span>
        <span class="n">z</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">z</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">z</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;input_x&#39; can not contain duplicate values, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got duplicated </span><span class="si">{</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> in the &#39;input_x&#39;.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;value min&#39;</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">x_value</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;value max&#39;</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">x_value</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_value</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_value</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_value</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;input[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="sa">f</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">y</span><span class="p">[</span><span class="n">value</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
            <span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">y</span><span class="p">)}</span></div>


<div class="viewcode-block" id="Argmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Argmax.html#mindspore.ops.Argmax">[docs]</a><span class="k">class</span> <span class="nc">Argmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the maximum value of a tensor across the axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.argmax` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Axis where the Argmax operation applies to. Default: ``-1`` .</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): An optional data type of ``mstype.int32`` .</span>
<span class="sd">            Default: ``mstype.int32``.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor. :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions. Support data type list as follows:</span>

<span class="sd">          - Ascend: Float16, Float32.</span>
<span class="sd">          - GPU: Float16, Float32.</span>
<span class="sd">          - CPU: Float16, Float32, Float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, indices of the max value of input tensor across the axis.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Argmax(output_type=mindspore.int32)(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Argmax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_types_same_and_valid</span><span class="p">({</span><span class="s1">&#39;output&#39;</span><span class="p">:</span> <span class="n">output_type</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_type&#39;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="Argmin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Argmin.html#mindspore.ops.Argmin">[docs]</a><span class="k">class</span> <span class="nc">Argmin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum value of a tensor across the axis.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Axis where the Argmin operation applies to. Default: ``-1`` .</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): An optional data type of ``mstype.int32`` and</span>
<span class="sd">            ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">          - Ascend: Float16, Float32, Float64, Int8, Int16, Int32, Int64, UInt8, UInt16, UInt32, UInt64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype is determined by `output_type`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `output_type` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2.0, 3.1, 1.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = ops.Argmin()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index)</span>
<span class="sd">        2</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Argmin&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;output_type&quot;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_type&#39;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ArgminV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum value of a tensor across the axis.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Note:</span>
<span class="sd">        This operator only supports dynamic shape. As for static shape, please use operator `Argmin` instead.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor.</span>
<span class="sd">          The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **axis** (int) - Axis where the Argmin operator applies to. Default: ``-1`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, indices of the min value of input tensor across the axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ArgMinV2DynatimicShape(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, gather_axis=1, argmin_axis=1):</span>
<span class="sd">        ...         super(ArgMinV2DynatimicShape, self).__init__()</span>
<span class="sd">        ...         self.unique = P.Unique()</span>
<span class="sd">        ...         self.gather = P.Gather()</span>
<span class="sd">        ...         self.argmin = ArgminV2()</span>
<span class="sd">        ...         self.gather_axis = gather_axis</span>
<span class="sd">        ...         self.argmin_axis = argmin_axis</span>
<span class="sd">        ...     def construct(self, x, indices):</span>
<span class="sd">        ...         unique_index, _ = self.unique(indices)</span>
<span class="sd">        ...         y = self.gather(x, unique_index, self.gather_axis)</span>
<span class="sd">        ...         z = self.argmin(y, self.argmin_axis)</span>
<span class="sd">        ...         return z</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[4, 8, 1, 6], [4, 3, 6, 2], [4, 4, 1, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor([1, 2], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = ArgMinV2DynatimicShape()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x, index)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        [1 0 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ArgminV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">_run_op</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>


<div class="viewcode-block" id="ArgMaxWithValue"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ArgMaxWithValue.html#mindspore.ops.ArgMaxWithValue">[docs]</a><span class="k">class</span> <span class="nc">ArgMaxWithValue</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the maximum value along with the given axis for the input tensor, and returns the maximum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple maximum values, the index of the first maximum value is used.</span>
<span class="sd">        - The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;x&quot;.</span>

<span class="sd">    Also see :func:`mindspore.ops.max`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension to reduce. Default: ``0`` .</span>
<span class="sd">        keep_dims (bool): Whether to reduce dimension, if ``True`` , the output will keep same dimension with the</span>
<span class="sd">                          input, the output will reduce dimension if ``false`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_N)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the maximum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **index** (Tensor) - The index for the maximum value of the input tensor, with dtype int32. If `keep_dims`</span>
<span class="sd">          is ``True`` , the shape of output tensors is :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`.</span>
<span class="sd">          Otherwise, the shape is :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)` .</span>
<span class="sd">        - **values** (Tensor) - The maximum value of input tensor, with the same shape as index, and same dtype as x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMaxWithValue()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        3 0.7</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMaxWithValue(keep_dims=True)(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        [3] [0.7]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ArgMaxWithValue&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;values&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_dims&#39;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span> <span class="o">=</span> <span class="n">keep_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dimension&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="ArgMinWithValue"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ArgMinWithValue.html#mindspore.ops.ArgMinWithValue">[docs]</a><span class="k">class</span> <span class="nc">ArgMinWithValue</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the minimum value along with the given axis for the input tensor, and returns the minimum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple minimum values, the index of the first minimum value is used.</span>
<span class="sd">        - The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;x&quot;.</span>

<span class="sd">    Also see :func:`mindspore.ops.min`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension to reduce. Default: ``0`` .</span>
<span class="sd">        keep_dims (bool): Whether to reduce dimension, if ``True`` the output will keep the same dimension as the</span>
<span class="sd">                          input, the output will reduce dimension if ``false`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_N)` .Complex tensor is not supported.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the minimum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **index** (Tensor) - The index for the minimum value of the input tensor, with dtype int32. If `keep_dims`</span>
<span class="sd">          is ``True`` , the shape of output tensors is :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`.</span>
<span class="sd">          Otherwise, the shape is :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)` .</span>
<span class="sd">        - **values** (Tensor) - The minimum value of input tensor, with the same</span>
<span class="sd">          shape as `index`, and same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMinWithValue()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        0 0.0</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMinWithValue(keep_dims=True)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        [0] [0.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ArgMinWithValue&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;values&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_dims&#39;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span> <span class="o">=</span> <span class="n">keep_dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dimension&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="Tile"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Tile.html#mindspore.ops.Tile">[docs]</a><span class="k">class</span> <span class="nc">Tile</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replicates an input tensor with given multiples times.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tile` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - 1-D or higher dimensional Tensor. Set the shape of input tensor as</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_S)` .</span>
<span class="sd">        - **multiples** (tuple[int]) - The parameter that specifies the number of replications,</span>
<span class="sd">          the parameter type is tuple, and the data type is int, i.e., :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">          The length of `multiples` cannot be smaller than the length of the shape of `input_x`.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type as the `input_x`. Suppose the length of `multiples` is `d`,</span>
<span class="sd">        the dimension of `input_x` is `input_x.dim`, and the shape of `input_x` is :math:`(x_1, x_2, ..., x_S)`.</span>

<span class="sd">        - If `input_x.dim = d`, then the shape of their corresponding positions can be multiplied, and</span>
<span class="sd">          the shape of Outputs is :math:`(x_1*y_1, x_2*y_2, ..., x_S*y_S)`.</span>
<span class="sd">        - If `input_x.dim &lt; d`, fill in multiple 1 in the length of the shape of `input_x` until their</span>
<span class="sd">          lengths are consistent. Such as set the shape of `input_x` as :math:`(1, ..., x_1, x_2, ..., x_S)`,</span>
<span class="sd">          then the shape of their corresponding positions can be multiplied, and the shape of Outputs is</span>
<span class="sd">          :math:`(1*y_1, ..., x_R*y_R, x_S*y_S)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; tile = ops.Tile()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; multiples = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = tile(input_x, multiples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  2.  1.  2.  1.  2.]</span>
<span class="sd">         [3.  4.  3.  4.  3.  4.]</span>
<span class="sd">         [1.  2.  1.  2.  1.  2.]</span>
<span class="sd">         [3.  4.  3.  4.  3.  4.]]</span>
<span class="sd">        &gt;&gt;&gt; multiples = (2, 3, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = tile(input_x, multiples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]]</span>
<span class="sd">         [[1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Tile&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;multiples&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_elim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_tensor</span><span class="p">,</span> <span class="n">multiplier</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;input_x&#39; must be Tensor, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">base_tensor</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">multiplier</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the type of &#39;multiplier&#39; must be tuple, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">multiplier</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">v</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">multiplier</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">base_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">multiplier</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()(</span><span class="n">base_tensor</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_shape_and_range</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;calculate tile shape and value&quot;&quot;&quot;</span>
        <span class="n">x_shp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">x_shp</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_shp</span><span class="p">},</span> <span class="kc">None</span>
        <span class="n">multiples_v</span> <span class="o">=</span> <span class="n">multiples</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">len_sub</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">multiples_v</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shp</span><span class="p">)</span>
        <span class="n">multiples_w</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">len_sub</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">multiples_w</span> <span class="o">=</span> <span class="n">multiples_v</span>
        <span class="k">if</span> <span class="n">len_sub</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">len_sub</span><span class="p">):</span>
                <span class="n">x_shp</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">multiples_w</span> <span class="o">=</span> <span class="n">multiples_v</span>
        <span class="k">elif</span> <span class="n">len_sub</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the length of &#39;multiples&#39; can not be smaller than &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the dimension of &#39;input_x&#39;, but got length of &#39;multiples&#39;: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">multiples_v</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;and dimension of &#39;input_x&#39;: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shp</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">multiples_w</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x_shp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">x_shp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">a</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">multiples_w</span><span class="p">))</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">x_shp</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">):</span>
        <span class="n">multiples_v</span> <span class="o">=</span> <span class="n">multiples</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">multiples_v</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="kc">None</span> <span class="ow">in</span> <span class="n">multiples_v</span><span class="p">:</span>
            <span class="k">if</span> <span class="s1">&#39;max_value&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">multiples</span> <span class="ow">or</span> <span class="s1">&#39;min_value&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">multiples</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">multiples_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">multiples</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">shape</span> <span class="o">=</span> <span class="n">multiples</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s1">, the dim of multiples must be 1.&#39;</span><span class="p">)</span>
                <span class="n">rank</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]),</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rank</span>
                <span class="k">if</span> <span class="o">-</span><span class="mi">2</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]:</span>
                    <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">return</span> <span class="p">{</span>
                    <span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">out_shape</span><span class="p">,</span>
                    <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                    <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span>
                <span class="p">}</span>
            <span class="n">out_shape</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_shape_and_range</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">)</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">out_shape</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">shape</span><span class="p">,</span>
                   <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                   <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
            <span class="k">return</span> <span class="n">out</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span>
            <span class="s2">&quot;multiples&quot;</span><span class="p">,</span> <span class="n">multiples_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">multiple</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">multiples_v</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span>
                <span class="n">multiple</span><span class="p">,</span> <span class="s2">&quot;multiples[</span><span class="si">%d</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span>
            <span class="s2">&quot;x[</span><span class="se">\&#39;</span><span class="s2">dtype</span><span class="se">\&#39;</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;dtype&quot;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">out_shp</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_shape_and_range</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">multiples</span><span class="p">)</span>
        <span class="n">shp</span> <span class="o">=</span> <span class="n">out_shp</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">shp</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="UnsortedSegmentSum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UnsortedSegmentSum.html#mindspore.ops.UnsortedSegmentSum">[docs]</a><span class="k">class</span> <span class="nc">UnsortedSegmentSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sum of a tensor along segments.</span>

<span class="sd">    Refer to :func:`mindspore.ops.unsorted_segment_sum` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input Tensor contains the data to be summed.</span>
<span class="sd">          The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **segment_ids** (Tensor) - The label indicates the segment to which each element belongs.</span>
<span class="sd">          Set the shape as :math:`(x_1, x_2, ..., x_N)`, where 0 &lt; N &lt;= R.</span>
<span class="sd">        - **num_segments** (int) - Set :math:`z` as num_segments, it can be an int or 0-D Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is :math:`(z, x_{N+1}, ..., x_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.UnsortedSegmentSum()(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 0.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 2, 5], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2, 3, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 6</span>
<span class="sd">        &gt;&gt;&gt; output = ops.UnsortedSegmentSum()(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 2. 5. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UnsortedSegmentSum&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;num_segments&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="UnsortedSegmentMin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UnsortedSegmentMin.html#mindspore.ops.UnsortedSegmentMin">[docs]</a><span class="k">class</span> <span class="nc">UnsortedSegmentMin</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of a tensor along segments.</span>

<span class="sd">    Refer to :func:`mindspore.ops.unsorted_segment_min` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          The data type must be float16, float32 or int32.</span>
<span class="sd">        - **segment_ids** (Tensor) - A `1-D` tensor whose shape is :math:`(x_1)`, the value must be non-negative tensor.</span>
<span class="sd">          The data type must be int32.</span>
<span class="sd">        - **num_segments** (int) - The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; unsorted_segment_min = ops.UnsortedSegmentMin()</span>
<span class="sd">        &gt;&gt;&gt; output = unsorted_segment_min(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UnsortedSegmentMin&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;num_segments&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__check__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">segment_ids_shape</span> <span class="o">=</span> <span class="n">segment_ids</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                      <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;segment_ids&quot;</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># support vmap : segment_ids_shape support batch rank</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;batch_rank&#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;rank of segment_ids_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">num_segments_type</span> <span class="o">=</span> <span class="n">num_segments</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;num_segments&quot;</span><span class="p">,</span> <span class="n">num_segments_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">):</span>
            <span class="c1"># only validate when both shapes fully known</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;first shape of input_x&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="s1">&#39;length of segments_id&#39;</span><span class="p">,</span> <span class="n">segment_ids_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">num_segments_v</span> <span class="o">=</span> <span class="n">num_segments</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;num_segments&#39;</span><span class="p">,</span> <span class="n">num_segments_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">num_segments_v</span><span class="p">,</span> <span class="s2">&quot;num_segments&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="UnsortedSegmentMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UnsortedSegmentMax.html#mindspore.ops.UnsortedSegmentMax">[docs]</a><span class="k">class</span> <span class="nc">UnsortedSegmentMax</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum along segments of a tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.unsorted_segment_max` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          The data type must be float16, float32 or int32.</span>
<span class="sd">        - **segment_ids** (Tensor) - A `1-D` tensor whose shape is :math:`(x_1)`, the value must be non-negative tensor.</span>
<span class="sd">          The data type must be int32.</span>
<span class="sd">        - **num_segments** (int) - The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Only have two num_segments, where is 0 and 1, and segment_ids=[0, 1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # num_segments = 2 indicates that there are two types of segment_id,</span>
<span class="sd">        &gt;&gt;&gt; # the first number &#39;0&#39; in [0, 1, 1] indicates input_x[0],</span>
<span class="sd">        &gt;&gt;&gt; # the second number &#39;1&#39; in [0, 1, 1] indicates input_x[1],</span>
<span class="sd">        &gt;&gt;&gt; # the third number &#39;1&#39; in [0, 1, 1] indicates input_x[2],</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0], which is [1, 2, 3] will not be compared to other segment_id.</span>
<span class="sd">        &gt;&gt;&gt; # Only the same segment_id will be compared.</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; unsorted_segment_max = ops.UnsortedSegmentMax()</span>
<span class="sd">        &gt;&gt;&gt; output = unsorted_segment_max(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # case 2: The segment_ids=[0, 0, 1, 1].</span>
<span class="sd">        &gt;&gt;&gt; # [1, 2, 3] will compare with [4, 2, 0],</span>
<span class="sd">        &gt;&gt;&gt; # and [4, 5, 6] will compare with [4, 2, 1].</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 2, 0], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; unsorted_segment_max = ops.UnsortedSegmentMax()</span>
<span class="sd">        &gt;&gt;&gt; output = unsorted_segment_max(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(input_x.shape)</span>
<span class="sd">            (4, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[4. 2. 3.]</span>
<span class="sd">             [4. 5. 6.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: If the input_x have three dimensions even more, what will happen?</span>
<span class="sd">        &gt;&gt;&gt; # The shape of input_x is (2, 4, 3),</span>
<span class="sd">        &gt;&gt;&gt; # and the length of segment_ids should be the same as the first dimension of input_x.</span>
<span class="sd">        &gt;&gt;&gt; # Because the segment_ids are different, input_x[0] will not be compared to input_x[1].</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1, 2, 3], [4, 2, 0], [4, 5, 6], [4, 2, 1]],</span>
<span class="sd">        ...                            [[1, 2, 3], [4, 2, 0], [4, 5, 6], [4, 2, 1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; unsorted_segment_max = ops.UnsortedSegmentMax()</span>
<span class="sd">        &gt;&gt;&gt; output = unsorted_segment_max(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(input_x.shape)</span>
<span class="sd">            (2, 4, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[[1. 2. 3.]</span>
<span class="sd">              [4. 2. 0.]</span>
<span class="sd">              [4. 5. 6.]</span>
<span class="sd">              [4. 2. 1.]]</span>
<span class="sd">             [[1. 2. 3.]</span>
<span class="sd">              [4. 2. 0.]</span>
<span class="sd">              [4. 5. 6.]</span>
<span class="sd">              [4. 2. 1.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: It has the same input with the 3rd case.</span>
<span class="sd">        &gt;&gt;&gt; # Because num_segments is equal to 2, there are two segment_ids, but currently only one 0 is used.</span>
<span class="sd">        &gt;&gt;&gt; # the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">        &gt;&gt;&gt; # the smallest possible value of the input_x&#39;s type.</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = unsorted_segment_max(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[[ 1.0000000e+00  2.0000000e+00  3.0000000e+00]</span>
<span class="sd">              [ 4.0000000e+00  2.0000000e+00  0.0000000e+00]</span>
<span class="sd">              [ 4.0000000e+00  5.0000000e+00  6.0000000e+00]</span>
<span class="sd">              [ 4.0000000e+00  2.0000000e+00  1.0000000e+00]]</span>
<span class="sd">             [[-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]</span>
<span class="sd">              [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]</span>
<span class="sd">              [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]</span>
<span class="sd">              [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UnsortedSegmentMax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;num_segments&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__check__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">segment_ids_shape</span> <span class="o">=</span> <span class="n">segment_ids</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">valid_type</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                      <span class="n">mstype</span><span class="o">.</span><span class="n">int16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">uint64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">valid_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">({</span><span class="s2">&quot;segment_ids&quot;</span><span class="p">:</span> <span class="n">segment_ids</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]},</span>
                                                      <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># support vmap : segment_ids_shape support batch rank</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;batch_rank&#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;rank of segment_ids_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">num_segments_type</span> <span class="o">=</span> <span class="n">num_segments</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;num_segments&quot;</span><span class="p">,</span> <span class="n">num_segments_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">number</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">segment_ids_shape</span><span class="p">):</span>
            <span class="c1"># only validate when both shapes fully known</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;first shape of input_x&#39;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="s1">&#39;length of segments_id&#39;</span><span class="p">,</span> <span class="n">segment_ids_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">num_segments_v</span> <span class="o">=</span> <span class="n">num_segments</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">num_segments_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;num_segments&#39;</span><span class="p">,</span> <span class="n">num_segments_v</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">num_segments_v</span><span class="p">,</span> <span class="s2">&quot;num_segments&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="UnsortedSegmentProd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UnsortedSegmentProd.html#mindspore.ops.UnsortedSegmentProd">[docs]</a><span class="k">class</span> <span class="nc">UnsortedSegmentProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the product of a tensor along segments.</span>

<span class="sd">    Refer to :func:`mindspore.ops.unsorted_segment_prod` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          With float16, float32 or int32 data type.</span>
<span class="sd">        - **segment_ids** (Tensor) - A `1-D` tensor whose shape is :math:`(x_1)`, the value must be non-negative tensor.</span>
<span class="sd">          Data type must be int32.</span>
<span class="sd">        - **num_segments** (int) - The value specifies the number of distinct `segment_ids`,</span>
<span class="sd">          must be greater than 0.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; unsorted_segment_prod = ops.UnsortedSegmentProd()</span>
<span class="sd">        &gt;&gt;&gt; output = unsorted_segment_prod(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 4. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UnsortedSegmentProd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;num_segments&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Concat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Concat.html#mindspore.ops.Concat">[docs]</a><span class="k">class</span> <span class="nc">Concat</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connect tensor in the specified axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.concat` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int, optional): The specified axis. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[tuple, list]) - A tuple or a list of input tensors.</span>
<span class="sd">          Suppose there are two tensors in this tuple or list, namely x1 and x2.</span>
<span class="sd">          To perform `Concat` in the axis 0 direction, except for the 0th axis, all other axes should be equal,</span>
<span class="sd">          that is, :math:`x1.shape[1] == x2.shape[1], x1.shape[2] == x2.shape[2], ..., x1.shape[R] == x2.shape[R]`,</span>
<span class="sd">          where the :math:`R` indicates the last axis.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - Tensor, the shape is :math:`(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)`.</span>
<span class="sd">          The data type is the same with `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Concat()</span>
<span class="sd">        &gt;&gt;&gt; output = op((input_x1, input_x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 1.]</span>
<span class="sd">         [0. 1.]</span>
<span class="sd">         [2. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Concat(1)</span>
<span class="sd">        &gt;&gt;&gt; output = op((input_x1, input_x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 0. 1.]</span>
<span class="sd">         [2. 1. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Concat&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Implement Concat infer value&quot;&quot;&quot;</span>
        <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">input_x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">input_x</span><span class="p">:</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_x</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">value</span></div>


<span class="k">class</span> <span class="nc">ConcatOffsetV1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    primitive for computing Concatâ€™s gradient.</span>

<span class="sd">    Computes offsets of concat inputs within its output. Accumulate offsets from zero along `axis`.</span>
<span class="sd">    If tensor element in `x` isn&#39;t along `axis`, they should be the same along their axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **axis** (Tensor): The specified axis, required to be 0-D Tensor object with dtype int32.</span>
<span class="sd">          Input `axis` should fall in :math:`[-numelement, numelement - 1]`,</span>
<span class="sd">          say numelement is the element number of first tensor in `x`.</span>
<span class="sd">        - **x** (tuple[Tensor], list[Tensor]) - A tuple or a list of input tensors.</span>
<span class="sd">          The tensors in `x` are all required to be a vector, in other word, 1-D Tensor object with dtype int32.</span>
<span class="sd">          Suppose there are two tensors in this tuple or list, namely x1 and x2.</span>
<span class="sd">          To perform `ConcatOffsetV1` in the axis 0 direction,</span>
<span class="sd">          except for the 0th axis, all elements in other axes should be equal,</span>
<span class="sd">          that is, :math:`x1[1] == x2[1], x1[2] == x2[2], ..., x1[R] == x2[R]`,</span>
<span class="sd">          where the :math:`R` indicates the last axis.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensors. A tuple of N 1-D Tensor objects.</span>
<span class="sd">        The data type is the same with the Inputs `x`, dtype int32.</span>
<span class="sd">        The shape is the same with the Inputs `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not a tensor.</span>
<span class="sd">        TypeError: If dtype of tensor in `axis` is not int32.</span>
<span class="sd">        TypeError: If `x` have different type of tensor.</span>
<span class="sd">        TypeError: If dtype of tensor in `x` is not int32.</span>
<span class="sd">        ValueError: If the shape rank of `axis` does not equal to 0.</span>
<span class="sd">        ValueError: If the number of tensors in `x` is less than 2.</span>
<span class="sd">        ValueError: If the shape rank of tensor in `x` does not equal to 1.</span>
<span class="sd">        ValueError: If the element number of tensor in `x` is less than 1.</span>
<span class="sd">        ValueError: If `x` have different shape of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; axis = Tensor(1, dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 5, 3]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; x3 = Tensor(np.array([1, 4, 3]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ConcatOffsetV1()</span>
<span class="sd">        &gt;&gt;&gt; output = op(axis, (x1, x2, x3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3,], dtype=Int32, value=[0, 0, 0]),</span>
<span class="sd">         Tensor(shape=[3,], dtype=Int32, value=[0, 2, 0]),</span>
<span class="sd">         Tensor(shape=[3,], dtype=Int32, value=[0, 7, 0]))</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ConcatOffsetV1&quot;&quot;&quot;</span>


<div class="viewcode-block" id="ParallelConcat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ParallelConcat.html#mindspore.ops.ParallelConcat">[docs]</a><span class="k">class</span> <span class="nc">ParallelConcat</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Concats input tensors along the first dimension.</span>

<span class="sd">    The difference between Concat and ParallelConcat is that Concat requires all of the inputs be computed</span>
<span class="sd">    before the operation will begin but doesn&#39;t require that the input shapes be known during graph construction.</span>
<span class="sd">    Parallel concat will copy pieces of the input into the output as they become available, in some situations</span>
<span class="sd">    this can provide a performance benefit.</span>

<span class="sd">    Note:</span>
<span class="sd">        The input tensors are all required to have size 1 in the first dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **values** (tuple, list) - A tuple or a list of input tensors. The data type and shape of these</span>
<span class="sd">          tensors must be the same and their rank should not be less than 1.</span>
<span class="sd">          The supported date type is Number on CPU, the same for Ascend except</span>
<span class="sd">          [float64, complex64, complex128].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, data type is the same as `values`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If any type of the inputs is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of these tensors are not the same.</span>
<span class="sd">        ValueError: If any tensor.shape[0] is not 1.</span>
<span class="sd">        ValueError: If rank of any Tensor in `values` is less than 1.</span>
<span class="sd">        ValueError: If the shape of these tensors are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; data1 = Tensor(np.array([[0, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; data2 = Tensor(np.array([[2, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ParallelConcat()</span>
<span class="sd">        &gt;&gt;&gt; output = op((data1, data2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">         [2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ParallelConcat&quot;&quot;&quot;</span></div>


<span class="k">def</span> <span class="nf">_get_stack_shape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;for stack output shape&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;len of input_x&quot;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input_x[0]&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>

    <span class="n">out_n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">out_n</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x_type</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">x_type</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{}</span><span class="se">\&#39;</span><span class="s2">, all types should be same, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prim_name</span><span class="p">,</span> <span class="n">x_type</span><span class="p">))</span>

    <span class="n">new_x_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">shp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">shp</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="n">new_x_shape</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;shape&quot;</span><span class="p">:</span> <span class="n">shp</span><span class="p">,</span> <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">})</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">new_x_shape</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;shape&quot;</span><span class="p">:</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">new_x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">]</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_x_shape</span><span class="p">)</span>

    <span class="n">rank_base</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;len of x_shape[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">new_x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;id&quot;</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">]),</span>
                        <span class="s1">&#39;len of x_shape[0]&#39;</span><span class="p">,</span> <span class="n">rank_base</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">rank_base</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">new_x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="n">new_x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="ow">and</span> \
                    <span class="n">new_x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">new_x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;shape&quot;</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{}</span><span class="se">\&#39;</span><span class="s2"> element </span><span class="si">{}</span><span class="s2"> shape in input can not pack with first element&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">prim_name</span><span class="p">,</span> <span class="n">new_x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;id&#39;</span><span class="p">]))</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">rank_base</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">rank_base</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">rank_base</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">out_shape</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">out_shape</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">out_n</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out_shape</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="n">out_shape</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">out_n</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_shape</span>


<span class="k">class</span> <span class="nc">Pack</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator Stack. Pack will be deprecated in the future.</span>
<span class="sd">    Please use Stack instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;Stack&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pack&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">x_type</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">))</span>
        <span class="n">all_shape</span> <span class="o">=</span> <span class="n">_get_stack_shape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">all_shape</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="Stack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Stack.html#mindspore.ops.Stack">[docs]</a><span class="k">class</span> <span class="nc">Stack</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks a list of tensors in specified axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.stack` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int):  Dimension to stack. Default: ``0`` .</span>
<span class="sd">            Negative values wrap around. The range is [-(R+1), R+1).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[tuple, list]) - A Tuple or list of Tensor objects with the same shape and type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor. A stacked Tensor with the same type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; data1 = Tensor(np.array([0, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; data2 = Tensor(np.array([2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; stack = ops.Stack()</span>
<span class="sd">        &gt;&gt;&gt; output = stack([data1, data2])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Stack&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="n">x_type</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">))</span>
        <span class="n">all_shape</span> <span class="o">=</span> <span class="n">_get_stack_shape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">x_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">tuple_value</span> <span class="o">=</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">input_array</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">infered_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">tuple_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">tuple_value</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tuple_value</span><span class="p">:</span>
                <span class="n">npy_item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
                <span class="n">input_array</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">npy_item</span><span class="p">)</span>
            <span class="n">infered_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">input_array</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">))</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="n">all_shape</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">all_shape</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="k">else</span> <span class="n">all_shape</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">shape</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">infered_value</span><span class="p">}</span>

        <span class="k">def</span> <span class="nf">unpack</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">unpack</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">x</span>

        <span class="k">if</span> <span class="s1">&#39;shape_value&#39;</span> <span class="ow">in</span> <span class="n">value</span> <span class="ow">and</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;shape_value&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_shape_value</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">value</span><span class="p">[</span><span class="s1">&#39;shape_value&#39;</span><span class="p">]:</span>
                <span class="n">item</span> <span class="o">=</span> <span class="n">unpack</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
                <span class="n">item</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
                <span class="n">input_shape_value</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
            <span class="n">infered_shape_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">input_shape_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
            <span class="n">infered_shape_value</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">infered_shape_value</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">out</span><span class="p">[</span><span class="s1">&#39;shape_value&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">infered_shape_value</span>
        <span class="k">return</span> <span class="n">out</span></div>


<span class="k">class</span> <span class="nc">Unpack</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator Unstack. Unpack will be deprecated in the future.</span>
<span class="sd">    Please use Unstack instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;Unstack&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Unpack&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">])</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="o">-</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="s1">&#39;axis value&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">+</span> <span class="n">dim</span>
        <span class="n">output_num</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">output_num</span><span class="p">,</span> <span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;num&#39;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>
        <span class="n">output_valid_check</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="n">output_num</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">output_valid_check</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                            <span class="s2">&quot;The dimension which to unstack divides output_num&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">out_shapes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">out_dtypes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">output_num</span><span class="p">):</span>
            <span class="n">out_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">out_shape</span><span class="p">))</span>
            <span class="n">out_dtypes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">])</span>
        <span class="n">out_shapes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out_shapes</span><span class="p">)</span>
        <span class="n">out_dtypes</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out_dtypes</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">out_shapes</span><span class="p">,</span>
               <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">out_dtypes</span><span class="p">,</span>
               <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
        <span class="k">return</span> <span class="n">out</span>


<div class="viewcode-block" id="Unstack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Unstack.html#mindspore.ops.Unstack">[docs]</a><span class="k">class</span> <span class="nc">Unstack</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unstacks tensor in specified axis.</span>

<span class="sd">    Unstacks a tensor of rank `R` along axis dimension, output tensors will have rank `(R-1)`.</span>

<span class="sd">    Given a tensor of shape :math:`(x_1, x_2, ..., x_R)`. If :math:`0 \le axis`,</span>
<span class="sd">    the shape of tensor in output is :math:`(x_1, x_2, ..., x_{axis}, x_{axis+2}, ..., x_R)`.</span>

<span class="sd">    This is the opposite of pack.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Dimension along which to unpack. Default: ``0`` .</span>
<span class="sd">            Negative values wrap around. The range is [-R, R).</span>
<span class="sd">        num (Union[None, int]): The number of output tensors.</span>
<span class="sd">            Automatically inferred by input_x and axis if ``None`` . Default: ``None`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          A tensor to be unstacked and the rank of the tensor must be greater than 0.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tuple of tensors, the shape of each objects is the same.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of the range [-len(input_x.shape), len(input_x.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; unstack = ops.Unstack()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; output = unstack(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[4], dtype=Int64, value= [1, 1, 1, 1]), Tensor(shape=[4], dtype=Int64, value= [2, 2, 2, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Unstack&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num&quot;</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Slice"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Slice.html#mindspore.ops.Slice">[docs]</a><span class="k">class</span> <span class="nc">Slice</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slices a tensor in the specified shape.</span>

<span class="sd">    Refer to :func:`mindspore.ops.slice` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **begin** (Union[tuple, list]) - The beginning of the slice. Only constant value(&gt;=0) is allowed.</span>
<span class="sd">        - **size** (Union[tuple, list]) - The size of the slice. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is: input `size`, the data type is the same as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[[1, 1, 1], [2, 2, 2]],</span>
<span class="sd">        ...                         [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">        ...                         [[5, 5, 5], [6, 6, 6]]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; slice_op = ops.Slice()</span>
<span class="sd">        &gt;&gt;&gt; output = slice_op(data, (1, 0, 0), (1, 1, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3 3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = slice_op(data, (1, 0, 0), (1, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = slice_op(data, (1, 0, 0), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = slice_op(data, (1, 1, 0), (1, 1, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4 4 4]]]</span>
<span class="sd">        &gt;&gt;&gt; output = slice_op(data, (1, 0, 1), (1, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize slice&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;begin&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">Coalesce</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the coalesced sparse tensor of the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x_indices** (Tensor) - A 2-D Tensor, represents the indices of the nonzero elements of the sparse tensor.</span>
<span class="sd">          Supported data type is int64. Its elements should be non-negative. The shape is :math:`(y, x)`.</span>
<span class="sd">        - **x_values** (Tensor) - A 1-D Tensor, represents the values corresponding to the indices in `x_indices`.</span>
<span class="sd">          Supported data types are float16 and float32. The shape is :math:`(x,)`.</span>
<span class="sd">        - **x_shape** (Tensor) - A 1-D Tensor, specifies the shape of the sparse tensor.</span>
<span class="sd">          Supported data type is int64. The shape is :math:`(y,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y_indices** (Tensor) - A 2-D Tensor, represents the indices of the nonzero elements of the sparse tensor.</span>
<span class="sd">          Data type is int64. It&#39;s elements are non-negative. The shape is :math:`(y, z)`.</span>
<span class="sd">          `z` represents the number of different indices in `x_indices`.</span>
<span class="sd">        - **y_values** (Tensor) - A 1-D Tensor, represents the values corresponding to the indices in `y_indices`.</span>
<span class="sd">          Data type is the same as `x_values`&#39;s. The shape is :math:`(z,)`.</span>
<span class="sd">        - **y_shape** (Tensor) - A 1-D Tensor, specifies the shape of the sparse tensor.</span>
<span class="sd">          Data type is int64. The shape is :math:`(y,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `x_values` is neither float32 nor float16.</span>
<span class="sd">        TypeError: If any of the data types of `x_indices` and `x_shape` is not int64.</span>
<span class="sd">        ValueError: If any of `x_values` and `x_shape` is not a 1-D tensor.</span>
<span class="sd">        ValueError: If `x_indices` is not a 2-D tensor.</span>
<span class="sd">        ValueError: If sizes of second dimension of `x_indices` and first dimension of `x_values` are not the same.</span>
<span class="sd">        ValueError: If sizes of first dimension of `x_indices` and first dimension of `x_shape` are not the same.</span>
<span class="sd">        ValueError: If any of the values of elements of `x_indices` is negative.</span>
<span class="sd">        ValueError: If any of the values of elements of `x_indices` exceed the limit set by `x_shape`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x_indices = Tensor([[0, 0, 1], [1, 1, 2]], dtype=mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; x_values = Tensor([1, 5, 4], dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; x_shape = Tensor([3, 3], dtype=mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; coalesce = ops.Coalesce()</span>
<span class="sd">        &gt;&gt;&gt; y_indices, y_values, y_shape = coalesce(x_indices, x_values, x_shape)</span>
<span class="sd">        &gt;&gt;&gt; print(y_indices)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">         [1 2]]</span>
<span class="sd">        &gt;&gt;&gt; print(y_values)</span>
<span class="sd">        [6. 4.]</span>
<span class="sd">        &gt;&gt;&gt; print(y_shape)</span>
<span class="sd">        [3 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Coalesce.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x_indices&#39;</span><span class="p">,</span> <span class="s1">&#39;x_values&#39;</span><span class="p">,</span> <span class="s1">&#39;x_shape&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y_indices&#39;</span><span class="p">,</span> <span class="s1">&#39;y_values&#39;</span><span class="p">,</span> <span class="s1">&#39;y_shape&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="ReverseV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReverseV2.html#mindspore.ops.ReverseV2">[docs]</a><span class="k">class</span> <span class="nc">ReverseV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses specific dimensions of a tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;input_x&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[tuple(int), list(int)]): The indices of the dimensions to reverse.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The data type is Number except float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is neither list nor tuple.</span>
<span class="sd">        TypeError: If element of `axis` is not an int.</span>
<span class="sd">        ValueError: There are multiple identical axes in `axis`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReverseV2(axis=[1])</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 3 2 1]</span>
<span class="sd">         [8 7 6 5]]</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReverseV2(axis=[1, 0])</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[8 7 6 5]</span>
<span class="sd">         [4 3 2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReverseV2.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">each</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;axis[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="n">each</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Rint"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Rint.html#mindspore.ops.Rint">[docs]</a><span class="k">class</span> <span class="nc">Rint</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an integer that is closest to `input_x` element-wise.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor, which must be one of the following types:</span>
<span class="sd">          float16, float32, float64. The shape is :math:`(N,*)` where :math:`*` means</span>
<span class="sd">          any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not in [float16, float32, float64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1.6, -0.1, 1.5, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Rint()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.  0.  2.  2.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-2.0, -1.9, -1.8, -1.7, -1.6],</span>
<span class="sd">        ...                            [-2.0, -1.9, -1.8, -1.7, -1.6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-2. -2. -2. -2. -2.]</span>
<span class="sd">         [-2. -2. -2. -2. -2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Rint.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Select.html#mindspore.ops.Select">[docs]</a><span class="k">class</span> <span class="nc">Select</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The conditional tensor determines whether the corresponding element in the output must be</span>
<span class="sd">    selected from `x` (if True) or `y` (if False) based on the value of each</span>
<span class="sd">    element.</span>

<span class="sd">    It can be defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        x_i, &amp; \text{if } condition_i \\</span>
<span class="sd">        y_i, &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **condition** (Tensor[bool]) - The condition tensor, decides which element is chosen.</span>
<span class="sd">          The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">        - **x** (Tensor) - The first tensor to be selected and the shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">        - **y** (Tensor) - The second tensor to be selected and the shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `condition`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of the three inputs are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; select = ops.Select()</span>
<span class="sd">        &gt;&gt;&gt; input_cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor([1,2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = select(input_cond, input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Select.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;condition&#39;</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">StridedSliceV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    StridedSliceV2 will be deprecated by StridedSlice in the future.</span>
<span class="sd">    Extracts a strided slice of a tensor.</span>
<span class="sd">    Refer to class StridedSlice for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_mask (int): Starting index of the slice. Default: ``0`` .</span>
<span class="sd">        end_mask (int): Ending index of the slice. Default: ``0`` .</span>
<span class="sd">        ellipsis_mask (int): An int mask. Default: ``0`` .</span>
<span class="sd">        new_axis_mask (int): An int mask. Default: ``0`` .</span>
<span class="sd">        shrink_axis_mask (int): An int mask. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor.</span>
<span class="sd">        - **begin** (tuple[int]) - A tuple which represents the location where to start. Only</span>
<span class="sd">          constant value is allowed.</span>
<span class="sd">        - **end** (tuple[int]) - A tuple or which represents the maximum location where to end.</span>
<span class="sd">          Only constant value is allowed.</span>
<span class="sd">        - **strides** (tuple[int]) - A tuple which represents the stride is continuously added</span>
<span class="sd">          before reaching the maximum location. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, The output is explained by following example.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin_mask`, `end_mask`, `ellipsis_mask`, `new_axis_mask` or `shrink_axis_mask` is not an int.</span>
<span class="sd">        TypeError: If `begin`, `end` or `strides` is not a tuple.</span>
<span class="sd">        ValueError: If `begin_mask`, `end_mask`, `ellipsis_mask`, `new_axis_mask` or `shrink_axis_mask` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">        ...                   [[5, 5, 5], [6, 6, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; strided_slice_v2 = ops.StridedSliceV2()</span>
<span class="sd">        &gt;&gt;&gt; output = strided_slice_v2(input_x, (1, 0, 2), (3, 1, 3), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3.]]</span>
<span class="sd">         [[5.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">begin_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">end_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">ellipsis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">new_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize StridedSliceV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;begin&#39;</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">,</span> <span class="s1">&#39;strides&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="StridedSlice"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.StridedSlice.html#mindspore.ops.StridedSlice">[docs]</a><span class="k">class</span> <span class="nc">StridedSlice</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Extracts a strided slice of a tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.strided_slice` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_mask (int, optional): Starting index of the slice. Default: ``0`` .</span>
<span class="sd">        end_mask (int, optional): Ending index of the slice. Default: ``0`` .</span>
<span class="sd">        ellipsis_mask (int, optional): An int mask, ignore slicing operation when set to 1. Default: ``0`` .</span>
<span class="sd">        new_axis_mask (int, optional): An int mask for adding new dims. Default: ``0`` .</span>
<span class="sd">        shrink_axis_mask (int, optional): An int mask for shrinking dims. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor to be extracted from.</span>
<span class="sd">        - **begin** (tuple[int]) - A tuple which represents the location where to start.</span>
<span class="sd">          Only non-negative int is allowed.</span>
<span class="sd">        - **end** (tuple[int]) - A tuple or which represents the maximum location where to end.</span>
<span class="sd">          Only non-negative int is allowed.</span>
<span class="sd">        - **strides** (tuple[int]) - A tuple which represents the strides is continuously added</span>
<span class="sd">          before reaching the maximum location. Only int is allowed, it can be negative</span>
<span class="sd">          which results in reversed slicing.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, return the extracts a strided slice of a Tensor based on `begin/end` index and `strides`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">        ...                   [[5, 5, 5], [6, 6, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; #         [[[1. 1. 1.]</span>
<span class="sd">        &gt;&gt;&gt; #           [2. 2. 2.]]</span>
<span class="sd">        &gt;&gt;&gt; #</span>
<span class="sd">        &gt;&gt;&gt; #          [[3. 3. 3.]</span>
<span class="sd">        &gt;&gt;&gt; #           [4. 4. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; #</span>
<span class="sd">        &gt;&gt;&gt; #          [[5. 5. 5.]</span>
<span class="sd">        &gt;&gt;&gt; #           [6. 6. 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # In order to visually view the multi-dimensional array, write the above as follows</span>
<span class="sd">        &gt;&gt;&gt; #         [</span>
<span class="sd">        &gt;&gt;&gt; #             [</span>
<span class="sd">        &gt;&gt;&gt; #                 [1,1,1]</span>
<span class="sd">        &gt;&gt;&gt; #                 [2,2,2]</span>
<span class="sd">        &gt;&gt;&gt; #             ]</span>
<span class="sd">        &gt;&gt;&gt; #             [</span>
<span class="sd">        &gt;&gt;&gt; #                 [3,3,3]</span>
<span class="sd">        &gt;&gt;&gt; #                 [4,4,4]</span>
<span class="sd">        &gt;&gt;&gt; #             ]</span>
<span class="sd">        &gt;&gt;&gt; #             [</span>
<span class="sd">        &gt;&gt;&gt; #                 [5,5,5]</span>
<span class="sd">        &gt;&gt;&gt; #                 [6,6,6]</span>
<span class="sd">        &gt;&gt;&gt; #             ]</span>
<span class="sd">        &gt;&gt;&gt; #         ]</span>
<span class="sd">        &gt;&gt;&gt; strided_slice = ops.StridedSlice()</span>
<span class="sd">        &gt;&gt;&gt; output = strided_slice(input_x, (1, 0, 2), (3, 1, 3), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; # Take this &quot; output = strided_slice(input_x, (1, 0, 2), (3, 1, 3), (1, 1, 1)) &quot; as an example,</span>
<span class="sd">        &gt;&gt;&gt; # start = [1, 0, 2] , end = [3, 1, 3], stride = [1, 1, 1], Find a segment of (start, end),</span>
<span class="sd">        &gt;&gt;&gt; # note that end is an open interval</span>
<span class="sd">        &gt;&gt;&gt; # To facilitate understanding, this operator can be divided into three steps:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Calculation of the first dimension:</span>
<span class="sd">        &gt;&gt;&gt; # start = 1, end = 3, stride = 1, So can take 1st, 2nd rows, and then gets the final output at this time.</span>
<span class="sd">        &gt;&gt;&gt; # output_1th =</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [3,3,3]</span>
<span class="sd">        &gt;&gt;&gt; #         [4,4,4]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [5,5,5]</span>
<span class="sd">        &gt;&gt;&gt; #         [6,6,6]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Calculation of the second dimension</span>
<span class="sd">        &gt;&gt;&gt; # 2nd dimension, start = 0, end = 1, stride = 1. So only 0th rows can be taken, and the output at this time.</span>
<span class="sd">        &gt;&gt;&gt; # output_2nd =</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [3,3,3]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [5,5,5]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 3: Calculation of the third dimension</span>
<span class="sd">        &gt;&gt;&gt; # 3nd dimension,start = 2, end = 3, stride = 1, So can take 2th cols,</span>
<span class="sd">        &gt;&gt;&gt; # and you get the final output at this time.</span>
<span class="sd">        &gt;&gt;&gt; # output_3ed =</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [3]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [5]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final output after finishing is:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3.]]</span>
<span class="sd">         [[5.]]]</span>
<span class="sd">        &gt;&gt;&gt; # another example like :</span>
<span class="sd">        &gt;&gt;&gt; output = strided_slice(input_x, (1, 0, 0), (2, 1, 3), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">begin_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">end_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">ellipsis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">new_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize StridedSlice&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;begin&#39;</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">,</span> <span class="s1">&#39;strides&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">begin_mask</span><span class="p">,</span> <span class="s1">&#39;begin_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">end_mask</span><span class="p">,</span> <span class="s1">&#39;end_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">ellipsis_mask</span><span class="p">,</span> <span class="s1">&#39;ellipsis_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="nb">bin</span><span class="p">(</span><span class="n">ellipsis_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, only support one ellipsis in the index, but got </span><span class="si">{</span><span class="n">ellipsis_mask</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">new_axis_mask</span><span class="p">,</span> <span class="s1">&#39;new_axis_mask&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">shrink_axis_mask</span><span class="p">,</span> <span class="s1">&#39;shrink_axis_mask&#39;</span><span class="p">,</span>
                                         <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">):</span>
        <span class="n">begin_v</span><span class="p">,</span> <span class="n">begin_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_get_value</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="s1">&#39;begin&#39;</span><span class="p">)</span>
        <span class="n">end_v</span><span class="p">,</span> <span class="n">end_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_get_value</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">)</span>
        <span class="n">strides_v</span><span class="p">,</span> <span class="n">strides_len</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_and_get_value</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="s1">&#39;strides&#39;</span><span class="p">)</span>

        <span class="n">is_dynamic_tuple</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_is_none_in_tuple</span><span class="p">(</span><span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>
                            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_none_in_tuple</span><span class="p">(</span><span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>
                            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_none_in_tuple</span><span class="p">(</span><span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]))</span>
        <span class="n">is_dynamic</span> <span class="o">=</span> <span class="kc">None</span> <span class="ow">in</span> <span class="p">(</span><span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_dynamic</span> <span class="ow">and</span> <span class="p">(</span><span class="n">begin_len</span> <span class="o">!=</span> <span class="n">strides_len</span> <span class="ow">or</span> <span class="n">end_len</span> <span class="o">!=</span> <span class="n">strides_len</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, &#39;begin&#39;, &#39;end&#39; and &#39;strides&#39; must be the same length, but got &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;&#39;begin&#39; length: </span><span class="si">{</span><span class="n">begin_len</span><span class="si">}</span><span class="s2">, &#39;end&#39; length: </span><span class="si">{</span><span class="n">end_len</span><span class="si">}</span><span class="s2">, &#39;strides&#39; length: </span><span class="si">{</span><span class="n">strides_len</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">is_dynamic</span> <span class="ow">or</span> <span class="n">is_dynamic_tuple</span> <span class="ow">or</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]):</span>
            <span class="n">ret_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dynamic_slicing_shape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">,</span> <span class="n">begin_len</span><span class="p">)</span>
            <span class="n">rets</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">ret_shape</span><span class="p">,</span>
                    <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                    <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>
            <span class="k">return</span> <span class="n">rets</span>

        <span class="n">ret_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_shape</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">ret_shape</span><span class="p">):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">init_func</span> <span class="o">=</span> <span class="n">Zero</span><span class="p">()</span>
            <span class="n">init_func</span><span class="o">.</span><span class="n">__enable_zero_dim__</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">element_type</span><span class="p">(),</span> <span class="n">shape</span><span class="o">=</span><span class="n">ret_shape</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="n">init_func</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;max_value&quot;</span> <span class="ow">in</span> <span class="n">x</span> <span class="ow">and</span> <span class="s2">&quot;min_value&quot;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;min_value&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;min_value&quot;</span><span class="p">],</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;max_value&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;max_value&quot;</span><span class="p">],</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">max_value_slice</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dynamic_slicing_value</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;max_value&quot;</span><span class="p">],</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">)</span>
            <span class="n">min_value_slice</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dynamic_slicing_value</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;min_value&quot;</span><span class="p">],</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">ret_shape</span><span class="p">,</span>
                    <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                    <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">,</span>
                    <span class="s1">&#39;max_value&#39;</span><span class="p">:</span> <span class="n">max_value_slice</span><span class="p">,</span>
                    <span class="s1">&#39;min_value&#39;</span><span class="p">:</span> <span class="n">min_value_slice</span><span class="p">}</span>

        <span class="k">if</span> <span class="s2">&quot;shape_value&quot;</span> <span class="ow">in</span> <span class="n">x</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape_value&quot;</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;shape_value&quot;</span><span class="p">],</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">shape_value_slice</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dynamic_slicing_value</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;shape_value&quot;</span><span class="p">],</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">ret_shape</span><span class="p">,</span>
                    <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                    <span class="s1">&#39;shape_value&#39;</span><span class="p">:</span> <span class="n">shape_value_slice</span><span class="p">,</span>
                    <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">ret_shape</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">value</span><span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_compute_slicing_len_for_positive_stride</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute slice length for positive stride.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">begin</span> <span class="o">&gt;=</span> <span class="n">end</span><span class="p">:</span>
                <span class="c1"># When slicing forward, if begin &gt;= end, the length of the slicing is 0.</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">return</span> <span class="n">slicing_length</span>
        <span class="c1"># When slicing forward, convert begin and end to positive numbers.</span>
        <span class="k">if</span> <span class="n">begin</span> <span class="o">&gt;=</span> <span class="n">x_dim</span> <span class="ow">or</span> <span class="n">end</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">x_dim</span><span class="p">:</span>
            <span class="c1"># When slicing forward, if begin &gt;= x_dim or end &lt; -x_dim, the length of the slicing is 0.</span>
            <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="o">-</span><span class="n">x_dim</span> <span class="o">&lt;=</span> <span class="n">begin</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">begin</span> <span class="o">+=</span> <span class="n">x_dim</span>
            <span class="k">if</span> <span class="n">begin</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">x_dim</span><span class="p">:</span>
                <span class="c1"># When slicing forward, if begin &lt; -x_dim, set begin = 0, which means start from the 0th element.</span>
                <span class="n">begin</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">if</span> <span class="o">-</span><span class="n">x_dim</span> <span class="o">&lt;=</span> <span class="n">end</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">end</span> <span class="o">+=</span> <span class="n">x_dim</span>
            <span class="k">if</span> <span class="n">end</span> <span class="o">&gt;</span> <span class="n">x_dim</span><span class="p">:</span>
                <span class="c1"># When slicing forward, if end &gt; x_dim, set end = x_dims, which means slice to the last element.</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">x_dim</span>
            <span class="k">if</span> <span class="n">begin</span> <span class="o">&gt;=</span> <span class="n">end</span><span class="p">:</span>
                <span class="c1"># When slicing forward, if begin &gt;= end, the length of the slicing is 0.</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">begin</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
        <span class="k">return</span> <span class="n">slicing_length</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_compute_slicing_len_for_negative_stride</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute slice length for negative stride.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x_dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">begin</span> <span class="o">&lt;=</span> <span class="n">end</span><span class="p">:</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">return</span> <span class="n">slicing_length</span>
        <span class="c1"># When slicing backward, convert begin and end to negative numbers.</span>
        <span class="k">if</span> <span class="n">begin</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">x_dim</span> <span class="ow">or</span> <span class="n">end</span> <span class="o">&gt;=</span> <span class="n">x_dim</span><span class="p">:</span>
            <span class="c1"># When slicing backward, if begin &lt; -x_dim or end &gt;= x_dim, the length of the slicing is 0.</span>
            <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">begin</span> <span class="o">&lt;</span> <span class="n">x_dim</span><span class="p">:</span>
                <span class="n">begin</span> <span class="o">+=</span> <span class="o">-</span><span class="n">x_dim</span>
            <span class="k">if</span> <span class="n">begin</span> <span class="o">&gt;=</span> <span class="n">x_dim</span><span class="p">:</span>
                <span class="n">begin</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">if</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">end</span> <span class="o">&lt;</span> <span class="n">x_dim</span><span class="p">:</span>
                <span class="n">end</span> <span class="o">+=</span> <span class="o">-</span><span class="n">x_dim</span>
            <span class="k">if</span> <span class="n">end</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">x_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="c1"># Slicing to the 0th element.</span>
                <span class="n">end</span> <span class="o">=</span> <span class="o">-</span><span class="n">x_dim</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">begin</span> <span class="o">&lt;=</span> <span class="n">end</span><span class="p">:</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">slicing_length</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">end</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">begin</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
        <span class="k">return</span> <span class="n">slicing_length</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_slice_value</span><span class="p">(</span><span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the slice value from value or shape_value.&quot;&quot;&quot;</span>
        <span class="n">begin_value</span> <span class="o">=</span> <span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">end_value</span> <span class="o">=</span> <span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">strides_value</span> <span class="o">=</span> <span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">begin_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">begin_value</span> <span class="o">=</span> <span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;shape_value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">end_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">end_value</span> <span class="o">=</span> <span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;shape_value&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">strides_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">strides_value</span> <span class="o">=</span> <span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;shape_value&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">begin_value</span><span class="p">,</span> <span class="n">end_value</span><span class="p">,</span> <span class="n">strides_value</span>

    <span class="k">def</span> <span class="nf">_is_none_in_tuple</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="kc">None</span> <span class="ow">in</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_compute_slicing_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the length of the slicing.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_len_for_positive_stride</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_len_for_negative_stride</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">slicing_length</span>

    <span class="k">def</span> <span class="nf">_compute_slicing_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the shape of the slicing.&quot;&quot;&quot;</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="n">slice_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">begin_v</span><span class="p">)</span>

        <span class="c1"># After the integer is converted to binary, it is a str and the first two chars are the flag char &#39;0b&#39;.</span>
        <span class="n">begin_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">begin_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">end_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">end_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">ellipsis_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ellipsis_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">new_axis_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_axis_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">shrink_axis_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shrink_axis_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">ret_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="n">has_ellipsis</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">x_rank</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">slice_len</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">slice_len</span><span class="p">:</span>
                <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">begin_v</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end_v</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">strides_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">ellipsis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ellipsis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="c1"># When there is ellipsis, the latter part of the ellipsis will be processed separately.</span>
                    <span class="n">has_ellipsis</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">begin_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">begin_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">begin</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">strides_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">end_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">end</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">strides_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_axis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">new_axis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">ret_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">shrink_axis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shrink_axis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="o">-</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">begin</span> <span class="o">&lt;</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="ow">or</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;strides[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39; cannot be negative number and &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;&#39;begin[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39; must be in [-</span><span class="si">{</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">) &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;when &#39;shrink_axis_mask&#39; is greater than 0, &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;but got &#39;shrink_axis_mask&#39;: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shrink_axis_mask</span><span class="si">}</span><span class="s2">, &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;&#39;strides[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39;: </span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2">, &#39;begin[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39;: </span><span class="si">{</span><span class="n">begin</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span>

            <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_length</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">ret_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slicing_length</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">has_ellipsis</span><span class="p">:</span>
            <span class="c1"># When there is ellipsis, handle the second half of the ellipsis split.</span>
            <span class="n">ellipsis_occupied_dims</span> <span class="o">=</span> <span class="n">x_rank</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="p">(</span><span class="n">slice_len</span> <span class="o">-</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> \
                                     <span class="nb">len</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="n">new_axis_pos</span><span class="p">[</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">slice_len</span><span class="p">])))</span>
            <span class="n">ret_shape</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">ellipsis_occupied_dims</span><span class="p">])</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="n">ellipsis_occupied_dims</span>

            <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">x_rank</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">slice_len</span><span class="p">:</span>
                <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">begin_v</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end_v</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">strides_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">begin_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">begin_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">begin</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">strides_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">end_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">end</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">strides_v</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_axis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">new_axis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">ret_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">shrink_axis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shrink_axis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="o">-</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">begin</span> <span class="o">&lt;</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="ow">or</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;strides[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39; can not be negative number and &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;&#39;begin[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39; must be in [-</span><span class="si">{</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">) &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;when &#39;shrink_axis_mask&#39; is greater than 0, &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;but got &#39;shrink_axis_mask&#39;: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">shrink_axis_mask</span><span class="si">}</span><span class="s2">, &quot;</span>
                                         <span class="sa">f</span><span class="s2">&quot;&#39;strides[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39;: </span><span class="si">{</span><span class="n">stride</span><span class="si">}</span><span class="s2">, &#39;begin[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&#39;: </span><span class="si">{</span><span class="n">begin</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>

                <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_length</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">ret_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slicing_length</span><span class="p">)</span>
                <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">ret_shape</span>

    <span class="k">def</span> <span class="nf">_compute_dynamic_slicing_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape_value</span><span class="p">,</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the length of the slicing for dynamic shape.&quot;&quot;&quot;</span>
        <span class="n">shape_value_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">shape_value</span><span class="p">)</span>
        <span class="n">slice_index</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">begin_i</span><span class="p">,</span> <span class="n">end_i</span><span class="p">,</span> <span class="n">strides_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">begin_i</span><span class="p">,</span> <span class="n">end_i</span><span class="p">,</span> <span class="n">strides_i</span><span class="p">)</span>
            <span class="n">slice_index</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">slice_index</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">slice_index</span><span class="p">)</span>
        <span class="n">shape_value_slice</span> <span class="o">=</span> <span class="n">shape_value_np</span><span class="p">[</span><span class="n">slice_index</span><span class="p">]</span>
        <span class="n">shape_value_slice</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shape_value_slice</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">shape_value_slice</span>

    <span class="k">def</span> <span class="nf">_compute_dynamic_slicing_length</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the length of the slicing for dynamic shape.&quot;&quot;&quot;</span>
        <span class="n">slicing_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">if</span> <span class="kc">None</span> <span class="ow">in</span> <span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span> <span class="ow">or</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">slicing_length</span>
        <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_length</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_dim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">slicing_length</span>

    <span class="k">def</span> <span class="nf">_compute_dynamic_slicing_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">,</span> <span class="n">slice_len</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the shape of the slicing for dynamic shape, mask is currently not supported.&quot;&quot;&quot;</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">is_dim_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">x_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="n">new_axis_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">new_axis_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">shrink_axis_pos</span> <span class="o">=</span> <span class="nb">bin</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shrink_axis_mask</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ellipsis_mask</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Ellipsis Mask is currently not supported in dynamic shape.&quot;</span><span class="p">)</span>
        <span class="n">ret_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="n">slice_has_special_value</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">begin_value</span><span class="p">,</span> <span class="n">end_value</span><span class="p">,</span> <span class="n">strides_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_slice_value</span><span class="p">(</span>
            <span class="n">begin_v</span><span class="p">,</span> <span class="n">end_v</span><span class="p">,</span> <span class="n">strides_v</span><span class="p">)</span>
        <span class="n">is_dynamic_tuple</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_is_none_in_tuple</span><span class="p">(</span><span class="n">begin_value</span><span class="p">)</span>
                            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_none_in_tuple</span><span class="p">(</span><span class="n">end_value</span><span class="p">)</span>
                            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_none_in_tuple</span><span class="p">(</span><span class="n">strides_value</span><span class="p">))</span>
        <span class="k">if</span> <span class="kc">None</span> <span class="ow">in</span> <span class="p">(</span><span class="n">begin_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">end_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">strides_v</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">])</span> <span class="ow">or</span> <span class="n">is_dynamic_tuple</span><span class="p">:</span>
            <span class="n">slice_has_special_value</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">x_rank</span> <span class="ow">or</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">slice_len</span><span class="p">:</span>
            <span class="n">slicing_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">slice_len</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_axis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">new_axis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">ret_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">shrink_axis_pos</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shrink_axis_pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span>
                    <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="kc">None</span> <span class="ow">in</span> <span class="p">(</span><span class="n">begin_value</span><span class="p">,</span> <span class="n">end_value</span><span class="p">,</span> <span class="n">strides_value</span><span class="p">):</span>
                    <span class="n">slicing_length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                <span class="k">elif</span> <span class="n">slice_has_special_value</span><span class="p">:</span>
                    <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dynamic_slicing_length</span><span class="p">(</span>
                        <span class="n">begin_value</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end_value</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">strides_value</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">slicing_length</span> <span class="o">=</span> \
                        <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_length</span><span class="p">(</span><span class="n">begin_value</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end_value</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">strides_value</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;StridedSlice&#39;, the index must be less than or equal to &quot;</span>
                                     <span class="sa">f</span><span class="s2">&quot;the dimension of &#39;input_x&#39;, but got the dimension of &#39;input_x&#39;: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
                                     <span class="sa">f</span><span class="s2">&quot;and the index: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
                <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">end</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">slicing_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_slicing_length</span><span class="p">(</span><span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">ret_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">slicing_length</span><span class="p">)</span>
            <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">ret_shape</span>

    <span class="k">def</span> <span class="nf">_check_and_get_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">slice_input</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check begin, end, strides. Get its length and value.&quot;&quot;&quot;</span>
        <span class="n">slice_value</span> <span class="o">=</span> <span class="n">slice_input</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">slice_min</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">slice_max</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">slice_special_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s2">&quot;min_value&quot;</span> <span class="ow">in</span> <span class="n">slice_input</span> <span class="ow">and</span> <span class="s2">&quot;max_value&quot;</span> <span class="ow">in</span> <span class="n">slice_input</span><span class="p">:</span>
            <span class="n">slice_min</span> <span class="o">=</span> <span class="n">slice_input</span><span class="p">[</span><span class="s2">&quot;min_value&quot;</span><span class="p">]</span>
            <span class="n">slice_max</span> <span class="o">=</span> <span class="n">slice_input</span><span class="p">[</span><span class="s2">&quot;max_value&quot;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="s2">&quot;shape_value&quot;</span> <span class="ow">in</span> <span class="n">slice_input</span><span class="p">:</span>
            <span class="n">slice_special_value</span> <span class="o">=</span> <span class="n">slice_input</span><span class="p">[</span><span class="s2">&quot;shape_value&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">slice_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">slice_input</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">slice_shape</span> <span class="o">=</span> <span class="n">slice_input</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">slice_shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, both the &#39;begins&#39;, &#39;ends&#39;, and &#39;strides&#39; must be 1-D, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; shape: </span><span class="si">{</span><span class="n">slice_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="c1"># not support scalar</span>
            <span class="n">slices</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">slice_value</span><span class="p">,</span>
                <span class="s1">&#39;shape_value&#39;</span><span class="p">:</span> <span class="n">slice_special_value</span><span class="p">,</span>
                <span class="s1">&#39;min_value&#39;</span><span class="p">:</span> <span class="n">slice_min</span><span class="p">,</span>
                <span class="s1">&#39;max_value&#39;</span><span class="p">:</span> <span class="n">slice_max</span>
            <span class="p">}</span>
            <span class="k">return</span> <span class="n">slices</span><span class="p">,</span> <span class="n">slice_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_value</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">slice_input</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">slice_value</span> <span class="o">=</span> <span class="n">slice_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">slice_value</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, both the &#39;begin&#39;, &#39;end&#39;, and &#39;strides&#39; must be a tuple or Tensor, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got &#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">slice_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">),</span> <span class="n">slice_value</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the elements of &#39;begin&#39;, &#39;end&#39;, and &#39;strides&#39; must be int, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">slice_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s1">&#39;strides&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">slice_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_value</span><span class="p">)):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, &#39;strides&#39; cannot contain 0, but got &#39;strides&#39;: </span><span class="si">{</span><span class="n">slice_value</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">slices</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="n">slice_value</span><span class="p">,</span>
            <span class="s1">&#39;shape_value&#39;</span><span class="p">:</span> <span class="n">slice_special_value</span><span class="p">,</span>
            <span class="s1">&#39;min_value&#39;</span><span class="p">:</span> <span class="n">slice_min</span><span class="p">,</span>
            <span class="s1">&#39;max_value&#39;</span><span class="p">:</span> <span class="n">slice_max</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">slices</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">slice_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="Diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Diag.html#mindspore.ops.Diag">[docs]</a><span class="k">class</span> <span class="nc">Diag</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Constructs a diagonal tensor with a given diagonal values.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.diag` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; diag = ops.Diag()</span>
<span class="sd">        &gt;&gt;&gt; output = diag(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0 0 0]</span>
<span class="sd">         [0 2 0 0]</span>
<span class="sd">         [0 0 3 0]</span>
<span class="sd">         [0 0 0 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Diag&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># do constant-folding only when x rank is 1</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DiagPart</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">    Extracts the diagonal elements from the given Tensor.</span>

<span class="sd">    If the `input_x` is a Tensor of shape :math:`[D_1,..., D_k, D_1,..., D_k]`, then the</span>
<span class="sd">    output will be a Tensor of rank k of shape :math:`[D_1,..., D_k]` where:</span>
<span class="sd">    :math:`output[i_1,..., i_k] = input_x[i_1,..., i_k, i_1,..., i_k]`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The rank of input tensor is 2k(k &gt; 0).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the extracted diagonal has the same dtype as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If rank of `input_x` is not even or zero.</span>
<span class="sd">        ValueError: If input_shape[i] is not equal to input_shape[i + len(input_shape)/2].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1, 0, 0, 0],</span>
<span class="sd">        ...                   [0, 2, 0, 0],</span>
<span class="sd">        ...                   [0, 0, 3, 0],</span>
<span class="sd">        ...                   [0, 0, 0, 4]])</span>
<span class="sd">        &gt;&gt;&gt; diag_part = ops.DiagPart()</span>
<span class="sd">        &gt;&gt;&gt; output = diag_part(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DiagPart&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># do constant-folding only when x rank is 2</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>


<div class="viewcode-block" id="Mvlgamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Mvlgamma.html#mindspore.ops.Mvlgamma">[docs]</a><span class="k">class</span> <span class="nc">Mvlgamma</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the multivariate log-gamma function element-wise for a given dimension `p`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.mvlgamma` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p(int): The number of dimensions. And the value of `p` must be greater than or equal to 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The tensor to compute the multivariate log-gamma function,</span>
<span class="sd">          which must be one of the following types: float32, float64.</span>
<span class="sd">          The shape is :math:`(N,*)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">          And the value of any element in `x` must be greater than :math:`(p - 1) / 2`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 5], [4, 2, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Mvlgamma(p=3)</span>
<span class="sd">        &gt;&gt;&gt; y = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.694925   5.402975   9.140645 ]</span>
<span class="sd">         [ 5.402975   1.5963125 13.640454 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Mvlgamma.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Eye"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Eye.html#mindspore.ops.Eye">[docs]</a><span class="k">class</span> <span class="nc">Eye</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with ones on the diagonal and zeros in the rest.</span>

<span class="sd">    Refer to :func:`mindspore.ops.eye` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **n** (int) - The number of rows of returned tensor. Constant value only.</span>
<span class="sd">        - **m** (int) - The number of columns of returned tensor. Constant value only.</span>
<span class="sd">        - **t** (mindspore.dtype) - MindSpore&#39;s dtype, the data type of the returned tensor.</span>
<span class="sd">          The data type can be bool or Number.</span>
<span class="sd">          Default: ``None`` , the data type of the returned tensor is mindspore.float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor with ones on the diagonal and the rest of elements are zero. The shape of `output` depends on</span>
<span class="sd">        the user&#39;s Inputs `n` and `m`. And the data type depends on Inputs `t`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; eye = ops.Eye()</span>
<span class="sd">        &gt;&gt;&gt; output = eye(2, 2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0]</span>
<span class="sd">         [0 1]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; output = eye(1, 2, mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float64</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Eye&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;t&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ScatterNd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNd.html#mindspore.ops.ScatterNd">[docs]</a><span class="k">class</span> <span class="nc">ScatterNd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a tensor into a new tensor depending on the specified indices.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - The index of scattering in the new tensor with int32 or int64 data type.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices_shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The source Tensor to be scattered.</span>
<span class="sd">          It has shape `indices_shape[:-1] + shape[indices_shape[-1]:]`.</span>
<span class="sd">        - **shape** (tuple[int]) - Define the shape of the output tensor, has the same data type as indices.</span>
<span class="sd">          The shape of `shape` is :math:`(x_1, x_2, ..., x_R)`, and the length of &#39;shape&#39; is greater than or equal to 2.</span>
<span class="sd">          In other words, the shape of `shape` is at least :math:`(x_1, x_2)`.</span>
<span class="sd">          And the value of any element in `shape` must be greater than or equal to 1.</span>
<span class="sd">          In other words, :math:`x_1` &gt;= 1, :math:`x_2` &gt;= 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the new tensor, has the same type as `update` and the same shape as `shape`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ScatterNd()</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (4, 4, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = op(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]</span>
<span class="sd">         [[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([3.2, 1.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = op(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; # In order to facilitate understanding, explain the operator pseudo-operation process step by step:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Generate an empty Tensor of the specified shape according to the shape</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Modify the data at the specified location according to the indicators</span>
<span class="sd">        &gt;&gt;&gt; # 0th row of indices is [0, 1], 0th row of updates is 3.2.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 0th row and 1st col set to 3.2</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # 1th row of indices is [1, 1], 1th row of updates is 1.1.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 1th row and 1st col set to 1.1</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 1.1  0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final result is as follows:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 3.2 0.]</span>
<span class="sd">         [0. 1.1 0.]</span>
<span class="sd">         [0. 0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterNd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;update&#39;</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ResizeNearestNeighbor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ResizeNearestNeighbor.html#mindspore.ops.ResizeNearestNeighbor">[docs]</a><span class="k">class</span> <span class="nc">ResizeNearestNeighbor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor to a given size by using the nearest neighbor algorithm. The nearest</span>
<span class="sd">    neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union[tuple, list]): The target size. The dimension of size must be 2.</span>
<span class="sd">        align_corners (bool): Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">                              and output tensors are aligned. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape of the tensor is :math:`(N, C, H, W)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is  :math:`(N, C, NEW\_H, NEW\_W)`.</span>
<span class="sd">        The data type is the same as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is neither tuple nor list.</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        ValueError: If length of `size` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[[[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ResizeNearestNeighbor(size=size)(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[-0.1  0.3]</span>
<span class="sd">           [ 0.4  0.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ResizeNearestNeighbor&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;length of size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">th value of size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;image_in&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;image_out&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ResizeNearestNeighborV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ResizeNearestNeighborV2.html#mindspore.ops.ResizeNearestNeighborV2">[docs]</a><span class="k">class</span> <span class="nc">ResizeNearestNeighborV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor to specific size by using the nearest neighbor algorithm.</span>

<span class="sd">    The nearest neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , the centers of the 4 corner pixels of the input and output</span>
<span class="sd">            tensors are aligned, preserving the values at the corner pixels. Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether half pixel center. If set to ``True`` ,</span>
<span class="sd">            `align_corners` should be False. Default: ``False`` .</span>
<span class="sd">        data_format (str, optional): An optional `string` that describes the</span>
<span class="sd">            format of the input `x`. Default: ``NHWC`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - 4-D with shape :math:`(batch, height, width, channels)`</span>
<span class="sd">          or :math:`(batch, channels, height, width)` depending on the attr &#39;data_format&#39;. Support</span>
<span class="sd">          type [`int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `float16`, `float32`, `float64`].</span>
<span class="sd">        - **size** (Tensor) - The new size for the images. A 1-D int32 Tensor</span>
<span class="sd">          of 2 elements: [`new_height, new_width`].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - The resized images. A 4-D with shape</span>
<span class="sd">          :math:`(batch, new\_height, new\_width, channels)`</span>
<span class="sd">          or :math:`(batch, channels, new\_height, new\_width)`</span>
<span class="sd">          depending on the attr `data_format`. It has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `size` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type  of `x` is not in supported list.</span>
<span class="sd">        TypeError: If the data type  of `size` is not int32.</span>
<span class="sd">        TypeError: If `align_corners` or `half_pixel_centers` is not bool.</span>
<span class="sd">        TypeError: If `data_format` is not string.</span>
<span class="sd">        ValueError: If `data_format` not in [`NHWC`, `NCHW`].</span>
<span class="sd">        ValueError: If any value of `size` is non positive.</span>
<span class="sd">        ValueError: If the dimension of `x` is not 4.</span>
<span class="sd">        ValueError: If the dimension of `size` is not 1.</span>
<span class="sd">        ValueError: If the elements number of `size` is not 2.</span>
<span class="sd">        ValueError: If attr `half_pixel_centers` and `align_corners` are True at the same time.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones((1, 4, 4, 1)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = Tensor([2, 2], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; resize = ops.ResizeNearestNeighborV2()</span>
<span class="sd">        &gt;&gt;&gt; output = resize(input_tensor, size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.]</span>
<span class="sd">           [1.]]</span>
<span class="sd">          [[1.]</span>
<span class="sd">           [1.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 2, 2, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ResizeNearestNeighborV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">align_corners</span><span class="p">,</span> <span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">half_pixel_centers</span><span class="p">,</span> <span class="s1">&#39;half_pixel_centers&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span></div>


<div class="viewcode-block" id="GatherNd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GatherNd.html#mindspore.ops.GatherNd">[docs]</a><span class="k">class</span> <span class="nc">GatherNd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers slices from a tensor by indices.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather_nd` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor to gather values.</span>
<span class="sd">        - **indices** (Tensor) - The index tensor, with int32 or int64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as `input_x` and the shape is indices_shape[:-1] + x_shape[indices_shape[-1]:].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; op = ops.GatherNd()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GatherNd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ScatterUpdate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterUpdate.html#mindspore.ops.ScatterUpdate">[docs]</a><span class="k">class</span> <span class="nc">ScatterUpdate</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates tensor values by using input indices and value.</span>

<span class="sd">    Using given values to update tensor value, along with the input indices.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] = \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is 0-D or :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor. With int32 data type.</span>
<span class="sd">          If there are duplicates in indices, the order for updating is undefined.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates.shape = indices.shape + input_x.shape[1:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; np_x = np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]])</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Parameter(Tensor(np_x, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; np_updates = np.array([[2.0, 1.2, 1.0], [3.0, 1.2, 1.0]])</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np_updates, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ScatterUpdate()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 1.2  1.]</span>
<span class="sd">         [3. 1.2  1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterUpdate&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterNdUpdate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdUpdate.html#mindspore.ops.ScatterNdUpdate">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdUpdate</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates tensor values by using input indices and value.</span>

<span class="sd">    Using given values to update tensor value, along with the input indices.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`, and its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor, with int32 or int64 data type.</span>
<span class="sd">        - **updates** (Tensor) - N-D(2D or 3D) Tensor The tensor to be updated to the input tensor,</span>
<span class="sd">          has the same type as input. The shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; np_x = np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]])</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Parameter(Tensor(np_x, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ScatterNdUpdate()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.   0.3   3.6]</span>
<span class="sd">         [0.4  2.2  -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterNdUpdate&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterMax.html#mindspore.ops.ScatterMax">[docs]</a><span class="k">class</span> <span class="nc">ScatterMax</span><span class="p">(</span><span class="n">_ScatterOpDynamic</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the maximum operation.</span>

<span class="sd">    Using given values to update tensor value through the max operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :]</span>
<span class="sd">        = \max(\text{input_x}[\text{indices}[i, ..., j], :], \text{updates}[i, ..., j, :])</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type. A RuntimeError will be reported</span>
<span class="sd">    when `updates` does not support conversion to the data type required by `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do max operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64.</span>
<span class="sd">        - **updates** (Tensor) - The tensor that performs the maximum operation with `input_x`,</span>
<span class="sd">          the data type is the same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32),</span>
<span class="sd">        ...                     name=&quot;input_x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.ones([2, 2, 3]) * 88, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_max = ops.ScatterMax()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_max(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[88. 88. 88.]</span>
<span class="sd">         [88. 88. 88.]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ScatterMin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterMin.html#mindspore.ops.ScatterMin">[docs]</a><span class="k">class</span> <span class="nc">ScatterMin</span><span class="p">(</span><span class="n">_ScatterOpDynamic</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the minimum operation.</span>

<span class="sd">    Using given values to update tensor value through the min operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :]</span>
<span class="sd">        = \min(\text{input_x}[\text{indices}[i, ..., j], :], \text{updates}[i, ..., j, :])</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type. A RuntimeError will be reported</span>
<span class="sd">    when `updates` does not support conversion to the data type required by `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do min operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the min operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 1.0, 2.0], [0.0, 0.0, 0.0]]), mindspore.float32),</span>
<span class="sd">        ...                     name=&quot;input_x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; update = Tensor(np.ones([2, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_min = ops.ScatterMin()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_min(input_x, indices, update)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 1.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ScatterAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterAdd.html#mindspore.ops.ScatterAdd">[docs]</a><span class="k">class</span> <span class="nc">ScatterAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the addition operation.</span>

<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{+}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        This is an in-place update operator. Therefore, the `input_x` will be updated after the operation is completed.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock.</span>
<span class="sd">            If ``True`` , `input_x` will be protected by the lock.</span>
<span class="sd">            Otherwise, the calculation result is undefined. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">        - **indices** (Tensor) - The index to do min operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the min operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape + x.shape[1:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.ones([2, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_add = ops.ScatterAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1. 1.]</span>
<span class="sd">         [3. 3. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [0.0, 0.0, 0.0] + [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [0.0, 0.0, 0.0] + [3.0, 3.0, 3.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [3.0, 3.0, 3.0] + [7.0, 7.0, 7.0] = [10.0, 10.0, 10.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [10.0, 10.0, 10.0] + [9.0, 9.0, 9.0] = [19.0, 19.0, 19.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_add = ops.ScatterAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  1.  1.]</span>
<span class="sd">         [19. 19. 19.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [0.0, 0.0, 0.0] + [3.0, 3.0, 3.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [0.0, 0.0, 0.0] + [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [1.0, 1.0, 1.0] + [7.0, 7.0, 7.0] = [8.0, 8.0, 8.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [8.0, 8.0, 8.0] + [9.0, 9.0, 9.0] = [17.0, 17.0, 17.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_add = ops.ScatterAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 3.  3.  3.]</span>
<span class="sd">         [17. 17. 17.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [0.0, 0.0, 0.0] + [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [0.0, 0.0, 0.0] + [3.0, 3.0, 3.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] + [7.0, 7.0, 7.0] = [8.0, 8.0, 8.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [3.0, 3.0, 3.0] + [9.0, 9.0, 9.0] = [12.0, 12.0, 12.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [0, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_add = ops.ScatterAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 8.  8.  8.]</span>
<span class="sd">         [12. 12. 12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterAdd&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterSub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterSub.html#mindspore.ops.ScatterSub">[docs]</a><span class="k">class</span> <span class="nc">ScatterSub</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the subtraction operation.</span>

<span class="sd">    Using given values to update tensor value through the subtraction operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{-}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do min operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the min operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices_shape + x_shape[1:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices_shape + x_shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_sub = ops.ScatterSub()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1. -1. -1.]</span>
<span class="sd">         [-1. -1. -1.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [0.0, 0.0, 0.0] - [1.0, 1.0, 1.0] = [-1.0, -1.0, -1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [0.0, 0.0, 0.0] - [3.0, 3.0, 3.0] = [-3.0, -3.0, -3.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [-3.0, -3.0, -3.0] - [7.0, 7.0, 7.0] = [-10.0, -10.0, -10.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [-10.0, -10.0, -10.0] - [9.0, 9.0, 9.0] = [-19.0, -19.0, -19.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_sub = ops.ScatterSub()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ -1.  -1.  -1.]</span>
<span class="sd">         [-19. -19. -19.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [0.0, 0.0, 0.0] - [3.0, 3.0, 3.0] = [-3.0, -3.0, -3.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [0.0, 0.0, 0.0] - [1.0, 1.0, 1.0] = [-1.0, -1.0, -1.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [-1.0, -1.0, -1.0] - [7.0, 7.0, 7.0] = [-8.0, -8.0, -8.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [-8.0, -8.0, -8.0] - [9.0, 9.0, 9.0] = [-17.0, -17.0, -17.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_sub = ops.ScatterSub()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ -3.  -3.  -3.]</span>
<span class="sd">         [-17. -17. -17.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [0.0, 0.0, 0.0] - [1.0, 1.0, 1.0] = [-1.0, -1.0, -1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [0.0, 0.0, 0.0] - [3.0, 3.0, 3.0] = [-3.0, -3.0, -3.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [-1.0, -1.0, -1.0] - [7.0, 7.0, 7.0] = [-8.0, -8.0, -8.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [-3.0, -3.0, -3.0] - [9.0, 9.0, 9.0] = [-12.0, -12.0, -12.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [0, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_sub = ops.ScatterSub()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ -8.  -8.  -8.]</span>
<span class="sd">         [-12. -12. -12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterSub&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Triu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Triu.html#mindspore.ops.Triu">[docs]</a><span class="k">class</span> <span class="nc">Triu</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the upper triangular portion of the 2-D matrix or the set of matrices</span>
<span class="sd">    in a batch. The remaining elements of the resulting Tensor are assigned a value of 0.</span>
<span class="sd">    The upper triangular section of the matrix comprises of the</span>
<span class="sd">    elements present on and above the main diagonal.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        diagonal (int, optional): The index of diagonal. Default: ``0`` , indicating the main diagonal.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor with shape :math:`(M, N, *)`</span>
<span class="sd">          where :math:`*` means any number of additional dimensions. The data type is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor has the same shape and data type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not an Tensor.</span>
<span class="sd">        TypeError: If `diagonal` is not an int.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; triu = ops.Triu()</span>
<span class="sd">        &gt;&gt;&gt; result = triu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  2  3  4]</span>
<span class="sd">         [ 0  6  7  8]</span>
<span class="sd">         [ 0  0 12 13]</span>
<span class="sd">         [ 0  0  0 17]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; triu = ops.Triu(diagonal=1)</span>
<span class="sd">        &gt;&gt;&gt; result = triu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 0  2  3  4]</span>
<span class="sd">         [ 0  0  7  8]</span>
<span class="sd">         [ 0  0  0 13]</span>
<span class="sd">         [ 0  0  0  0]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; triu = ops.Triu(diagonal=-1)</span>
<span class="sd">        &gt;&gt;&gt; result = triu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  2  3  4]</span>
<span class="sd">         [ 5  6  7  8]</span>
<span class="sd">         [ 0 11 12 13]</span>
<span class="sd">         [ 0  0 16 17]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Triu&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;diagonal&quot;</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">diagonal</span> <span class="o">=</span> <span class="n">diagonal</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ScatterMul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterMul.html#mindspore.ops.ScatterMul">[docs]</a><span class="k">class</span> <span class="nc">ScatterMul</span><span class="p">(</span><span class="n">_ScatterOpDynamic</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the multiply operation.</span>

<span class="sd">    Using given values to update tensor value through the mul operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{*}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do multiply operation whose data type must be mstype.int32 or</span>
<span class="sd">          mstype.int64.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the multiply operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_mul = ops.ScatterMul()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 2. 2.]</span>
<span class="sd">         [4. 4. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [3.0, 3.0, 3.0] = [6.0, 6.0, 6.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [6.0, 6.0, 6.0] * [7.0, 7.0, 7.0] = [42.0, 42.0, 42.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [42.0, 42.0, 42.0] * [9.0, 9.0, 9.0] = [378.0, 378.0, 378.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_mul = ops.ScatterMul()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  1.   1.   1.]</span>
<span class="sd">         [378. 378. 378.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [3.0, 3.0, 3.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [1.0, 1.0, 1.0] = [2.0, 2.0, 2.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [7.0, 7.0, 7.0] = [14.0, 14.0, 14.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [14.0, 14.0, 14.0] * [9.0, 9.0, 9.0] = [126.0, 126.0, 126.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_mul = ops.ScatterMul()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  3.   3.   3.]</span>
<span class="sd">         [126. 126. 126.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [3.0, 3.0, 3.0] = [6.0, 6.0, 6.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [7.0, 7.0, 7.0] = [7.0, 7.0, 7.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [6.0, 6.0, 6.0] * [9.0, 9.0, 9.0] = [54.0, 54.0, 54.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_mul = ops.ScatterMul()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 7.  7.  7.]</span>
<span class="sd">         [54. 54. 54.]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ScatterDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterDiv.html#mindspore.ops.ScatterDiv">[docs]</a><span class="k">class</span> <span class="nc">ScatterDiv</span><span class="p">(</span><span class="n">_ScatterOpDynamic</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input tensor through the divide operation.</span>

<span class="sd">    Using given values to update tensor value through the div operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{/}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type. A RuntimeError will be reported</span>
<span class="sd">    when `updates` does not support conversion to the data type required by `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do divide operation whose data type must be mstype.int32 or</span>
<span class="sd">          mstype.int64.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the divide operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[6.0, 6.0, 6.0], [2.0, 2.0, 2.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_div = ops.ScatterDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 3. 3.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [1.0, 1.0, 1.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [3.0, 3.0, 3.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [105.0, 105.0, 105.0] / [5.0, 5.0, 5.0] = [21.0, 21.0, 21.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [21.0, 21.0, 21.0] / [7.0, 7.0, 7.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_div = ops.ScatterDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[105. 105. 105.]</span>
<span class="sd">         [  3.   3.   3.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [3.0, 3.0, 3.0] = [35.0, 35.0, 35.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [1.0, 1.0, 1.0] = [315.0, 315.0, 315.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [5.0, 5.0, 5.0] = [63.0 63.0 63.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [63.0 63.0 63.0] / [7.0, 7.0, 7.0] = [9.0, 9.0, 9.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_div = ops.ScatterDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[35. 35. 35.]</span>
<span class="sd">         [ 9.  9.  9.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mstype.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [1.0, 1.0, 1.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [3.0, 3.0, 3.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [5.0, 5.0, 5.0] = [21.0, 21.0, 21.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [105.0, 105.0, 105.0] / [7.0, 7.0, 7.0] = [15.0, 15.0, 15.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_div = ops.ScatterDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[21. 21. 21.]</span>
<span class="sd">         [15. 15. 15.]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ScatterNdAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdAdd.html#mindspore.ops.ScatterNdAdd">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse addition to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd_add` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool, optional): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do add operation whose data type must be mindspore.int32.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the add operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_add = ops.ScatterNdAdd(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 10.  9.  4. 12.  6.  7. 17.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_add = ops.ScatterNdAdd(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _ScatterOp&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterNdSub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdSub.html#mindspore.ops.ScatterNdSub">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdSub</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse subtraction to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the subtraction operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd_sub` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool, optional): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">        - **indices** (Tensor) - The index to do sub operation whose data type must be mindspore.int32.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the sub operation with `input_x`,</span>
<span class="sd">          the data type is same as `input_x`, the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_sub = ops.ScatterNdSub(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. -6. -3.  4. -2.  6.  7. -1.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_sub = ops.ScatterNdSub(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-1 -1 -1 -1]</span>
<span class="sd">          [-2 -2 -2 -2]</span>
<span class="sd">          [-3 -3 -3 -3]</span>
<span class="sd">          [-4 -4 -4 -4]]</span>
<span class="sd">         [[ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]]</span>
<span class="sd">         [[-5 -5 -5 -5]</span>
<span class="sd">          [-6 -6 -6 -6]</span>
<span class="sd">          [-7 -7 -7 -7]</span>
<span class="sd">          [-8 -8 -8 -8]]</span>
<span class="sd">         [[ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterNdSub&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;use_locking&#39;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterNdMul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdMul.html#mindspore.ops.ScatterNdMul">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdMul</span><span class="p">(</span><span class="n">_ScatterNdOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse multiplication to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update parameter value through the multiplication operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd_mul` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool, optional): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">        - **indices** (Tensor) - The index to do mul operation whose data type must be int32 or int64.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to do the mul operation with `input_x`.</span>
<span class="sd">          The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_mul = ops.ScatterNdMul()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 18.  4. 35.  6.  7. 72.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_mul = ops.ScatterNdMul()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ScatterNdDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdDiv.html#mindspore.ops.ScatterNdDiv">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdDiv</span><span class="p">(</span><span class="n">_ScatterNdOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse division to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the division operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd_div` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool, optional): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target tensor, with data type of Parameter.</span>
<span class="sd">        - **indices** (Tensor) - The index to do div operation whose data type must be int32 or int64.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to do the div operation with `input_x`.</span>
<span class="sd">          The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_div = ops.ScatterNdDiv(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.         0.25       0.5        4.         0.71428573 6.</span>
<span class="sd">         7.         0.8888889 ]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_div = ops.ScatterNdDiv(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.         1.         1.         1.        ]</span>
<span class="sd">          [0.5        0.5        0.5        0.5       ]</span>
<span class="sd">          [0.33333334 0.33333334 0.33333334 0.33333334]</span>
<span class="sd">          [0.25       0.25       0.25       0.25      ]]</span>
<span class="sd">         [[1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]]</span>
<span class="sd">         [[0.2        0.2        0.2        0.2       ]</span>
<span class="sd">          [0.16666667 0.16666667 0.16666667 0.16666667]</span>
<span class="sd">          [0.14285715 0.14285715 0.14285715 0.14285715]</span>
<span class="sd">          [0.125      0.125      0.125      0.125     ]]</span>
<span class="sd">         [[1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="ScatterNdMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdMax.html#mindspore.ops.ScatterNdMax">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdMax</span><span class="p">(</span><span class="n">_ScatterNdOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse maximum to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update parameter value through the maximum operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd_max` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool, optional): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) -The target tensor, with data type of Parameter.</span>
<span class="sd">        - **indices** (Tensor) - The index to do maximum operation whose data type must be int32 or int64.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to do the max operation with `input_x`.</span>
<span class="sd">          The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_max = ops.ScatterNdMax()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_max(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 8. 6.  4. 7.  6.  7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_max = ops.ScatterNdMax()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_max(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterNdMax&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">use_locking</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterNdMin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNdMin.html#mindspore.ops.ScatterNdMin">[docs]</a><span class="k">class</span> <span class="nc">ScatterNdMin</span><span class="p">(</span><span class="n">_ScatterNdOp</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse minimum to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the minimum operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd_min` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool, optional): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) -The target tensor, with data type of Parameter.</span>
<span class="sd">        - **indices** (Tensor) - The index to do minimum operation whose data type must be int32 or int64.</span>
<span class="sd">          The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to do the max operation with `input_x`.</span>
<span class="sd">          The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones(8) * 10, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_min = ops.ScatterNdMin(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_min(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10.  8.  6. 10.  7. 10. 10.  9.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)) * 10, mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; use_locking = False</span>
<span class="sd">        &gt;&gt;&gt; scatter_nd_min = ops.ScatterNdMin(use_locking)</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_nd_min(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1  1  1  1]</span>
<span class="sd">          [ 2  2  2  2]</span>
<span class="sd">          [ 3  3  3  3]</span>
<span class="sd">          [ 4  4  4  4]]</span>
<span class="sd">         [[10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]]</span>
<span class="sd">         [[ 5  5  5  5]</span>
<span class="sd">          [ 6  6  6  6]</span>
<span class="sd">          [ 7  7  7  7]</span>
<span class="sd">          [ 8  8  8  8]]</span>
<span class="sd">         [[10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterNdMin&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">use_locking</span><span class="p">)</span></div>


<div class="viewcode-block" id="ScatterNonAliasingAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNonAliasingAdd.html#mindspore.ops.ScatterNonAliasingAdd">[docs]</a><span class="k">class</span> <span class="nc">ScatterNonAliasingAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse addition to the input using individual values or slices.</span>

<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Parameter) - The target parameter. The data type must be float16, float32 or int32.</span>
<span class="sd">        - **indices** (Tensor) - The index to perform the addition operation whose data type must be mindspore.int32.</span>
<span class="sd">        - **updates** (Tensor) - The tensor that performs the addition operation with `input_x`,</span>
<span class="sd">          the data type is the same as `input_x`, the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Parameter, the updated `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of float16, float32, int32.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scatter_non_aliasing_add = ops.ScatterNonAliasingAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = scatter_non_aliasing_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 10.  9.  4. 12.  6.  7. 17.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterNonAliasingAdd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SpaceToDepth"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SpaceToDepth.html#mindspore.ops.SpaceToDepth">[docs]</a><span class="k">class</span> <span class="nc">SpaceToDepth</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearrange blocks of spatial data into depth.</span>

<span class="sd">    The output tensor&#39;s `height` dimension is :math:`height / block\_size`.</span>

<span class="sd">    The output tensor&#39;s `weight` dimension is :math:`weight / block\_size`.</span>

<span class="sd">    The depth of output tensor is :math:`block\_size * block\_size * input\_depth`.</span>

<span class="sd">    The input tensor&#39;s height and width must be divisible by `block_size`.</span>
<span class="sd">    The data format is &quot;NCHW&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_size (int): The block size used to divide spatial data. It must be &gt;= 2.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The target tensor. The data type is Number. It must be a 4-D tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the same data type as `x`. It must be a 4-D tensor. Tensor of shape</span>
<span class="sd">        :math:`(N, (C_{in} * \text{block_size} * 2), H_{in} / \text{block_size}, W_{in} / \text{block_size})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_size` is not an int.</span>
<span class="sd">        ValueError: If `block_size` is less than 2.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(1,3,2,2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; block_size = 2</span>
<span class="sd">        &gt;&gt;&gt; space_to_depth = ops.SpaceToDepth(block_size)</span>
<span class="sd">        &gt;&gt;&gt; output = space_to_depth(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 12, 1, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SpaceToDepth&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="DepthToSpace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DepthToSpace.html#mindspore.ops.DepthToSpace">[docs]</a><span class="k">class</span> <span class="nc">DepthToSpace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearrange blocks of depth data into spatial dimensions.</span>

<span class="sd">    This is the reverse operation of SpaceToDepth.</span>

<span class="sd">    The depth of output tensor is :math:`input\_depth / (block\_size * block\_size)`.</span>

<span class="sd">    The output tensor&#39;s `height` dimension is :math:`height * block\_size`.</span>

<span class="sd">    The output tensor&#39;s `weight` dimension is :math:`weight * block\_size`.</span>

<span class="sd">    The input tensor&#39;s depth must be divisible by `block_size * block_size`.</span>
<span class="sd">    The data format is &quot;NCHW&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_size (int): The block size used to divide depth data. It must be &gt;= 2.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The target tensor. It must be a 4-D tensor with shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          The data type is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, C_{in} / \text{block_size} ^ 2, H_{in} * \text{block_size},</span>
<span class="sd">        W_{in} * \text{block_size})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_size` is not an int.</span>
<span class="sd">        ValueError: If `block_size` is less than 2.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(1, 12, 1, 1), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; block_size = 2</span>
<span class="sd">        &gt;&gt;&gt; depth_to_space = ops.DepthToSpace(block_size)</span>
<span class="sd">        &gt;&gt;&gt; output = depth_to_space(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 3, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DepthToSpace&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">SpaceToBatch</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SpaceToBatch is deprecated. Please use :class:`mindspore.ops.SpaceToBatchND` instead.</span>
<span class="sd">    Divides spatial dimensions into blocks and combines the block size with the original batch.</span>

<span class="sd">    This operation will divide spatial dimensions (H, W) into blocks with `block_size`, the output tensor&#39;s H and W</span>
<span class="sd">    dimension is the corresponding number of blocks after division. The output tensor&#39;s batch dimension is the</span>
<span class="sd">    product of the original batch and the square of block_size. Before division, the spatial dimensions</span>
<span class="sd">    of the input are zero padded according to paddings if necessary.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_size (int): The block size of dividing blocks with value greater than or equal to 2.</span>
<span class="sd">        paddings (Union[tuple, list]): The padding values for H and W dimension, containing 2 subtraction lists.</span>
<span class="sd">            Each subtraction list contains 2 integer value. All values must be greater than 0.</span>
<span class="sd">            paddings[i] specifies the paddings for the spatial dimension i, which corresponds to the</span>
<span class="sd">            input dimension i+2. It is required that input_shape[i+2]+paddings[i][0]+paddings[i][1]</span>
<span class="sd">            is divisible by block_size.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. It must be a 4-D tensor. The data type is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the output tensor with the same data type as input. Assume input shape is :math:`(n, c, h, w)` with</span>
<span class="sd">        :math:`block\_size` and :math:`paddings`. The shape of the output tensor will be :math:`(n&#39;, c&#39;, h&#39;, w&#39;)`,</span>
<span class="sd">        where</span>

<span class="sd">        :math:`n&#39; = n*(block\_size*block\_size)`</span>

<span class="sd">        :math:`c&#39; = c`</span>

<span class="sd">        :math:`h&#39; = (h+paddings[0][0]+paddings[0][1])//block\_size`</span>

<span class="sd">        :math:`w&#39; = (w+paddings[1][0]+paddings[1][1])//block\_size`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_size` is not an int.</span>
<span class="sd">        ValueError: If `block_size` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; block_size = 2</span>
<span class="sd">        &gt;&gt;&gt; paddings = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; space_to_batch = ops.SpaceToBatch(block_size, paddings)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, 2], [3, 4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = space_to_batch(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.]]]</span>
<span class="sd">         [[[2.]]]</span>
<span class="sd">         [[[3.]]]</span>
<span class="sd">         [[[4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SpaceToBatch&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN_DEPRECATED: The usage of SpaceToBatch is deprecated.&quot;</span>
                       <span class="s2">&quot; Please use SpaceToBatchND.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;paddings shape&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">paddings</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="s1">&#39;paddings element&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;paddings element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span>


<div class="viewcode-block" id="BatchToSpace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BatchToSpace.html#mindspore.ops.BatchToSpace">[docs]</a><span class="k">class</span> <span class="nc">BatchToSpace</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides batch dimension with blocks and interleaves these blocks back into spatial dimensions.</span>

<span class="sd">    This operation will divide batch dimension N into blocks with block_size, the output tensor&#39;s N dimension</span>
<span class="sd">    is the corresponding number of blocks after division. The output tensor&#39;s H, W dimension is product of</span>
<span class="sd">    original H, W dimension and block_size with given amount to crop from dimension, respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_size (int): The block size of division, has the value not less than 2.</span>
<span class="sd">        crops (Union[list(int), tuple(int)]): The crop value for H and W dimension, containing 2 subtraction lists.</span>
<span class="sd">            Each list contains 2 integers.</span>
<span class="sd">            All values must be not less than 0. crops[i] specifies the crop values for the spatial dimension i, which</span>
<span class="sd">            corresponds to the input dimension i+2. It is required that</span>
<span class="sd">            :math:`input\_shape[i+2]*block\_size &gt; crops[i][0]+crops[i][1]` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. It must be a 4-D tensor, dimension 0 must be divisible by</span>
<span class="sd">          product of `block_shape`. The data type is float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the output tensor with the same type as input. Assume input shape is :math:`(n, c, h, w)` with</span>
<span class="sd">        block_size and crops. The output shape will be :math:`(n&#39;, c&#39;, h&#39;, w&#39;)`, where</span>

<span class="sd">        :math:`n&#39; = n//(block\_size*block\_size)`</span>

<span class="sd">        :math:`c&#39; = c`</span>

<span class="sd">        :math:`h&#39; = h*block\_size-crops[0][0]-crops[0][1]`</span>

<span class="sd">        :math:`w&#39; = w*block\_size-crops[1][0]-crops[1][1]`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_size` or element of `crops` is not an int.</span>
<span class="sd">        TypeError: If `crops` is neither list nor tuple.</span>
<span class="sd">        ValueError: If `block_size` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; block_size = 2</span>
<span class="sd">        &gt;&gt;&gt; crops = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; batch_to_space = ops.BatchToSpace(block_size, crops)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1]]], [[[2]]], [[[3]]], [[[4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = batch_to_space(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.  2.]</span>
<span class="sd">           [3.  4.]]]]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">crops</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BatchToSpace&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN_DEPRECATED: The usage of BatchToSpace is deprecated.&quot;</span>
                       <span class="s2">&quot; Please use BatchToSpaceND.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;crops type&#39;</span><span class="p">,</span> <span class="n">crops</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;crops shape&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">crops</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">crops</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="s1">&#39;crops element&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;crops element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crops</span> <span class="o">=</span> <span class="n">crops</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;rank of input_x&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">x_block_prod</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>
            <span class="n">crops_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crops</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">crops</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x block shape prod&quot;</span><span class="p">,</span> <span class="n">x_block_prod</span><span class="p">,</span> <span class="s1">&#39;crops sum&#39;</span><span class="p">,</span> <span class="n">crops_sum</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">out_shape</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_block_prod</span> <span class="o">-</span> <span class="n">crops_sum</span>
        <span class="n">block_size_prod</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>
        <span class="k">if</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_size_prod</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of output with index 0 must be divided exactly &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;by block_size_prod, but got the shape of output: </span><span class="si">{</span><span class="n">out_shape</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;block_size_prod: </span><span class="si">{</span><span class="n">block_size_prod</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">out_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">block_size_prod</span>
        <span class="k">return</span> <span class="n">out_shape</span></div>


<div class="viewcode-block" id="SpaceToBatchND"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SpaceToBatchND.html#mindspore.ops.SpaceToBatchND">[docs]</a><span class="k">class</span> <span class="nc">SpaceToBatchND</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides spatial dimensions into blocks and combines the block size with the original batch.</span>

<span class="sd">    This operation will divide spatial dimensions into blocks with `block_shape`, and then the output tensor&#39;s spatial</span>
<span class="sd">    dimension is the corresponding number of blocks after division. The output tensor&#39;s batch dimension is the</span>
<span class="sd">    product of the original batch and all elements in `block_shape`.</span>
<span class="sd">    Before division, the spatial dimensions of the input are zero padded according to paddings if necessary.</span>

<span class="sd">    Args:</span>
<span class="sd">        block_shape (Union[list(int), tuple(int), int]): The block shape of dividing block</span>
<span class="sd">            with all elements greater than or euqal to 1. If `block_shape` is a list or tuple,</span>
<span class="sd">            the length of `block_shape` is the number of spatial dimensions, called M later.</span>
<span class="sd">            If `block_shape` is an int, the block size of M dimensions are the same, equal to `block_shape`.</span>
<span class="sd">            In this case of Ascend, M must be 2.</span>
<span class="sd">        paddings (Union[tuple, list]): The padding values for spatial dimensions, containing M subtraction list.</span>
<span class="sd">            Each contains 2 integer values. All values must be greater than or equal to 0.</span>
<span class="sd">            `paddings[i]` specifies the paddings for the spatial dimension i,</span>
<span class="sd">            which corresponds to the input dimension i + offset,where offset = N-M,</span>
<span class="sd">            and N is the number of input dimensions.</span>
<span class="sd">            For each i, input_shape[i + offset]+paddings[i][0]+paddings[i][1]</span>
<span class="sd">            should be divisible by block_shape[i].</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The input tensor must be a 4-D tensor on Ascend.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the output tensor with the same data type as the input.</span>
<span class="sd">        Assume the input shape is :math:`(n, c_1, ... c_k, w_1, ..., w_M)` with</span>
<span class="sd">        :math:`block\_shape` and :math:`paddings`.</span>
<span class="sd">        The shape of the output tensor will be :math:`(n&#39;, c_1, ... c_k, w&#39;_1, ..., w&#39;_M)`,</span>
<span class="sd">        where</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                n&#39; = n*(block\_shape[0]*...*block\_shape[M-1]) \\</span>
<span class="sd">                w&#39;_i = (w_i+paddings[i-1][0]+paddings[i-1][1])//block\_shape[i-1]</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_shape` is not one of list, tuple, int.</span>
<span class="sd">        TypeError: If `paddings` is neither list nor tuple.</span>
<span class="sd">        ValueError: If `block_shape` is not one dimensional when `block_shape` is a list or tuple.</span>
<span class="sd">        ValueError: If the length of `block_shape` is not 2 on Ascend.</span>
<span class="sd">        ValueError: If shape of `paddings` is not (M, 2), where M is the length of `block_shape`.</span>
<span class="sd">        ValueError: If the element of `block_shape` is not an integer larger than or equal to 1.</span>
<span class="sd">        ValueError: If the element of `paddings` is not an integer larger than or euqal to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; block_shape = [2, 2]</span>
<span class="sd">        &gt;&gt;&gt; paddings = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; space_to_batch_nd = ops.SpaceToBatchND(block_shape, paddings)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, 2], [3, 4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = space_to_batch_nd(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.]]]</span>
<span class="sd">         [[[2.]]]</span>
<span class="sd">         [[[3.]]]</span>
<span class="sd">         [[[4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SpaceToBatchND&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;paddings type&#39;</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;paddings length&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">paddings</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block_shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">block_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_shape</span><span class="p">,)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;block_shape&quot;</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_shape type&#39;</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_shape shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
                        <span class="s1">&#39;default value&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">block_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_shape length&#39;</span><span class="p">,</span> <span class="n">block_rank</span><span class="p">,</span> <span class="s1">&#39;default value&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">block_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_shape element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="s1">&#39;min value&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_shape element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span> <span class="o">=</span> <span class="n">block_shape</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span>
            <span class="s1">&#39;paddings shape&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">paddings</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;default value&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">block_rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">paddings</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="s1">&#39;paddings element&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;paddings element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span></div>


<div class="viewcode-block" id="BatchToSpaceND"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BatchToSpaceND.html#mindspore.ops.BatchToSpaceND">[docs]</a><span class="k">class</span> <span class="nc">BatchToSpaceND</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :class:`mindspore.ops.BatchToSpaceND` is deprecated from version 2.0 and will be removed in a future version,</span>
<span class="sd">    use :func:`mindspore.ops.batch_to_space_nd` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; block_size = 2</span>
<span class="sd">        &gt;&gt;&gt; crops = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; batch_to_space = ops.BatchToSpaceND(block_size, crops)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1]]], [[[2]]], [[[3]]], [[[4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = batch_to_space(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.  2.]</span>
<span class="sd">           [3.  4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2.0&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.batch_to_space_nd&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BatchToSpaceND&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block_shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">block_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_shape</span><span class="p">,)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">crops</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;block_shape&quot;</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_shape type&#39;</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_shape shape&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">block_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">block_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_shape length&#39;</span><span class="p">,</span> <span class="n">block_rank</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">block_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;block_shape element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_shape element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_shape</span> <span class="o">=</span> <span class="n">block_shape</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;crops type&#39;</span><span class="p">,</span> <span class="n">crops</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;crops length&#39;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">crops</span><span class="p">),</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;crops shape&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">crops</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">block_rank</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elem</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="o">*</span><span class="n">crops</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">elem</span><span class="p">,</span> <span class="s1">&#39;crops element&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;crops element&#39;</span><span class="p">,</span> <span class="n">elem</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crops</span> <span class="o">=</span> <span class="n">crops</span></div>


<div class="viewcode-block" id="BatchToSpaceNDV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BatchToSpaceNDV2.html#mindspore.ops.BatchToSpaceNDV2">[docs]</a><span class="k">class</span> <span class="nc">BatchToSpaceNDV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides batch dimension with blocks and interleaves these blocks back into spatial dimensions.</span>

<span class="sd">    Refer to :func:`mindspore.ops.batch_to_space_nd` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. It must be greater or equal to 2-D</span>
<span class="sd">          tensor(equal to 4-D tensor on Ascend), batch dimension must be divisible by product of `block_shape`.</span>
<span class="sd">        - **block_shape** (Union[list(int), tuple(int), int]) - The block shape of dividing block with all value greater</span>
<span class="sd">          than or equal to 1. If `block_shape` is a tuple or list, the length of `block_shape` is M corresponding</span>
<span class="sd">          to the number of spatial dimensions. If `block_shape` is an int, the block size of M dimensions are the</span>
<span class="sd">          same, equal to `block_shape`. In this case of Ascend, M must be 2.</span>
<span class="sd">        - **crops** (Union[list(int), tuple(int)]) - The crops values for spatial dimensions, containing</span>
<span class="sd">          M subtraction list. Each contains 2 integer values. All values must be &gt;= 0. crops[i] specifies</span>
<span class="sd">          the crops values for spatial dimension i, which corresponds to input dimension i + offset,</span>
<span class="sd">          where offset = N-M, and N is the number of input dimensions. It is required that</span>
<span class="sd">          :math:`input\_shape[i+offset]*block\_shape[i] &gt; crops[i][0]+crops[i][1]`</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, contains the result of batch division and rearrangement of the original Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BatchToSpaceNDV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;block_shape&#39;</span><span class="p">,</span> <span class="s1">&#39;crops&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;origin_format&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">)</span></div>


<div class="viewcode-block" id="BroadcastTo"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BroadcastTo.html#mindspore.ops.BroadcastTo">[docs]</a><span class="k">class</span> <span class="nc">BroadcastTo</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts input tensor to a given shape.</span>

<span class="sd">    Refer to :func:`mindspore.ops.broadcast_to` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (tuple): The target shape to broadcast. Can be fully specified, or have -1 in one position</span>
<span class="sd">            where it will be substituted by the input tensor&#39;s shape in that position, see example.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the given `shape` and the same data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.BroadcastTo(shape=shape)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [1. 2. 3.]]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; shape = (-1, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1], [2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.BroadcastTo(shape=shape)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [2. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BroadcastTo&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;dimension of x&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;target shape index -&gt; &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ix</span><span class="p">),</span> <span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;shape element&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;shape element min limit&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span></div>


<div class="viewcode-block" id="Meshgrid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Meshgrid.html#mindspore.ops.Meshgrid">[docs]</a><span class="k">class</span> <span class="nc">Meshgrid</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates coordinate matrices from given coordinate tensors.</span>

<span class="sd">    Refer to :func:`mindspore.ops.meshgrid` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        indexing (str, optional): Cartesian (&#39;xy&#39;) or</span>
<span class="sd">            matrix (&#39;ij&#39;) indexing of output. Valid options: xy&#39; or &#39;ij&#39;. In the 2-D case with</span>
<span class="sd">            inputs of length `M` and `N`, the outputs are of shape :math:`(N, M)`</span>
<span class="sd">            for &#39;xy&#39; indexing and :math:`(M, N)` for &#39;ij&#39; indexing. In the 3-D</span>
<span class="sd">            case with inputs of length `M`, `N` and `P`, outputs are of shape</span>
<span class="sd">            :math:`(N, M, P)` for &#39;xy&#39; indexing and :math:`(M, N, P)` for &#39;ij&#39; indexing.</span>
<span class="sd">            Default: &#39;xy&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Union[tuple]) - A Tuple of N 1-D Tensor objects.</span>
<span class="sd">          The length of input should be greater than 1. The data type is Number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensors, A Tuple of N N-D Tensor objects. The data type is the same with the Inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([5, 6, 7]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; z = Tensor(np.array([8, 9, 0, 1, 2]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; inputs = (x, y, z)</span>
<span class="sd">        &gt;&gt;&gt; meshgrid = ops.Meshgrid(indexing=&#39;xy&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = meshgrid(inputs)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]],</span>
<span class="sd">          [[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]],</span>
<span class="sd">          [[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]]]),</span>
<span class="sd">         Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5]],</span>
<span class="sd">          [[6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6]],</span>
<span class="sd">          [[7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7]]]),</span>
<span class="sd">         Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]],</span>
<span class="sd">          [[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]],</span>
<span class="sd">          [[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s2">&quot;xy&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Meshgrid.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;indexing&quot;</span><span class="p">,</span> <span class="n">indexing</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">indexing</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s2">&quot;xy&quot;</span><span class="p">,</span> <span class="s2">&quot;ij&quot;</span><span class="p">],</span> <span class="s2">&quot;indexing&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">indexing</span> <span class="o">=</span> <span class="n">indexing</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;len of input&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
        <span class="n">shape_0</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;each input rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">shape_0</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">indexing</span> <span class="o">==</span> <span class="s2">&quot;xy&quot;</span><span class="p">:</span>
            <span class="n">shape_0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">shape_0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape_0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">shape_0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">shape_0</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input[0]&quot;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_type</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;x_type[</span><span class="si">%d</span><span class="s1">]&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">x_type</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">&#39;base&#39;</span><span class="p">,</span> <span class="n">x_type</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_type</span></div>


<div class="viewcode-block" id="ReverseSequence"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReverseSequence.html#mindspore.ops.ReverseSequence">[docs]</a><span class="k">class</span> <span class="nc">ReverseSequence</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses variable length slices.</span>

<span class="sd">    Args:</span>
<span class="sd">        seq_dim (int): The dimension where reversal is performed. Required.</span>
<span class="sd">        batch_dim (int): The input is sliced in this dimension. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input to reverse, supporting all number types including bool.</span>
<span class="sd">        - **seq_lengths** (Tensor) - Must be a 1-D vector with int32 or int64 types.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `seq_dim` or `batch_dim` is not an int.</span>
<span class="sd">        ValueError: If value of `batch_dim` is equal to or greater than length of shape of `x` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; reverse_sequence = ops.ReverseSequence(seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = reverse_sequence(x, seq_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; reverse_sequence = ops.ReverseSequence(seq_dim=0, batch_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = reverse_sequence(x, seq_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 5. 9.]</span>
<span class="sd">         [4. 2. 6.]</span>
<span class="sd">         [7. 8. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([2, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; reverse_sequence = ops.ReverseSequence(seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = reverse_sequence(x, seq_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 1. 3.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([3, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; reverse_sequence = ops.ReverseSequence(seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = reverse_sequence(x, seq_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 2. 1.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([4, 4]))</span>
<span class="sd">        &gt;&gt;&gt; reverse_sequence = ops.ReverseSequence(seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = reverse_sequence(x, seq_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 3. 2. 1.]</span>
<span class="sd">         [8. 7. 6. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReverseSequence&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;seq_lengths&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seq_dim&quot;</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_dim_</span> <span class="o">=</span> <span class="n">seq_dim</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;batch_dim&quot;</span><span class="p">,</span> <span class="n">batch_dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_dim_</span> <span class="o">=</span> <span class="n">batch_dim</span></div>


<div class="viewcode-block" id="EditDistance"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.EditDistance.html#mindspore.ops.EditDistance">[docs]</a><span class="k">class</span> <span class="nc">EditDistance</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Levenshtein Edit Distance. It is used to measure the similarity of two sequences. The inputs are</span>
<span class="sd">    variable-length sequences provided by SparseTensors (hypothesis_indices, hypothesis_values, hypothesis_shape)</span>
<span class="sd">    and (truth_indices, truth_values, truth_shape).</span>

<span class="sd">    .. math::</span>

<span class="sd">        \operatorname{lev}_{a, b}(i, j)=\left\{\begin{array}{ll}</span>
<span class="sd">        \max (i, j)  \qquad \qquad \qquad \qquad \qquad \quad \  \text { if } \min (i, j)=0 \\</span>
<span class="sd">        \min \left\{\begin{array}{ll}</span>
<span class="sd">        \operatorname{lev}_{a, b}(i-1, j)+1 &amp; \\</span>
<span class="sd">        \operatorname{lev}_{a, b}(i, j-1)+1 &amp; \text { otherwise. } \\</span>
<span class="sd">        \operatorname{lev}_{a, b}(i-1, j-1)+1_{\left(a_{i} \neq b_{j}\right)}</span>
<span class="sd">        \end{array}\right. &amp;</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Where the :math:`a` indicates the hypothesis and the :math:`b` indicates the truth. For ease of understanding,</span>
<span class="sd">    i and j here in may be considered as lengths of a and b.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Unorded `truth_indices` or `hypothesis_indices` might lead to expected result, so it is suggested to</span>
<span class="sd">        make sure `truth_indices` and `hypothesis_indices` are both in ascending order before</span>
<span class="sd">        calling this API.</span>

<span class="sd">    Args:</span>
<span class="sd">        normalize (bool): If ``True`` , edit distances are normalized by length of truth. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **hypothesis_indices** (Tensor) - The indices of the hypothesis list SparseTensor. With int64 data type.</span>
<span class="sd">          The shape of tensor is :math:`(N, R)`.</span>
<span class="sd">        - **hypothesis_values** (Tensor) - The values of the hypothesis list SparseTensor.</span>
<span class="sd">          Must be 1-D vector with length of N.</span>
<span class="sd">        - **hypothesis_shape** (Tensor) - The shape of the hypothesis list SparseTensor.</span>
<span class="sd">          Must be R-length vector with int64 data type. Only constant value is allowed.</span>
<span class="sd">        - **truth_indices** (Tensor) - The indices of the truth list SparseTensor. With int64 data type.</span>
<span class="sd">          The shape of tensor is :math:`(M, R)`.</span>
<span class="sd">        - **truth_values** (Tensor) - The values of the truth list SparseTensor. Must be 1-D vector with length of M.</span>
<span class="sd">        - **truth_shape** (Tensor) - The shape of the truth list SparseTensor.</span>
<span class="sd">          Must be R-length vector with int64 data type. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a dense tensor with rank `R-1` and float32 data type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `normalize` is not a bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; class EditDistance(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, hypothesis_shape, truth_shape, normalize=True):</span>
<span class="sd">        ...         super(EditDistance, self).__init__()</span>
<span class="sd">        ...         self.edit_distance = ops.EditDistance(normalize)</span>
<span class="sd">        ...         self.hypothesis_shape = hypothesis_shape</span>
<span class="sd">        ...         self.truth_shape = truth_shape</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, hypothesis_indices, hypothesis_values, truth_indices, truth_values):</span>
<span class="sd">        ...         return self.edit_distance(hypothesis_indices, hypothesis_values, self.hypothesis_shape,</span>
<span class="sd">        ...                                   truth_indices, truth_values, self.truth_shape)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; hypothesis_indices = Tensor(np.array([[0, 0, 0], [1, 0, 1], [1, 1, 1]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; hypothesis_values = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; hypothesis_shape = Tensor(np.array([1, 1, 2]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; truth_indices = Tensor(np.array([[0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; truth_values = Tensor(np.array([1, 3, 2, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; truth_shape = Tensor(np.array([2, 2, 2]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; edit_distance = EditDistance(hypothesis_shape, truth_shape)</span>
<span class="sd">        &gt;&gt;&gt; output = edit_distance(hypothesis_indices, hypothesis_values, truth_indices, truth_values)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize EditDistance&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;normalize&quot;</span><span class="p">,</span> <span class="n">normalize</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">TransShape</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transforms the shape of input tensor to target shape.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A input tensor.</span>
<span class="sd">        - **out_shape** (tuple[int]) - The shape of output data.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor whose data type is same as &#39;input_x&#39;, and the shape is the same as the `out_shape`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TransShape.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">__infer__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="n">shp</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span> <span class="o">+</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;out_shape&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">shp</span><span class="p">))</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;shape&#39;</span><span class="p">:</span> <span class="n">shp</span><span class="p">,</span>
                <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">,</span>
                <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">}</span>


<div class="viewcode-block" id="Sort"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sort.html#mindspore.ops.Sort">[docs]</a><span class="k">class</span> <span class="nc">Sort</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sorts the elements of the input tensor along the given dimension in the specified order.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, the data types of Float16 is well supported.</span>
<span class="sd">        Using Float32 might cause loss of accuracy.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension to sort along. Default: ``-1``, means the last dimension.</span>
<span class="sd">            The Ascend backend only supports sorting the last dimension.</span>
<span class="sd">        descending (bool): Controls the sort order. If descending is ``True`` then the elements</span>
<span class="sd">            are sorted in descending order by value. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor of any dimension, with a type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y1** (Tensor) - A tensor whose values are the sorted values, with the same shape and data type as input.</span>
<span class="sd">        - **y2** (Tensor) - the indices of the elements in the original input tensor. Data type is int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `descending` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is not in range of [-len(x.shape), len(x.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8, 2, 1], [5, 9, 3], [4, 6, 7]]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; sort = ops.Sort()</span>
<span class="sd">        &gt;&gt;&gt; output = sort(x)</span>
<span class="sd">        &gt;&gt;&gt; # The output below is based on the Ascend platform.</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 3], dtype=Float16, value=</span>
<span class="sd">        [[ 1.0000e+00,  2.0000e+00,  8.0000e+00],</span>
<span class="sd">         [ 3.0000e+00,  5.0000e+00,  9.0000e+00],</span>
<span class="sd">         [ 4.0000e+00,  6.0000e+00,  7.0000e+00]]), Tensor(shape=[3, 3], dtype=Int32, value=</span>
<span class="sd">        [[2, 1, 0],</span>
<span class="sd">         [2, 0, 1],</span>
<span class="sd">         [0, 1, 2]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Sort&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">descending</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;descending&quot;</span><span class="p">,</span> <span class="n">descending</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y1&#39;</span><span class="p">,</span> <span class="s1">&#39;y2&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="EmbeddingLookup"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.EmbeddingLookup.html#mindspore.ops.EmbeddingLookup">[docs]</a><span class="k">class</span> <span class="nc">EmbeddingLookup</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a slice of input tensor based on the specified indices.</span>

<span class="sd">    This Primitive has the similar functionality as GatherV2 operating on `axis = 0`, but has one more inputs:</span>
<span class="sd">    `offset`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_params** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          This represents a Tensor slice, instead of the entire Tensor. Currently, the dimension is restricted to be 2.</span>
<span class="sd">        - **input_indices** (Tensor) - The shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">          Specifies the indices of elements of the original Tensor. Values can be out of range of `input_params`,</span>
<span class="sd">          and the exceeding part will be filled with 0 in the output. Values do not support negative and the result</span>
<span class="sd">          is undefined if values are negative. The data type should be int32 or int64.</span>
<span class="sd">        - **offset** (int) - Specifies the offset value of this `input_params` slice. Thus the real indices</span>
<span class="sd">          are equal to `input_indices` minus `offset`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(z_1, z_2, ..., z_N)`. The data type is the same with `input_params`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_indices` is not int.</span>
<span class="sd">        ValueError: If length of shape of `input_params` is greater than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[8, 9], [10, 11], [12, 13], [14, 15]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([[5, 2], [8, 5]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; offset = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.EmbeddingLookup()(input_params, input_indices, offset)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[10. 11.]</span>
<span class="sd">          [ 0.  0.]]</span>
<span class="sd">         [[ 0.  0.]</span>
<span class="sd">          [10. 11.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize EmbeddingLookup.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setattr_flag__</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;bprop_return_sparse&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__check__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">offset</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;params&quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indices&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">[</span><span class="s1">&#39;dtype&#39;</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">indices_shp</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">indices_shp</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the dimension of &#39;input_indices&#39; should not &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;be zero, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shp</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">params_shp</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">params_shp</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the dimension of &#39;input_params&#39; must &lt;= 2, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">params_shp</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="GatherD"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GatherD.html#mindspore.ops.GatherD">[docs]</a><span class="k">class</span> <span class="nc">GatherD</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather_elements` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">        - **dim** (int) - The axis along which to index. It must be int32 or int64.</span>
<span class="sd">        - **index** (Tensor) - The indices of elements to gather. It can be one of the following data types:</span>
<span class="sd">          int32, int64. The value range of each index element is [-x_rank[dim], x_rank[dim]).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type with `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.GatherD()(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GatherD&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Identity"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Identity.html#mindspore.ops.Identity">[docs]</a><span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The `mindspore.ops.Identity` interface is deprecated, please use the :func:`mindspore.ops.deepcopy` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<span class="k">class</span> <span class="nc">IdentityN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a tuple of tensors with the same shapes and contents as the input.</span>

<span class="sd">    This op can be used to override the gradient for complicated functions. For</span>
<span class="sd">    example, suppose :math:`y = f(x)` and we wish to apply a custom function g for backprop</span>
<span class="sd">    such that :math:`dx=g(dy)`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[tuple[Tensor], list[Tensor]]) - Input, the data type is RealNumber.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensors - tuple(Tensor), the shape of tensor and the data type are the same as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not tuple(Tensor) or List(Tensor).</span>
<span class="sd">        TypeError: If input `x` type is not RealNumber.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = [Tensor(np.array([1, 2, 3, 4]), mstype.int64), Tensor(np.array([4, 3, 1, 1]), mstype.int64)]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.IdentityN()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(np.allclose(output[0].asnumpy(), x[0].asnumpy()))</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; print(np.allclose(output[1].asnumpy(), x[1].asnumpy()))</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[4], dtype=Int64, value= [1, 2, 3, 4]), Tensor(shape=[4], dtype=Int64, value= [4, 3, 1, 1]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IdentityN&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Range"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Range.html#mindspore.ops.Range">[docs]</a><span class="k">class</span> <span class="nc">Range</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start` and extlimits by increments of</span>
<span class="sd">    `delta` up to but not including `limit`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.range` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        maxlen (int, optional): Memory that can fit `maxlen` many elements</span>
<span class="sd">            will be allocated for the output. Optional, must be positive. Default: 1000000.</span>
<span class="sd">            If the output has more than `maxlen` elements, a runtime error</span>
<span class="sd">            will occur.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) - A scalar Tensor. The first number in the sequence. Must have</span>
<span class="sd">          type: int32 ,int64, float32 or float64.</span>
<span class="sd">        - **limit** (Tensor) - A scalar Tensor. Upper limit of the sequence, exclusive. Must</span>
<span class="sd">          have type: int32 ,int64, float32 or float64.</span>
<span class="sd">        - **delta** (Tensor) - A scalar Tensor. Number that increments `start`. Must have</span>
<span class="sd">          type: int32 ,int64, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">       A 1-D Tensor, with the same type as the inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(0, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; limit = Tensor(10, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(4, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Range()(start, limit, delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 4 8]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;start&#39;</span><span class="p">,</span> <span class="s1">&#39;limit&#39;</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">maxlen</span> <span class="o">=</span> <span class="n">maxlen</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;maxlen&#39;</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_shape</span><span class="p">,</span> <span class="n">limit_shape</span><span class="p">,</span> <span class="n">delta_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">start_shape</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;start_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">start_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">limit_shape</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;limit_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">limit_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_shape_unknown</span><span class="p">(</span><span class="n">delta_shape</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;delta_shape&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">delta_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_dtype</span><span class="p">,</span> <span class="n">limit_dtype</span><span class="p">,</span> <span class="n">delta_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;start&quot;</span><span class="p">:</span> <span class="n">start_dtype</span><span class="p">,</span> <span class="s2">&quot;limit&quot;</span><span class="p">:</span> <span class="n">limit_dtype</span><span class="p">,</span> <span class="s2">&quot;delta&quot;</span><span class="p">:</span> <span class="n">delta_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_value</span><span class="p">,</span> <span class="n">limit_value</span><span class="p">,</span> <span class="n">delat_value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Infer the value of input for Range.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">start_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">limit_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">delat_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">start_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">limit</span> <span class="o">=</span> <span class="n">limit_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">delat</span> <span class="o">=</span> <span class="n">delat_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delat</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">start_value</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<span class="k">class</span> <span class="nc">RangeV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start`, ends at `limit` but not including `limit`</span>
<span class="sd">    and extends by increments of `delta`.</span>

<span class="sd">    The types of all 3 inputs must be the same. The type of the resulting tensor is</span>
<span class="sd">    the same as the type of the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        maxlen (int): Memory that can fit `maxlen` many elements</span>
<span class="sd">            will be allocated for the output. Optional, must be positive, defaults to 1000000.</span>
<span class="sd">            If the output has more than `maxlen` elements, a `ValueError` will occur.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) - A scalar Tensor. The first number in the sequence. Must have</span>
<span class="sd">          type: int32 or float32 or int64 or float64</span>
<span class="sd">        - **limit** (Tensor) - A scalar Tensor. Upper limit of the sequence, exclusive. Must</span>
<span class="sd">          have type: int32 or float32 or int64 or float64</span>
<span class="sd">        - **delta** (Tensor) - A scalar Tensor. Number that increments `start`. Must have</span>
<span class="sd">          type: int32 or float32 or int64 or float64</span>

<span class="sd">    Outputs:</span>
<span class="sd">       A 1D Tensor, with the same type as the inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If datatype of `start`, `limit` and `delta` not supported.</span>
<span class="sd">        TypeError: If datatype of `start`, `limit` and `delta` not same.</span>
<span class="sd">        TypeError: If attr `max_len` is not int.</span>
<span class="sd">        TypeError: If `start` or `limit` or `delta` is not scalar Tensor.</span>
<span class="sd">        ValueError: If value of `max_len` is negative.</span>
<span class="sd">        ValueError: If `delta` &gt;= 0 when `start` &gt; `limit`.</span>
<span class="sd">        ValueError: If `delta` &lt;= 0 when `start` &lt; `limit`.</span>
<span class="sd">        ValueError: If the output has more than `maxlen` elements</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(0, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; limit = Tensor(10, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(4, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.RangeV2()(start, limit, delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 4 8]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize RangeV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;start&#39;</span><span class="p">,</span> <span class="s1">&#39;limit&#39;</span><span class="p">,</span> <span class="s1">&#39;delta&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="MaskedFill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaskedFill.html#mindspore.ops.MaskedFill">[docs]</a><span class="k">class</span> <span class="nc">MaskedFill</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills elements with value where mask is True.</span>

<span class="sd">    Note:</span>
<span class="sd">        If `value` is a floating-point number of Python, it will be converted to float32 later by default.</span>
<span class="sd">        In this case, if `input_x` is a float16 Tensor, it will be converted to float32 for calculation,</span>
<span class="sd">        and the result type will be converted back to float16 on the CPU and Ascend platforms, which may</span>
<span class="sd">        cause the performance penalty. A TypeError may be raised on the GPU platform. Therefore,</span>
<span class="sd">        it is recommended that &#39;value&#39; should use a Tensor with the same dtype as `input_x`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.masked_fill` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The source tensor whose data type is one of float16, float32, int8, int32.</span>
<span class="sd">        - **mask** (Tensor[bool]) - The boolean mask.</span>
<span class="sd">        - **value** (Union[float, Tensor]) â€“ The value to fill in with, which dtype is the same as `input`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([True, True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.MaskedFill()(input, mask, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5 0.5 3.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">MaskedScatter</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value in the input with value in `updates` according to the `mask`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor): The input Tensor to be updated.</span>
<span class="sd">        - **mask** (Tensor[bool]): The mask Tensor indicating which elements should be modified or replaced.</span>
<span class="sd">          The shapes of `mask` and `x` must be the same or broadcastable.</span>
<span class="sd">        - **updates** (Tensor): The values to scatter into the target tensor `x`. It has the same data type as `x`. The</span>
<span class="sd">          number of elements must be greater than or equal to the number of True&#39;s in `mask`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x`, `mask` or `updates` is not a Tensor.</span>
<span class="sd">        TypeError: If data type of `x` is not be supported.</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>
<span class="sd">        TypeError: If the dim of `x` less than the dim of `mask`.</span>
<span class="sd">        ValueError: If `mask` can not be broadcastable to `x`.</span>
<span class="sd">        ValueError: If the number of elements in `updates` is less than number of True&#39;s in `mask`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x= Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([True, True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([5., 6., 7.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.MaskedScatter()(input_X, mask, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 3. 7.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaskedScatter&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="MaskedSelect"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaskedSelect.html#mindspore.ops.MaskedSelect">[docs]</a><span class="k">class</span> <span class="nc">MaskedSelect</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new 1-D Tensor which indexes the `x` tensor according to the boolean `mask`.</span>
<span class="sd">    The shapes of the `mask` tensor and the `x` tensor don&#39;t need to match, but they must be broadcastable.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        - **mask** (Tensor[bool]) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 1-D Tensor, with the same type as x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `mask` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([1, 0, 1, 0]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.MaskedSelect()(x, mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">mask_shape</span><span class="p">):</span>
        <span class="n">get_broadcast_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">mask_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">arg_name1</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">arg_name2</span><span class="o">=</span><span class="s2">&quot;mask&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mask_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">mask_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SearchSorted"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SearchSorted.html#mindspore.ops.SearchSorted">[docs]</a><span class="k">class</span> <span class="nc">SearchSorted</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices correspond to the positions where the given numbers in `values` should be inserted</span>
<span class="sd">    into `sorted_sequence` so that the order of the sequence is maintained.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.searchsorted` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): Output data type. An optional data type of</span>
<span class="sd">            ``mstype.int32`` and ``mstype.int64``. Default: ``mstype.int64``.</span>
<span class="sd">        right (bool, optional): Search Strategy. If ``True`` , return the last suitable index found;</span>
<span class="sd">            if ``False`` , return the first such index. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **sorted_sequence** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R-1, x_R)` or `(x_1)`.</span>
<span class="sd">          It must contain a monotonically increasing sequence on the innermost dimension.</span>
<span class="sd">        - **values** (Tensor) - The value that should be inserted.</span>
<span class="sd">          The shape of tensor is :math:`(x_1, x_2, ..., x_R-1, x_S)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor containing the indices from the innermost dimension of `sorted_sequence` such that,</span>
<span class="sd">        if insert the corresponding value in the `values` tensor, the order of `sorted_sequence` would be preserved,</span>
<span class="sd">        whose datatype is int32 if out_int32 is True, otherwise int64, and shape is the same as the shape of `values`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; sorted_sequence = Tensor(np.array([[0, 1, 3, 5, 7], [2, 4, 6, 8, 10]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([[3, 6, 9], [3, 6, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.SearchSorted()(sorted_sequence, values)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2 4 5]</span>
<span class="sd">         [1 2 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SearchSorted&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span>
            <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;right&#39;</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sorted_sequence&#39;</span><span class="p">,</span> <span class="s1">&#39;values&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">_TensorScatterOp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Defines TensorScatter Base Operators</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">indices_shape</span> <span class="o">!=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the dimension of &#39;indices&#39; cannot be less than 2,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">indices_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">indices_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_x_shape</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the last dimension of &#39;indices&#39; must be less than or equal to &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;the dimension of &#39;input_x&#39;, but got the &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;last dimension of &#39;indices&#39;: </span><span class="si">{</span><span class="n">indices_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> and the dimension of &#39;input_x&#39;: &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">input_x_shape</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="n">updates_shape_check</span> <span class="o">=</span> <span class="n">indices_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input_x_shape</span><span class="p">[</span><span class="n">indices_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_shape</span><span class="p">(</span><span class="n">updates_shape_check</span><span class="p">,</span> <span class="n">updates_shape</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the shape of &#39;update&#39; must be equal to updates_shape_check, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;where updates_shape_check = indices_shape[:-1] + input_x_shape[indices_shape[-1]:] &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got the shape of &#39;update&#39;: </span><span class="si">{</span><span class="n">updates_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;updates_shape_check: </span><span class="si">{</span><span class="n">updates_shape_check</span><span class="si">}</span><span class="s2">, indices_shape: </span><span class="si">{</span><span class="n">indices_shape</span><span class="si">}</span><span class="s2"> and &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;input_x_shape: </span><span class="si">{</span><span class="n">input_x_shape</span><span class="si">}</span><span class="s2">. Please check input_x_shape and indices_shape.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">input_x_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">updates_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="n">updates_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x_dtype</span>

    <span class="k">def</span> <span class="nf">_check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">expect</span><span class="p">,</span> <span class="n">real</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;check shape&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="o">-</span><span class="mi">2</span> <span class="ow">in</span> <span class="n">expect</span> <span class="ow">or</span> <span class="o">-</span><span class="mi">2</span> <span class="ow">in</span> <span class="n">real</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">expect</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">expect</span><span class="p">,</span> <span class="n">real</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">a</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="n">b</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">a</span> <span class="o">!=</span> <span class="n">b</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="TensorScatterUpdate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterUpdate.html#mindspore.ops.TensorScatterUpdate">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterUpdate</span><span class="p">(</span><span class="n">_TensorScatterOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by updating the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `update`. This operation is almost equivalent to using</span>
<span class="sd">    `mindspore.ops.ScatterNdUpdate` , except that the updates are applied on `input_x` instead of a zero tensor.</span>

<span class="sd">    `indices` must have rank at least 2, the last axis is the depth of each index</span>
<span class="sd">    vectors. For each index vector, there must be a corresponding value in `update`. If</span>
<span class="sd">    the depth of each index tensor matches the rank of `input_x`, then each index</span>
<span class="sd">    vector corresponds to a scalar in `input_x` and each `update` updates a scalar. If</span>
<span class="sd">    the depth of each index tensor is less than the rank of `input_x`, then each index</span>
<span class="sd">    vector corresponds to a slice in `input_x`, and each `update` updates a slice.</span>

<span class="sd">    The order in which updates are applied is nondeterministic, meaning that if there</span>
<span class="sd">    are multiple index vectors in `indices` that correspond to the same position, the</span>
<span class="sd">    value of that position in the output will be nondeterministic.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means,any number of additional dimensions.</span>
<span class="sd">          The data type is Number.</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **update** (Tensor) - The tensor to update the input tensor, has the same type as input, and</span>
<span class="sd">          :math:`update.shape = indices.shape[:-1]+input_x.shape[indices.shape[-1]:]`</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        ValueError: If the value of `input_x` are not match with input `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; update = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterUpdate()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, update)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.   0.3  3.6]</span>
<span class="sd">         [ 0.4  2.2 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_infer_specified_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate min/max value for output of TensorScatterUpdate op&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">input_x_value</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">input_x_value</span> <span class="o">=</span> <span class="n">input_x_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">indices_value</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">updates_value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices_value</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">indices_value</span> <span class="o">=</span> <span class="n">indices_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">updates_value</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
            <span class="n">updates_value</span> <span class="o">=</span> <span class="n">updates_value</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">)</span>
        <span class="n">updates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">updates_value</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">indice</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">indices_value</span><span class="p">):</span>
            <span class="n">input_x</span><span class="p">[</span><span class="n">indice</span><span class="p">]</span> <span class="o">=</span> <span class="n">updates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">input_x</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">_infer_min_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_value</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_infer_max_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_value</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">updates_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;input_x&quot;</span><span class="p">:</span> <span class="n">input_x_dtype</span><span class="p">,</span> <span class="s2">&quot;updates&quot;</span><span class="p">:</span> <span class="n">updates_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">,)</span> <span class="o">+</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x_dtype</span>

    <span class="k">def</span> <span class="nf">_infer_shape_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_infer_specified_value</span><span class="p">(</span><span class="n">input_x_value</span><span class="p">,</span> <span class="n">indices_value</span><span class="p">,</span> <span class="n">updates_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="TensorScatterMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterMax.html#mindspore.ops.TensorScatterMax">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterMax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By comparing the value at the position indicated by `indices` in `x` with the value in the `updates`,</span>
<span class="sd">    the value at the index will eventually be equal to the largest one to create a new tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_max` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterMax()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the max operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = Max(input_x[0][0], updates[0]) = [[1.0, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the max operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = Max(input_x[0][0], updates[1]) = [[2.2, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.2  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TensorScatterMin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterMin.html#mindspore.ops.TensorScatterMin">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterMin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By comparing the value at the position indicated by `indices` in `input_x` with the value in the `updates`,</span>
<span class="sd">    the value at the index will eventually be equal to the smallest one to create a new tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_min` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterMin()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the min operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = Min(input_x[0][0], updates[0]) = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the min operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = Min(input_x[0][0], updates[1]) = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ -0.1  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TensorScatterSub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterSub.html#mindspore.ops.TensorScatterSub">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterSub</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by subtracting the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are provided for the same</span>
<span class="sd">    index, the result of the update will be to subtract these values respectively. This operation is almost</span>
<span class="sd">    equivalent to using :class:`mindspore.ops.ScatterNdSub` , except that the updates are applied on output `Tensor`</span>
<span class="sd">    instead of input `Parameter`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_sub` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterSub()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the subtract operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = input_x[0][0] - updates[0] = [[-1.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the subtract operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = input_x[0][0] - updates[1] = [[-3.3, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3.3000002  0.3        3.6      ]</span>
<span class="sd">         [ 0.4        0.5       -3.2      ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TensorScatterAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterAdd.html#mindspore.ops.TensorScatterAdd">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by adding the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are given for the same</span>
<span class="sd">    index, the updated result will be the sum of all values. This operation is almost</span>
<span class="sd">    equivalent to using :class:`mindspore.ops.ScatterNdAdd`, except that the updates are applied on output `Tensor`</span>
<span class="sd">    instead of input `Parameter`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_add` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates. Shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterAdd()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the addition operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = input_x[0][0] + updates[0] = [[0.9, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the addition operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = input_x[0][0] + updates[1] = [[3.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 3.1  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TensorScatterMul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterMul.html#mindspore.ops.TensorScatterMul">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterMul</span><span class="p">(</span><span class="n">_TensorScatterOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by multiplying the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are provided for the same</span>
<span class="sd">    index, the result of the update will be to multiply these values respectively.</span>
<span class="sd">    The updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_mul` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterMul()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the multiply operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = input_x[0][0] * updates[0] = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the multiply operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = input_x[0][0] * updates[1] = [[-0.22, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.22  0.3   3.6  ]</span>
<span class="sd">         [ 0.4   0.5   -3.2 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TensorScatterDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterDiv.html#mindspore.ops.TensorScatterDiv">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterDiv</span><span class="p">(</span><span class="n">_TensorScatterOp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by dividing the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">    index, the result of the update will be to divided these values respectively. Except that</span>
<span class="sd">    the updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_div` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        - **indices** (Tensor) - The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">          The rank must be at least 2.</span>
<span class="sd">        - **updates** (Tensor) - The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">          and updates.shape should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterDiv()</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the division operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = input_x[0][0] / updates[0] = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the division operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = input_x[0][0] * updates[1] = [[-0.05, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.05  0.3  3.6  ]</span>
<span class="sd">         [ 0.4   0.5  -3.2 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">ListDiff</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function calculates the disparity between two numerical lists.</span>

<span class="sd">    It generates a list of all elements that are present in list `x` but not in list `y`.</span>
<span class="sd">    The output list `out` retains the same order as the original `x` including duplicate elements.</span>

<span class="sd">    Additionally, this class outputs a list `idx` that identifies the position of each element</span>
<span class="sd">    in `out` within the original `x`. That is to say:</span>
<span class="sd">    :code:`out[i] = x[idx[i]] for i in [0, 1, ..., len(out) - 1]` .</span>

<span class="sd">    Args:</span>
<span class="sd">        out_idx (:class:`mindspore.dtype`, optional): The dtype of `idx`,</span>
<span class="sd">            an optioanal datatype of ``mstype.int32`` and ``mstype.int64`` .</span>
<span class="sd">            Default: ``mstype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** - Values to keep. A 1-D `Tensor`.</span>
<span class="sd">        - **y** - Values to remove. A 1-D `Tensor`. Must have the same type as `x`. 1-D.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **out** - The kept values. A 1-D `Tensor`. Has the same type as `x`.</span>
<span class="sd">        - **idx** - The original index of kept values. A 1-D `Tensor` of type `out_idx`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `x` or `y` shape is not 1D.</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor.</span>
<span class="sd">        TypeError: If `x` or `y` date type is not int or uint.</span>
<span class="sd">        TypeError: If `x` has different data type with `y`.</span>
<span class="sd">        TypeError: If attr `out_idx` not in [mstype.int32, mstype.int64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1, 7, 1), dtype=mindspore.dtype.int32) # [1, 2, 3, 4, 5, 6]</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor([1, 3, 5], dtype=mindspore.dtype.int32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ListDiff() # out_idx default is mindspore.dtype.int32</span>
<span class="sd">        &gt;&gt;&gt; out, idx = op(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [2 4 6]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [1 3 5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_idx</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ListDiff&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">,</span> <span class="s1">&#39;idx&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;out_idx&quot;</span><span class="p">,</span> <span class="n">out_idx</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;out_idx&quot;</span><span class="p">,</span> <span class="n">out_idx</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">excp_cls</span><span class="o">=</span><span class="ne">TypeError</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_idx</span> <span class="o">=</span> <span class="n">out_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;out_idx&#39;</span><span class="p">,</span> <span class="n">out_idx</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SplitV</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor into `num_split` tensors along the given dimension.</span>

<span class="sd">    The `input_x` tensor will be split into sub-tensors with individual shapes given</span>
<span class="sd">    by `size_splits` along the split dimension. This requires that `input_x.shape(split_dim)`</span>
<span class="sd">    is equal to the sum of `size_splits`.</span>

<span class="sd">    The shape of `input_x` is :math:`(x_1, x_2, ..., x_M, ..., x_R)` whose rank</span>
<span class="sd">    is `R`. Set the given `split_dim` as M, and :math:`-R \le M &lt; R`. Set the given `num_split`</span>
<span class="sd">    as `N`, the given `size_splits` as :math:`(x_{m_1}, x_{m_2}, ..., x_{m_N})`,</span>
<span class="sd">    :math:`x_M=\sum_{i=1}^Nx_{m_i}`. The output is a list of tensor objects, for the</span>
<span class="sd">    :math:`i`-th tensor, it has the shape of :math:`(x_1, x_2, ..., x_{m_i}, ..., x_R)`.</span>
<span class="sd">    :math:`x_{m_i}` is the :math:`M`-th dimension of the :math:`i`-th tensor.</span>
<span class="sd">    Then, the shape of the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        ((x_1, x_2, ..., x_{m_1}, ..., x_R), (x_1, x_2, ..., x_{m_2}, ..., x_R), ...,</span>
<span class="sd">         (x_1, x_2, ..., x_{m_N}, ..., x_R))</span>

<span class="sd">    Args:</span>
<span class="sd">        size_splits (Union[tuple, list]): A tuple or list of sizes of each output tensor along the split</span>
<span class="sd">            dimension, and the sum of these sizes should equal to the dimension of the</span>
<span class="sd">            input tensor along `split_dim`. The list may also contain a single instance of</span>
<span class="sd">            the value -1, which indicates that the size of that dimension should be inferred.</span>
<span class="sd">        split_dim (int): An int indicates the dimension along which to split.</span>
<span class="sd">            Must be in the range [-len(input_x.shape), len(input_x.shape)).</span>
<span class="sd">        num_split (int): The number of output tensors. Must be positive int.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ...,x_M ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a list of `num_split` Tensor objects with the shape :math:`((x_1, x_2, ..., x_{m_1}, ..., x_R),</span>
<span class="sd">        (x_1, x_2, ..., x_{m_2}, ..., x_R), ..., (x_1, x_2, ..., x_{m_N}, ..., x_R))`, :math:`x_M=\sum_{i=1}^Nx_{m_i}`.</span>
<span class="sd">        The data type is the same with `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `size_splits` is not a tuple or a list.</span>
<span class="sd">        TypeError: If element of `size_splits` is not an int.</span>
<span class="sd">        TypeError: If `split_dim` or `num_split` is not an int.</span>
<span class="sd">        ValueError: If rank of the `size_splits` is not equal to `num_split`.</span>
<span class="sd">        ValueError: If sum of the `size_splits` is not equal to the dimension of value along `split_dim`.</span>
<span class="sd">        ValueError: If `split_dim` is out of the range [-len(input_x.shape), len(input_x.shape)).</span>
<span class="sd">        ValueError: If the `num_split` is less than or equal to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SplitV(size_splits=[1, -1], split_dim=1, num_split=2)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [4],</span>
<span class="sd">         [7]]), Tensor(shape=[3, 2], dtype=Int32, value=</span>
<span class="sd">        [[2, 3],</span>
<span class="sd">         [5, 6],</span>
<span class="sd">         [8, 9]]))</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SplitV(size_splits=[2, 1], split_dim=0, num_split=2)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 3], dtype=Int32, value=</span>
<span class="sd">        [[1, 2, 3],</span>
<span class="sd">         [4, 5, 6]]), Tensor(shape=[1, 3], dtype=Int32, value=</span>
<span class="sd">        [[7, 8, 9]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_splits</span><span class="p">,</span> <span class="n">split_dim</span><span class="p">,</span> <span class="n">num_split</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SplitV&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;size_splits&quot;</span><span class="p">,</span> <span class="n">size_splits</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">elements_of_size_splits</span> <span class="ow">in</span> <span class="n">size_splits</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;elements of size_splits&quot;</span><span class="p">,</span> <span class="n">elements_of_size_splits</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">elements_of_size_splits</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">elements_of_size_splits</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="se">\&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="se">\&#39;</span><span class="s2">, all elements of size_splits must be positive (except at most &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;one default value -1), but got: </span><span class="si">{</span><span class="n">elements_of_size_splits</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;split_dim&quot;</span><span class="p">,</span> <span class="n">split_dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num_split&quot;</span><span class="p">,</span> <span class="n">num_split</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">num_split</span><span class="p">,</span> <span class="s2">&quot;num_split&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="TensorScatterElements"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorScatterElements.html#mindspore.ops.TensorScatterElements">[docs]</a><span class="k">class</span> <span class="nc">TensorScatterElements</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates the value of the input Tensor through specified reduction operation.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_elements` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If there are multiple index vectors in `indices` that correspond to the same position,</span>
<span class="sd">        the value of that position in the output will be nondeterministic.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int, optional): Specify which axis to do scatter operation. Default: ``0`` .</span>
<span class="sd">        reduction (str, optional): Which reduction operation to scatter, default is ``&quot;none&quot;`` . Other option: &quot;add&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **data** (Tensor) - The target tensor. Its rank must be at least 1.</span>
<span class="sd">        - **indices** (Tensor) - The index to do scatter operation whose data type must be int32 or</span>
<span class="sd">          int64. It has the same rank as `data`. And accepted range is [-s, s) where s is the size along axis.</span>
<span class="sd">        - **updates** (Tensor) - The tensor doing the scatter operation with `data`,</span>
<span class="sd">          it has the same shape and type as `data`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `data`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterElements(0, &quot;none&quot;)</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0, 2], [0, 2, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[0, 0, 0], [0, 0, 0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(data, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.0  0.0  3.0]</span>
<span class="sd">         [ 0.0  5.0  0.0]</span>
<span class="sd">         [ 7.0  0.0  0.0]]</span>
<span class="sd">        &gt;&gt;&gt; op = ops.TensorScatterElements(1, &quot;add&quot;)</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[8, 8]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(data, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1  2  11  4  13]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TensorScatterElements&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;add&quot;</span><span class="p">],</span> <span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;ascend&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Currently Ascend device_target only support `reduction`=&#39;none&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="ExtractVolumePatches"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ExtractVolumePatches.html#mindspore.ops.ExtractVolumePatches">[docs]</a><span class="k">class</span> <span class="nc">ExtractVolumePatches</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract patches from input and put them in the &quot;depth&quot; output dimension.</span>
<span class="sd">    &quot;depth&quot; dimension is the second dim of output.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int], list[int]]): A list of ints which&#39;s length is 3 or 5.</span>
<span class="sd">            The size of the sliding window for each dimension of input. Must be: :math:`[1, 1, k_d, k_h, k_w]` or</span>
<span class="sd">            :math:`[k_d, k_h, k_w]`. If :math:`k_d = k_h = k_w`, you can enter an integer.</span>
<span class="sd">        strides (Union[int, tuple[int], list[int]]): A list of ints which&#39;s length is 3 or 5.</span>
<span class="sd">            How far the centers of two consecutive patches are in input. Must be: :math:`[1, 1, s_d, s_h, s_w]` or</span>
<span class="sd">            :math:`[s_d, s_h, s_w]`. If :math:`s_d = s_h = s_w`, you can enter an integer.</span>
<span class="sd">        padding (str): A string from: ``&quot;SAME&quot;`` , ``&quot;VALID&quot;`` . The type of padding algorithm to use.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor. 5-D Tensor with shape :math:`(x_n, x_c, x_d, x_h, x_w)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">        If padding is &quot;VALID&quot;, the shape is :math:`(x_n, k_d * k_h * k_w * x_c, 1 + (x_d - k_d) / s_d,</span>
<span class="sd">        1 + (x_h - k_h) / s_h, 1 + (x_w - k_w) / s_w)`; if padding is &quot;SAME&quot;, the shape is :math:`(</span>
<span class="sd">        x_n, k_d * k_h * k_w * x_c, (x_d + s_d - 1) / s_d, (x_h + s_h - 1) / s_h, (x_w + s_w - 1) / s_w)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If kernel_size or strides is not a list, a tuple or an int.</span>
<span class="sd">        TypeError: If input_x is not a tensor.</span>
<span class="sd">        TypeError: If padding is not str.</span>
<span class="sd">        ValueError: If the length of kernel_size is neither 3 nor 5 and kernel_size is not an integer.</span>
<span class="sd">        ValueError: If the length of strides is neither 3 nor 5 and strides is not an integer.</span>
<span class="sd">        ValueError: If padding is neither ``&quot;VALID&quot;`` nor ``&quot;SAME&quot;`` .</span>
<span class="sd">        ValueError: If elements of kernel_size or strides are not positive integer.</span>
<span class="sd">        ValueError: If input_x is not a tensor in dimension 5.</span>
<span class="sd">        ValueError: If input_x&#39;s shape has zero.</span>
<span class="sd">        ValueError: If one of kernel_size or strides&#39; first two numbers is not 1.</span>
<span class="sd">        ValueError: If padding = &quot;VALID&quot; and :math:`input\_x - kernel\_size` is less than 0 in d, h or w dimension.</span>
<span class="sd">        ValueError: If padding = &quot;SAME&quot; and :math:`padding\_needed = ((input\_x + strides - 1) / strides - 1) *</span>
<span class="sd">                    strides + kernel\_size - input\_x` is less than 0 in d, h or w dimension.</span>
<span class="sd">        ValueError: If x_h is not 1 or x_w is not 1 and :math:`x_w + padding\_needed - k_w - s_w` is less than 0.</span>
<span class="sd">        ValueError: If :math:`x_d * x_h * x_w` is greater than 2048.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; kernel_size = (1, 1, 2, 2, 2)</span>
<span class="sd">        &gt;&gt;&gt; strides = (1, 1, 1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; padding = &quot;VALID&quot;</span>
<span class="sd">        &gt;&gt;&gt; input_x = ops.Reshape()(Tensor(np.arange(1, 28), mstype.float16), (1, 1, 3, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output_y = ops.ExtractVolumePatches(kernel_size, strides, padding)(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_y.shape)</span>
<span class="sd">        (1, 8, 2, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;kernel_size[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;kernel_size[1]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">strides</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">strides</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;strides[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;strides[1]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                  <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                              <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;padding_dtype&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">padding</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ScatterAddWithAxis</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    &#39;ops.ScatterAddWithAxis&#39; is deprecated from version 2.0 and will be removed in a future version,</span>
<span class="sd">    use &#39;ops.TensorScatterElements&#39; instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ScatterAddWithAxis(0)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0, 2], [0, 2, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[1, 1, 1], [1, 1, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.  3.  3.]</span>
<span class="sd">         [ 5.  5.  7.]</span>
<span class="sd">         [ 7.  9.  10.]]</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ScatterAddWithAxis(1)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4, 5]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[8, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1  2  11  4  13]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;updates&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2.0&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.TensorScatterElements&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ScatterAddWithAxis&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Lstsq</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the solutions of the least squares and minimum norm problems of full-rank</span>
<span class="sd">    matrix `x` of size :math:`(m \times n)` and matrix `a` of size :math:`(m \times k)`.</span>

<span class="sd">    If :math:`m \geq n`, `Lstsq` solves the least-squares problem:</span>

<span class="sd">    .. math::</span>

<span class="sd">       \begin{array}{ll}</span>
<span class="sd">       \min_y &amp; \|xy-a\|_2</span>
<span class="sd">       \end{array}</span>

<span class="sd">    If :math:`m &lt; n`, `Lstsq` solves the least-norm problem:</span>

<span class="sd">    .. math::</span>

<span class="sd">       \begin{array}{llll}</span>
<span class="sd">       \min_y &amp; \|y\|_2 &amp; \text{subject to} &amp; xy = a</span>
<span class="sd">       \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        fast (bool, optional): Solving algorithm. Default: ``True`` .</span>

<span class="sd">            - If `fast` is True, then the solution is computed by solving</span>
<span class="sd">              the normal equations using Cholesky decomposition.</span>
<span class="sd">            - If `fast` is False, an algorithm based on numerically robust</span>
<span class="sd">              completed orthogonal decomposition is used.</span>

<span class="sd">        l2_regularizer (float, optional): L2 regularization coefficient. Default: ``0.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - :math:`(m \times n)` matrix `x`. The input tensor whose data type is</span>
<span class="sd">          float16, float32 or float64.</span>
<span class="sd">        - **a** (Tensor) - :math:`(m \times k)` matrix `a`. The input tensor whose data type is</span>
<span class="sd">          float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the least squares or minimum norm problems solution, which has shape</span>
<span class="sd">        :math:`(n \times k)`. The data type is the same with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input `x` or `a` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` or `a` is not one of: float16, float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `x` and `a` are not the same.</span>
<span class="sd">        ValueError: If the dimension of `x` is not equal to 2.</span>
<span class="sd">        ValueError: If the dimension of `a` is not equal to 2 or 1.</span>
<span class="sd">        ValueError: If the length of x_dims[0] is not equal to the length of a_dims[0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2,1,5],[3,5,1],[1,1,1]]),mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([[10,5],[15,8],[7,4]]),mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Lstsq()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[17.000002  11.000002 ]</span>
<span class="sd">         [-6.5000005 -4.500001 ]</span>
<span class="sd">         [-3.500002  -2.5000017]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fast</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Lstsq&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;fast&quot;</span><span class="p">,</span> <span class="n">fast</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;l2_regularizer&quot;</span><span class="p">,</span> <span class="n">l2_regularizer</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast</span> <span class="o">=</span> <span class="n">fast</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2_regularizer</span> <span class="o">=</span> <span class="n">l2_regularizer</span>


<span class="k">class</span> <span class="nc">LowerBound</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find the index of the lower bound of `values` in sorted sequence `sorted_x` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_type (:class:`mindspore.dtype`, optional): An optional data type of</span>
<span class="sd">            ``mindspore.dtype.int32`` and ``mindspore.dtype.int64`` .</span>
<span class="sd">            Default: ``mindspore.dtype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **sorted_x** (Tensor) - The input tensor whose dtype is real number and</span>
<span class="sd">          the data of each row must be sorted in ascending order. The rank must be 2.</span>
<span class="sd">        - **values** (Tensor) - The input tensor whose dtype is the same as `sorted_x`</span>
<span class="sd">          and the first dimension of the shape of `values` must be equal to that of</span>
<span class="sd">          `sorted_x` . The rank must be 2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype is determined by `out_type` and whose shape is the same</span>
<span class="sd">        as that of `values`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sorted_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `values` is not a Tensor.</span>
<span class="sd">        TypeError: If `out_type` is invalid.</span>
<span class="sd">        TypeError: If the type of `sorted_x` is not the same as that of `values`.</span>
<span class="sd">        ValueError: If rank of the `sorted_x` is not equal to 2.</span>
<span class="sd">        ValueError: If rank of the `values` is not equal to 2.</span>
<span class="sd">        ValueError: If the first dimension of the shape of `sorted_x` is not equal to that of `values`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; lowerbound = ops.LowerBound(out_type = mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sorted_x = Tensor(np.arange(12).reshape(3, 4).astype(np.int8))</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([[3], [4], [8]]).astype(np.int8))</span>
<span class="sd">        &gt;&gt;&gt; output = lowerbound(sorted_x, values)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3]</span>
<span class="sd">         [0]</span>
<span class="sd">         [0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LowerBound&quot;&quot;&quot;</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;out_type&quot;</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sorted_x&#39;</span><span class="p">,</span> <span class="s1">&#39;values&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">UpperBound</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor that contains the index for finding the upper bound of the value of</span>
<span class="sd">    the input values element in the input sorted_x.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_type (:class:`mindspore.dtype`, optional): Specified output type.</span>
<span class="sd">            Supported types: ``mindspore.dtype.int32`` and ``mindspore.dtype.int64`` .</span>
<span class="sd">            Default: ``mindspore.dtype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **sorted_x** (Tensor) - The input tensor whose dtype is real number. The rank must be 2.</span>
<span class="sd">          Each row of the `sorted_x` needs to be sorted in ascending order.</span>
<span class="sd">        - **values** (Tensor) - The input tensor whose dtype is the same as `sorted_x`. The rank must be 2.</span>
<span class="sd">          The shape[0] of the two inputs must be consistent.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype is determined by `out_type` and whose shape is consistent with `values`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sorted_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `values` is not a Tensor.</span>
<span class="sd">        TypeError: If the type of `sorted_x` is not the same as that of `values`.</span>
<span class="sd">        ValueError: If rank of the `sorted_x` is not equal to 2.</span>
<span class="sd">        ValueError: If rank of the `values` is not equal to 2.</span>
<span class="sd">        ValueError: If the number of rows of `sorted_x` is not consistent with that of `values`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; upperbound = ops.UpperBound(out_type = mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sorted_x = Tensor(np.arange(12).reshape(3, 4).astype(np.int8))</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([[3], [6], [9]]).astype(np.int8))</span>
<span class="sd">        &gt;&gt;&gt; output = upperbound(sorted_x, values)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4]</span>
<span class="sd">         [3]</span>
<span class="sd">         [2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UpperBound&quot;&quot;&quot;</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;out_type&quot;</span><span class="p">,</span> <span class="n">out_type</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sorted_x&#39;</span><span class="p">,</span> <span class="s1">&#39;values&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Cummax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cummax.html#mindspore.ops.Cummax">[docs]</a><span class="k">class</span> <span class="nc">Cummax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cumulative maximum of elements and the index.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cummax` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The axis to accumulate the tensor&#39;s value. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tuple of 2 Tensors(values, indices), containing the cumulative maximum of elements and the index,</span>
<span class="sd">        The shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; cummax = ops.Cummax(axis=0)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = cummax(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 3.  6.  7. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [[0 0 0 0]</span>
<span class="sd">         [0 1 1 0]</span>
<span class="sd">         [2 1 2 0]</span>
<span class="sd">         [2 1 2 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Cummax&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="RightShift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RightShift.html#mindspore.ops.RightShift">[docs]</a><span class="k">class</span> <span class="nc">RightShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift the value of each position of Tensor `input_x` to the right by corresponding bits in Tensor `input_y`.</span>
<span class="sd">    The inputs are two tensors, dtypes of them must be consistent, and the</span>
<span class="sd">    shapes of them could be broadcast.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;out_{i} =x_{i} &gt;&gt; y_{i}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor, will be shifted to the right</span>
<span class="sd">          by `input_y` bits element-wise.</span>
<span class="sd">        - **input_y** (Tensor) - Number of bits shifted, the tensor must have the same type as `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - The output tensor, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `input_y` is not tensor.</span>
<span class="sd">        TypeError: If `input_x` and `input_y` could not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; rightshift = ops.RightShift()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]).astype(np.uint8))</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 1]).astype(np.uint8))</span>
<span class="sd">        &gt;&gt;&gt; output = rightshift(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize RightShift.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;input_y&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="LogSpace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogSpace.html#mindspore.ops.LogSpace">[docs]</a><span class="k">class</span> <span class="nc">LogSpace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a 1-D Tensor with a length of steps. The tensor&#39;s</span>
<span class="sd">    values are uniformly distributed on a logarithmic scale, ranging from</span>
<span class="sd">    :math:`base^{start}` to :math:`base^{end}`, including both endpoints.</span>
<span class="sd">    The logarithmic scale is based on the specified `base`.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;step = (end - start)/(steps - 1)\\</span>
<span class="sd">        &amp;output = [base^{start}, base^{start + 1 * step}, ... , base^{start + (steps-2) * step}, base^{end}]</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        steps (int, optional): The steps must be a non-negative integer. Default: ``10`` .</span>
<span class="sd">        base (int, optional): The base must be a non-negative integer. Default: ``10`` .</span>
<span class="sd">        dtype (mindspore.dtype, optional): The dtype of output, include ``mstype.float16`` ,</span>
<span class="sd">            ``mstype.float32`` or ``mstype.float64`` . Default: ``mstype.float32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) - Start value of interval, with shape of 0-D,</span>
<span class="sd">          dtype is float16, float32 or float64.</span>
<span class="sd">        - **end** (Tensor) - End value of interval, with shape of 0-D,</span>
<span class="sd">          dtype is float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor has the shape as :math:`(step, )`. Its datatype is set by the attr &#39;dtype&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `steps` is not an int.</span>
<span class="sd">        TypeError: If `base` is not an int.</span>
<span class="sd">        TypeError: If `dtype` is not mstype.float16, mstype.float32 or</span>
<span class="sd">            mstype.float64.</span>
<span class="sd">        ValueError: If `steps` is not a non-negative integer.</span>
<span class="sd">        ValueError: If `base` is not a non-negative integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logspace = ops.LogSpace(steps = 10, base = 10, dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(10, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = logspace(start, end)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.e+01 1.e+02 1.e+03 1.e+04 1.e+05 1.e+06 1.e+07 1.e+08 1.e+09 1.e+10]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Logspace.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="n">base</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">steps</span><span class="p">,</span> <span class="s2">&quot;steps&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">base</span><span class="p">,</span> <span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;start&#39;</span><span class="p">,</span> <span class="s1">&#39;end&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="NonZero"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NonZero.html#mindspore.ops.NonZero">[docs]</a><span class="k">class</span> <span class="nc">NonZero</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a tensor of the positions of all non-zero values.</span>

<span class="sd">    Refer to :func:`mindspore.ops.nonzero` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor), 2-D Tensor of data type int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import NonZero</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; nonzero = NonZero()</span>
<span class="sd">        &gt;&gt;&gt; output = nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">         [0 1 0]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 0, 2, 0, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; nonzero = NonZero()</span>
<span class="sd">        &gt;&gt;&gt; output = nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0]</span>
<span class="sd">         [2]</span>
<span class="sd">         [4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Tril"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Tril.html#mindspore.ops.Tril">[docs]</a><span class="k">class</span> <span class="nc">Tril</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the lower triangular portion of the 2-D matrix or the set of matrices</span>
<span class="sd">    in a batch. The remaining elements of the resulting Tensor are assigned a value of 0.</span>
<span class="sd">    The lower triangular section of the matrix comprises of the</span>
<span class="sd">    elements present on and below the main diagonal.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        diagonal (int, optional): An optional attribute indicates the diagonal to consider, default: ``0`` ,</span>
<span class="sd">            indicating the main didiagonal.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A Tensor with shape :math:`(x_1, x_2, ..., x_R)`. The rank must be at least 2.</span>
<span class="sd">          Supporting all number types including bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the same shape and data type as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `diagonal` is not an int.</span>
<span class="sd">        TypeError: If the type of `x` is neither number nor bool.</span>
<span class="sd">        ValueError: If the rank of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; tril = ops.Tril()</span>
<span class="sd">        &gt;&gt;&gt; result = tril(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  0  0  0]</span>
<span class="sd">         [ 5  6  0  0]</span>
<span class="sd">         [10 11 12  0]</span>
<span class="sd">         [14 15 16 17]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; tril = ops.Tril(diagonal=1)</span>
<span class="sd">        &gt;&gt;&gt; result = tril(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  2  0  0]</span>
<span class="sd">         [ 5  6  7  0]</span>
<span class="sd">         [10 11 12 13]</span>
<span class="sd">         [14 15 16 17]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; tril = ops.Tril(diagonal=-1)</span>
<span class="sd">        &gt;&gt;&gt; result = tril(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 0  0  0  0]</span>
<span class="sd">         [ 5  0  0  0]</span>
<span class="sd">         [10 11  0  0]</span>
<span class="sd">         [14 15 16  0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Tril.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;diagonal&quot;</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="IndexFill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.IndexFill.html#mindspore.ops.IndexFill">[docs]</a><span class="k">class</span> <span class="nc">IndexFill</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills the elements under the `dim` dimension of the input Tensor `x` with the input `value`</span>
<span class="sd">    by selecting the indices in the order given in `index`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.index_fill` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor.</span>
<span class="sd">        - **dim** (Union[int, Tensor]) - Dimension along which to fill the input tensor. Only supports</span>
<span class="sd">          a 0-dimensional tensor or an int number.</span>
<span class="sd">        - **index** (Tensor) - Indices of the input tensor to fill in.</span>
<span class="sd">        - **value** (Union[bool, int, float, Tensor]) - Value to fill the input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same type and shape as input tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; index_fill = ops.IndexFill()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor([0, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(-2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = index_fill(x, 1, index, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[-2. 2. -2.]</span>
<span class="sd">         [-2. 5. -2.]</span>
<span class="sd">         [-2. 8. -2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IndexFill&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">IndexPut</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    According to the index number of `indexes`, replace the value corresponding to `x1` with the value in `x2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        accumulate (int): If accumulate is 1, the elements in x2 are added to x1,</span>
<span class="sd">            else the elements in x2 replace the corresponding element in x1, should be 0 or 1. Default: ``0`` .</span>
<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The assigned target tensor, 1-D or higher dimensional.</span>
<span class="sd">        - **x2** (Tensor) - 1-D Tensor of the same type as `x1`. If the size of `x2` is 1,</span>
<span class="sd">          it will broadcast to the same size as `x1`.</span>
<span class="sd">        - **indices** (tuple[Tensor], list[Tensor]) - the indices of type int32 or int64, used to index into x1.</span>
<span class="sd">        The rank of tensors in indices should be 1-D, size of indices should &lt;= x1.rank and the tensors in indices</span>
<span class="sd">        should be broadcastable.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x1`.</span>

<span class="sd">    Raises:</span>
<span class="sd">            TypeError: If the dtype of `x1` is not equal to the dtype of `x2`.</span>
<span class="sd">            TypeError: If `indices` is not tuple[Tensor] or list[Tensor].</span>
<span class="sd">            TypeError: If the dtype of tensors in `indices` are not int32 or int64.</span>
<span class="sd">            TypeError: If the dtype of tensors in `indices` are inconsistent.</span>
<span class="sd">            TypeError: If the dtype of `accumulate` are not int.</span>
<span class="sd">            ValueError: If rank(x2) is not 1-D.</span>
<span class="sd">            ValueError: If size(x2) is not 1 or max size of the tensors in `indices` when rank(x1) == size(indices).</span>
<span class="sd">            ValueError: If size(x2) is not 1 or x1.shape[-1] when rank(x1) &gt; size(indices).</span>
<span class="sd">            ValueError: If the rank of tensors in `indices` is not 1-D.</span>
<span class="sd">            ValueError: If the tensors in `indices` is not be broadcastable.</span>
<span class="sd">            ValueError: If size(indices) &gt; rank(x1).</span>
<span class="sd">            ValueError: If `accumulate` is not equal to 0 or 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 2, 3], [4, 5, 6]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([3]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = [Tensor(np.array([0, 0]).astype(np.int32)), Tensor(np.array([0, 1]).astype(np.int32))]</span>
<span class="sd">        &gt;&gt;&gt; accumulate = 1</span>
<span class="sd">        &gt;&gt;&gt; op = ops.IndexPut(accumulate = accumulate)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x1, x2, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">         [[4 5 3]</span>
<span class="sd">         [4 5 6]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">accumulate</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulate</span> <span class="o">=</span> <span class="n">accumulate</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;accumulate&#39;</span><span class="p">,</span> <span class="n">accumulate</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">SegmentMax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum along segments of a Tensor.</span>

<span class="sd">    Specifically, it generates a new Tensor `output` such that :math:`output_i=max_j(input\_x_j)`</span>
<span class="sd">    in which the maximum value is obtained from all elements corresponding</span>
<span class="sd">    to :math:`j` that meets :math:`segment\_ids[j] == i`.</span>
<span class="sd">    If a segment contains no elements for a given segment :math:`i`,</span>
<span class="sd">    then the corresponding element in the output Tensor is set to zero: :math:`output[i] = 0`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is real number and whose rank is not less than 1.</span>
<span class="sd">        - **segment_ids** (Tensor) - A 1-D tensor whose dtype is int32 or int64. The size of tensor must be equal to</span>
<span class="sd">          the first dimension of the shape of `input_x`. Values must be sorted in ascending order and need not cover</span>
<span class="sd">          all values in the full range of valid values, but must be positive integer. Only constant values is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype and the dimension of the shape is the same as `input_x`. The first dimension of the shape</span>
<span class="sd">        is equal to the value of the last element of `segment_ids` plus one, and the other dimensions are the same as</span>
<span class="sd">        those of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is invalid.</span>
<span class="sd">        TypeError: If the dtype of `segment_ids` is invalid.</span>
<span class="sd">        ValueError: If the rank of `input_x` is less than 1.</span>
<span class="sd">        ValueError: If the rank of `segment_ids` is not equal to 1.</span>
<span class="sd">        ValueError: If the size of `segment_ids` is not equal to the first dimension of the shape of `input_x`.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are negative.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are not sorted in ascending order.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 2], mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SegmentMax()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 5. 6.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [7. 8. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SegmentMax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">SegmentMin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum along segments of a Tensor.</span>

<span class="sd">    Specifically, it generates a new Tensor `output` such that :math:`output_i=min_j(input\_x_j)`</span>
<span class="sd">    in which the minimum value is obtained from all elements corresponding</span>
<span class="sd">    to :math:`j` that meets :math:`segment\_ids[j] == i`.</span>
<span class="sd">    If a segment contains no elements for a given segment :math:`i`,</span>
<span class="sd">    then the corresponding element in the output Tensor is set to zero: :math:`output[i] = 0`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is real number and whose rank is not less than 1.</span>
<span class="sd">        - **segment_ids** (Tensor) - A 1-D tensor whose dtype is int32 or int64. The size of tensor must be equal to</span>
<span class="sd">          the first dimension of the shape of `input_x`. Values must be sorted in ascending order and need not cover</span>
<span class="sd">          all values in the full range of valid values, but must be positive integer. Only constant values is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype and the dimension of the shape is the same as `input_x`. The first dimension of the shape</span>
<span class="sd">        is equal to the value of the last element of `segment_ids` plus one, and the other dimensions are the same as</span>
<span class="sd">        those of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is invalid.</span>
<span class="sd">        TypeError: If the dtype of `segment_ids` is invalid.</span>
<span class="sd">        ValueError: If the rank of `input_x` is less than 1.</span>
<span class="sd">        ValueError: If the rank of `segment_ids` is not equal to 1.</span>
<span class="sd">        ValueError: If the size of `segment_ids` is not equal to the first dimension of the shape of `input_x`.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are negative.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are not sorted in ascending order.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 2], mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SegmentMin()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [7. 8. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SegmentMin&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">SegmentSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum along segments of a Tensor.</span>

<span class="sd">    Specifically, it generates a new Tensor `output` such that :math:`output_i = \sum_j input\_x_j`</span>
<span class="sd">    in which the cumulative sum is obtained from all elements corresponding</span>
<span class="sd">    to :math:`j` that meets :math:`segment\_ids[j] == i`.</span>
<span class="sd">    If a segment contains no elements for a given segment :math:`i`,</span>
<span class="sd">    then the corresponding element in the output Tensor is set to 0: :math:`output[i] = 0`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the dtype of `input_x` is complex number, the gradient can not be calculated.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is real number or complex number and whose rank is not</span>
<span class="sd">          less than 1.</span>
<span class="sd">        - **segment_ids** (Tensor) - A 1-D tensor whose dtype is int32 or int64. The size of tensor must be equal to</span>
<span class="sd">          the first dimension of the shape of `input_x`. Values must be sorted in ascending order and need not cover</span>
<span class="sd">          all values in the full range of valid values, but must be positive integer. Only constant values is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype and the dimension of the shape is the same as `input_x`. The first dimension of the shape</span>
<span class="sd">        is equal to the value of the last element of `segment_ids` plus one, and the other dimensions are the same as</span>
<span class="sd">        those of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is invalid.</span>
<span class="sd">        TypeError: If the dtype of `segment_ids` is invalid.</span>
<span class="sd">        ValueError: If the rank of `input_x` is less than 1.</span>
<span class="sd">        ValueError: If the rank of `segment_ids` is not equal to 1.</span>
<span class="sd">        ValueError: If the size of `segment_ids` is not equal to the first dimension of the shape of `input_x`.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are negative.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are not sorted in ascending order.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 2], mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SegmentSum()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5. 7. 9.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [7. 8. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SegmentSum&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="LeftShift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LeftShift.html#mindspore.ops.LeftShift">[docs]</a><span class="k">class</span> <span class="nc">LeftShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift the value of each position of the tensor to the left several bits.</span>
<span class="sd">    The inputs are two tensors, dtypes of them must be consistent, and the</span>
<span class="sd">    shapes of them could be broadcast.</span>
<span class="sd">    The output does not support implicit type conversion.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;out_{i} =x_{i} &lt;&lt; y_{i}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - The target tensor whose dtype supports int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, will be shifted to the left by x2 in element-wise.</span>
<span class="sd">        - **x2** (Tensor) - The tensor must have the same dtype as x1. And the tensor must have the same shape as x1</span>
<span class="sd">          or could be broadcast with x1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - The output tensor, has the same dtype as x1.</span>
<span class="sd">          And the shape of the output tensor is the same shape as x1, or the same shape</span>
<span class="sd">          as x1 and x2 after broadcasting.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x1` or `x2` has wrong type.</span>
<span class="sd">        TypeError: If `x1` or `x2` is not tensor.</span>
<span class="sd">        ValueError: If `x1` and `x2` could not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; left_shift = ops.LeftShift()</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3]).astype(np.int8))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([0, 1, -1]).astype(np.int8))</span>
<span class="sd">        &gt;&gt;&gt; output = left_shift(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 4 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LeftShift&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="FillDiagonal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FillDiagonal.html#mindspore.ops.FillDiagonal">[docs]</a><span class="k">class</span> <span class="nc">FillDiagonal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills the main diagonal of a Tensor in-place with a specified value and returns the result.</span>
<span class="sd">    The input has at least 2 dimensions, and all dimensions of input must be equal in length</span>
<span class="sd">    when the dimension of input is greater than 2.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        fill_value (float): The value to fill the diagonal of `input_x`.</span>
<span class="sd">        wrap (bool, optional): Controls whether the diagonal elements continue onto the</span>
<span class="sd">            remaining rows in case of a tall matrix(A matrix has more rows than columns).</span>
<span class="sd">            Examples blow demonstrates how it works on a tall matrix if `wrap` is set ``True`` .</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          The data type must be float32, int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Tensor, has the same shape and data type as the input `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `input_x` is not one of the following: float32, int32, int64.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not greater than 1.</span>
<span class="sd">        ValueError: If the size of each dimension is not equal, when the dimension is greater than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; fill_value = 9.9</span>
<span class="sd">        &gt;&gt;&gt; fill_diagonal = ops.FillDiagonal(fill_value)</span>
<span class="sd">        &gt;&gt;&gt; y = fill_diagonal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[9.9 2.  3. ]</span>
<span class="sd">         [4.  9.9 6. ]</span>
<span class="sd">         [7.  8.  9.9]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; fill_value = 9.0</span>
<span class="sd">        &gt;&gt;&gt; fill_diagonal = ops.FillDiagonal(fill_value)</span>
<span class="sd">        &gt;&gt;&gt; y = fill_diagonal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[9 0 0]</span>
<span class="sd">         [1 9 1]</span>
<span class="sd">         [2 2 9]</span>
<span class="sd">         [3 3 3]</span>
<span class="sd">         [4 4 4]</span>
<span class="sd">         [5 5 5]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3],</span>
<span class="sd">        ...                      [4, 4, 4], [5, 5, 5], [6, 6, 6]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; fill_value = 9.0</span>
<span class="sd">        &gt;&gt;&gt; wrap = True</span>
<span class="sd">        &gt;&gt;&gt; fill_diagonal = FillDiagonal(fill_value, wrap)</span>
<span class="sd">        &gt;&gt;&gt; y = fill_diagonal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[9 0 0]</span>
<span class="sd">         [1 9 1]</span>
<span class="sd">         [2 2 9]</span>
<span class="sd">         [3 3 3]</span>
<span class="sd">         [9 4 4]</span>
<span class="sd">         [5 9 5]</span>
<span class="sd">         [6 6 9]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">,</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FillDiagonal&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;fill_value&#39;</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fill_value</span> <span class="o">=</span> <span class="n">fill_value</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;wrap&#39;</span><span class="p">,</span> <span class="n">wrap</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="HammingWindow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HammingWindow.html#mindspore.ops.HammingWindow">[docs]</a><span class="k">class</span> <span class="nc">HammingWindow</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the hamming window function with input window length.</span>

<span class="sd">    .. math::</span>

<span class="sd">        w[n] = \alpha - \beta\ \cos \left( \frac{2 \pi n}{N - 1} \right),</span>

<span class="sd">    where :math:`N` is the full window size.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        periodic (bool, optional): a flag determines whether the returned window trims off</span>
<span class="sd">            the last duplicate value from the symmetric window. Default: ``True`` .</span>

<span class="sd">            - If True, returns a window to be used as periodic function, in above formula,</span>
<span class="sd">              :math:`N = \text{length} + 1`.</span>
<span class="sd">            - If False, return a symmetric window, :math:`N = \text{length}`.</span>

<span class="sd">        alpha (float, optional): The coefficient :math:`\alpha` in the equation above. Default: ``0.54`` .</span>
<span class="sd">        beta (float, optional): The coefficient :math:`\beta` in the equation above. Default: ``0.46`` .</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): An optional data type of ``mstype.float16`` ,</span>
<span class="sd">            ``mstype.float32`` and ``mstype.float64`` . Default: ``mstype.float32``.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **length** (Tensor) - a positive integer tensor controlling the returned window size, must be 1D.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, A 1-D tensor containing the window, whose shape is :math:`(\text{length},)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `length` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `length` is not integer data type.</span>
<span class="sd">        TypeError: If `periodic` is not a bool.</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If `beta` is not a float.</span>
<span class="sd">        TypeError: If `dtype` is not mindspore.float16, mindspore.float32 or mindspore.float64.</span>
<span class="sd">        ValueError: If dimension of `length` is not 1.</span>
<span class="sd">        ValueError: If data of `length` is negative.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: periodic=True.</span>
<span class="sd">        &gt;&gt;&gt; length = Tensor(np.array([6]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; hamming_window = ops.HammingWindow(periodic=True)</span>
<span class="sd">        &gt;&gt;&gt; y = hamming_window(length)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0.08000001 0.31       0.77000004 1.         0.77000004 0.31      ]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: periodic=False.</span>
<span class="sd">        &gt;&gt;&gt; length = Tensor(np.array([7]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; hamming_window = ops.HammingWindow(periodic=False)</span>
<span class="sd">        &gt;&gt;&gt; y = hamming_window(length)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [0.08000001 0.31       0.77000004 1.         0.77000004 0.31       0.08000001]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.54</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.46</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize HammingWindow&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;periodic&quot;</span><span class="p">,</span> <span class="n">periodic</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;length&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span></div>


<div class="viewcode-block" id="AffineGrid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AffineGrid.html#mindspore.ops.AffineGrid">[docs]</a><span class="k">class</span> <span class="nc">AffineGrid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a 2D or 3D flow field (sampling grid) based on a batch of affine matrices `theta`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.affine_grid` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool, optional): Geometrically, each pixel of input is viewed as a squqre instead of dot.</span>
<span class="sd">            If True, consider extremum -1 and 1 referring to the centers of the pixels rather than pixel corners.</span>
<span class="sd">            The default value is ``False`` , extremum -1 and 1 refer to the corners of the pixels, so that sampling is</span>
<span class="sd">            irrelevant to resolution of the image. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **theta** (Tensor) - The input tensor of flow field whose dtype is float16, float32.</span>
<span class="sd">          Input batch of affine matrices with shape :math:`(N, 2, 3)` for 2D grid or :math:`(N, 3, 4)` for 3D grid.</span>
<span class="sd">        - **output_size** (tuple[int]) - The target output image size.</span>
<span class="sd">          The value of target output with format :math:`(N, C, H, W)` for 2D grid</span>
<span class="sd">          or :math:`(N, C, D, H, W)` for 3D grid.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor whose data type is same as &#39;theta&#39;, and the shape is :math:`(N, H, W, 2)` for 2D grid</span>
<span class="sd">        or :math:`(N, D, H, W, 3)` for 3D grid.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; affinegrid = ops.AffineGrid(align_corners=False)</span>
<span class="sd">        &gt;&gt;&gt; theta = Tensor([[[0.8, 0.5, 0],[-0.5, 0.8, 0]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; out_size = (1, 3, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = affinegrid(theta, out_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[-0.78333336 -0.06666666]</span>
<span class="sd">        [-0.25       -0.4       ]</span>
<span class="sd">        [ 0.28333336 -0.73333335]]</span>
<span class="sd">        [[-0.28333336  0.73333335]</span>
<span class="sd">        [ 0.25        0.4       ]</span>
<span class="sd">        [ 0.78333336  0.06666666]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AffineGrid.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">SegmentMean</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the mean along segments of a Tensor.</span>

<span class="sd">    Specifically, it generates a new Tensor `output` such that :math:`output_i=mean_j(input\_x_j)`</span>
<span class="sd">    in which the mean value is obtained from all elements corresponding</span>
<span class="sd">    to :math:`j` that meets :math:`segment\_ids[j] == i`.</span>
<span class="sd">    If a segment contains no elements for a given segment :math:`i`,</span>
<span class="sd">    then the corresponding element in the output Tensor is set to zero: :math:`output[i] = 0`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the dtype of `input_x` is complex number, the gradient can not be calculated.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is real number or complex number and whose rank is not</span>
<span class="sd">          less than 1.</span>
<span class="sd">        - **segment_ids** (Tensor) - A 1-D tensor whose dtype is int32 or int64. The size of tensor must be equal to</span>
<span class="sd">          the first dimension of the shape of `input_x`. Values must be sorted in ascending order and need not cover</span>
<span class="sd">          all values in the full range of valid values, but must be positive integer. Only constant values is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype and the dimension of the shape is the same as `input_x`. The first dimension of the shape</span>
<span class="sd">        is equal to the value of the last element of `segment_ids` plus one, and the other dimensions are the same as</span>
<span class="sd">        those of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is invalid.</span>
<span class="sd">        TypeError: If the dtype of `segment_ids` is invalid.</span>
<span class="sd">        ValueError: If the rank of `input_x` is less than 1.</span>
<span class="sd">        ValueError: If the rank of `segment_ids` is not equal to 1.</span>
<span class="sd">        ValueError: If the size of `segment_ids` is not equal to the first dimension of the shape of `input_x`.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are negative.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are not sorted in ascending order.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [1, 2, 3], [7, 8, 9]], mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 2], mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SegmentMean()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [7. 8. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SegmentMean&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">SegmentProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative product along segments of a Tensor.</span>

<span class="sd">    Specifically, it generates a new Tensor `output` such that :math:`output_i = \prod_j input\_x_j`</span>
<span class="sd">    in which the cumulative product is obtained from all elements corresponding</span>
<span class="sd">    to :math:`j` that meets :math:`segment\_ids[j] == i`.</span>
<span class="sd">    If a segment contains no elements for a given segment :math:`i`,</span>
<span class="sd">    then the corresponding element in the output Tensor is set to 1: :math:`output[i] = 1`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the dtype of `input_x` is complex number, the gradient can not be calculated.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor whose dtype is real number or complex number and whose rank is not</span>
<span class="sd">          less than 1.</span>
<span class="sd">        - **segment_ids** (Tensor) - A 1-D tensor whose dtype is int32 or int64. The size of tensor must be equal to</span>
<span class="sd">          the first dimension of the shape of `input_x`. Values must be sorted in ascending order and need not cover</span>
<span class="sd">          all values in the full range of valid values, but must be positive integer. Only constant values is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype and the dimension of the shape is the same as `input_x`. The first dimension of the shape</span>
<span class="sd">        is equal to the value of the last element of `segment_ids` plus one, and the other dimensions are the same as</span>
<span class="sd">        those of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `segment_ids` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is invalid.</span>
<span class="sd">        TypeError: If the dtype of `segment_ids` is invalid.</span>
<span class="sd">        ValueError: If the rank of `input_x` is less than 1.</span>
<span class="sd">        ValueError: If the rank of `segment_ids` is not equal to 1.</span>
<span class="sd">        ValueError: If the size of `segment_ids` is not equal to the first dimension of the shape of `input_x`.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are negative.</span>
<span class="sd">        ValueError: If the values of `segment_ids` are not sorted in ascending order.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mstype.float64)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 2], mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.SegmentProd()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, segment_ids)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 4. 10. 18.]</span>
<span class="sd">         [ 1.  1.  1.]</span>
<span class="sd">         [ 7.  8.  9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SegmentProd&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;segment_ids&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="PopulationCount"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.PopulationCount.html#mindspore.ops.PopulationCount">[docs]</a><span class="k">class</span> <span class="nc">PopulationCount</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes element-wise population count(a.k.a bitsum, bitcount).</span>

<span class="sd">    Refer to :func:`mindspore.ops.population_count` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension. The data type must be int16 or uint16 (Ascend).</span>
<span class="sd">          The data type must be int8, int16, int32, int64, uint8, uint16, uint32, uint64 (CPU and GPU).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape as the input, and the data type is uint8.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([0, 1, 3], mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.PopulationCount()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PopulationCount&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="TopK"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TopK.html#mindspore.ops.TopK">[docs]</a><span class="k">class</span> <span class="nc">TopK</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values and indices of the `k` largest entries along the last dimension.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If sorted is set to False, it will use the aicpu operator, the performance may be reduced. In addition, due to</span>
<span class="sd">          different memory layout and traversal methods on different platforms, the display order of calculation results</span>
<span class="sd">          may be inconsistent when `sorted` is False.</span>

<span class="sd">    If the `input_x` is a one-dimensional Tensor, finds the `k` largest entries in the Tensor,</span>
<span class="sd">    and outputs its value and index as a Tensor. values[`k`] is the `k` largest item in `input_x`,</span>
<span class="sd">    and its index is indices [`k`].</span>

<span class="sd">    For a multi-dimensional matrix,</span>
<span class="sd">    calculates the first `k` entries in each row (corresponding vector along the last dimension), therefore:</span>

<span class="sd">    .. math::</span>

<span class="sd">        values.shape = indices.shape = input.shape[:-1] + [k].</span>

<span class="sd">    If the two compared elements are the same, the one with the smaller index value is returned first.</span>

<span class="sd">    Args:</span>
<span class="sd">        sorted (bool, optional): If ``True`` , the obtained elements will be sorted by the values in descending order.</span>
<span class="sd">            If ``False`` , the obtained elements will not be sorted. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input to be computed, data type must be float16, float32 or int32 on CPU,</span>
<span class="sd">          and float16 or float32 on GPU.</span>
<span class="sd">        - **k** (int) - The number of top elements to be computed along the last dimension, constant input is needed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tuple consisting of `values` and `indexes`.</span>

<span class="sd">        - **values** (Tensor) - The `k` largest elements in each slice of the last dimension.</span>
<span class="sd">        - **indices** (Tensor) - The indices of values within the last dimension of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sorted` is not a bool.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If `k` is not an int.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not one of the following: float16, float32 or int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 5], mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; k = 3</span>
<span class="sd">        &gt;&gt;&gt; values, indices = ops.TopK(sorted=True)(input_x, k)</span>
<span class="sd">        &gt;&gt;&gt; print((values, indices))</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float16, value= [ 5.0000e+00,  4.0000e+00,  3.0000e+00]), Tensor(shape=[3],</span>
<span class="sd">          dtype=Int32, value= [4, 3, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TopK.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sorted</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sorted&quot;</span><span class="p">,</span> <span class="nb">sorted</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;sorted&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sorted</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;values&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Bincount"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Bincount.html#mindspore.ops.Bincount">[docs]</a><span class="k">class</span> <span class="nc">Bincount</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Counts the number of occurrences of each value in an integer array.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **array** (Tensor) - A Tensor of type int32, whose value can not be less than zero.</span>
<span class="sd">        - **size** (Tensor) - A non-negative Tensor of type int32.</span>
<span class="sd">        - **weights** (Tensor) - A Tensor with the same shape as array, or a length-0 Tensor, in which case it acts as</span>
<span class="sd">          all weights equal to 1. Must be one of the following types: int32, int64, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor. Has the same type as weights.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `array` is not int32.</span>
<span class="sd">        TypeError: If dtype of `size` is not int32.</span>
<span class="sd">        ValueError: If `size` is negative.</span>
<span class="sd">        ValueError: If `weights` are empty.</span>
<span class="sd">        ValueError: If size of `weights` is not zero and the shape of `weights` is different with the shape of `array`.</span>
<span class="sd">        TypeError: If dtype of `weights` is not in int32,int64,float32,float64</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; array = Tensor(np.array([1, 2, 2, 3, 3, 3, 4, 4, 4, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; size = Tensor(5, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; weights = Tensor(np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bincount = ops.Bincount()</span>
<span class="sd">        &gt;&gt;&gt; bins = bincount(array, size, weights)</span>
<span class="sd">        &gt;&gt;&gt; print(bins)</span>
<span class="sd">        [0. 1. 2. 3. 4.]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Bincount&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;array&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="s1">&#39;weights&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bins&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">CountNonZero</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the total number of non-zero entries in the input tensor along the</span>
<span class="sd">    specified dimensions.</span>

<span class="sd">    Refer to :func:`mindspore.ops.count_nonzero` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        dims (Union[int, tuple(int), list(int)], optional): The dimensions to reduce.</span>
<span class="sd">            Default: ``None`` , reduce over all dimensions.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input data is used to count non-zero numbers. With shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">          Tensor, number of nonzero element across axis specified by `dims`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 0, 1], [1, 1, 2], [0, 0, 1]], dtype=mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; countnonzero = ops.CountNonZero(dims=[1])</span>
<span class="sd">        &gt;&gt;&gt; y = countnonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1 3 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">dims</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">dims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;dims&#39;</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="s2">&quot;CountNonZero&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dims</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">each</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dims</span><span class="p">):</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dims[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span> <span class="n">each</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;CountNonZero&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dims</span> <span class="o">=</span> <span class="n">dims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;dims&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dims</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>