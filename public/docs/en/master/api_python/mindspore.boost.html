<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.boost &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore.numpy" href="mindspore.numpy.html" />
    <link rel="prev" title="mindspore.rewrite" href="mindspore.rewrite.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Native Distributed Parallel Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/migrator_with_tools.html">Application Practice Guide for Network Migration Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax —— Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax —— Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax —— Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.boost</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api_python/mindspore.boost.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="module-mindspore.boost">
<span id="mindspore-boost"></span><h1>mindspore.boost<a class="headerlink" href="#module-mindspore.boost" title="Permalink to this headline"></a></h1>
<p>Boost provide auto accelerating for network, such as Less BN, Gradient Freeze, Gradient
accumulation and so on.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is a beta feature, and we are still improving its functionality.</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.AdaSum">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">AdaSum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">rank</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameter_tuple</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/adasum.html#AdaSum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.AdaSum" title="Permalink to this definition"></a></dt>
<dd><p>The Adaptive Summation, or AdaSum, is a novel algorithm for improving distributed data
parallel training of Deep Learning models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rank</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Rank number.</p></li>
<li><p><strong>device_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Device number.</p></li>
<li><p><strong>group_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Group number.</p></li>
<li><p><strong>parameter_tuple</strong> (<em>Tuple</em><em>(</em><a class="reference internal" href="mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>)</em>) – Tuple of parameters.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>delta_weights</strong> (Tuple(Tensor)) - Tuple of gradients.</p></li>
<li><p><strong>parameters</strong> (Tuple(Parameter)) - Tuple of current parameters.</p></li>
<li><p><strong>old_parameters</strong> (Tuple(Parameter)) - Tuple of last parameters.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>adasum_parameters</strong> (Tuple(Tensor)) - Tuple of parameters after adasum process.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.AutoBoost">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">AutoBoost</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'O0'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boost_config_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.AutoBoost" title="Permalink to this definition"></a></dt>
<dd><p>Provide auto accelerating for network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>level</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Boost config level. Default: <code class="docutils literal notranslate"><span class="pre">&quot;O0&quot;</span></code> .</p></li>
<li><p><strong>boost_config_dict</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – <p>User config hyperparameter dict, recommended config format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;adasum&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;loss_scale_group&quot;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">},</span>
    <span class="s2">&quot;common&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;gradient_split_groups&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
        <span class="s2">&quot;device_number&quot;</span><span class="p">:</span> <span class="mi">8</span>
    <span class="p">},</span>
    <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;fn_flag&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;gc_flag&quot;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">},</span>
    <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;freeze_type&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;freeze_p&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="mi">65536</span>
    <span class="p">}</span>
    <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">:</span> <span class="mi">1</span>
    <span class="p">},</span>
    <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="mf">0.55</span><span class="p">,</span>
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="s2">&quot;sigma&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
        <span class="s2">&quot;n_components&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;pca_mat_path&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;weight_load_dir&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="mi">1800</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code> .</p>
<ul>
<li><p>boost:</p>
<ul class="simple">
<li><p>mode (str): How to set the boost. Supports [“auto”, “manual”, “enable_all”, “disable_all”].
Default: <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> .</p>
<ul>
<li><p>auto: Depend on the argument “boost_level” in class Model.</p></li>
<li><p>manual: Depend on “boost_config_dict”.</p></li>
<li><p>enable_all: Set all boost functions true.</p></li>
<li><p>disable_all: Set all boost functions false.</p></li>
</ul>
</li>
<li><p>less_bn (bool): Whether to apply less_bn function. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p>grad_freeze: (bool): Whether to apply grad_freeze function. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p>adasum (bool): Whether to apply adasum function. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p>grad_accumulation (bool): Whether to apply grad_accumulation function. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p>dim_reduce (bool): Whether to apply dim_reduce function. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p>loss_scale_group (bool): Whether to apply loss_scale_group function. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
</ul>
<p>If set dim_reduce true, other functions will be false.
If set grad_freeze true and dim_reduce false, other functions will be false.</p>
</li>
<li><p>common:</p>
<ul class="simple">
<li><p>gradient_split_groups (list): The gradient split point of this network. Default: <code class="docutils literal notranslate"><span class="pre">[50,</span> <span class="pre">100]</span></code> .</p></li>
<li><p>device_number (int): Device number. Default: <code class="docutils literal notranslate"><span class="pre">8</span></code> .</p></li>
</ul>
</li>
<li><p>less_bn:</p>
<ul class="simple">
<li><p>fn_flag (bool): Whether changing fc to fn. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code> .</p></li>
<li><p>gc_flag (bool): Whether to apply gc. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code> .</p></li>
</ul>
</li>
<li><p>grad_freeze:</p>
<ul class="simple">
<li><p>param_groups (int): The number of parameter groups. Default: <code class="docutils literal notranslate"><span class="pre">10</span></code> .</p></li>
<li><p>freeze_type (int): Gradient freeze grouping strategy, select from [0, 1]. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
<li><p>freeze_p (float): Gradient freezing probability. Default: <code class="docutils literal notranslate"><span class="pre">0.7</span></code> .</p></li>
<li><p>total_steps (int): Total training steps. Default: <code class="docutils literal notranslate"><span class="pre">65536</span></code> .</p></li>
</ul>
</li>
<li><p>grad_accumulation:</p>
<ul class="simple">
<li><p>grad_accumulation_step (int): Steps to accumulate gradients. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
</ul>
</li>
<li><p>dim_reduce:</p>
<p>The leading principles of dim_reduce:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k &amp;= pca\_mat \cdot grad\\
dk &amp;= - bk \cdot grad\_k\\
sk &amp;= rho ^ m \cdot dk\\
delta\_loss &amp;= sigma \cdot grad\_k.T \cdot sk
\end{align}\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p>pca_mat (array): Shape <span class="math notranslate nohighlight">\((k*n)\)</span>, k is part of n_components, n is the size of weight.</p></li>
<li><p>bk (array): Shape <span class="math notranslate nohighlight">\((k*k)\)</span>, is the symmetric positive definite matrix in Quasi-Newton method.</p></li>
</ul>
<p>we need to find the m satisfy:</p>
<div class="math notranslate nohighlight">
\[new\_loss &lt; old\_loss + delta\_loss\]</div>
<p>Then, get delta_grad to update the weights for model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k\_proj &amp;= pca\_mat.T \cdot grad\_k\\
new\_grad\_momentum &amp;= gamma \cdot old\_grad\_momentum + grad - grad\_k\_proj\\
delta\_grad &amp;= alpha \cdot new\_grad\_momentum - pca\_mat.T \cdot sk
\end{align}\end{split}\]</div>
<ul class="simple">
<li><p>rho (float): Generally, it does not need to be modified. Default: <code class="docutils literal notranslate"><span class="pre">0.55</span></code> .</p></li>
<li><p>gamma (float): Generally, it does not need to be modified. Default: <code class="docutils literal notranslate"><span class="pre">0.9</span></code> .</p></li>
<li><p>alpha (float): Generally, it does not need to be modified. Default: <code class="docutils literal notranslate"><span class="pre">0.001</span></code> .</p></li>
<li><p>sigma (float): Generally, it does not need to be modified. Default: <code class="docutils literal notranslate"><span class="pre">0.4</span></code> .</p></li>
<li><p>n_components (int): PCA component. Default: <code class="docutils literal notranslate"><span class="pre">32</span></code> .</p></li>
<li><p>pca_mat_path (str): The path to load pca mat. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p>weight_load_dir (str): The directory to load weight files saved as ckpt. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p>timeout (int): Waiting time to load local pca mat. Default: <code class="docutils literal notranslate"><span class="pre">1800</span> <span class="pre">(second)</span></code> .</p></li>
</ul>
</li>
</ul>
<p>User can load the config through the JSON file or use the dictionary directly.
The unconfigured parameters will adopt the default values.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – The boost mode not in [“auto”, “manual”, “enable_all”, “disable_all”].</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">AutoBoost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) when configuring the dict directly:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">(</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) when loading the dict from a json file:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_json</span> <span class="o">=</span> <span class="s2">&quot;/path/boost_config.json&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">boost_json</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">boost_config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">(</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.AutoBoost.network_auto_process_eval">
<span class="sig-name descname"><span class="pre">network_auto_process_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost.network_auto_process_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.AutoBoost.network_auto_process_eval" title="Permalink to this definition"></a></dt>
<dd><p>Boost network eval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The inference network.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.AutoBoost.network_auto_process_train">
<span class="sig-name descname"><span class="pre">network_auto_process_train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost.network_auto_process_train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.AutoBoost.network_auto_process_train" title="Permalink to this definition"></a></dt>
<dd><p>Boost network train.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">BoostTrainOneStepCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell" title="Permalink to this definition"></a></dt>
<dd><p>Boost Network training package class.</p>
<p>Wraps the network with an optimizer. The resulting Cell is trained with input ‘*inputs’.
The backward graph will be created in the construct function to update the parameter, and different
parallel modes are available for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network. The network only supports single output.</p></li>
<li><p><strong>optimizer</strong> (<em>Union</em><em>[</em><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a><em>]</em>) – Optimizer for updating the weights.</p></li>
<li><p><strong>sens</strong> (<a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a>) – The scaling number to be filled as the input of backpropagation.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> , which is <code class="docutils literal notranslate"><span class="pre">1.0</span></code> .</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>*inputs</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a tensor means the loss value, the shape of which is usually <span class="math notranslate nohighlight">\(()\)</span>.</p>
<ul class="simple">
<li><p>loss(Tensor): A scalar Tensor.</p></li>
<li><p>overflow(Tensor): A scalar Tensor which type is bool.</p></li>
<li><p>loss scaling value(Tensor): A scalar Tensor.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>sens</cite> is not a number.</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) Using the WithLossCell existing provide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Using user-defined WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyWithLossCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
<span class="gp">... </span>       <span class="nb">super</span><span class="p">(</span><span class="n">MyWithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="gp">... </span>       <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="nd">@property</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">backbone_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.adasum_process">
<span class="sig-name descname"><span class="pre">adasum_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.adasum_process"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.adasum_process" title="Permalink to this definition"></a></dt>
<dd><p>Adasum algorithm process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>grads</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of gradient tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Network loss, tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.check_adasum_enable">
<span class="sig-name descname"><span class="pre">check_adasum_enable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.check_adasum_enable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.check_adasum_enable" title="Permalink to this definition"></a></dt>
<dd><p>Check adasum enable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>enable_adasum</strong> (bool) - Check whether the Adasum algorithm is enabled.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable">
<span class="sig-name descname"><span class="pre">check_dim_reduce_enable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.check_dim_reduce_enable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable" title="Permalink to this definition"></a></dt>
<dd><p>Check dim_reduce enable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>enable_dim_reduce</strong> (bool) - Check whether the dimensionality reduction second-order training
algorithm is enabled.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process">
<span class="sig-name descname"><span class="pre">gradient_accumulation_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.gradient_accumulation_process"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process" title="Permalink to this definition"></a></dt>
<dd><p>Gradient accumulation algorithm process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>grads</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of gradient tensors.</p></li>
<li><p><strong>sens</strong> (<a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Network loss, tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process">
<span class="sig-name descname"><span class="pre">gradient_freeze_process</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.gradient_freeze_process"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process" title="Permalink to this definition"></a></dt>
<dd><p>Gradient freeze algorithm process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss</strong> (Tensor) -  Network loss, tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.BoostTrainOneStepWithLossScaleCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">BoostTrainOneStepWithLossScaleCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_sense</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepWithLossScaleCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepWithLossScaleCell" title="Permalink to this definition"></a></dt>
<dd><p>Boost Network training with loss scaling.</p>
<p>This is a training step with loss scaling. It takes a network, an optimizer and possibly a scale update
Cell as args. The loss scale value can be updated in both host side or device side. The
BoostTrainOneStepWithLossScaleCell will be compiled to be graph which takes <cite>*inputs</cite> as input data.
The Tensor type of <cite>scale_sense</cite> is acting as loss scaling value. If you want to update it on host side,
the value must be provided. If the Tensor type of <cite>scale_sense</cite> is not given, the loss scale update logic
must be provide by Cell type of <cite>scale_sense</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network. The network only supports single output.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
<li><p><strong>scale_sense</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a><em>]</em>) – If this value is Cell type, the loss scaling update logic cell.If this value
is Tensor type, Tensor with shape <span class="math notranslate nohighlight">\(()\)</span> or <span class="math notranslate nohighlight">\((1,)\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>*inputs</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the loss, overflow flag and current loss scaling value.</p>
<ul class="simple">
<li><p><strong>loss</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>overflow</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>, type is bool.</p></li>
<li><p><strong>loss scaling value</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>scale_sense</cite> is neither Cell nor Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If shape of <cite>scale_sense</cite> is neither <span class="math notranslate nohighlight">\((1,)\)</span> nor <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) when the type of scale_sense is Cell:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manager</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DynamicLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_window</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_features</span><span class="p">,]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) when the type of scale_sense is Tensor:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">scaling_sens</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.DimReduce">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">DimReduce</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pca_mat_local</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_components</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rho</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sigma</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/dim_reduce.html#DimReduce"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.DimReduce" title="Permalink to this definition"></a></dt>
<dd><p>The dimension reduce training, is a novel algorithm for accelerating convergence of Deep Learning models.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k &amp;= pca\_mat \cdot grad\\
dk &amp;= - bk \cdot grad\_k\\
sk &amp;= rho ^ m \cdot dk\\
delta\_loss &amp;= sigma \cdot grad\_k.T \cdot sk
\end{align}\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p>pca_mat (array): Shape <span class="math notranslate nohighlight">\((k*n)\)</span>, k is part of n_components, n is the size of weight.</p></li>
<li><p>bk (array): Shape <span class="math notranslate nohighlight">\((k*k)\)</span>, is the symmetric positive definite matrix in Quasi-Newton method.</p></li>
</ul>
<p>we need to find the m satisfy:</p>
<div class="math notranslate nohighlight">
\[new\_loss &lt; old\_loss + delta\_loss\]</div>
<p>Then, get delta_grad to update the weights for model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k\_proj &amp;= pca\_mat.T \cdot grad\_k\\
new\_grad\_momentum &amp;= gamma \cdot old\_grad\_momentum + grad - grad\_k\_proj\\
delta\_grad &amp;= alpha \cdot new\_grad\_momentum - pca\_mat.T \cdot sk
\end{align}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network. The network only supports single output.</p></li>
<li><p><strong>optimizer</strong> (<em>Union</em><em>[</em><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a><em>]</em>) – Optimizer for updating the weights.</p></li>
<li><p><strong>weight</strong> (<em>Tuple</em><em>(</em><a class="reference internal" href="mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>)</em>) – Tuple of parameters.</p></li>
<li><p><strong>pca_mat_local</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><em>numpy.ndarray</em></a>) – For PCA operation, k*n, k is part of n_components, n is the size of weight.</p></li>
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – PCA.components.</p></li>
<li><p><strong>rho</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>rank</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Rank number.</p></li>
<li><p><strong>rank_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Rank size.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>old_grad</strong> (Tuple(Tensor)) - Tuple of gradient tensors.</p></li>
<li><p><strong>weight</strong> (Tuple(Tensor)) - Tuple of parameters.</p></li>
<li><p><strong>weight_clone</strong> (Tuple(Tensor)) - clone of weight</p></li>
<li><p><strong>*inputs</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.FreezeOpt">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">FreezeOpt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">opt</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_parameter_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#FreezeOpt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.FreezeOpt" title="Permalink to this definition"></a></dt>
<dd><p>Optimizer that supports gradients freezing training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>opt</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – non-freezing optimizer instance, such as ‘Momentum’, ‘SGD’.</p></li>
<li><p><strong>train_parameter_groups</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em>) – Groups of parameters for gradients freezing training.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>train_strategy</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – Strategy for gradients freezing training.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.GradientAccumulation">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">GradientAccumulation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">max_accumulation_step</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_accumulation.html#GradientAccumulation"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientAccumulation" title="Permalink to this definition"></a></dt>
<dd><p>After accumulating the gradients of multiple steps, call to optimize its update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_accumulation_step</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Steps to accumulate gradients.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">GradientFreeze</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">param_groups</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze" title="Permalink to this definition"></a></dt>
<dd><p>Gradients freezing algorithm, freezing the gradients of some layers randomly,
to improve network training performance. The number and
probability of frozen layers can be configured by users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_groups</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em>) – Groups of parameters for gradients freezing training.</p></li>
<li><p><strong>freeze_type</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Strategy of gradients freezing training.</p></li>
<li><p><strong>freeze_p</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – probability of gradients freezing training.</p></li>
<li><p><strong>total_steps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Steps of the whole training.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>       <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                               <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>       <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_freeze_class</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">GradientFreeze</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">gradient_freeze_class</span><span class="o">.</span><span class="n">freeze_generate</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze.freeze_generate">
<span class="sig-name descname"><span class="pre">freeze_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.freeze_generate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.freeze_generate" title="Permalink to this definition"></a></dt>
<dd><p>Generate freeze network and optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze.generate_freeze_index_sequence">
<span class="sig-name descname"><span class="pre">generate_freeze_index_sequence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameter_groups_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_strategy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_p</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_steps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.generate_freeze_index_sequence"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.generate_freeze_index_sequence" title="Permalink to this definition"></a></dt>
<dd><p>Generate index sequence for gradient freezing training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameter_groups_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of parameter groups.</p></li>
<li><p><strong>freeze_strategy</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Gradient freeze grouping strategy, select from [0, 1].</p></li>
<li><p><strong>freeze_p</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Gradient freezing probability.</p></li>
<li><p><strong>total_steps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Total training steps.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GradientFreeze.split_parameters_groups">
<span class="sig-name descname"><span class="pre">split_parameters_groups</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">net</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_para_groups_number</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.split_parameters_groups"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.split_parameters_groups" title="Permalink to this definition"></a></dt>
<dd><p>Split parameter groups for gradients freezing training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>net</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>freeze_para_groups_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of gradient freeze groups.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">GroupLossScaleManager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">init_loss_scale</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_scale_groups</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager" title="Permalink to this definition"></a></dt>
<dd><p>Enhanced hybrid precision algorithm supports multi-layer application of different loss scales and
dynamic updating of loss scales.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_loss_scale</strong> (<em>Number</em>) – The initialized loss scale value.</p></li>
<li><p><strong>loss_scale_groups</strong> (<em>List</em>) – The loss scale groups, which are divided from the param list.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The output of last operator.</p></li>
<li><p><strong>layer1</strong> (Int) - Current network layer value.</p></li>
<li><p><strong>layer2</strong> (Int) - Last network layer value.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The output of <cite>_DynamicLossScale</cite> operator.</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enhanced_amp</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">num_class</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span> <span class="o">=</span> <span class="n">enhanced_amp</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">GroupLossScaleManager</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="p">[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">loss_scale_manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_group1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_group2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">param_group1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">param_group2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span><span class="o">.</span><span class="n">loss_scale_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_group1</span><span class="p">,</span> <span class="n">param_group2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;manual&quot;</span><span class="p">,</span> <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;adasum&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>                     <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;loss_scale_group&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">boost_level</span><span class="o">=</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="o">=</span><span class="n">boost_config_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.get_loss_scale">
<span class="sig-name descname"><span class="pre">get_loss_scale</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.get_loss_scale"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.get_loss_scale" title="Permalink to this definition"></a></dt>
<dd><p>Get loss scale value.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>bool, <cite>loss_scale</cite> value.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.get_update_cell">
<span class="sig-name descname"><span class="pre">get_update_cell</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.get_update_cell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.get_update_cell" title="Permalink to this definition"></a></dt>
<dd><p>Returns the instance of <a class="reference internal" href="#mindspore.boost.GroupLossScaleManager" title="mindspore.boost.GroupLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.boost.GroupLossScaleManager</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mindspore.boost.GroupLossScaleManager" title="mindspore.boost.GroupLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.boost.GroupLossScaleManager</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.set_loss_scale_status">
<span class="sig-name descname"><span class="pre">set_loss_scale_status</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss_scale_number</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_loss_scale</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.set_loss_scale_status"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.set_loss_scale_status" title="Permalink to this definition"></a></dt>
<dd><p>Generate dynamic loss scale tuple and set overflow status list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_scale_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of loss scale.</p></li>
<li><p><strong>init_loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The initialized loss scale.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.GroupLossScaleManager.update_loss_scale_status">
<span class="sig-name descname"><span class="pre">update_loss_scale_status</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_ratio</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.update_loss_scale_status"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.update_loss_scale_status" title="Permalink to this definition"></a></dt>
<dd><p>Update dynamic loss scale.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Current layer.</p></li>
<li><p><strong>update_ratio</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The ratio of loss scale update.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Outputs:</dt><dd><p>float, new loss scale.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.LessBN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">LessBN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fn_flag</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/less_batch_normalization.html#LessBN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.LessBN" title="Permalink to this definition"></a></dt>
<dd><p>Reduce the number of BN automatically to improve the network performance
and ensure the network accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Network to be modified.</p></li>
<li><p><strong>fn_flag</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Replace FC with FN. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>       <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                               <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>       <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="n">output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">LessBN</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">OptimizerProcess</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">opt</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess" title="Permalink to this definition"></a></dt>
<dd><p>Process optimizer for Boost. Currently, this class supports adding GC(grad centralization) tags
and creating new optimizers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>opt</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer used.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">OptimizerProcess</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_process</span> <span class="o">=</span> <span class="n">OptimizerProcess</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_process</span><span class="o">.</span><span class="n">add_grad_centralization</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_process</span><span class="o">.</span><span class="n">generate_new_optimizer</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.add_grad_centralization">
<span class="sig-name descname"><span class="pre">add_grad_centralization</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.add_grad_centralization"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.add_grad_centralization" title="Permalink to this definition"></a></dt>
<dd><p>Add gradient centralization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.build_gc_params_group">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">build_gc_params_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">params_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.build_gc_params_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.build_gc_params_group" title="Permalink to this definition"></a></dt>
<dd><p>Build the parameter’s group with grad centralization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params_dict</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – The network’s parameter dict.</p></li>
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s parameter list.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.build_params_dict">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">build_params_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.build_params_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.build_params_dict" title="Permalink to this definition"></a></dt>
<dd><p>Build the parameter’s dict of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.OptimizerProcess.generate_new_optimizer">
<span class="sig-name descname"><span class="pre">generate_new_optimizer</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.generate_new_optimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.generate_new_optimizer" title="Permalink to this definition"></a></dt>
<dd><p>Generate new optimizer.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.boost.ParameterProcess">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">ParameterProcess</span></span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess" title="Permalink to this definition"></a></dt>
<dd><p>Process parameter for Boost. Currently, this class supports creating group parameters
and automatically setting gradient segmentation point.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">ParameterProcess</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight2&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">output2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span> <span class="o">+</span> <span class="n">output2</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_parameter</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[:</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="n">ParameterProcess</span><span class="o">.</span><span class="n">generate_group_params</span><span class="p">(</span><span class="n">new_parameter</span><span class="p">,</span> <span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.ParameterProcess.assign_parameter_group">
<span class="sig-name descname"><span class="pre">assign_parameter_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_point</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess.assign_parameter_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess.assign_parameter_group" title="Permalink to this definition"></a></dt>
<dd><p>Assign parameter group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s parameter list.</p></li>
<li><p><strong>split_point</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The gradient split point of this network. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.boost.ParameterProcess.generate_group_params">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">generate_group_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">origin_params</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess.generate_group_params"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess.generate_group_params" title="Permalink to this definition"></a></dt>
<dd><p>Generate group parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s parameter list.</p></li>
<li><p><strong>origin_params</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s origin parameter list.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mindspore.boost.freeze_cell">
<span class="sig-prename descclassname"><span class="pre">mindspore.boost.</span></span><span class="sig-name descname"><span class="pre">freeze_cell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">reducer_flag</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sens</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_grad_accumulation</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_accumulation_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#freeze_cell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.boost.freeze_cell" title="Permalink to this definition"></a></dt>
<dd><p>Generate freeze network and optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reducer_flag</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Reducer flag.</p></li>
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
<li><p><strong>sens</strong> (<a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a>) – The scaling number.</p></li>
<li><p><strong>grad</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of gradient tensors.</p></li>
<li><p><strong>use_grad_accumulation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use gradient accumulation flag.</p></li>
<li><p><strong>mean</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Gradients mean flag. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>degree</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Device number. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>max_accumulation_step</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Max accumulation steps. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost.grad_freeze</span> <span class="kn">import</span> <span class="n">freeze_cell</span><span class="p">,</span> <span class="n">FreezeOpt</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">FreezeOpt</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">freeze_nets</span> <span class="o">=</span> <span class="n">freeze_cell</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.rewrite.html" class="btn btn-neutral float-left" title="mindspore.rewrite" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.numpy.html" class="btn btn-neutral float-right" title="mindspore.numpy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>