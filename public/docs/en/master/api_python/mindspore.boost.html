

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.boost &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore.numpy" href="mindspore.numpy.html" />
    <link rel="prev" title="mindspore.rewrite" href="mindspore.rewrite.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption"><span class="caption-text">Specification</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/master/README.md#table-of-contents">Network List↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">Syntax Support</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>mindspore.boost</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/api_python/mindspore.boost.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-mindspore.boost">
<span id="mindspore-boost"></span><h1>mindspore.boost<a class="headerlink" href="#module-mindspore.boost" title="Permalink to this headline">¶</a></h1>
<p>Boost provide auto accelerating for network, such as Less BN, Gradient Freeze, Gradient
accumulation and so on.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is a beta feature, and we are still improving its functionality.</p>
</div>
<dl class="class">
<dt id="mindspore.boost.AutoBoost">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">AutoBoost</code><span class="sig-paren">(</span><em class="sig-param">level=&quot;O0&quot;</em>, <em class="sig-param">boost_config_dict=&quot;&quot;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.AutoBoost" title="Permalink to this definition">¶</a></dt>
<dd><p>Provide auto accelerating for network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>level</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Boost config level. Default: “O0”.</p></li>
<li><p><strong>boost_config_dict</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – <p>User config hyperparameter dict, recommended config format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;adasum&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;loss_scale_group&quot;</span><span class="p">:</span> <span class="kc">False</span>
    <span class="p">},</span>
    <span class="s2">&quot;common&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;gradient_split_groups&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
        <span class="s2">&quot;device_number&quot;</span><span class="p">:</span> <span class="mi">8</span>
    <span class="p">},</span>
    <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;fn_flag&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="s2">&quot;gc_flag&quot;</span><span class="p">:</span> <span class="kc">True</span>
    <span class="p">},</span>
    <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;freeze_type&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;freeze_p&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="mi">65536</span>
    <span class="p">}</span>
    <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;grad_accumulation_step&quot;</span><span class="p">:</span> <span class="mi">1</span>
    <span class="p">},</span>
    <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;rho&quot;</span><span class="p">:</span> <span class="mf">0.55</span><span class="p">,</span>
        <span class="s2">&quot;gamma&quot;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="s2">&quot;sigma&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
        <span class="s2">&quot;n_components&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
        <span class="s2">&quot;pca_mat_path&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;weight_load_dir&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;timeout&quot;</span><span class="p">:</span> <span class="mi">1800</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<ul>
<li><p>boost:</p>
<ul class="simple">
<li><p>mode (str): How to set the boost. Supports [“auto”, “manual”, “enable_all”, “disable_all”].
Default: “auto”.</p>
<ul>
<li><p>auto: Depend on the argument “boost_level” in class Model.</p></li>
<li><p>manual: Depend on “boost_config_dict”.</p></li>
<li><p>enable_all: Set all boost functions true.</p></li>
<li><p>disable_all: Set all boost functions false.</p></li>
</ul>
</li>
<li><p>less_bn (bool): Whether to apply less_bn function. Default: False.</p></li>
<li><p>grad_freeze: (bool): Whether to apply grad_freeze function. Default: False.</p></li>
<li><p>adasum (bool): Whether to apply adasum function. Default: False.</p></li>
<li><p>grad_accumulation (bool): Whether to apply grad_accumulation function. Default: False.</p></li>
<li><p>dim_reduce (bool): Whether to apply dim_reduce function. Default: False.</p></li>
<li><p>loss_scale_group (bool): Whether to apply loss_scale_group function. Default: False.</p></li>
</ul>
<p>If set dim_reduce true, other functions will be false.
If set grad_freeze true and dim_reduce false, other functions will be false.</p>
</li>
<li><p>common:</p>
<ul class="simple">
<li><p>gradient_split_groups (list): The gradient split point of this network. Default: [50, 100].</p></li>
<li><p>device_number (int): Device number. Default: 8.</p></li>
</ul>
</li>
<li><p>less_bn:</p>
<ul class="simple">
<li><p>fn_flag (bool): Whether changing fc to fn. Default: True.</p></li>
<li><p>gc_flag (bool): Whether to apply gc. Default: True.</p></li>
</ul>
</li>
<li><p>grad_freeze:</p>
<ul class="simple">
<li><p>param_groups (int): The number of parameter groups. Default: 10.</p></li>
<li><p>freeze_type (int): Gradient freeze grouping strategy, select from [0, 1]. Default: 1.</p></li>
<li><p>freeze_p (float): Gradient freezing probability. Default: 0.7.</p></li>
<li><p>total_steps (int): Total training steps. Default: 65536.</p></li>
</ul>
</li>
<li><p>grad_accumulation:</p>
<ul class="simple">
<li><p>grad_accumulation_step (int): Steps to accumulate gradients. Default: 1.</p></li>
</ul>
</li>
<li><p>dim_reduce:</p>
<p>The leading principles of dim_reduce:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k &amp;= pca\_mat \cdot grad\\
dk &amp;= - bk \cdot grad\_k\\
sk &amp;= rho ^ m \cdot dk\\
delta\_loss &amp;= sigma \cdot grad\_k.T \cdot sk
\end{align}\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p>pca_mat (array): Shape <span class="math notranslate nohighlight">\((k*n)\)</span>, k is part of n_components, n is the size of weight.</p></li>
<li><p>bk (array): Shape <span class="math notranslate nohighlight">\((k*k)\)</span>, is the symmetric positive definite matrix in Quasi-Newton method.</p></li>
</ul>
<p>we need to find the m satisfy:</p>
<div class="math notranslate nohighlight">
\[new\_loss &lt; old\_loss + delta\_loss\]</div>
<p>Then, get delta_grad to update the weights for model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k\_proj &amp;= pca\_mat.T \cdot grad\_k\\
new\_grad\_momentum &amp;= gamma \cdot old\_grad\_momentum + grad - grad\_k\_proj\\
delta\_grad &amp;= alpha \cdot new\_grad\_momentum - pca\_mat.T \cdot sk
\end{align}\end{split}\]</div>
<ul class="simple">
<li><p>rho (float): Generally, it does not need to be modified. Default: 0.55.</p></li>
<li><p>gamma (float): Generally, it does not need to be modified. Default: 0.9.</p></li>
<li><p>alpha (float): Generally, it does not need to be modified. Default: 0.001.</p></li>
<li><p>sigma (float): Generally, it does not need to be modified. Default: 0.4.</p></li>
<li><p>n_components (int): PCA component. Default: 32.</p></li>
<li><p>pca_mat_path (str): The path to load pca mat. Default: None.</p></li>
<li><p>weight_load_dir (str): The directory to load weight files saved as ckpt. Default: None.</p></li>
<li><p>timeout (int): Waiting time to load local pca mat. Default: 1800 (second).</p></li>
</ul>
</li>
</ul>
<p>User can load the config through the JSON file or use the dictionary directly.
The unconfigured parameters will adopt the default values.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – The boost mode not in [“auto”, “manual”, “enable_all”, “disable_all”].</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">AutoBoost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) when configuring the dict directly:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;auto&quot;</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">(</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) when loading the dict from a json file:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_json</span> <span class="o">=</span> <span class="s2">&quot;/path/boost_config.json&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">boost_json</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">boost_config_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">(</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.boost.AutoBoost.network_auto_process_eval">
<code class="sig-name descname">network_auto_process_eval</code><span class="sig-paren">(</span><em class="sig-param">network</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost.network_auto_process_eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.AutoBoost.network_auto_process_eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Boost network eval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The inference network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.AutoBoost.network_auto_process_train">
<code class="sig-name descname">network_auto_process_train</code><span class="sig-paren">(</span><em class="sig-param">network</em>, <em class="sig-param">optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost.html#AutoBoost.network_auto_process_train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.AutoBoost.network_auto_process_train" title="Permalink to this definition">¶</a></dt>
<dd><p>Boost network train.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.boost.OptimizerProcess">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">OptimizerProcess</code><span class="sig-paren">(</span><em class="sig-param">opt</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess" title="Permalink to this definition">¶</a></dt>
<dd><p>Process optimizer for Boost. Currently, this class supports adding GC(grad centralization) tags
and creating new optimizers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>opt</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer used.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">OptimizerProcess</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_process</span> <span class="o">=</span> <span class="n">OptimizerProcess</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_process</span><span class="o">.</span><span class="n">add_grad_centralization</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_process</span><span class="o">.</span><span class="n">generate_new_optimizer</span><span class="p">()</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.boost.OptimizerProcess.add_grad_centralization">
<code class="sig-name descname">add_grad_centralization</code><span class="sig-paren">(</span><em class="sig-param">network</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.add_grad_centralization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.add_grad_centralization" title="Permalink to this definition">¶</a></dt>
<dd><p>Add gradient centralization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.OptimizerProcess.build_gc_params_group">
<em class="property">static </em><code class="sig-name descname">build_gc_params_group</code><span class="sig-paren">(</span><em class="sig-param">params_dict</em>, <em class="sig-param">parameters</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.build_gc_params_group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.build_gc_params_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the parameter’s group with grad centralization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params_dict</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – The network’s parameter dict.</p></li>
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s parameter list.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.OptimizerProcess.build_params_dict">
<em class="property">static </em><code class="sig-name descname">build_params_dict</code><span class="sig-paren">(</span><em class="sig-param">network</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.build_params_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.build_params_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the parameter’s dict of the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.OptimizerProcess.generate_new_optimizer">
<code class="sig-name descname">generate_new_optimizer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#OptimizerProcess.generate_new_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.OptimizerProcess.generate_new_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate new optimizer.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.boost.ParameterProcess">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">ParameterProcess</code><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess" title="Permalink to this definition">¶</a></dt>
<dd><p>Process parameter for Boost. Currently, this class supports creating group parameters
and automatically setting gradient segmentation point.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost</span> <span class="kn">import</span> <span class="n">ParameterProcess</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight2&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">output2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span> <span class="o">+</span> <span class="n">output2</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_parameter</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()[:</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">group_params</span> <span class="o">=</span> <span class="n">ParameterProcess</span><span class="o">.</span><span class="n">generate_group_params</span><span class="p">(</span><span class="n">new_parameter</span><span class="p">,</span> <span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.boost.ParameterProcess.assign_parameter_group">
<code class="sig-name descname">assign_parameter_group</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">split_point=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess.assign_parameter_group"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess.assign_parameter_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Assign parameter group.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s parameter list.</p></li>
<li><p><strong>split_point</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The gradient split point of this network. default: None.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.ParameterProcess.generate_group_params">
<em class="property">static </em><code class="sig-name descname">generate_group_params</code><span class="sig-paren">(</span><em class="sig-param">parameters</em>, <em class="sig-param">origin_params</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/base.html#ParameterProcess.generate_group_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.ParameterProcess.generate_group_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate group parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameters</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s parameter list.</p></li>
<li><p><strong>origin_params</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – The network’s origin parameter list.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.boost.BoostTrainOneStepCell">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">BoostTrainOneStepCell</code><span class="sig-paren">(</span><em class="sig-param">network</em>, <em class="sig-param">optimizer</em>, <em class="sig-param">sens=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Boost Network training package class.</p>
<p>Wraps the network with an optimizer. The resulting Cell is trained with input ‘*inputs’.
The backward graph will be created in the construct function to update the parameter, and different
parallel modes are available for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network. The network only supports single output.</p></li>
<li><p><strong>optimizer</strong> (<em>Union</em><em>[</em><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a><em>]</em>) – Optimizer for updating the weights.</p></li>
<li><p><strong>sens</strong> (<a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a>) – The scaling number to be filled as the input of backpropagation. Default value is 1.0.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>*inputs</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tensor, a tensor means the loss value, the shape of which is usually <span class="math notranslate nohighlight">\(()\)</span>.</p>
<ul class="simple">
<li><p>loss(Tensor): A scalar Tensor.</p></li>
<li><p>overflow(Tensor): A scalar Tensor which type is bool.</p></li>
<li><p>loss scaling value(Tensor): A scalar Tensor.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>sens</cite> is not a number.</p>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) Using the WithLossCell existing provide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) Using user-defined WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyWithLossCell</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
<span class="gp">... </span>       <span class="nb">super</span><span class="p">(</span><span class="n">MyWithLossCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span> <span class="o">=</span> <span class="n">backbone</span>
<span class="gp">... </span>       <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="gp">... </span>       <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="nd">@property</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">backbone_network</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backbone</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">MyWithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_net</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">loss_net</span><span class="p">,</span> <span class="n">optim</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.boost.BoostTrainOneStepCell.adasum_process">
<code class="sig-name descname">adasum_process</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">grads</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.adasum_process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.adasum_process" title="Permalink to this definition">¶</a></dt>
<dd><p>Adasum algorithm process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>grads</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of gradient tensors.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Network loss, tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.BoostTrainOneStepCell.check_adasum_enable">
<code class="sig-name descname">check_adasum_enable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.check_adasum_enable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.check_adasum_enable" title="Permalink to this definition">¶</a></dt>
<dd><p>Check adasum enable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>enable_adasum</strong> (bool) - Check whether the Adasum algorithm is enabled.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable">
<code class="sig-name descname">check_dim_reduce_enable</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.check_dim_reduce_enable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable" title="Permalink to this definition">¶</a></dt>
<dd><p>Check dim_reduce enable.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>enable_dim_reduce</strong> (bool) - Check whether the dimensionality reduction second-order training
algorithm is enabled.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process">
<code class="sig-name descname">gradient_accumulation_process</code><span class="sig-paren">(</span><em class="sig-param">loss</em>, <em class="sig-param">grads</em>, <em class="sig-param">sens</em>, <em class="sig-param">*inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.gradient_accumulation_process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient accumulation algorithm process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>grads</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of gradient tensors.</p></li>
<li><p><strong>sens</strong> (<a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a>) – Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Network loss, tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process">
<code class="sig-name descname">gradient_freeze_process</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepCell.gradient_freeze_process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient freeze algorithm process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>loss</strong> (Tensor) -  Network loss, tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.boost.BoostTrainOneStepWithLossScaleCell">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">BoostTrainOneStepWithLossScaleCell</code><span class="sig-paren">(</span><em class="sig-param">network</em>, <em class="sig-param">optimizer</em>, <em class="sig-param">scale_sense</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/boost_cell_wrapper.html#BoostTrainOneStepWithLossScaleCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.BoostTrainOneStepWithLossScaleCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Boost Network training with loss scaling.</p>
<p>This is a training step with loss scaling. It takes a network, an optimizer and possibly a scale update
Cell as args. The loss scale value can be updated in both host side or device side. The
BoostTrainOneStepWithLossScaleCell will be compiled to be graph which takes <cite>*inputs</cite> as input data.
The Tensor type of <cite>scale_sense</cite> is acting as loss scaling value. If you want to update it on host side,
the value must be provided. If the Tensor type of <cite>scale_sense</cite> is not given, the loss scale update logic
must be provide by Cell type of <cite>scale_sense</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network. The network only supports single output.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
<li><p><strong>scale_sense</strong> (<em>Union</em><em>[</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a><em>]</em>) – If this value is Cell type, the loss scaling update logic cell.If this value
is Tensor type, Tensor with shape <span class="math notranslate nohighlight">\(()\)</span> or <span class="math notranslate nohighlight">\((1,)\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>*inputs</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><p>Tuple of 3 Tensor, the loss, overflow flag and current loss scaling value.</p>
<ul class="simple">
<li><p><strong>loss</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>overflow</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>, type is bool.</p></li>
<li><p><strong>loss scaling value</strong> (Tensor) -  Tensor with shape <span class="math notranslate nohighlight">\(()\)</span></p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>scale_sense</cite> is neither Cell nor Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If shape of <cite>scale_sense</cite> is neither <span class="math notranslate nohighlight">\((1,)\)</span> nor <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">WithLossCell</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#1) when the type of scale_sense is Cell:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manager</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DynamicLossScaleUpdateCell</span><span class="p">(</span><span class="n">loss_scale_value</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">12</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale_window</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_features</span><span class="p">,</span> <span class="n">in_features</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">out_features</span><span class="p">,]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#2) when the type of scale_sense is Tensor:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_with_loss</span> <span class="o">=</span> <span class="n">WithLossCell</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">size</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">net_with_loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="o">=</span><span class="n">scaling_sens</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">train_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.boost.LessBN">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">LessBN</code><span class="sig-paren">(</span><em class="sig-param">network</em>, <em class="sig-param">fn_flag=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/less_batch_normalization.html#LessBN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.LessBN" title="Permalink to this definition">¶</a></dt>
<dd><p>Reduce the number of BN automatically to improve the network performance
and ensure the network accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Network to be modified.</p></li>
<li><p><strong>fn_flag</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Replace FC with FN. default: False.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">LessBN</span><span class="p">(</span><span class="n">network</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.boost.GradientFreeze">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">GradientFreeze</code><span class="sig-paren">(</span><em class="sig-param">param_groups</em>, <em class="sig-param">freeze_type</em>, <em class="sig-param">freeze_p</em>, <em class="sig-param">total_steps</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradients freezing algorithm, freezing the gradients of some layers randomly,
to improve network training performance. The number and
probability of frozen layers can be configured by users.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>param_groups</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em>) – Groups of parameters for gradients freezing training.</p></li>
<li><p><strong>freeze_type</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Strategy of gradients freezing training.</p></li>
<li><p><strong>freeze_p</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – probability of gradients freezing training.</p></li>
<li><p><strong>total_steps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Steps of the whole training.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gradient_freeze_class</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">GradientFreeze</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">gradient_freeze_class</span><span class="o">.</span><span class="n">freeze_generate</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.boost.GradientFreeze.freeze_generate">
<code class="sig-name descname">freeze_generate</code><span class="sig-paren">(</span><em class="sig-param">network</em>, <em class="sig-param">optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.freeze_generate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.freeze_generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate freeze network and optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.GradientFreeze.generate_freeze_index_sequence">
<code class="sig-name descname">generate_freeze_index_sequence</code><span class="sig-paren">(</span><em class="sig-param">parameter_groups_number</em>, <em class="sig-param">freeze_strategy</em>, <em class="sig-param">freeze_p</em>, <em class="sig-param">total_steps</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.generate_freeze_index_sequence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.generate_freeze_index_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate index sequence for gradient freezing training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parameter_groups_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of parameter groups.</p></li>
<li><p><strong>freeze_strategy</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Gradient freeze grouping strategy, select from [0, 1].</p></li>
<li><p><strong>freeze_p</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Gradient freezing probability.</p></li>
<li><p><strong>total_steps</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Total training steps.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.GradientFreeze.split_parameters_groups">
<code class="sig-name descname">split_parameters_groups</code><span class="sig-paren">(</span><em class="sig-param">net</em>, <em class="sig-param">freeze_para_groups_number</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#GradientFreeze.split_parameters_groups"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GradientFreeze.split_parameters_groups" title="Permalink to this definition">¶</a></dt>
<dd><p>Split parameter groups for gradients freezing training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>net</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>freeze_para_groups_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of gradient freeze groups.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="mindspore.boost.FreezeOpt">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">FreezeOpt</code><span class="sig-paren">(</span><em class="sig-param">opt</em>, <em class="sig-param">train_parameter_groups=None</em>, <em class="sig-param">train_strategy=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#FreezeOpt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.FreezeOpt" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimizer that supports gradients freezing training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>opt</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – non-freezing optimizer instance, such as ‘Momentum’, ‘SGD’.</p></li>
<li><p><strong>train_parameter_groups</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>]</em>) – Groups of parameters for gradients freezing training.</p></li>
<li><p><strong>train_strategy</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>)</em><em>, </em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em>) – Strategy for gradients freezing training.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="mindspore.boost.freeze_cell">
<code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">freeze_cell</code><span class="sig-paren">(</span><em class="sig-param">reducer_flag</em>, <em class="sig-param">network</em>, <em class="sig-param">optimizer</em>, <em class="sig-param">sens</em>, <em class="sig-param">grad</em>, <em class="sig-param">use_grad_accumulation</em>, <em class="sig-param">mean=None</em>, <em class="sig-param">degree=None</em>, <em class="sig-param">max_accumulation_step=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_freeze.html#freeze_cell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.freeze_cell" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate freeze network and optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>reducer_flag</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Reducer flag.</p></li>
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights.</p></li>
<li><p><strong>sens</strong> (<a class="reference external" href="https://docs.python.org/library/numbers.html#numbers.Number" title="(in Python v3.8)"><em>numbers.Number</em></a>) – The scaling number.</p></li>
<li><p><strong>grad</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>(</em><a class="reference internal" href="mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>)</em>) – Tuple of gradient tensors.</p></li>
<li><p><strong>use_grad_accumulation</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Use gradient accumulation flag.</p></li>
<li><p><strong>mean</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Gradients mean flag. default: None.</p></li>
<li><p><strong>degree</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Device number. default: None.</p></li>
<li><p><strong>max_accumulation_step</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Max accumulation steps. default: 1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.boost.grad_freeze</span> <span class="kn">import</span> <span class="n">freeze_cell</span><span class="p">,</span> <span class="n">FreezeOpt</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                <span class="n">name</span><span class="o">=</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">output</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">network</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">FreezeOpt</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">freeze_nets</span> <span class="o">=</span> <span class="n">freeze_cell</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="mindspore.boost.GradientAccumulation">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">GradientAccumulation</code><span class="sig-paren">(</span><em class="sig-param">max_accumulation_step</em>, <em class="sig-param">optimizer</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/grad_accumulation.html#GradientAccumulation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GradientAccumulation" title="Permalink to this definition">¶</a></dt>
<dd><p>After accumulating the gradients of multiple steps, call to optimize its update.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>max_accumulation_step</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Steps to accumulate gradients.</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer used.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.boost.AdaSum">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">AdaSum</code><span class="sig-paren">(</span><em class="sig-param">rank</em>, <em class="sig-param">device_number</em>, <em class="sig-param">group_number</em>, <em class="sig-param">parameter_tuple</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/adasum.html#AdaSum"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.AdaSum" title="Permalink to this definition">¶</a></dt>
<dd><p>The Adaptive Summation, or AdaSum, is a novel algorithm for improving distributed data
parallel training of Deep Learning models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>rank</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Rank number.</p></li>
<li><p><strong>device_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Device number.</p></li>
<li><p><strong>group_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Group number.</p></li>
<li><p><strong>parameter_tuple</strong> (<em>Tuple</em><em>(</em><a class="reference internal" href="mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>)</em>) – Tuple of parameters.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>delta_weights</strong> (Tuple(Tensor)) - Tuple of gradients.</p></li>
<li><p><strong>parameters</strong> (Tuple(Parameter)) - Tuple of current parameters.</p></li>
<li><p><strong>old_parameters</strong> (Tuple(Parameter)) - Tuple of last parameters.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>adasum_parameters</strong> (Tuple(Tensor)) - Tuple of parameters after adasum process.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.boost.DimReduce">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">DimReduce</code><span class="sig-paren">(</span><em class="sig-param">network</em>, <em class="sig-param">optimizer</em>, <em class="sig-param">weight</em>, <em class="sig-param">pca_mat_local</em>, <em class="sig-param">n_components</em>, <em class="sig-param">rho</em>, <em class="sig-param">gamma</em>, <em class="sig-param">alpha</em>, <em class="sig-param">sigma</em>, <em class="sig-param">rank</em>, <em class="sig-param">rank_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/dim_reduce.html#DimReduce"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.DimReduce" title="Permalink to this definition">¶</a></dt>
<dd><p>The dimension reduce training, is a novel algorithm for accelerating convergence of Deep Learning models.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k &amp;= pca\_mat \cdot grad\\
dk &amp;= - bk \cdot grad\_k\\
sk &amp;= rho ^ m \cdot dk\\
delta\_loss &amp;= sigma \cdot grad\_k.T \cdot sk
\end{align}\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p>pca_mat (array): Shape <span class="math notranslate nohighlight">\((k*n)\)</span>, k is part of n_components, n is the size of weight.</p></li>
<li><p>bk (array): Shape <span class="math notranslate nohighlight">\((k*k)\)</span>, is the symmetric positive definite matrix in Quasi-Newton method.</p></li>
</ul>
<p>we need to find the m satisfy:</p>
<div class="math notranslate nohighlight">
\[new\_loss &lt; old\_loss + delta\_loss\]</div>
<p>Then, get delta_grad to update the weights for model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
grad\_k\_proj &amp;= pca\_mat.T \cdot grad\_k\\
new\_grad\_momentum &amp;= gamma \cdot old\_grad\_momentum + grad - grad\_k\_proj\\
delta\_grad &amp;= alpha \cdot new\_grad\_momentum - pca\_mat.T \cdot sk
\end{align}\end{split}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – The training network. The network only supports single output.</p></li>
<li><p><strong>optimizer</strong> (<em>Union</em><em>[</em><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a><em>]</em>) – Optimizer for updating the weights.</p></li>
<li><p><strong>weight</strong> (<em>Tuple</em><em>(</em><a class="reference internal" href="mindspore/mindspore.Parameter.html#mindspore.Parameter" title="mindspore.Parameter"><em>Parameter</em></a><em>)</em>) – Tuple of parameters.</p></li>
<li><p><strong>pca_mat_local</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.17)"><em>numpy.ndarray</em></a>) – For PCA operation, k*n, k is part of n_components, n is the size of weight.</p></li>
<li><p><strong>n_components</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – PCA.components.</p></li>
<li><p><strong>rho</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Coefficient.</p></li>
<li><p><strong>rank</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Rank number.</p></li>
<li><p><strong>rank_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Rank size.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
<li><p><strong>old_grad</strong> (Tuple(Tensor)) - Tuple of gradient tensors.</p></li>
<li><p><strong>weight</strong> (Tuple(Tensor)) - Tuple of parameters.</p></li>
<li><p><strong>weight_clone</strong> (Tuple(Tensor)) - clone of weight</p></li>
<li><p><strong>*inputs</strong> (Tuple(Tensor)) - Tuple of input tensors with shape <span class="math notranslate nohighlight">\((N, \ldots)\)</span>.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>loss</strong> (Tensor) - Tensor with shape <span class="math notranslate nohighlight">\(()\)</span>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="mindspore.boost.GroupLossScaleManager">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.boost.</code><code class="sig-name descname">GroupLossScaleManager</code><span class="sig-paren">(</span><em class="sig-param">init_loss_scale</em>, <em class="sig-param">loss_scale_groups</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager" title="Permalink to this definition">¶</a></dt>
<dd><p>Enhanced hybrid precision algorithm supports multi-layer application of different loss scales and
dynamic updating of loss scales.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_loss_scale</strong> (<em>Number</em>) – The initialized loss scale value.</p></li>
<li><p><strong>loss_scale_groups</strong> (<em>List</em>) – The loss scale groups, which are divided from the param list.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Inputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The output of last operator.</p></li>
<li><p><strong>layer1</strong> (Int) - Current network layer value.</p></li>
<li><p><strong>layer2</strong> (Int) - Last network layer value.</p></li>
</ul>
</dd>
<dt>Outputs:</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - The output of <cite>_DynamicLossScale</cite> operator.</p></li>
</ul>
</dd>
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">boost</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enhanced_amp</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">84</span><span class="p">,</span> <span class="n">num_class</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s1">&#39;ones&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span> <span class="o">=</span> <span class="n">enhanced_amp</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enhanced_amp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">boost</span><span class="o">.</span><span class="n">GroupLossScaleManager</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="p">[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">loss_scale_manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_group1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">param_group2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">param_group1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">else</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">param_group2</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span><span class="o">.</span><span class="n">loss_scale_groups</span> <span class="o">=</span> <span class="p">[</span><span class="n">param_group1</span><span class="p">,</span> <span class="n">param_group2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boost_config_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;boost&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;manual&quot;</span><span class="p">,</span> <span class="s2">&quot;less_bn&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;adasum&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>         <span class="o">&gt;&gt;&gt;</span>                      <span class="s2">&quot;grad_accumulation&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;dim_reduce&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;loss_scale_group&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">,</span>         <span class="o">&gt;&gt;&gt;</span>               <span class="n">boost_level</span><span class="o">=</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span> <span class="n">boost_config_dict</span><span class="o">=</span><span class="n">boost_config_dict</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># For details about how to build the dataset, please refer to the variable `dataset_train` in tutorial</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># document on the official website:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://www.mindspore.cn/tutorials/zh-CN/master/beginner/quick_start.html</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_custom_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore.boost.GroupLossScaleManager.get_loss_scale">
<code class="sig-name descname">get_loss_scale</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.get_loss_scale"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.get_loss_scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Get loss scale value.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>bool, <cite>loss_scale</cite> value.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.GroupLossScaleManager.get_update_cell">
<code class="sig-name descname">get_update_cell</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.get_update_cell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.get_update_cell" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the instance of <a class="reference internal" href="#mindspore.boost.GroupLossScaleManager" title="mindspore.boost.GroupLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.boost.GroupLossScaleManager</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><a class="reference internal" href="#mindspore.boost.GroupLossScaleManager" title="mindspore.boost.GroupLossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.boost.GroupLossScaleManager</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.GroupLossScaleManager.set_loss_scale_status">
<code class="sig-name descname">set_loss_scale_status</code><span class="sig-paren">(</span><em class="sig-param">loss_scale_number</em>, <em class="sig-param">init_loss_scale</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.set_loss_scale_status"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.set_loss_scale_status" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate dynamic loss scale tuple and set overflow status list.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss_scale_number</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The number of loss scale.</p></li>
<li><p><strong>init_loss_scale</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The initialized loss scale.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore.boost.GroupLossScaleManager.update_loss_scale_status">
<code class="sig-name descname">update_loss_scale_status</code><span class="sig-paren">(</span><em class="sig-param">layer</em>, <em class="sig-param">update_ratio</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/boost/group_loss_scale_manager.html#GroupLossScaleManager.update_loss_scale_status"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore.boost.GroupLossScaleManager.update_loss_scale_status" title="Permalink to this definition">¶</a></dt>
<dd><p>Update dynamic loss scale.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>layer</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Current layer.</p></li>
<li><p><strong>update_ratio</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – The ratio of loss scale update.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Outputs:</dt><dd><p>float, new loss scale.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.numpy.html" class="btn btn-neutral float-right" title="mindspore.numpy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.rewrite.html" class="btn btn-neutral float-left" title="mindspore.rewrite" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>