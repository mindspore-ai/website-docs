<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore.nn.Cell" href="nn/mindspore.nn.Cell.html" />
    <link rel="prev" title="mindspore.run_check" href="mindspore/mindspore.run_check.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Native Distributed Parallel Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-block">Basic Block</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Cell.html">mindspore.nn.Cell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GraphCell.html">mindspore.nn.GraphCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LossBase.html">mindspore.nn.LossBase</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Optimizer.html">mindspore.nn.Optimizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#container">Container</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.CellList.html">mindspore.nn.CellList</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SequentialCell.html">mindspore.nn.SequentialCell</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#wrapper-layer">Wrapper Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.DistributedGradReducer.html">mindspore.nn.DistributedGradReducer</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.DynamicLossScaleUpdateCell.html">mindspore.nn.DynamicLossScaleUpdateCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.FixedLossScaleUpdateCell.html">mindspore.nn.FixedLossScaleUpdateCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ForwardValueAndGrad.html">mindspore.nn.ForwardValueAndGrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GetNextSingleOp.html">mindspore.nn.GetNextSingleOp</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MicroBatchInterleaved.html">mindspore.nn.MicroBatchInterleaved</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ParameterUpdate.html">mindspore.nn.ParameterUpdate</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.PipelineCell.html">mindspore.nn.PipelineCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TimeDistributed.html">mindspore.nn.TimeDistributed</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TrainOneStepCell.html">mindspore.nn.TrainOneStepCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TrainOneStepWithLossScaleCell.html">mindspore.nn.TrainOneStepWithLossScaleCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.WithEvalCell.html">mindspore.nn.WithEvalCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.WithLossCell.html">mindspore.nn.WithLossCell</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#convolutional-layer">Convolutional Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Conv1d.html">mindspore.nn.Conv1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Conv1dTranspose.html">mindspore.nn.Conv1dTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Conv2d.html">mindspore.nn.Conv2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Conv2dTranspose.html">mindspore.nn.Conv2dTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Conv3d.html">mindspore.nn.Conv3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Conv3dTranspose.html">mindspore.nn.Conv3dTranspose</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Unfold.html">mindspore.nn.Unfold</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#recurrent-layer">Recurrent Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.RNN.html">mindspore.nn.RNN</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.RNNCell.html">mindspore.nn.RNNCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GRU.html">mindspore.nn.GRU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GRUCell.html">mindspore.nn.GRUCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LSTM.html">mindspore.nn.LSTM</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LSTMCell.html">mindspore.nn.LSTMCell</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#transformer-layer">Transformer Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MultiheadAttention.html">mindspore.nn.MultiheadAttention</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TransformerEncoderLayer.html">mindspore.nn.TransformerEncoderLayer</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TransformerDecoderLayer.html">mindspore.nn.TransformerDecoderLayer</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TransformerEncoder.html">mindspore.nn.TransformerEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TransformerDecoder.html">mindspore.nn.TransformerDecoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Transformer.html">mindspore.nn.Transformer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#embedding-layer">Embedding Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Embedding.html">mindspore.nn.Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.EmbeddingLookup.html">mindspore.nn.EmbeddingLookup</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MultiFieldEmbeddingLookup.html">mindspore.nn.MultiFieldEmbeddingLookup</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nonlinear-activation-layer">Nonlinear Activation Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.CELU.html">mindspore.nn.CELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ELU.html">mindspore.nn.ELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.FastGelu.html">mindspore.nn.FastGelu</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GELU.html">mindspore.nn.GELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GLU.html">mindspore.nn.GLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.get_activation.html">mindspore.nn.get_activation</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Hardtanh.html">mindspore.nn.Hardtanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.HShrink.html">mindspore.nn.HShrink</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.HSigmoid.html">mindspore.nn.HSigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.HSwish.html">mindspore.nn.HSwish</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LeakyReLU.html">mindspore.nn.LeakyReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LogSigmoid.html">mindspore.nn.LogSigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LogSoftmax.html">mindspore.nn.LogSoftmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LRN.html">mindspore.nn.LRN</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Mish.html">mindspore.nn.Mish</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Softsign.html">mindspore.nn.Softsign</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.PReLU.html">mindspore.nn.PReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReLU.html">mindspore.nn.ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReLU6.html">mindspore.nn.ReLU6</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.RReLU.html">mindspore.nn.RReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SeLU.html">mindspore.nn.SeLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SiLU.html">mindspore.nn.SiLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Sigmoid.html">mindspore.nn.Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Softmin.html">mindspore.nn.Softmin</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Softmax.html">mindspore.nn.Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Softmax2d.html">mindspore.nn.Softmax2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SoftShrink.html">mindspore.nn.SoftShrink</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Tanh.html">mindspore.nn.Tanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Tanhshrink.html">mindspore.nn.Tanhshrink</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Threshold.html">mindspore.nn.Threshold</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#linear-layer">Linear Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Dense.html">mindspore.nn.Dense</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.BiDense.html">mindspore.nn.BiDense</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dropout-layer">Dropout Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Dropout.html">mindspore.nn.Dropout</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Dropout1d.html">mindspore.nn.Dropout1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Dropout2d.html">mindspore.nn.Dropout2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Dropout3d.html">mindspore.nn.Dropout3d</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#normalization-layer">Normalization Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.BatchNorm1d.html">mindspore.nn.BatchNorm1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.BatchNorm2d.html">mindspore.nn.BatchNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.BatchNorm3d.html">mindspore.nn.BatchNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GroupNorm.html">mindspore.nn.GroupNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.InstanceNorm1d.html">mindspore.nn.InstanceNorm1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.InstanceNorm2d.html">mindspore.nn.InstanceNorm2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.InstanceNorm3d.html">mindspore.nn.InstanceNorm3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LayerNorm.html">mindspore.nn.LayerNorm</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SyncBatchNorm.html">mindspore.nn.SyncBatchNorm</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pooling-layer">Pooling Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaptiveAvgPool1d.html">mindspore.nn.AdaptiveAvgPool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaptiveAvgPool2d.html">mindspore.nn.AdaptiveAvgPool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaptiveAvgPool3d.html">mindspore.nn.AdaptiveAvgPool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaptiveMaxPool1d.html">mindspore.nn.AdaptiveMaxPool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaptiveMaxPool2d.html">mindspore.nn.AdaptiveMaxPool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaptiveMaxPool3d.html">mindspore.nn.AdaptiveMaxPool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AvgPool1d.html">mindspore.nn.AvgPool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AvgPool2d.html">mindspore.nn.AvgPool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AvgPool3d.html">mindspore.nn.AvgPool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.FractionalMaxPool3d.html">mindspore.nn.FractionalMaxPool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LPPool1d.html">mindspore.nn.LPPool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LPPool2d.html">mindspore.nn.LPPool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MaxPool1d.html">mindspore.nn.MaxPool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MaxPool2d.html">mindspore.nn.MaxPool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MaxPool3d.html">mindspore.nn.MaxPool3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MaxUnpool1d.html">mindspore.nn.MaxUnpool1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MaxUnpool2d.html">mindspore.nn.MaxUnpool2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MaxUnpool3d.html">mindspore.nn.MaxUnpool3d</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#padding-layer">Padding Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Pad.html">mindspore.nn.Pad</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ConstantPad1d.html">mindspore.nn.ConstantPad1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ConstantPad2d.html">mindspore.nn.ConstantPad2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ConstantPad3d.html">mindspore.nn.ConstantPad3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReflectionPad1d.html">mindspore.nn.ReflectionPad1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReflectionPad2d.html">mindspore.nn.ReflectionPad2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReflectionPad3d.html">mindspore.nn.ReflectionPad3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReplicationPad1d.html">mindspore.nn.ReplicationPad1d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReplicationPad2d.html">mindspore.nn.ReplicationPad2d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ReplicationPad3d.html">mindspore.nn.ReplicationPad3d</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ZeroPad2d.html">mindspore.nn.ZeroPad2d</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#loss-function">Loss Function</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.BCELoss.html">mindspore.nn.BCELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.BCEWithLogitsLoss.html">mindspore.nn.BCEWithLogitsLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.CosineEmbeddingLoss.html">mindspore.nn.CosineEmbeddingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.CrossEntropyLoss.html">mindspore.nn.CrossEntropyLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.CTCLoss.html">mindspore.nn.CTCLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.DiceLoss.html">mindspore.nn.DiceLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.FocalLoss.html">mindspore.nn.FocalLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.GaussianNLLLoss.html">mindspore.nn.GaussianNLLLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.HingeEmbeddingLoss.html">mindspore.nn.HingeEmbeddingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.HuberLoss.html">mindspore.nn.HuberLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.KLDivLoss.html">mindspore.nn.KLDivLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.L1Loss.html">mindspore.nn.L1Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MarginRankingLoss.html">mindspore.nn.MarginRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MSELoss.html">mindspore.nn.MSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MultiClassDiceLoss.html">mindspore.nn.MultiClassDiceLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MultilabelMarginLoss.html">mindspore.nn.MultilabelMarginLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MultiLabelSoftMarginLoss.html">mindspore.nn.MultiLabelSoftMarginLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.MultiMarginLoss.html">mindspore.nn.MultiMarginLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.NLLLoss.html">mindspore.nn.NLLLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.PoissonNLLLoss.html">mindspore.nn.PoissonNLLLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.RMSELoss.html">mindspore.nn.RMSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SampledSoftmaxLoss.html">mindspore.nn.SampledSoftmaxLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SmoothL1Loss.html">mindspore.nn.SmoothL1Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SoftMarginLoss.html">mindspore.nn.SoftMarginLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SoftmaxCrossEntropyWithLogits.html">mindspore.nn.SoftmaxCrossEntropyWithLogits</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.TripletMarginLoss.html">mindspore.nn.TripletMarginLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#optimizer">Optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Adadelta.html">mindspore.nn.Adadelta</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Adagrad.html">mindspore.nn.Adagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Adam.html">mindspore.nn.Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaMax.html">mindspore.nn.AdaMax</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdamOffload.html">mindspore.nn.AdamOffload</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdamWeightDecay.html">mindspore.nn.AdamWeightDecay</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaSumByDeltaWeightWrapCell.html">mindspore.nn.AdaSumByDeltaWeightWrapCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.AdaSumByGradWrapCell.html">mindspore.nn.AdaSumByGradWrapCell</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ASGD.html">mindspore.nn.ASGD</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.FTRL.html">mindspore.nn.FTRL</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Lamb.html">mindspore.nn.Lamb</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LARS.html">mindspore.nn.LARS</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.LazyAdam.html">mindspore.nn.LazyAdam</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Momentum.html">mindspore.nn.Momentum</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ProximalAdagrad.html">mindspore.nn.ProximalAdagrad</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.RMSProp.html">mindspore.nn.RMSProp</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Rprop.html">mindspore.nn.Rprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.SGD.html">mindspore.nn.SGD</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.thor.html">mindspore.nn.thor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#experimental-optimizer">Experimental Optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.optim_ex.Optimizer.html">mindspore.nn.optim_ex.Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.optim_ex.Adam.html">mindspore.nn.optim_ex.Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.optim_ex.AdamW.html">mindspore.nn.optim_ex.AdamW</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.optim_ex.SGD.html">mindspore.nn.optim_ex.SGD</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-learning-rate">Dynamic Learning Rate</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#learningrateschedule-class">LearningRateSchedule Class</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.CosineDecayLR.html">mindspore.nn.CosineDecayLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.ExponentialDecayLR.html">mindspore.nn.ExponentialDecayLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.InverseDecayLR.html">mindspore.nn.InverseDecayLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.NaturalExpDecayLR.html">mindspore.nn.NaturalExpDecayLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.PolynomialDecayLR.html">mindspore.nn.PolynomialDecayLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.WarmUpLR.html">mindspore.nn.WarmUpLR</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-lr-function">Dynamic LR Function</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.cosine_decay_lr.html">mindspore.nn.cosine_decay_lr</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.exponential_decay_lr.html">mindspore.nn.exponential_decay_lr</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.inverse_decay_lr.html">mindspore.nn.inverse_decay_lr</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.natural_exp_decay_lr.html">mindspore.nn.natural_exp_decay_lr</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.piecewise_constant_lr.html">mindspore.nn.piecewise_constant_lr</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.polynomial_decay_lr.html">mindspore.nn.polynomial_decay_lr</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.warmup_lr.html">mindspore.nn.warmup_lr</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#lrscheduler-class">LRScheduler Class</a><ul>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.ChainedScheduler.html">mindspore.nn.ChainedScheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.ExponentialLR.html">mindspore.nn.ExponentialLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.LinearLR.html">mindspore.nn.LinearLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.LRScheduler.html">mindspore.nn.LRScheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.PolynomialLR.html">mindspore.nn.PolynomialLR</a></li>
<li class="toctree-l4"><a class="reference internal" href="nn/mindspore.nn.StepLR.html">mindspore.nn.StepLR</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#image-processing-layer">Image Processing Layer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.PixelShuffle.html">mindspore.nn.PixelShuffle</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.PixelUnshuffle.html">mindspore.nn.PixelUnshuffle</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ResizeBilinear.html">mindspore.nn.ResizeBilinear</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Upsample.html">mindspore.nn.Upsample</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tools">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.ChannelShuffle.html">mindspore.nn.ChannelShuffle</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Flatten.html">mindspore.nn.Flatten</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Identity.html">mindspore.nn.Identity</a></li>
<li class="toctree-l3"><a class="reference internal" href="nn/mindspore.nn.Unflatten.html">mindspore.nn.Unflatten</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.scipy.html">mindspore.scipy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/typical_api_comparision.html">Differences Between MindSpore and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/migrator_with_tools.html">Application Practice Guide for Network Migration Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax —— Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax —— Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax —— Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.nn</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api_python/mindspore.nn.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-nn">
<h1>mindspore.nn<a class="headerlink" href="#mindspore-nn" title="Permalink to this headline"></a></h1>
<p>Neural Network Cell</p>
<p>For building predefined building blocks or computational units in neural networks.</p>
<p>For more information about dynamic shape support status, please refer to <a class="reference external" href="https://mindspore.cn/docs/en/master/note/dynamic_shape_nn.html">Dynamic Shape Support Status of nn Interface</a> .</p>
<p>Compared with the previous version, the added, deleted and supported platforms change information of <cite>mindspore.nn</cite> operators in MindSpore, please refer to the link <a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/resource/api_updates/nn_api_updates_en.md">mindspore.nn API Interface Change</a> .</p>
<section id="basic-block">
<h2>Basic Block<a class="headerlink" href="#basic-block" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Cell</span></code></a></p></td>
<td><p>The basic building block of neural networks in MindSpore.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.GraphCell.html#mindspore.nn.GraphCell" title="mindspore.nn.GraphCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GraphCell</span></code></a></p></td>
<td><p>Base class for running the graph loaded from a MindIR.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LossBase.html#mindspore.nn.LossBase" title="mindspore.nn.LossBase"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LossBase</span></code></a></p></td>
<td><p>Base class for other losses.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Optimizer.html#mindspore.nn.Optimizer" title="mindspore.nn.Optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Optimizer</span></code></a></p></td>
<td><p>Base class for updating parameters.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="container">
<h2>Container<a class="headerlink" href="#container" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.CellList.html#mindspore.nn.CellList" title="mindspore.nn.CellList"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.CellList</span></code></a></p></td>
<td><p>Holds Cells in a list.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.SequentialCell.html#mindspore.nn.SequentialCell" title="mindspore.nn.SequentialCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SequentialCell</span></code></a></p></td>
<td><p>Sequential Cell container.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="wrapper-layer">
<h2>Wrapper Layer<a class="headerlink" href="#wrapper-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.DistributedGradReducer.html#mindspore.nn.DistributedGradReducer" title="mindspore.nn.DistributedGradReducer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.DistributedGradReducer</span></code></a></p></td>
<td><p>A distributed optimizer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.DynamicLossScaleUpdateCell.html#mindspore.nn.DynamicLossScaleUpdateCell" title="mindspore.nn.DynamicLossScaleUpdateCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.DynamicLossScaleUpdateCell</span></code></a></p></td>
<td><p>Dynamic Loss scale update cell.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.FixedLossScaleUpdateCell.html#mindspore.nn.FixedLossScaleUpdateCell" title="mindspore.nn.FixedLossScaleUpdateCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.FixedLossScaleUpdateCell</span></code></a></p></td>
<td><p>Update cell with fixed loss scaling value.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ForwardValueAndGrad.html#mindspore.nn.ForwardValueAndGrad" title="mindspore.nn.ForwardValueAndGrad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ForwardValueAndGrad</span></code></a></p></td>
<td><p>Encapsulate training network.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.GetNextSingleOp.html#mindspore.nn.GetNextSingleOp" title="mindspore.nn.GetNextSingleOp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GetNextSingleOp</span></code></a></p></td>
<td><p>Cell to run for getting the next operation.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MicroBatchInterleaved.html#mindspore.nn.MicroBatchInterleaved" title="mindspore.nn.MicroBatchInterleaved"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MicroBatchInterleaved</span></code></a></p></td>
<td><p>This function splits the input at the 0th into interleave_num pieces and then performs the computation of the wrapped cell.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ParameterUpdate.html#mindspore.nn.ParameterUpdate" title="mindspore.nn.ParameterUpdate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ParameterUpdate</span></code></a></p></td>
<td><p>Cell that updates parameter.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.PipelineCell.html#mindspore.nn.PipelineCell" title="mindspore.nn.PipelineCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PipelineCell</span></code></a></p></td>
<td><p>Wrap the network with Micro Batch.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.TimeDistributed.html#mindspore.nn.TimeDistributed" title="mindspore.nn.TimeDistributed"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TimeDistributed</span></code></a></p></td>
<td><p>The time distributed layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.TrainOneStepCell.html#mindspore.nn.TrainOneStepCell" title="mindspore.nn.TrainOneStepCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TrainOneStepCell</span></code></a></p></td>
<td><p>Network training package class.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.TrainOneStepWithLossScaleCell.html#mindspore.nn.TrainOneStepWithLossScaleCell" title="mindspore.nn.TrainOneStepWithLossScaleCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TrainOneStepWithLossScaleCell</span></code></a></p></td>
<td><p>Network training with loss scaling.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.WithEvalCell.html#mindspore.nn.WithEvalCell" title="mindspore.nn.WithEvalCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.WithEvalCell</span></code></a></p></td>
<td><p>Wraps the forward network with the loss function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.WithLossCell.html#mindspore.nn.WithLossCell" title="mindspore.nn.WithLossCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.WithLossCell</span></code></a></p></td>
<td><p>Cell with loss function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="convolutional-layer">
<h2>Convolutional Layer<a class="headerlink" href="#convolutional-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Conv1d.html#mindspore.nn.Conv1d" title="mindspore.nn.Conv1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Conv1d</span></code></a></p></td>
<td><p>Calculates the 1D convolution on the input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Conv1dTranspose.html#mindspore.nn.Conv1dTranspose" title="mindspore.nn.Conv1dTranspose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Conv1dTranspose</span></code></a></p></td>
<td><p>Calculates a 1D transposed convolution, which can be regarded as Conv1d for the gradient of the input, also called deconvolution (although it is not an actual deconvolution).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Conv2d.html#mindspore.nn.Conv2d" title="mindspore.nn.Conv2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Conv2d</span></code></a></p></td>
<td><p>Calculates the 2D convolution on the input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Conv2dTranspose.html#mindspore.nn.Conv2dTranspose" title="mindspore.nn.Conv2dTranspose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Conv2dTranspose</span></code></a></p></td>
<td><p>Calculates a 2D transposed convolution, which can be regarded as Conv2d for the gradient of the input, also called deconvolution (although it is not an actual deconvolution).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Conv3d.html#mindspore.nn.Conv3d" title="mindspore.nn.Conv3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Conv3d</span></code></a></p></td>
<td><p>Calculates the 3D convolution on the input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Conv3dTranspose.html#mindspore.nn.Conv3dTranspose" title="mindspore.nn.Conv3dTranspose"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Conv3dTranspose</span></code></a></p></td>
<td><p>Calculates a 3D transposed convolution, which can be regarded as Conv3d for the gradient of the input.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Unfold.html#mindspore.nn.Unfold" title="mindspore.nn.Unfold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Unfold</span></code></a></p></td>
<td><p>Extracts patches from images.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="recurrent-layer">
<h2>Recurrent Layer<a class="headerlink" href="#recurrent-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.RNN.html#mindspore.nn.RNN" title="mindspore.nn.RNN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.RNN</span></code></a></p></td>
<td><p>Stacked Elman RNN layers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.RNNCell.html#mindspore.nn.RNNCell" title="mindspore.nn.RNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.RNNCell</span></code></a></p></td>
<td><p>An Elman RNN cell with tanh or ReLU non-linearity.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.GRU.html#mindspore.nn.GRU" title="mindspore.nn.GRU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GRU</span></code></a></p></td>
<td><p>Stacked GRU (Gated Recurrent Unit) layers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.GRUCell.html#mindspore.nn.GRUCell" title="mindspore.nn.GRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GRUCell</span></code></a></p></td>
<td><p>A GRU(Gated Recurrent Unit) cell.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LSTM.html#mindspore.nn.LSTM" title="mindspore.nn.LSTM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LSTM</span></code></a></p></td>
<td><p>Stacked LSTM (Long Short-Term Memory) layers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LSTMCell.html#mindspore.nn.LSTMCell" title="mindspore.nn.LSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LSTMCell</span></code></a></p></td>
<td><p>A LSTM (Long Short-Term Memory) cell.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="transformer-layer">
<h2>Transformer Layer<a class="headerlink" href="#transformer-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MultiheadAttention.html#mindspore.nn.MultiheadAttention" title="mindspore.nn.MultiheadAttention"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MultiheadAttention</span></code></a></p></td>
<td><p>This is an implementation of multihead attention in the paper <a class="reference external" href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention is all you need</a>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.TransformerEncoderLayer.html#mindspore.nn.TransformerEncoderLayer" title="mindspore.nn.TransformerEncoderLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TransformerEncoderLayer</span></code></a></p></td>
<td><p>Transformer Encoder Layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.TransformerDecoderLayer.html#mindspore.nn.TransformerDecoderLayer" title="mindspore.nn.TransformerDecoderLayer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TransformerDecoderLayer</span></code></a></p></td>
<td><p>Transformer Decoder Layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.TransformerEncoder.html#mindspore.nn.TransformerEncoder" title="mindspore.nn.TransformerEncoder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TransformerEncoder</span></code></a></p></td>
<td><p>Transformer Encoder module with multi-layer stacked of <cite>TransformerEncoderLayer</cite>, including multihead self attention and feedforward layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.TransformerDecoder.html#mindspore.nn.TransformerDecoder" title="mindspore.nn.TransformerDecoder"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TransformerDecoder</span></code></a></p></td>
<td><p>Transformer Decoder module with multi-layer stacked of <cite>TransformerDecoderLayer</cite>, including multihead self attention, cross attention and feedforward layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Transformer.html#mindspore.nn.Transformer" title="mindspore.nn.Transformer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Transformer</span></code></a></p></td>
<td><p>Transformer module including encoder and decoder.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="embedding-layer">
<h2>Embedding Layer<a class="headerlink" href="#embedding-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Embedding.html#mindspore.nn.Embedding" title="mindspore.nn.Embedding"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Embedding</span></code></a></p></td>
<td><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.EmbeddingLookup.html#mindspore.nn.EmbeddingLookup" title="mindspore.nn.EmbeddingLookup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.EmbeddingLookup</span></code></a></p></td>
<td><p>EmbeddingLookup layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MultiFieldEmbeddingLookup.html#mindspore.nn.MultiFieldEmbeddingLookup" title="mindspore.nn.MultiFieldEmbeddingLookup"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MultiFieldEmbeddingLookup</span></code></a></p></td>
<td><p>Returns a slice of input tensor based on the specified indices and the field ids.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="nonlinear-activation-layer">
<h2>Nonlinear Activation Layer<a class="headerlink" href="#nonlinear-activation-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.CELU.html#mindspore.nn.CELU" title="mindspore.nn.CELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.CELU</span></code></a></p></td>
<td><p>Continuously differentiable exponential linear units activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ELU.html#mindspore.nn.ELU" title="mindspore.nn.ELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ELU</span></code></a></p></td>
<td><p>Exponential Linear Unit activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.FastGelu.html#mindspore.nn.FastGelu" title="mindspore.nn.FastGelu"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.FastGelu</span></code></a></p></td>
<td><p>Fast Gaussian error linear unit activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.GELU.html#mindspore.nn.GELU" title="mindspore.nn.GELU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GELU</span></code></a></p></td>
<td><p>Gaussian error linear unit activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.GLU.html#mindspore.nn.GLU" title="mindspore.nn.GLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GLU</span></code></a></p></td>
<td><p>The gated linear unit function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.get_activation.html#mindspore.nn.get_activation" title="mindspore.nn.get_activation"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.get_activation</span></code></a></p></td>
<td><p>Gets the activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Hardtanh.html#mindspore.nn.Hardtanh" title="mindspore.nn.Hardtanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Hardtanh</span></code></a></p></td>
<td><p>Applies the Hardtanh function element-wise.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.HShrink.html#mindspore.nn.HShrink" title="mindspore.nn.HShrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.HShrink</span></code></a></p></td>
<td><p>Hard Shrink activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.HSigmoid.html#mindspore.nn.HSigmoid" title="mindspore.nn.HSigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.HSigmoid</span></code></a></p></td>
<td><p>Hard sigmoid activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.HSwish.html#mindspore.nn.HSwish" title="mindspore.nn.HSwish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.HSwish</span></code></a></p></td>
<td><p>Applies hswish-type activation element-wise.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LeakyReLU.html#mindspore.nn.LeakyReLU" title="mindspore.nn.LeakyReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LeakyReLU</span></code></a></p></td>
<td><p>Leaky ReLU activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LogSigmoid.html#mindspore.nn.LogSigmoid" title="mindspore.nn.LogSigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LogSigmoid</span></code></a></p></td>
<td><p>Applies logsigmoid activation element-wise.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LogSoftmax.html#mindspore.nn.LogSoftmax" title="mindspore.nn.LogSoftmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LogSoftmax</span></code></a></p></td>
<td><p>Applies the LogSoftmax function to n-dimensional input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LRN.html#mindspore.nn.LRN" title="mindspore.nn.LRN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LRN</span></code></a></p></td>
<td><p>Local Response Normalization.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Mish.html#mindspore.nn.Mish" title="mindspore.nn.Mish"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Mish</span></code></a></p></td>
<td><p>Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Softsign.html#mindspore.nn.Softsign" title="mindspore.nn.Softsign"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Softsign</span></code></a></p></td>
<td><p>Softsign activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.PReLU.html#mindspore.nn.PReLU" title="mindspore.nn.PReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PReLU</span></code></a></p></td>
<td><p>PReLU activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ReLU.html#mindspore.nn.ReLU" title="mindspore.nn.ReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReLU</span></code></a></p></td>
<td><p>Rectified Linear Unit activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ReLU6.html#mindspore.nn.ReLU6" title="mindspore.nn.ReLU6"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReLU6</span></code></a></p></td>
<td><p>Compute ReLU6 activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.RReLU.html#mindspore.nn.RReLU" title="mindspore.nn.RReLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.RReLU</span></code></a></p></td>
<td><p>Randomized Leaky ReLU activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.SeLU.html#mindspore.nn.SeLU" title="mindspore.nn.SeLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SeLU</span></code></a></p></td>
<td><p>Activation function SeLU (Scaled exponential Linear Unit).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.SiLU.html#mindspore.nn.SiLU" title="mindspore.nn.SiLU"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SiLU</span></code></a></p></td>
<td><p>Sigmoid Linear Unit activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Sigmoid.html#mindspore.nn.Sigmoid" title="mindspore.nn.Sigmoid"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Sigmoid</span></code></a></p></td>
<td><p>Sigmoid activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Softmin.html#mindspore.nn.Softmin" title="mindspore.nn.Softmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Softmin</span></code></a></p></td>
<td><p>Softmin activation function, which is a two-category function <a class="reference internal" href="nn/mindspore.nn.Sigmoid.html#mindspore.nn.Sigmoid" title="mindspore.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Sigmoid</span></code></a> in the promotion of multi-classification, and the purpose is to show the results of multi-classification in the form of probability.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Softmax.html#mindspore.nn.Softmax" title="mindspore.nn.Softmax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Softmax</span></code></a></p></td>
<td><p>Softmax activation function, which is a two-category function <a class="reference internal" href="nn/mindspore.nn.Sigmoid.html#mindspore.nn.Sigmoid" title="mindspore.nn.Sigmoid"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.Sigmoid</span></code></a> in the promotion of multi-classification, the purpose is to show the results of multi-classification in the form of probability.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Softmax2d.html#mindspore.nn.Softmax2d" title="mindspore.nn.Softmax2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Softmax2d</span></code></a></p></td>
<td><p>Softmax function applied to 2D features data.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.SoftShrink.html#mindspore.nn.SoftShrink" title="mindspore.nn.SoftShrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SoftShrink</span></code></a></p></td>
<td><p>Applies the SoftShrink function element-wise.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Tanh.html#mindspore.nn.Tanh" title="mindspore.nn.Tanh"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Tanh</span></code></a></p></td>
<td><p>Applies the Tanh function element-wise, returns a new tensor with the hyperbolic tangent of the elements of input, The input is a Tensor with any valid shape.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Tanhshrink.html#mindspore.nn.Tanhshrink" title="mindspore.nn.Tanhshrink"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Tanhshrink</span></code></a></p></td>
<td><p>Tanhshrink activation function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Threshold.html#mindspore.nn.Threshold" title="mindspore.nn.Threshold"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Threshold</span></code></a></p></td>
<td><p>Thresholds each element of the input Tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="linear-layer">
<h2>Linear Layer<a class="headerlink" href="#linear-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Dense.html#mindspore.nn.Dense" title="mindspore.nn.Dense"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Dense</span></code></a></p></td>
<td><p>The dense connected layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.BiDense.html#mindspore.nn.BiDense" title="mindspore.nn.BiDense"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.BiDense</span></code></a></p></td>
<td><p>The bilinear dense connected layer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="dropout-layer">
<h2>Dropout Layer<a class="headerlink" href="#dropout-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Dropout.html#mindspore.nn.Dropout" title="mindspore.nn.Dropout"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Dropout</span></code></a></p></td>
<td><p>Dropout layer for the input.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Dropout1d.html#mindspore.nn.Dropout1d" title="mindspore.nn.Dropout1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Dropout1d</span></code></a></p></td>
<td><p>During training, randomly zeroes entire channels of the input tensor with probability <cite>p</cite> from a Bernoulli distribution (For a 3-dimensional tensor with a shape of <span class="math notranslate nohighlight">\((N, C, L)\)</span>, the channel feature map refers to a 1-dimensional feature map with the shape of <span class="math notranslate nohighlight">\(L\)</span>).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Dropout2d.html#mindspore.nn.Dropout2d" title="mindspore.nn.Dropout2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Dropout2d</span></code></a></p></td>
<td><p>During training, randomly zeroes some channels of the input tensor with probability <cite>p</cite> from a Bernoulli distribution (For a 4-dimensional tensor with a shape of <span class="math notranslate nohighlight">\(NCHW\)</span>, the channel feature map refers to a 2-dimensional feature map with the shape of <span class="math notranslate nohighlight">\(HW\)</span>).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Dropout3d.html#mindspore.nn.Dropout3d" title="mindspore.nn.Dropout3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Dropout3d</span></code></a></p></td>
<td><p>During training, randomly zeroes some channels of the input tensor with probability <cite>p</cite> from a Bernoulli distribution (For a 5-dimensional tensor with a shape of <span class="math notranslate nohighlight">\(NCDHW\)</span>, the channel feature map refers to a 3-dimensional feature map with a shape of <span class="math notranslate nohighlight">\(DHW\)</span>).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="normalization-layer">
<h2>Normalization Layer<a class="headerlink" href="#normalization-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.BatchNorm1d.html#mindspore.nn.BatchNorm1d" title="mindspore.nn.BatchNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm1d</span></code></a></p></td>
<td><p>This layer applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D or 2D inputs) to reduce internal covariate shift.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.BatchNorm2d.html#mindspore.nn.BatchNorm2d" title="mindspore.nn.BatchNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm2d</span></code></a></p></td>
<td><p>Batch Normalization is widely used in convolutional networks.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.BatchNorm3d.html#mindspore.nn.BatchNorm3d" title="mindspore.nn.BatchNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.BatchNorm3d</span></code></a></p></td>
<td><p>Batch Normalization is widely used in convolutional networks.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.GroupNorm.html#mindspore.nn.GroupNorm" title="mindspore.nn.GroupNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GroupNorm</span></code></a></p></td>
<td><p>Group Normalization over a mini-batch of inputs.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.InstanceNorm1d.html#mindspore.nn.InstanceNorm1d" title="mindspore.nn.InstanceNorm1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.InstanceNorm1d</span></code></a></p></td>
<td><p>This layer applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with additional channel dimension).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.InstanceNorm2d.html#mindspore.nn.InstanceNorm2d" title="mindspore.nn.InstanceNorm2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.InstanceNorm2d</span></code></a></p></td>
<td><p>This layer applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.InstanceNorm3d.html#mindspore.nn.InstanceNorm3d" title="mindspore.nn.InstanceNorm3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.InstanceNorm3d</span></code></a></p></td>
<td><p>This layer applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LayerNorm.html#mindspore.nn.LayerNorm" title="mindspore.nn.LayerNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LayerNorm</span></code></a></p></td>
<td><p>Applies Layer Normalization over a mini-batch of inputs.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.SyncBatchNorm.html#mindspore.nn.SyncBatchNorm" title="mindspore.nn.SyncBatchNorm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SyncBatchNorm</span></code></a></p></td>
<td><p>Sync Batch Normalization layer over a N-dimension input.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="pooling-layer">
<h2>Pooling Layer<a class="headerlink" href="#pooling-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaptiveAvgPool1d.html#mindspore.nn.AdaptiveAvgPool1d" title="mindspore.nn.AdaptiveAvgPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaptiveAvgPool1d</span></code></a></p></td>
<td><p>Applies a 1D adaptive average pooling over an input Tensor which can be regarded as a composition of 1D input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaptiveAvgPool2d.html#mindspore.nn.AdaptiveAvgPool2d" title="mindspore.nn.AdaptiveAvgPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaptiveAvgPool2d</span></code></a></p></td>
<td><p>This operator applies a 2D adaptive average pooling to an input signal composed of multiple input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaptiveAvgPool3d.html#mindspore.nn.AdaptiveAvgPool3d" title="mindspore.nn.AdaptiveAvgPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaptiveAvgPool3d</span></code></a></p></td>
<td><p>This operator applies a 3D adaptive average pooling to an input signal composed of multiple input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaptiveMaxPool1d.html#mindspore.nn.AdaptiveMaxPool1d" title="mindspore.nn.AdaptiveMaxPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaptiveMaxPool1d</span></code></a></p></td>
<td><p>Applies a 1D adaptive maximum pooling over an input Tensor which can be regarded as a composition of 1D input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaptiveMaxPool2d.html#mindspore.nn.AdaptiveMaxPool2d" title="mindspore.nn.AdaptiveMaxPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaptiveMaxPool2d</span></code></a></p></td>
<td><p>This operator applies a 2D adaptive max pooling to an input signal composed of multiple input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaptiveMaxPool3d.html#mindspore.nn.AdaptiveMaxPool3d" title="mindspore.nn.AdaptiveMaxPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaptiveMaxPool3d</span></code></a></p></td>
<td><p>Calculates the 3D adaptive max pooling for an input Tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AvgPool1d.html#mindspore.nn.AvgPool1d" title="mindspore.nn.AvgPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AvgPool1d</span></code></a></p></td>
<td><p>Applies a 1D average pooling over an input Tensor which can be regarded as a composition of 1D input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AvgPool2d.html#mindspore.nn.AvgPool2d" title="mindspore.nn.AvgPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AvgPool2d</span></code></a></p></td>
<td><p>Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AvgPool3d.html#mindspore.nn.AvgPool3d" title="mindspore.nn.AvgPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AvgPool3d</span></code></a></p></td>
<td><p>Applies a 3D average pooling over an input Tensor which can be regarded as a composition of 3D input planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.FractionalMaxPool3d.html#mindspore.nn.FractionalMaxPool3d" title="mindspore.nn.FractionalMaxPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.FractionalMaxPool3d</span></code></a></p></td>
<td><p>Applies the 3D FractionalMaxPool operatin over <cite>input</cite>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LPPool1d.html#mindspore.nn.LPPool1d" title="mindspore.nn.LPPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LPPool1d</span></code></a></p></td>
<td><p>Applying 1D LPPooling operation on an input Tensor can be regarded as forming a 1D input plane.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LPPool2d.html#mindspore.nn.LPPool2d" title="mindspore.nn.LPPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LPPool2d</span></code></a></p></td>
<td><p>Applying 2D LPPooling operation on an input Tensor can be regarded as forming a 1D input plane.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MaxPool1d.html#mindspore.nn.MaxPool1d" title="mindspore.nn.MaxPool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool1d</span></code></a></p></td>
<td><p>Applies a 1D max pooling over an input Tensor which can be regarded as a composition of 1D planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MaxPool2d.html#mindspore.nn.MaxPool2d" title="mindspore.nn.MaxPool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool2d</span></code></a></p></td>
<td><p>Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MaxPool3d.html#mindspore.nn.MaxPool3d" title="mindspore.nn.MaxPool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool3d</span></code></a></p></td>
<td><p>3D max pooling operation.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MaxUnpool1d.html#mindspore.nn.MaxUnpool1d" title="mindspore.nn.MaxUnpool1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MaxUnpool1d</span></code></a></p></td>
<td><p>Computes the inverse of <a class="reference internal" href="nn/mindspore.nn.MaxPool1d.html#mindspore.nn.MaxPool1d" title="mindspore.nn.MaxPool1d"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool1d</span></code></a>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MaxUnpool2d.html#mindspore.nn.MaxUnpool2d" title="mindspore.nn.MaxUnpool2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MaxUnpool2d</span></code></a></p></td>
<td><p>Computes the inverse of <a class="reference internal" href="nn/mindspore.nn.MaxPool2d.html#mindspore.nn.MaxPool2d" title="mindspore.nn.MaxPool2d"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool2d</span></code></a>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MaxUnpool3d.html#mindspore.nn.MaxUnpool3d" title="mindspore.nn.MaxUnpool3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MaxUnpool3d</span></code></a></p></td>
<td><p>Computes the inverse of <a class="reference internal" href="nn/mindspore.nn.MaxPool3d.html#mindspore.nn.MaxPool3d" title="mindspore.nn.MaxPool3d"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.MaxPool3d</span></code></a>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="padding-layer">
<h2>Padding Layer<a class="headerlink" href="#padding-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Pad.html#mindspore.nn.Pad" title="mindspore.nn.Pad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Pad</span></code></a></p></td>
<td><p>Pads the input tensor according to the paddings and mode.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ConstantPad1d.html#mindspore.nn.ConstantPad1d" title="mindspore.nn.ConstantPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ConstantPad1d</span></code></a></p></td>
<td><p>Using a given constant value to pads the last dimensions of input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ConstantPad2d.html#mindspore.nn.ConstantPad2d" title="mindspore.nn.ConstantPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ConstantPad2d</span></code></a></p></td>
<td><p>Using a given constant value to pads the last two dimensions of input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ConstantPad3d.html#mindspore.nn.ConstantPad3d" title="mindspore.nn.ConstantPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ConstantPad3d</span></code></a></p></td>
<td><p>Using a given constant value to pads the last three dimensions of input tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ReflectionPad1d.html#mindspore.nn.ReflectionPad1d" title="mindspore.nn.ReflectionPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReflectionPad1d</span></code></a></p></td>
<td><p>Using a given padding to do reflection pad on the given tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ReflectionPad2d.html#mindspore.nn.ReflectionPad2d" title="mindspore.nn.ReflectionPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReflectionPad2d</span></code></a></p></td>
<td><p>Using a given padding to do reflection pad the given tensor.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ReflectionPad3d.html#mindspore.nn.ReflectionPad3d" title="mindspore.nn.ReflectionPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReflectionPad3d</span></code></a></p></td>
<td><p>Pad the given tensor in a reflecting way using the input boundaries as the axis of symmetry.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ReplicationPad1d.html#mindspore.nn.ReplicationPad1d" title="mindspore.nn.ReplicationPad1d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReplicationPad1d</span></code></a></p></td>
<td><p>Pad on W dimension of input <cite>x</cite> according to <cite>padding</cite>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ReplicationPad2d.html#mindspore.nn.ReplicationPad2d" title="mindspore.nn.ReplicationPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReplicationPad2d</span></code></a></p></td>
<td><p>Pad on HW dimension of input <cite>x</cite> according to <cite>padding</cite>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ReplicationPad3d.html#mindspore.nn.ReplicationPad3d" title="mindspore.nn.ReplicationPad3d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ReplicationPad3d</span></code></a></p></td>
<td><p>Pad on DHW dimension of input <cite>x</cite> according to <cite>padding</cite>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ZeroPad2d.html#mindspore.nn.ZeroPad2d" title="mindspore.nn.ZeroPad2d"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ZeroPad2d</span></code></a></p></td>
<td><p>Pads the last two dimensions of input tensor with zero.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="loss-function">
<h2>Loss Function<a class="headerlink" href="#loss-function" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.BCELoss.html#mindspore.nn.BCELoss" title="mindspore.nn.BCELoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.BCELoss</span></code></a></p></td>
<td><p>BCELoss creates a criterion to measure the binary cross entropy between the true labels and predicted labels.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.BCEWithLogitsLoss.html#mindspore.nn.BCEWithLogitsLoss" title="mindspore.nn.BCEWithLogitsLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.BCEWithLogitsLoss</span></code></a></p></td>
<td><p>Adds sigmoid activation function to input logits, and uses the given logits to compute binary cross entropy between the logits and the labels.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code>  <code class="docutils literal notranslate"><span class="pre">GPU</span></code>  <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.CosineEmbeddingLoss.html#mindspore.nn.CosineEmbeddingLoss" title="mindspore.nn.CosineEmbeddingLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.CosineEmbeddingLoss</span></code></a></p></td>
<td><p>CosineEmbeddingLoss creates a criterion to measure the similarity between two tensors using cosine distance.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.CrossEntropyLoss.html#mindspore.nn.CrossEntropyLoss" title="mindspore.nn.CrossEntropyLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.CrossEntropyLoss</span></code></a></p></td>
<td><p>The cross entropy loss between input and target.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.CTCLoss.html#mindspore.nn.CTCLoss" title="mindspore.nn.CTCLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.CTCLoss</span></code></a></p></td>
<td><p>Calculates the CTC (Connectionist Temporal Classification) loss.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.DiceLoss.html#mindspore.nn.DiceLoss" title="mindspore.nn.DiceLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.DiceLoss</span></code></a></p></td>
<td><p>The Dice coefficient is a set similarity loss, which is used to calculate the similarity between two samples.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.FocalLoss.html#mindspore.nn.FocalLoss" title="mindspore.nn.FocalLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.FocalLoss</span></code></a></p></td>
<td><p>It is a loss function to solve the imbalance of categories and the difference of classification difficulty.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.GaussianNLLLoss.html#mindspore.nn.GaussianNLLLoss" title="mindspore.nn.GaussianNLLLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.GaussianNLLLoss</span></code></a></p></td>
<td><p>Gaussian negative log likelihood loss.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.HingeEmbeddingLoss.html#mindspore.nn.HingeEmbeddingLoss" title="mindspore.nn.HingeEmbeddingLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.HingeEmbeddingLoss</span></code></a></p></td>
<td><p>Calculate the Hinge Embedding Loss value based on the input 'logits' and' labels' (only including 1 or -1).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.HuberLoss.html#mindspore.nn.HuberLoss" title="mindspore.nn.HuberLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.HuberLoss</span></code></a></p></td>
<td><p>HuberLoss calculate the error between the predicted value and the target value.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.KLDivLoss.html#mindspore.nn.KLDivLoss" title="mindspore.nn.KLDivLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.KLDivLoss</span></code></a></p></td>
<td><p>Computes the Kullback-Leibler divergence between the <cite>logits</cite> and the <cite>labels</cite>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.L1Loss.html#mindspore.nn.L1Loss" title="mindspore.nn.L1Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.L1Loss</span></code></a></p></td>
<td><p>L1Loss is used to calculate the mean absolute error between the predicted value and the target value.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MarginRankingLoss.html#mindspore.nn.MarginRankingLoss" title="mindspore.nn.MarginRankingLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MarginRankingLoss</span></code></a></p></td>
<td><p>MarginRankingLoss creates a criterion that measures the loss.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MSELoss.html#mindspore.nn.MSELoss" title="mindspore.nn.MSELoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MSELoss</span></code></a></p></td>
<td><p>Calculates the mean squared error between the predicted value and the label value.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MultiClassDiceLoss.html#mindspore.nn.MultiClassDiceLoss" title="mindspore.nn.MultiClassDiceLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MultiClassDiceLoss</span></code></a></p></td>
<td><p>When there are multiple classifications, label is transformed into multiple binary classifications by one hot.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MultilabelMarginLoss.html#mindspore.nn.MultilabelMarginLoss" title="mindspore.nn.MultilabelMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MultilabelMarginLoss</span></code></a></p></td>
<td><p>Creates a loss criterion that minimizes the hinge loss for multi-class classification tasks.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.MultiLabelSoftMarginLoss.html#mindspore.nn.MultiLabelSoftMarginLoss" title="mindspore.nn.MultiLabelSoftMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MultiLabelSoftMarginLoss</span></code></a></p></td>
<td><p>Calculates the MultiLabelSoftMarginLoss.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.MultiMarginLoss.html#mindspore.nn.MultiMarginLoss" title="mindspore.nn.MultiMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.MultiMarginLoss</span></code></a></p></td>
<td><p>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input <span class="math notranslate nohighlight">\(x\)</span> (a 2D mini-batch <cite>Tensor</cite>) and output <span class="math notranslate nohighlight">\(y\)</span> (which is a 1D tensor of target class indices, <span class="math notranslate nohighlight">\(0 \leq y \leq \text{x.size}(1)-1\)</span>):</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.NLLLoss.html#mindspore.nn.NLLLoss" title="mindspore.nn.NLLLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.NLLLoss</span></code></a></p></td>
<td><p>Gets the negative log likelihood loss between logits and labels.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.PoissonNLLLoss.html#mindspore.nn.PoissonNLLLoss" title="mindspore.nn.PoissonNLLLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PoissonNLLLoss</span></code></a></p></td>
<td><p>Poisson negative log likelihood loss.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.RMSELoss.html#mindspore.nn.RMSELoss" title="mindspore.nn.RMSELoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.RMSELoss</span></code></a></p></td>
<td><p>RMSELoss creates a criterion to measure the root mean square error between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> element-wise, where <span class="math notranslate nohighlight">\(x\)</span> is the input and <span class="math notranslate nohighlight">\(y\)</span> is the labels.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.SampledSoftmaxLoss.html#mindspore.nn.SampledSoftmaxLoss" title="mindspore.nn.SampledSoftmaxLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SampledSoftmaxLoss</span></code></a></p></td>
<td><p>Computes the sampled softmax training loss.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.SmoothL1Loss.html#mindspore.nn.SmoothL1Loss" title="mindspore.nn.SmoothL1Loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SmoothL1Loss</span></code></a></p></td>
<td><p>SmoothL1 loss function, if the absolute error element-wise between the predicted value and the target value is less than the set threshold <cite>beta</cite>, the square term is used, otherwise the absolute error term is used.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.SoftMarginLoss.html#mindspore.nn.SoftMarginLoss" title="mindspore.nn.SoftMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SoftMarginLoss</span></code></a></p></td>
<td><p>A loss class for two-class classification problems.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.SoftmaxCrossEntropyWithLogits.html#mindspore.nn.SoftmaxCrossEntropyWithLogits" title="mindspore.nn.SoftmaxCrossEntropyWithLogits"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SoftmaxCrossEntropyWithLogits</span></code></a></p></td>
<td><p>Computes softmax cross entropy between logits and labels.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.TripletMarginLoss.html#mindspore.nn.TripletMarginLoss" title="mindspore.nn.TripletMarginLoss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.TripletMarginLoss</span></code></a></p></td>
<td><p>TripletMarginLoss operation.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="optimizer">
<h2>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Adadelta.html#mindspore.nn.Adadelta" title="mindspore.nn.Adadelta"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Adadelta</span></code></a></p></td>
<td><p>Implements the Adadelta algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Adagrad.html#mindspore.nn.Adagrad" title="mindspore.nn.Adagrad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Adagrad</span></code></a></p></td>
<td><p>Implements the Adagrad algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Adam.html#mindspore.nn.Adam" title="mindspore.nn.Adam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Adam</span></code></a></p></td>
<td><p>Implements the Adaptive Moment Estimation (Adam) algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code>  <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaMax.html#mindspore.nn.AdaMax" title="mindspore.nn.AdaMax"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaMax</span></code></a></p></td>
<td><p>Implements the AdaMax algorithm, a variant of Adaptive Movement Estimation (Adam) based on the infinity norm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AdamOffload.html#mindspore.nn.AdamOffload" title="mindspore.nn.AdamOffload"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdamOffload</span></code></a></p></td>
<td><p>This optimizer will offload Adam optimizer to host CPU and keep parameters being updated on the device, to minimize the memory cost.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AdamWeightDecay.html#mindspore.nn.AdamWeightDecay" title="mindspore.nn.AdamWeightDecay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdamWeightDecay</span></code></a></p></td>
<td><p>Implements the Adam algorithm with weight decay.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaSumByDeltaWeightWrapCell.html#mindspore.nn.AdaSumByDeltaWeightWrapCell" title="mindspore.nn.AdaSumByDeltaWeightWrapCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaSumByDeltaWeightWrapCell</span></code></a></p></td>
<td><p>Enable the adasum in &quot;auto_parallel/semi_auto_parallel&quot; mode.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.AdaSumByGradWrapCell.html#mindspore.nn.AdaSumByGradWrapCell" title="mindspore.nn.AdaSumByGradWrapCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.AdaSumByGradWrapCell</span></code></a></p></td>
<td><p>Enable the adasum in &quot;auto_parallel/semi_auto_parallel&quot; mode.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ASGD.html#mindspore.nn.ASGD" title="mindspore.nn.ASGD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ASGD</span></code></a></p></td>
<td><p>Implements Average Stochastic Gradient Descent.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.FTRL.html#mindspore.nn.FTRL" title="mindspore.nn.FTRL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.FTRL</span></code></a></p></td>
<td><p>Implements the FTRL algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Lamb.html#mindspore.nn.Lamb" title="mindspore.nn.Lamb"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Lamb</span></code></a></p></td>
<td><p>Implements the Lamb(Layer-wise Adaptive Moments optimizer for Batching training) algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LARS.html#mindspore.nn.LARS" title="mindspore.nn.LARS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LARS</span></code></a></p></td>
<td><p>Implements the LARS algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LazyAdam.html#mindspore.nn.LazyAdam" title="mindspore.nn.LazyAdam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LazyAdam</span></code></a></p></td>
<td><p>Implements the Adaptive Moment Estimation (Adam) algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Momentum.html#mindspore.nn.Momentum" title="mindspore.nn.Momentum"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Momentum</span></code></a></p></td>
<td><p>Implements the Momentum algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ProximalAdagrad.html#mindspore.nn.ProximalAdagrad" title="mindspore.nn.ProximalAdagrad"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ProximalAdagrad</span></code></a></p></td>
<td><p>Implements the ProximalAdagrad algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.RMSProp.html#mindspore.nn.RMSProp" title="mindspore.nn.RMSProp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.RMSProp</span></code></a></p></td>
<td><p>Implements Root Mean Squared Propagation (RMSProp) algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Rprop.html#mindspore.nn.Rprop" title="mindspore.nn.Rprop"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Rprop</span></code></a></p></td>
<td><p>Implements Resilient backpropagation.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.SGD.html#mindspore.nn.SGD" title="mindspore.nn.SGD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.SGD</span></code></a></p></td>
<td><p>Implements stochastic gradient descent.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.thor.html#mindspore.nn.thor" title="mindspore.nn.thor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.thor</span></code></a></p></td>
<td><p>Updates gradients by second-order algorithm--THOR.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="experimental-optimizer">
<h2>Experimental Optimizer<a class="headerlink" href="#experimental-optimizer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.optim_ex.Optimizer.html#mindspore.nn.optim_ex.Optimizer" title="mindspore.nn.optim_ex.Optimizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.optim_ex.Optimizer</span></code></a></p></td>
<td><p>Base class for all optimizers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.optim_ex.Adam.html#mindspore.nn.optim_ex.Adam" title="mindspore.nn.optim_ex.Adam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.optim_ex.Adam</span></code></a></p></td>
<td><p>Implements Adam algorithm..</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.optim_ex.AdamW.html#mindspore.nn.optim_ex.AdamW" title="mindspore.nn.optim_ex.AdamW"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.optim_ex.AdamW</span></code></a></p></td>
<td><p>Implements Adam Weight Decay algorithm.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.optim_ex.SGD.html#mindspore.nn.optim_ex.SGD" title="mindspore.nn.optim_ex.SGD"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.optim_ex.SGD</span></code></a></p></td>
<td><p>Stochastic Gradient Descent optimizer.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="dynamic-learning-rate">
<h2>Dynamic Learning Rate<a class="headerlink" href="#dynamic-learning-rate" title="Permalink to this headline"></a></h2>
<section id="learningrateschedule-class">
<h3>LearningRateSchedule Class<a class="headerlink" href="#learningrateschedule-class" title="Permalink to this headline"></a></h3>
<p>The dynamic learning rates in this module are all subclasses of LearningRateSchedule. Pass the instance of
LearningRateSchedule to an optimizer. During the training process, the optimizer calls the instance taking current step
as input to get the current learning rate.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">min_lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">max_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">decay_steps</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">cosine_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CosineDecayLR</span><span class="p">(</span><span class="n">min_lr</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">cosine_decay_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.CosineDecayLR.html#mindspore.nn.CosineDecayLR" title="mindspore.nn.CosineDecayLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.CosineDecayLR</span></code></a></p></td>
<td><p>Calculates learning rate based on cosine decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ExponentialDecayLR.html#mindspore.nn.ExponentialDecayLR" title="mindspore.nn.ExponentialDecayLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ExponentialDecayLR</span></code></a></p></td>
<td><p>Calculates learning rate based on exponential decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.InverseDecayLR.html#mindspore.nn.InverseDecayLR" title="mindspore.nn.InverseDecayLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.InverseDecayLR</span></code></a></p></td>
<td><p>Calculates learning rate base on inverse-time decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.NaturalExpDecayLR.html#mindspore.nn.NaturalExpDecayLR" title="mindspore.nn.NaturalExpDecayLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.NaturalExpDecayLR</span></code></a></p></td>
<td><p>Calculates learning rate base on natural exponential decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.PolynomialDecayLR.html#mindspore.nn.PolynomialDecayLR" title="mindspore.nn.PolynomialDecayLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PolynomialDecayLR</span></code></a></p></td>
<td><p>Calculates learning rate base on polynomial decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.WarmUpLR.html#mindspore.nn.WarmUpLR" title="mindspore.nn.WarmUpLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.WarmUpLR</span></code></a></p></td>
<td><p>Gets learning rate warming up.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="dynamic-lr-function">
<h3>Dynamic LR Function<a class="headerlink" href="#dynamic-lr-function" title="Permalink to this headline"></a></h3>
<p>The dynamic learning rates in this module are all functions. Call the function and pass the result to an optimizer.
During the training process, the optimizer takes result[current step] as current learning rate.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">min_lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">max_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">total_step</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">step_per_epoch</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">decay_epoch</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">lr</span><span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">cosine_decay_lr</span><span class="p">(</span><span class="n">min_lr</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">,</span> <span class="n">decay_epoch</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.cosine_decay_lr.html#mindspore.nn.cosine_decay_lr" title="mindspore.nn.cosine_decay_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.cosine_decay_lr</span></code></a></p></td>
<td><p>Calculates learning rate base on cosine decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.exponential_decay_lr.html#mindspore.nn.exponential_decay_lr" title="mindspore.nn.exponential_decay_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.exponential_decay_lr</span></code></a></p></td>
<td><p>Calculates learning rate base on exponential decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.inverse_decay_lr.html#mindspore.nn.inverse_decay_lr" title="mindspore.nn.inverse_decay_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.inverse_decay_lr</span></code></a></p></td>
<td><p>Calculates learning rate base on inverse-time decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.natural_exp_decay_lr.html#mindspore.nn.natural_exp_decay_lr" title="mindspore.nn.natural_exp_decay_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.natural_exp_decay_lr</span></code></a></p></td>
<td><p>Calculates learning rate base on natural exponential decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.piecewise_constant_lr.html#mindspore.nn.piecewise_constant_lr" title="mindspore.nn.piecewise_constant_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.piecewise_constant_lr</span></code></a></p></td>
<td><p>Get piecewise constant learning rate.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.polynomial_decay_lr.html#mindspore.nn.polynomial_decay_lr" title="mindspore.nn.polynomial_decay_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.polynomial_decay_lr</span></code></a></p></td>
<td><p>Calculates learning rate base on polynomial decay function.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.warmup_lr.html#mindspore.nn.warmup_lr" title="mindspore.nn.warmup_lr"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.warmup_lr</span></code></a></p></td>
<td><p>Gets learning rate warming up.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="lrscheduler-class">
<h3>LRScheduler Class<a class="headerlink" href="#lrscheduler-class" title="Permalink to this headline"></a></h3>
<p>The dynamic learning rates in this module are all subclasses of LRScheduler, this module should be used with optimizers
in mindspore.nn.optim_ex, pass the optimizer instance to a LRScheduler when used. During the training process, the
LRScheduler subclass dynamically changes the learning rate by calling the <cite>step</cite> method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">optim_ex</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">forward_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
    <span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
    <span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">need_download</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ChainedScheduler.html#mindspore.nn.ChainedScheduler" title="mindspore.nn.ChainedScheduler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ChainedScheduler</span></code></a></p></td>
<td><p>Save the learning rate scheduler chain list of multiple learning rate schedulers, and call the step() function to execute the step() function of each learning rate scheduler.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.ExponentialLR.html#mindspore.nn.ExponentialLR" title="mindspore.nn.ExponentialLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ExponentialLR</span></code></a></p></td>
<td><p>For each epoch, the learning rate decays exponentially, multiplied by gamma.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.LinearLR.html#mindspore.nn.LinearLR" title="mindspore.nn.LinearLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LinearLR</span></code></a></p></td>
<td><p>Decays the learning rate of each parameter group by linearly changing small multiplicative factor until the number of epoch reaches a pre-defined milestone: total_iters.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.LRScheduler.html#mindspore.nn.LRScheduler" title="mindspore.nn.LRScheduler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.LRScheduler</span></code></a></p></td>
<td><p>Basic class of learning rate schedule.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.PolynomialLR.html#mindspore.nn.PolynomialLR" title="mindspore.nn.PolynomialLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PolynomialLR</span></code></a></p></td>
<td><p>For each epoch, the learning rate is adjusted by polynomial fitting.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.StepLR.html#mindspore.nn.StepLR" title="mindspore.nn.StepLR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.StepLR</span></code></a></p></td>
<td><p>Decays the learning rate of each parameter group by gamma every step_size epochs.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="image-processing-layer">
<h2>Image Processing Layer<a class="headerlink" href="#image-processing-layer" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.PixelShuffle.html#mindspore.nn.PixelShuffle" title="mindspore.nn.PixelShuffle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PixelShuffle</span></code></a></p></td>
<td><p>Applies the PixelShuffle operation over input which implements sub-pixel convolutions with stride <span class="math notranslate nohighlight">\(1/r\)</span> .</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.PixelUnshuffle.html#mindspore.nn.PixelUnshuffle" title="mindspore.nn.PixelUnshuffle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.PixelUnshuffle</span></code></a></p></td>
<td><p>Applies the PixelUnshuffle operation over input which is the inverse of PixelShuffle.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ResizeBilinear.html#mindspore.nn.ResizeBilinear" title="mindspore.nn.ResizeBilinear"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ResizeBilinear</span></code></a></p></td>
<td><p>'nn.ResizeBilinear' is deprecated from version 2.0 and will be removed in a future version, use <a class="reference internal" href="ops/mindspore.ops.ResizeBilinearV2.html#mindspore.ops.ResizeBilinearV2" title="mindspore.ops.ResizeBilinearV2"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.ops.ResizeBilinearV2</span></code></a> or <a class="reference internal" href="ops/mindspore.ops.interpolate.html#mindspore.ops.interpolate" title="mindspore.ops.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.ops.interpolate()</span></code></a> instead.</p></td>
<td><p>Deprecated</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Upsample.html#mindspore.nn.Upsample" title="mindspore.nn.Upsample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Upsample</span></code></a></p></td>
<td><p>For details, please refer to <a class="reference internal" href="ops/mindspore.ops.interpolate.html#mindspore.ops.interpolate" title="mindspore.ops.interpolate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.ops.interpolate()</span></code></a>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="tools">
<h2>Tools<a class="headerlink" href="#tools" title="Permalink to this headline"></a></h2>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 64%" />
<col style="width: 27%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>API Name</strong></p></td>
<td><p><strong>Description</strong></p></td>
<td><p><strong>Supported Platforms</strong></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.ChannelShuffle.html#mindspore.nn.ChannelShuffle" title="mindspore.nn.ChannelShuffle"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.ChannelShuffle</span></code></a></p></td>
<td><p>Divide the channels of Tensor whose shape is <span class="math notranslate nohighlight">\((*, C, H, W)\)</span> into <span class="math notranslate nohighlight">\(g\)</span> groups to obtain a Tensor with shape <span class="math notranslate nohighlight">\((*, C \frac g, g, H, W)\)</span>, and transpose along the corresponding axis of <span class="math notranslate nohighlight">\(C\)</span>, <span class="math notranslate nohighlight">\(\frac{g}{}\)</span> and <span class="math notranslate nohighlight">\(g\)</span> to restore Tensor to the original shape.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Flatten.html#mindspore.nn.Flatten" title="mindspore.nn.Flatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Flatten</span></code></a></p></td>
<td><p>Flatten the input Tensor along dimensions from <cite>start_dim</cite> to <cite>end_dim</cite>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="nn/mindspore.nn.Identity.html#mindspore.nn.Identity" title="mindspore.nn.Identity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Identity</span></code></a></p></td>
<td><p>A placeholder identity operator that returns the same as input.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="nn/mindspore.nn.Unflatten.html#mindspore.nn.Unflatten" title="mindspore.nn.Unflatten"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mindspore.nn.Unflatten</span></code></a></p></td>
<td><p>Summary:</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore/mindspore.run_check.html" class="btn btn-neutral float-left" title="mindspore.run_check" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="nn/mindspore.nn.Cell.html" class="btn btn-neutral float-right" title="mindspore.nn.Cell" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
        <script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>