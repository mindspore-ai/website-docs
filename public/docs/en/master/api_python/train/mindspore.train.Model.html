<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.train.Model &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.train.BackupAndRestore" href="mindspore.train.BackupAndRestore.html" />
    <link rel="prev" title="mindspore.train" href="../mindspore.train.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.train.html#model">Model</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.train.Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.train.html#callback">Callback</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.train.html#evaluation-metrics">Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.train.html#utils">Utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.train.html#thor">Thor</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.train.html">mindspore.train</a> &raquo;</li>
      <li>mindspore.train.Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/train/mindspore.train.Model.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-train-model">
<h1>mindspore.train.Model<a class="headerlink" href="#mindspore-train-model" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.train.Model">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.train.</span></span><span class="sig-name descname"><span class="pre">Model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_network</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_indexes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amp_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'O0'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">boost_level</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'O0'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model" title="Permalink to this definition"></a></dt>
<dd><p>High-Level API for training or inference.</p>
<p><cite>Model</cite> groups layers into an object with training and inference features based on the arguments.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>If use mixed precision functions, need to set parameter <cite>optimizer</cite> at the same time,
otherwise mixed precision functions do not take effect.
When uses mixed precision functions, <cite>global_step</cite> in optimizer may be different from <cite>cur_step_num</cite>
in Model.</p></li>
<li><p>After using <cite>custom_mixed_precision</cite> or <cite>auto_mixed_precision</cite> for precision conversion, it is not supported
to perform the precision conversion again. If  <cite>Model</cite> is used to train a converted network, <cite>amp_level</cite>
need to be configured to <code class="docutils literal notranslate"><span class="pre">O0</span></code> to avoid the duplicated accuracy conversion.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>network</strong> (<a class="reference internal" href="../nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – A training or testing network.</p></li>
<li><p><strong>loss_fn</strong> (<a class="reference internal" href="../nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Objective function. If <cite>loss_fn</cite> is None, the <cite>network</cite> should contain the calculation of loss
and parallel if needed. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>optimizer</strong> (<a class="reference internal" href="../nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Optimizer for updating the weights. If <cite>optimizer</cite> is None, the <cite>network</cite> needs to
do backpropagation and update weights. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>metrics</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#set" title="(in Python v3.8)"><em>set</em></a><em>]</em>) – A Dictionary or a set of metrics for model evaluation.
eg: {‘accuracy’, ‘recall’}. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>eval_network</strong> (<a class="reference internal" href="../nn/mindspore.nn.Cell.html#mindspore.nn.Cell" title="mindspore.nn.Cell"><em>Cell</em></a>) – Network for evaluation. If not defined, <cite>network</cite> and <cite>loss_fn</cite> would be wrapped as
<cite>eval_network</cite> . Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>eval_indexes</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – It is used when eval_network is defined. If <cite>eval_indexes</cite> is None by default, all outputs
of the <cite>eval_network</cite> would be passed to metrics. If <cite>eval_indexes</cite> is set, it must contain
three elements: the positions of loss value, predicted value and label in outputs of the
<cite>eval_network</cite>. In this case, the loss value will be passed to the <cite>Loss</cite> metric, the
predicted value and label will be passed to other metrics.
<a class="reference internal" href="mindspore.train.Metric.html#mindspore.train.Metric.set_indexes" title="mindspore.train.Metric.set_indexes"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.train.Metric.set_indexes()</span></code></a> is recommended instead of <cite>eval_indexes</cite>.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>amp_level</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Option for argument <cite>level</cite> in <a class="reference internal" href="../amp/mindspore.amp.build_train_network.html#mindspore.amp.build_train_network" title="mindspore.amp.build_train_network"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.amp.build_train_network()</span></code></a>, level for mixed
precision training. Supports [“O0”, “O1”, “O2”, “O3”, “auto”]. Default: <code class="docutils literal notranslate"><span class="pre">&quot;O0&quot;</span></code> .</p>
<ul>
<li><p>”O0”: Do not change.</p></li>
<li><p>”O1”: Cast the operators in white_list to float16, the remaining operators are kept in float32.
The operators in the whitelist: [Conv1d, Conv2d, Conv3d, Conv1dTranspose, Conv2dTranspose,
Conv3dTranspose, Dense, LSTMCell, RNNCell, GRUCell, MatMul, BatchMatMul, PReLU, ReLU, Ger].</p></li>
<li><p>”O2”: Cast network to float16, keep BatchNorm run in float32, using dynamic loss scale.</p></li>
<li><p>”O3”: Cast network to float16, the BatchNorm is also cast to float16, loss scale will not be used.</p></li>
<li><p>”auto”: Set level to recommended level in different devices. Set level to “O2” on GPU, set
level to “O3” on Ascend. The recommended level is chosen by the expert experience, not applicable to all
scenarios. User should specify the level for special network.</p></li>
</ul>
<p>”O2” is recommended on GPU, “O3” is recommended on Ascend.
The BatchNorm strategy can be changed by <cite>keep_batchnorm_fp32</cite> settings in <cite>kwargs</cite>. <cite>keep_batchnorm_fp32</cite>
must be a bool. The loss scale strategy can be changed by <cite>loss_scale_manager</cite> setting in <cite>kwargs</cite>.
<cite>loss_scale_manager</cite> should be a subclass of <a class="reference internal" href="../amp/mindspore.amp.LossScaleManager.html#mindspore.amp.LossScaleManager" title="mindspore.amp.LossScaleManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.amp.LossScaleManager</span></code></a>.
The more detailed explanation of <cite>amp_level</cite> setting can be found at <cite>mindspore.amp.build_train_network</cite>.</p>
</p></li>
<li><p><strong>boost_level</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – <p>Option for argument <cite>level</cite> in <cite>mindspore.boost</cite>, level for boost mode
training. Supports [“O0”, “O1”, “O2”]. Default: <code class="docutils literal notranslate"><span class="pre">&quot;O0&quot;</span></code> .</p>
<ul>
<li><p>”O0”: Do not change.</p></li>
<li><p>”O1”: Enable the boost mode, the performance is improved by about 20%, and
the accuracy is the same as the original accuracy.</p></li>
<li><p>”O2”: Enable the boost mode, the performance is improved by about 30%, and
the accuracy is reduced by less than 3%.</p></li>
</ul>
<p>If you want to config boost mode by yourself, you can set boost_config_dict as <cite>boost.py</cite>.
In order for this function to work, you need to set the optimizer, eval_network or metric parameters
at the same time.</p>
<p>Notice: The current optimization enabled by default only applies to some networks, and not all networks
can obtain the same benefits.  It is recommended to enable this function on
the Graph mode + Ascend platform, and for better acceleration, refer to the documentation to configure
boost_config_dict.</p>
</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train_network</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">predict_network</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">eval_network</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sink_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.build" title="Permalink to this definition"></a></dt>
<dd><p>Build computational graphs and data graphs with the sink mode.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is an experimental API that is subject to change or deletion.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The interface builds the computational graphs, when the interface is executed first, ‘Model.train’ only
performs the graphs execution. Pre-build process only supports <cite>GRAPH_MODE</cite> and <cite>Ascend</cite> target currently.
It only supports dataset sink mode.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset</strong> (<em>Dataset</em>) – A training dataset iterator. If <cite>train_dataset</cite> is defined, training graphs will be
built. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>valid_dataset</strong> (<em>Dataset</em>) – An evaluating dataset iterator. If <cite>valid_dataset</cite> is defined, evaluation graphs
will be built, and <cite>metrics</cite> in <cite>Model</cite> can not be None. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>sink_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Control the number of steps for each sinking. Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code> .</p></li>
<li><p><strong>epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Control the training epochs. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.amp</span> <span class="kn">import</span> <span class="n">FixedLossScaleManager</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">FixedLossScaleManager</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>                 <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">valid_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_sink_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.eval" title="Permalink to this definition"></a></dt>
<dd><p>Evaluation API.</p>
<p>Configure to pynative mode or CPU, the evaluating process will be performed with dataset non-sink mode.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If dataset_sink_mode is True, data will be sent to device. At this point, the dataset will be bound to this
model, so the dataset cannot be used by other models. If the device is Ascend, features
of data will be transferred one by one. The limitation of data transmission per time is 256M.</p>
<p>The interface builds the computational graphs and then executes the computational graphs. However, when
the <cite>Model.build</cite> is executed first, it only performs the graphs execution.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>valid_dataset</strong> (<em>Dataset</em>) – Dataset to evaluate the model.</p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>(</em><a class="reference internal" href="mindspore.train.Callback.html#mindspore.train.Callback" title="mindspore.train.Callback"><em>Callback</em></a><em>)</em><em>, </em><a class="reference internal" href="mindspore.train.Callback.html#mindspore.train.Callback" title="mindspore.train.Callback"><em>Callback</em></a><em>]</em>) – List of callback objects or callback object,
which should be executed while evaluation.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>dataset_sink_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Determines whether to pass the data through dataset channel.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict, the key is the metric name defined by users and the value is the metrics value for
the model in the test mode.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;acc&#39;</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tutorial Examples:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/model.html#training-and-saving-model">Advanced Encapsulation: Model - Train and Save Model</a></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.train.Model.eval_network">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">eval_network</span></span><a class="headerlink" href="#mindspore.train.Model.eval_network" title="Permalink to this definition"></a></dt>
<dd><p>Get the model’s eval network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Object, the instance of evaluate network.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_dataset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_frequency</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_sink_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">valid_dataset_sink_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sink_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit API.</p>
<p>Evaluation process will be performed during training process if <cite>valid_dataset</cite> is provided.</p>
<p>More details please refer to <a class="reference internal" href="#mindspore.train.Model.train" title="mindspore.train.Model.train"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.train.Model.train()</span></code></a> and
<a class="reference internal" href="#mindspore.train.Model.eval" title="mindspore.train.Model.eval"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.train.Model.eval()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Total training epochs. Generally, train network will be trained on complete dataset per epoch.
If <cite>dataset_sink_mode</cite> is set to True and <cite>sink_size</cite> is greater than 0, each epoch will
train <cite>sink_size</cite> steps instead of total steps of dataset.
If <cite>epoch</cite> used with <cite>initial_epoch</cite>, it is to be understood as “final epoch”.</p></li>
<li><p><strong>train_dataset</strong> (<em>Dataset</em>) – A training dataset iterator. If <cite>loss_fn</cite> is defined, the data and label will be
passed to the <cite>network</cite> and the <cite>loss_fn</cite> respectively, so a tuple (data, label)
should be returned from dataset. If there is multiple data or labels, set <cite>loss_fn</cite>
to None and implement calculation of loss in <cite>network</cite>,
then a tuple (data1, data2, data3, …) with all data returned from dataset
will be passed to the <cite>network</cite>.</p></li>
<li><p><strong>valid_dataset</strong> (<em>Dataset</em>) – Dataset to evaluate the model. If <cite>valid_dataset</cite> is provided, evaluation process
will be performed on the end of training process. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>valid_frequency</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a>) – Only relevant if <cite>valid_dataset</cite> is provided.  If an integer, specifies
how many training epochs to run before a new validation run is performed,
e.g. <cite>valid_frequency=2</cite> runs validation every 2 epochs.
If a list, specifies the epochs on which to run validation,
e.g. <cite>valid_frequency=[1, 5]</cite> runs validation at the end of the 1st, 5th epochs.
Default: <code class="docutils literal notranslate"><span class="pre">1</span></code> .</p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.train.Callback.html#mindspore.train.Callback" title="mindspore.train.Callback"><em>Callback</em></a><em>]</em><em>, </em><a class="reference internal" href="mindspore.train.Callback.html#mindspore.train.Callback" title="mindspore.train.Callback"><em>Callback</em></a><em>]</em>) – List of callback objects or callback object,
which should be executed while training.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>dataset_sink_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Determines whether to pass the train data through dataset channel.
Configure pynative mode or CPU, the training process will be performed with
dataset not sink. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p><strong>valid_dataset_sink_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Determines whether to pass the validation data through dataset channel.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code> .</p></li>
<li><p><strong>sink_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Control the number of steps for each sinking.
<cite>sink_size</cite> is invalid if <cite>dataset_sink_mode</cite> is False.
If sink_size = -1, sink the complete dataset for each epoch.
If sink_size &gt; 0, sink sink_size data for each epoch.
Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code> .</p></li>
<li><p><strong>initial_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Epoch at which to start train, it useful for resuming a previous training run.
Default: <code class="docutils literal notranslate"><span class="pre">0</span></code> .</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">valid_dataset</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt>Tutorial Examples:</dt><dd><ul class="simple">
<li><p><a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/model.html#training-and-saving-model">Advanced Encapsulation: Model - Train and Save Model</a></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.infer_predict_layout">
<span class="sig-name descname"><span class="pre">infer_predict_layout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">predict_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_backend_compile</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.infer_predict_layout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.infer_predict_layout" title="Permalink to this definition"></a></dt>
<dd><p>Generate parameter layout for the predict network in ‘AUTO_PARALLEL’ or ‘SEMI_AUTO_PARALLEL’ mode.</p>
<p>Data could be a single tensor or multiple tensors.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Batch data should be put together in one tensor.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_data</strong> (<em>Union</em><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em><em>]</em><em>, </em><em>optional</em>) – The predict data, can be a single tensor,
a list of tensor, or a tuple of tensor.</p></li>
<li><p><strong>skip_backend_compile</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Only run the frontend compile process,
skip the compile process on the device side. Set this flag to True may
lead to recompiling process can not hit cache.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict, Parameter layout dictionary used for load distributed checkpoint.
Using as one of input parameters of load_distributed_checkpoint, always.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If not in GRAPH_MODE.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># This example should be run with multiple devices. Refer to the tutorial &gt; Distributed Training on</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># mindspore.cn.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">full_batch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">Net</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predict_map</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">infer_predict_layout</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.infer_train_layout">
<span class="sig-name descname"><span class="pre">infer_train_layout</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_sink_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sink_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.infer_train_layout"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.infer_train_layout" title="Permalink to this definition"></a></dt>
<dd><p>Generate parameter layout for the train network in ‘AUTO_PARALLEL’ or ‘SEMI_AUTO_PARALLEL’ mode.
Only dataset sink mode is supported for now.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This is an experimental API that is subject to change or deletion.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a pre-compile function. The arguments should be the same as model.train() function.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataset</strong> (<em>Dataset</em>) – A training dataset iterator. If there is no
loss_fn, a tuple with multiple data (data1, data2, data3, …) should be
returned and passed to the network. Otherwise, a tuple (data, label) should
be returned. The data and label would be passed to the network and loss
function respectively.</p></li>
<li><p><strong>dataset_sink_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Determines whether to pass the data through dataset channel.
Configure pynative mode or CPU, the training process will be performed with
dataset not sink. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code> .</p></li>
<li><p><strong>sink_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Control the number of steps for each sinking.
If sink_size = -1, sink the complete dataset for each epoch.
If sink_size &gt; 0, sink sink_size data for each epoch.
If dataset_sink_mode is False, set sink_size as invalid.
Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code> .</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict, Parameter layout dictionary used for load distributed checkpoint</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># This example should be run with multiple devices. Refer to the tutorial &gt; Distributed Training on</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># mindspore.cn.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">init</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">FixedLossScaleManager</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>                 <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">layout_dict</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">infer_train_layout</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">predict_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.predict" title="Permalink to this definition"></a></dt>
<dd><p>Generate output predictions for the input samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predict_data</strong> (<em>Union</em><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#tuple" title="(in Python v3.8)"><em>tuple</em></a><em>[</em><a class="reference internal" href="../mindspore/mindspore.Tensor.html#mindspore.Tensor" title="mindspore.Tensor"><em>Tensor</em></a><em>]</em><em>]</em><em>, </em><em>optional</em>) – The predict data, can be a single tensor,
a list of tensor, or a tuple of tensor.</p></li>
<li><p><strong>backend</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Select predict backend, this parameter is an experimental feature
and is mainly used for MindSpore Lite cloud-side inference. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code> .</p></li>
<li><p><strong>config</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a><em>, </em><em>optional</em>) – <p>The config includes two parts: config_path (configPath, str) and config_item (str, dict).
When the config_item is set, its priority is higher than the config_path. Set the ranking
table file for inference. The content of the configuration file is as follows:</p>
<p>config_path defines the path of the configuration file, which is used to pass user-defined
options during model building. In the following scenarios, users may need to set parameters.
For example: “/home/user/config.ini”. Default value: <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code> , here is the content of the
config.ini file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">rank_table_file</span> <span class="o">=</span> <span class="p">[</span><span class="n">path_a</span><span class="p">](</span><span class="n">storage</span> <span class="n">initial</span> <span class="n">path</span> <span class="n">of</span> <span class="n">the</span> <span class="n">rank</span> <span class="n">table</span> <span class="n">file</span><span class="p">)</span>
<span class="p">[</span><span class="n">execution_plan</span><span class="p">]</span>
<span class="p">[</span><span class="n">op_name1</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_type</span><span class="p">:</span><span class="n">float16</span> <span class="p">(</span><span class="n">operator</span> <span class="n">named</span> <span class="n">op_name1</span> <span class="ow">is</span> <span class="nb">set</span> <span class="n">to</span> <span class="n">data</span> <span class="nb">type</span> <span class="n">Float16</span><span class="p">)</span>
<span class="p">[</span><span class="n">op_name2</span><span class="p">]</span> <span class="o">=</span> <span class="n">data_type</span><span class="p">:</span><span class="n">float32</span> <span class="p">(</span><span class="n">operator</span> <span class="n">named</span> <span class="n">op_name2</span> <span class="ow">is</span> <span class="nb">set</span> <span class="n">to</span> <span class="n">data</span> <span class="nb">type</span> <span class="n">Float32</span><span class="p">)</span>
</pre></div>
</div>
<p>When only the config_path is configured, it is done as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configPath&quot;</span> <span class="p">:</span> <span class="s2">&quot;/home/user/config.ini&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p>When only the config_dict is configured, it is done as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;ascend_context&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;rank_table_file&quot;</span> <span class="p">:</span> <span class="s2">&quot;path_b&quot;</span><span class="p">},</span>
          <span class="s2">&quot;execution_plan&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;op_name1&quot;</span> <span class="p">:</span> <span class="s2">&quot;data_type:float16&quot;</span><span class="p">,</span> <span class="s2">&quot;op_name2&quot;</span> <span class="p">:</span> <span class="s2">&quot;data_type:float32&quot;</span><span class="p">}}</span>
</pre></div>
</div>
<p>When both the <cite>config_path</cite> and the <cite>config_dict</cite> are configured, it is done as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;configPath&quot;</span> <span class="p">:</span> <span class="s2">&quot;/home/user/config.ini&quot;</span><span class="p">,</span>
          <span class="s2">&quot;ascend_context&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;rank_table_file&quot;</span> <span class="p">:</span> <span class="s2">&quot;path_b&quot;</span><span class="p">},</span>
          <span class="s2">&quot;execution_plan&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;op_name3&quot;</span> <span class="p">:</span> <span class="s2">&quot;data_type:float16&quot;</span><span class="p">,</span> <span class="s2">&quot;op_name4&quot;</span> <span class="p">:</span> <span class="s2">&quot;data_type:float32&quot;</span><span class="p">}}</span>
</pre></div>
</div>
<dl class="simple">
<dt>Note that both the “configPath” is configured in the config_dict and the config_item,</dt><dd><p>in this case, the path_b in the config_dict takes precedence.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor, array(s) of predictions.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">LeNet5</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.train.Model.predict_network">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">predict_network</span></span><a class="headerlink" href="#mindspore.train.Model.predict_network" title="Permalink to this definition"></a></dt>
<dd><p>Get the model’s predict network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Object, the instance of predict network.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore.train.Model.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_dataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_sink_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sink_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initial_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/train/model.html#Model.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore.train.Model.train" title="Permalink to this definition"></a></dt>
<dd><p>Training API.</p>
<p>When setting pynative mode or CPU, the training process will be performed with dataset not sink.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If dataset_sink_mode is True, data will be sent to device. If the device is Ascend, features
of data will be transferred one by one. The limitation of data transmission per time is 256M.</p>
<p>When dataset_sink_mode is True, the <cite>step_end</cite> method of the instance of Callback will be called at the end
of step in PyNative mode， or will be called at the end of epoch in Graph mode.</p>
<p>If dataset_sink_mode is True, dataset will be bound to this model and cannot be used by other models.</p>
<p>If sink_size &gt; 0, each epoch of the dataset can be traversed unlimited times until you get sink_size
elements of the dataset. The next epoch continues to traverse from the end position of the previous
traversal.</p>
<p>The interface builds the computational graphs and then executes the computational graphs. However, when
the <cite>Model.build</cite> is executed first, it only performs the graphs execution.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Total training epochs. Generally, train network will be trained on complete dataset per epoch.
If <cite>dataset_sink_mode</cite> is set to True and <cite>sink_size</cite> is greater than 0, each epoch will
train <cite>sink_size</cite> steps instead of total steps of dataset.
If <cite>epoch</cite> used with <cite>initial_epoch</cite>, it is to be understood as “final epoch”.</p></li>
<li><p><strong>train_dataset</strong> (<em>Dataset</em>) – A training dataset iterator. If <cite>loss_fn</cite> is defined, the data and label will be
passed to the <cite>network</cite> and the <cite>loss_fn</cite> respectively, so a tuple (data, label)
should be returned from dataset. If there is multiple data or labels, set <cite>loss_fn</cite>
to None and implement calculation of loss in <cite>network</cite>,
then a tuple (data1, data2, data3, …) with all data returned from dataset will be
passed to the <cite>network</cite>.</p></li>
<li><p><strong>callbacks</strong> (<em>Optional</em><em>[</em><a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore.train.Callback.html#mindspore.train.Callback" title="mindspore.train.Callback"><em>Callback</em></a><em>]</em><em>, </em><a class="reference internal" href="mindspore.train.Callback.html#mindspore.train.Callback" title="mindspore.train.Callback"><em>Callback</em></a><em>]</em>) – List of callback objects or callback object,
which should be executed while training.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>dataset_sink_mode</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Determines whether to pass the data through dataset channel.
Configure pynative mode or CPU, the training process will be performed with
dataset not sink. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>sink_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Control the number of steps for each sinking.
<cite>sink_size</cite> is invalid if <cite>dataset_sink_mode</cite> is False.
If sink_size = -1, sink the complete dataset for each epoch.
If sink_size &gt; 0, sink sink_size data for each epoch.
Default: -1.</p></li>
<li><p><strong>initial_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Epoch at which to start train, it used for resuming a previous training run.
Default: 0.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create the dataset taking MNIST as an example. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/mnist.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Define the network structure of LeNet5. Refer to</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># https://gitee.com/mindspore/docs/blob/master/docs/mindspore/code/lenet.py</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">LeNet5</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss_scale_manager</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="mf">1024.</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optim</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>                 <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale_manager</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore.train.Model.train_network">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">train_network</span></span><a class="headerlink" href="#mindspore.train.Model.train_network" title="Permalink to this definition"></a></dt>
<dd><p>Get the model’s train network.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Object, the instance of train network.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../mindspore.train.html" class="btn btn-neutral float-left" title="mindspore.train" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore.train.BackupAndRestore.html" class="btn btn-neutral float-right" title="mindspore.train.BackupAndRestore" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>