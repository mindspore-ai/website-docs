<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Combination of Dynamic and Static Graphs &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distributed Parallel Native" href="distributed_training_design.html" />
    <link rel="prev" title="Functional and Object-Oriented Fusion Programming Paradigm" href="programming_paradigm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Combination of Dynamic and Static Graphs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-concept-of-static-and-dynamic-graphs">The Concept of Static and Dynamic Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#mindspore-static-graph">MindSpore Static Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#graph-mode-execution-principle">Graph Mode Execution Principle</a></li>
<li class="toctree-l3"><a class="reference internal" href="#graph-mode-auto-differentiation-principle">Graph Mode Auto-differentiation Principle</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#mindspore-dynamic-graph">MindSpore Dynamic Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pynative-mode-execution-principle">PyNative Mode Execution Principle</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pynative-mode-auto-differentiation-principle">PyNative Mode Auto-differentiation Principle</a></li>
<li class="toctree-l3"><a class="reference internal" href="#control-flow-in-pynative-mode">Control flow in PyNative Mode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-and-static-unification">Dynamic and Static Unification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interconversion-of-dynamic-and-static-graphs">Interconversion of Dynamic and Static Graphs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#combination-of-static-and-dynamic">Combination of Static and Dynamic</a></li>
<li class="toctree-l3"><a class="reference internal" href="#static-graph-syntax-enhancement">Static Graph Syntax Enhancement</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#calling-the-third-party-libraries">Calling the Third-party Libraries</a></li>
<li class="toctree-l4"><a class="reference internal" href="#supporting-the-use-of-custom-classes">Supporting the Use of Custom Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#basic-operators-support-more-data-type">Basic Operators Support More Data Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="#base-type">Base Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="#built-in-functions-support-more-data-types">Built-in Functions Support More Data Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#supporting-control-flow">Supporting Control Flow</a></li>
<li class="toctree-l4"><a class="reference internal" href="#support-property-setting-and-modification">Support Property Setting and Modification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#supporting-derivation">Supporting Derivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#annotation-type">Annotation Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="#instructions-for-use">Instructions for Use</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Combination of Dynamic and Static Graphs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/design/dynamic_graph_and_static_graph.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="combination-of-dynamic-and-static-graphs">
<h1>Combination of Dynamic and Static Graphs<a class="headerlink" href="#combination-of-dynamic-and-static-graphs" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_en/design/dynamic_graph_and_static_graph.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="the-concept-of-static-and-dynamic-graphs">
<h2>The Concept of Static and Dynamic Graphs<a class="headerlink" href="#the-concept-of-static-and-dynamic-graphs" title="Permalink to this headline"></a></h2>
<p>There are two execution modes of the mainstream deep learning frameworks, namely static graph mode and dynamic graph mode.</p>
<p>In static graph mode, the program firstly generates the graph structure of the neural network, then executes the computational operations involved in the graph during compilation execution. Therefore, in static graph mode, the compiler uses techniques such as graph optimization to optimize the execution graph to a greater extent, resulting in better execution performance that helps scale deployment and cross-platform operation.</p>
<p>In dynamic graph mode, the program is executed in the order in which the code is written, and the reverse execution graph is dynamically generated during the execution of the forward process based on the principle of backward propagation. In this mode, the compiler sends down the individual operators in the neural network for execution one by one, making it easy for the user to write and debug the neural network model.</p>
</section>
<section id="mindspore-static-graph">
<h2>MindSpore Static Graph<a class="headerlink" href="#mindspore-static-graph" title="Permalink to this headline"></a></h2>
<p>In MindSpore, the static graph mode, also known as Graph mode, can be set to static graph mode by <code class="docutils literal notranslate"><span class="pre">set_context(mode=GRAPH_MODE)</span></code>. Static graph mode is more suitable for scenarios where the network is fixed and high performance is required. In static graph mode, the compiler can perform global optimization for the graph based on techniques such as graph optimization, whole graph offloading of computational graph. Therefore, better performance can be obtained under static graph, but the execution graph is converted from the source code. Not all Python syntax is supported under static graphs.</p>
<section id="graph-mode-execution-principle">
<h3>Graph Mode Execution Principle<a class="headerlink" href="#graph-mode-execution-principle" title="Permalink to this headline"></a></h3>
<p>In Graph mode, MindSpore converts Python source code into IR by means of source code conversion, then performs relevant graph optimization based on this, and finally executes the optimized graph on hardware devices. MindSpore uses a functional IR based on graph representation, i.e. MindIR, by using a semantics close to the ANF functional style. The Graph mode is compiled and optimized based on MindIR. To use the Graph mode, you need to use the <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> class and write the execution code in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function or call the <code class="docutils literal notranslate"><span class="pre">&#64;jit</span></code> decorator.</p>
<p>An code example for the Graph model is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[ 4. 10. 18.]
</pre></div>
</div>
</section>
<section id="graph-mode-auto-differentiation-principle">
<h3>Graph Mode Auto-differentiation Principle<a class="headerlink" href="#graph-mode-auto-differentiation-principle" title="Permalink to this headline"></a></h3>
<p>In MindSpore, the principle of auto-differentiation in Graph mode can be found in <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/autograd.html">Auto-differentiation</a>.</p>
</section>
</section>
<section id="mindspore-dynamic-graph">
<h2>MindSpore Dynamic Graph<a class="headerlink" href="#mindspore-dynamic-graph" title="Permalink to this headline"></a></h2>
<p>In MindSpore, dynamic graph mode is also known as PyNative mode, which can be set to dynamic graph mode by <code class="docutils literal notranslate"><span class="pre">set_context(mode=PYNATIVE_MODE)</span></code>. In script development and network flow debugging, it is recommended to use dynamic graph mode for debugging, which supports the execution of single operators, common functions and networks, and separate gradient solving operations.</p>
<section id="pynative-mode-execution-principle">
<h3>PyNative Mode Execution Principle<a class="headerlink" href="#pynative-mode-execution-principle" title="Permalink to this headline"></a></h3>
<p>In PyNative mode, users can use the full Python API. In addition, for using the API provided by MindSpore, the framework will execute the operations of the operator API on the corresponding hardware platform according to the hardware platform (Ascend, GPU, CPU) selected by the user and return the corresponding results. The overall execution process of the framework is as follows:</p>
<p><img alt="process" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/docs/mindspore/source_zh_cn/design/images/framework.png" /></p>
<p>Through the front-end Python API, call to the framework layer, and finally to the corresponding hardware devices to perform calculations. For example, to complete an addition</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[[[2. 2. 2. 2.]
   [2. 2. 2. 2.]
   [2. 2. 2. 2.]]

  [[2. 2. 2. 2.]
   [2. 2. 2. 2.]
   [2. 2. 2. 2.]]

  [[2. 2. 2. 2.]
   [2. 2. 2. 2.]
   [2. 2. 2. 2.]]]]
</pre></div>
</div>
<p>In this example, when the Python interface ops.add(x, y) is called, the Python interface call is called to the C++ layer of the framework via Pybind11, and converted to C++ call. Then the framework will select the corresponding hardware device according to the device_target set by the users, and execute the add operation on that hardware device.</p>
<p>From the above principle, we can see that in PyNative mode, Python script code will be executed according to Python syntax, and the execution process involves MindSpore’s API, which will be accelerated by executing on different hardware according to user settings. Therefore, in PyNative mode, users can use Python syntax and debugging methods at will, for example, you can use common IDEs such as PyCharm and VS Code to debug code.</p>
</section>
<section id="pynative-mode-auto-differentiation-principle">
<h3>PyNative Mode Auto-differentiation Principle<a class="headerlink" href="#pynative-mode-auto-differentiation-principle" title="Permalink to this headline"></a></h3>
<p>In the previous introduction, we can see that the execution of the forward procedure under PyNative is performed exactly according to Python syntax. Under PyNative, backward propagation is implemented based on Tensor. We record all the operations applied to Tensor during the execution of the forward process, and for each operation find its reverse, and string all the reverse processes together to form the overall backward propagation graph (reverse graph for short). Eventually, the reverse graph is executed on the device to calculate the gradient.</p>
<p>The following code is an example of the reverse composition process: multiply the matrix x with a fixed parameter z, then perform matrix multiplication with y, and finally derives x.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">MatMul</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">z</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">GradNetWrtX</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GradNetWrtX</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">gradient_function</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gradient_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mf">0.11</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.1</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">GradNetWrtX</span><span class="p">(</span><span class="n">Net</span><span class="p">())(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[9.02      5.4       7.2000003]
 [9.02      5.4       7.2000003]]
</pre></div>
</div>
<p>According to the above composition principle under PyNative, it can be seen that in the forward propagation process, we record the calculation process of Mul. According to the definition of reverse bprop corresponding to the Mul, we get the reverse MulGrad operator. According to the definition of Mul operator’s bprop, as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore.ops._grad.grad_base</span> <span class="kn">import</span> <span class="n">bprop_getters</span>

<span class="nd">@bprop_getters</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">Mul</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">get_bprop_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Grad definition for `Mul` operation.&quot;&quot;&quot;</span>
    <span class="n">mul_func</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">bprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="n">bc_dx</span> <span class="o">=</span> <span class="n">mul_func</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="n">bc_dy</span> <span class="o">=</span> <span class="n">mul_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">binop_grad_common</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bc_dx</span><span class="p">,</span> <span class="n">bc_dy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">bprop</span>
</pre></div>
</div>
<p>It can be seen that reversing the input to Mul requires backward propagation gradient values of two input and output, at which point z can be connected to MulGrad based on the actual input values. And so on, for the next operator Matmul, the MatmulGrad information is obtained accordingly, and then the contextual gradient propagation is connected according to the input and output of bprop.</p>
<p>Similarly for the input y derivation, the same procedure can be used for the derivation.</p>
</section>
<section id="control-flow-in-pynative-mode">
<h3>Control flow in PyNative Mode<a class="headerlink" href="#control-flow-in-pynative-mode" title="Permalink to this headline"></a></h3>
<p>In the PyNative mode, scripts are executed according to the Python syntax, so in MindSpore, there is no special treatment for the control flow syntax, which is directly expanded and executed according to the Python syntax, and automatic differentiation is performed on the expanded execution operator. For example, for a for loop, the statements in the for loop are continuously executed under PyNative and automatic differentiation is performed on the operators according to the specific number of loops.</p>
</section>
</section>
<section id="dynamic-and-static-unification">
<h2>Dynamic and Static Unification<a class="headerlink" href="#dynamic-and-static-unification" title="Permalink to this headline"></a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h3>
<p>The industry currently supports both dynamic and static graph modes. Dynamic graphs are executed by interpretation, with dynamic syntax affinity and flexible expression, and static graphs are executed by using jit compilation optimization, more inclined to static syntax and more restrictions in syntax. For dynamic and static graph modes, firstly MindSpore unifies the API expression, uses the same API in both modes, secondly, unifies the underlying differential mechanism of dynamic and static graphs.</p>
</section>
<section id="interconversion-of-dynamic-and-static-graphs">
<h3>Interconversion of Dynamic and Static Graphs<a class="headerlink" href="#interconversion-of-dynamic-and-static-graphs" title="Permalink to this headline"></a></h3>
<p>In MindSpore, we can switch the execution between using dynamic or static graphs by controlling the mode input parameters. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">)</span>
</pre></div>
</div>
<p>Since there are restrictions on Python syntax under static graphs, switching from dynamic to static graphs requires compliance with the syntax restrictions of static graphs in order to execute correctly by using static graphs. For more syntax restrictions for static graphs, refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Restrictions</a>.</p>
</section>
<section id="combination-of-static-and-dynamic">
<h3>Combination of Static and Dynamic<a class="headerlink" href="#combination-of-static-and-dynamic" title="Permalink to this headline"></a></h3>
<p>MindSpore supports mixed execution by using static compilation under dynamic graphs. The function objects that need to be executed with static graphs by using jit modification, and in this way you can achieve mixed execution of dynamic and static graphs. For more use of jit, refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/accelerate_with_static_graph.html#decorator-based-startup-method">jit documentation</a>.</p>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="k">class</span> <span class="nc">AddMulMul</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AddMulMul</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

    <span class="nd">@ms</span><span class="o">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">param</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">CellCallSingleCell</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CellCallSingleCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_init</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">gamma_init</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_mul_mul</span> <span class="o">=</span> <span class="n">AddMulMul</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_mul_mul</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">PYNATIVE_MODE</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">CellCallSingleCell</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[[[[15.99984]]

  [[15.99984]]]]
</pre></div>
</div>
</section>
<section id="static-graph-syntax-enhancement">
<h3>Static Graph Syntax Enhancement<a class="headerlink" href="#static-graph-syntax-enhancement" title="Permalink to this headline"></a></h3>
<p>In the MindSpore static graph mode, users need to follow MindSpore <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a> when writing programs, and there are constraints on the use of syntax. In dynamic graph mode, Python script code will be executed according to Python syntax, and users can use any Python syntax. It can be seen that the syntax constraints of static and dynamic graphs are different.</p>
<p>JIT Fallback considers the unification of static and dynamic graphs from the perspective of static graphs. When an unsupported syntax is found during compilation, the syntax is Fallback to the Python interpreter for interpretation execution. Through the JIT Fallback feature, static graphs can support as much dynamic graph syntax as possible, so that static graphs provide a syntax experience close to dynamic graphs, so as to achieve dynamic and static unity.</p>
<p>In the graph mode scenario, the MindSpore framework will report an error when it encounters unsupported syntax or symbols during graph compilation, mostly in the type inference stage. In the graph compilation stage, the Python source code written by the user is parsed, and then subsequent static analysis, type derivation, optimization and other steps are performed. Therefore, the JIT Fallback feature needs to be pre-detected for unsupported syntax. Common unsupported syntax mainly includes: calling methods of third-party libraries, calling class names to create objects, calling unsupported Python built-in functions, etc. Interpret execution of unsupported syntax Fallback to the Python interpreter. Since the graph mode uses <a class="reference external" href="https://www.mindspore.cn/docs/en/master/design/all_scenarios.html#mindspore-ir-mindir">MindSpore IR (MindIR)</a>, it is necessary to convert the statement executed by the interpretation to the intermediate representation and record the information required by the interpreter.</p>
<p>The following mainly introduces the static graph syntax supported using the JIT Fallback extension. The default value of the JIT syntax support level option jit_syntax_level is ‘LAX’, extending the static graph syntax with the ability of JIT Fallback.</p>
<section id="calling-the-third-party-libraries">
<h4>Calling the Third-party Libraries<a class="headerlink" href="#calling-the-third-party-libraries" title="Permalink to this headline"></a></h4>
<p>Complete support for third-party libraries such as NumPy and SciPy. The static graph mode supports many third-party library data types such as np.ndarray and their operation operations, supports obtaining properties and methods that call third-party libraries, and supports interacting with third-party libraries such as NumPy through methods such as Tensor’s asnumpy(). In other words, users can call MindSpore’s own interface and operator in static graph mode, or directly call the interface of the three-party library, or use them together.</p>
<ul class="simple">
<li><p>Supporting data types of third-party libraries (such as NumPy and SciPy), allowing calling and returning objects of third-party libraries.</p></li>
<li><p>Supporting calling methods of third-party libraries.</p></li>
<li><p>Supporting creating Tensor instances by using the data types of the third-party library NumPy.</p></li>
<li><p>The assignment of subscripts for data types in third-party libraries is not currently supported.</p></li>
</ul>
<p>For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#calling-the-third-party-libraries">Calling the Third-party Libraries</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="supporting-the-use-of-custom-classes">
<h4>Supporting the Use of Custom Classes<a class="headerlink" href="#supporting-the-use-of-custom-classes" title="Permalink to this headline"></a></h4>
<p>Custom classes that do not use ‘&#64;jit_class’ decorations and do not inherit ‘nn. Cell`。 Through the JIT Fallback technical solution, static graph mode allows creating and referencing instances of custom classes, can directly obtain and call properties and methods of custom class instances, and allows modifying properties(Inplace operations).</p>
<p>For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#supporting-the-use-of-custom-classes">Supporting the Use of Custom Classes</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="basic-operators-support-more-data-type">
<h4>Basic Operators Support More Data Type<a class="headerlink" href="#basic-operators-support-more-data-type" title="Permalink to this headline"></a></h4>
<p>In the syntax of graph mode, the following basic operators in the list is overloaded: [‘+’, ‘-’, ‘*’, ‘/’, ‘//’, ‘%’, ‘**’, ‘&lt;&lt;’, ‘&gt;&gt;’, ‘&amp;’, ‘|’, ‘^’, ‘not’, ‘==’, ‘!=’, ‘&lt;’, ‘&gt;’, ‘&lt;=’, ‘&gt;=’, ‘in’, ‘not in’, ‘y=x[0]’]. For more details, please refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax/operators.html">Operators</a>. When getting unsupported input type, those operators need to use extended static graph syntax to support, and make the output consistent with the output in the pynative mode.</p>
<p>For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#basic-operators-support-more-data-type">Basic Operators Support More Data Type</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="base-type">
<h4>Base Type<a class="headerlink" href="#base-type" title="Permalink to this headline"></a></h4>
<p>Use the JIT Fallback feature to extend support for Python’s native data types ‘List’, ‘Dictionary’, ‘None’. For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#base-type">Base Type</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
<section id="supporting-list-inplace-modification-operations">
<h5>Supporting List Inplace Modification Operations<a class="headerlink" href="#supporting-list-inplace-modification-operations" title="Permalink to this headline"></a></h5>
<ul class="simple">
<li><p>Support for getting the original ‘List’ object from a global variable.</p></li>
<li><p>Inplace operations on input ‘List’ objects are not supported.</p></li>
<li><p>Support for in-place modification of some ‘List’ built-in functions.</p></li>
</ul>
</section>
<section id="supporting-the-high-level-usage-of-dictionary">
<h5>Supporting the High-Level Usage of Dictionary<a class="headerlink" href="#supporting-the-high-level-usage-of-dictionary" title="Permalink to this headline"></a></h5>
<ul class="simple">
<li><p>Supporting Top Graph Return Dictionary</p></li>
<li><p>Supporting Dictionary Index Value Retrieval and Assignment</p></li>
</ul>
</section>
<section id="supporting-the-usage-of-none">
<h5>Supporting the Usage of None<a class="headerlink" href="#supporting-the-usage-of-none" title="Permalink to this headline"></a></h5>
<p>‘None’ is a special value in Python that represents null and can be assigned to any variable. Functions that do not have a return value statement are considered to return ‘None’. At the same time, ‘None’ is also supported as the input parameter or return value of the top graph or subgraph. Support ‘None’ as a subscript of a slice as input to ‘List’, ‘Tuple’, ‘Dictionary’.</p>
</section>
</section>
<section id="built-in-functions-support-more-data-types">
<h4>Built-in Functions Support More Data Types<a class="headerlink" href="#built-in-functions-support-more-data-types" title="Permalink to this headline"></a></h4>
<p>Extend the support for built-in functions. Python built-in functions perfectly support more input types, such as third-party library data types. More support for built-in functions can be found in the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax/python_builtin_functions.html">Python built-in functions</a> section.</p>
</section>
<section id="supporting-control-flow">
<h4>Supporting Control Flow<a class="headerlink" href="#supporting-control-flow" title="Permalink to this headline"></a></h4>
<p>In order to improve the support of Python standard syntax, realize dynamic and static unification, and extend the support for more data types in the use of control flow statements. Control flow statements refer to flow control statements such as ‘if’, ‘for’, and ‘while’. Theoretically, by extending the supported syntax, it is also supported in control flow scenarios. For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#supporting-control-flow">Supports Control Flow</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="support-property-setting-and-modification">
<h4>Support Property Setting and Modification<a class="headerlink" href="#support-property-setting-and-modification" title="Permalink to this headline"></a></h4>
<p>More types of inplace operations are supported. The previous version only supported value modification of the Parameter type through the Inplace operator, and in the static graph mode of MindSpore version 2.1, the properties of custom classes, Cell subclasses, and jit_class classes were supported. In addition to supporting changing the properties of class self and global variables, it also supports inplace operations such as extend(), reverse(), insert(), pop() of the List type. For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#supporting-property-setting-and-modification">Supporting Property Setting and Modification</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
<ul class="simple">
<li><p>Set and modify properties of custom class objects and third-party types.</p></li>
<li><p>Make changes to the Cell’s self object.</p></li>
<li><p>Set and modify Cell objects and jit_class objects in the static graph.</p></li>
</ul>
</section>
<section id="supporting-derivation">
<h4>Supporting Derivation<a class="headerlink" href="#supporting-derivation" title="Permalink to this headline"></a></h4>
<p>The static graph syntax supported by JIT Fallback also supports its use in derivation. For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#supporting-derivation">Supporting Derivation</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="annotation-type">
<h4>Annotation Type<a class="headerlink" href="#annotation-type" title="Permalink to this headline"></a></h4>
<p>For the syntax supported by the runtime extensions, nodes are generated that cannot be derived by type and are called ‘Any’ types. Since the type cannot derive the correct type at compile time, this ‘Any’ will be operated with a default maximum precision ‘Float64’ to prevent loss of precision. To optimize performance, it is recommended to minimize the generation of ‘Any’ types. When the user knows exactly what type of statement will be generated through the extension, it is recommended to use <code class="docutils literal notranslate"><span class="pre">Annotation</span> <span class="pre">&#64;jit.typing:</span></code> to specify the corresponding Python statement type, thereby determining the type of the interpretation node and avoiding the generation of ‘Any’ types. For more usage, please refer to the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html#annotation-type">Annotation Type</a> section in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/static_graph_syntax_support.html">Static Graph Syntax Support</a>.</p>
</section>
<section id="instructions-for-use">
<h4>Instructions for Use<a class="headerlink" href="#instructions-for-use" title="Permalink to this headline"></a></h4>
<p>When using the static graph extension support syntax, note the following points:</p>
<ol class="arabic simple">
<li><p>In order to match the support capability of the dynamic graph. That is, it must be within the scope of dynamic graph syntax, including but not limited to data types.</p></li>
<li><p>When extending the static graph syntax, more syntax is supported, but the execution performance may be affected and is not optimal.</p></li>
<li><p>When extending the static graph syntax, more syntax is supported, and the ability to import and export cannot be used with MindIR due to use Python.</p></li>
<li><p>It is not currently supported that the repeated definition of global variables with the same name across Python files, and these global variables are used in the network.</p></li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="programming_paradigm.html" class="btn btn-neutral float-left" title="Functional and Object-Oriented Fusion Programming Paradigm" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed_training_design.html" class="btn btn-neutral float-right" title="Distributed Parallel Native" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>