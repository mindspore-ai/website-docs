

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Model Analysis and Preparation &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Constructing MindSpore Network" href="model_development/model_development.html" />
    <link rel="prev" title="Environment Preparation and Information Acquisition" href="enveriment_preparation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/programming_paradigm.html">Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">Functional Differential Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">MindSpore IR (MindIR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">Full-scenarios Unification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">Distributed Training Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">Environment Preparation and Information Acquisition</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">Constructing MindSpore Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">Migration from a Third-party Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Model Analysis and Preparation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/analysis_and_preparation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="model-analysis-and-preparation">
<h1>Model Analysis and Preparation<a class="headerlink" href="#model-analysis-and-preparation" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_en/migration_guide/analysis_and_preparation.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<section id="obtaining-sample-code">
<h2>Obtaining Sample Code<a class="headerlink" href="#obtaining-sample-code" title="Permalink to this headline"></a></h2>
<p>When you obtain a paper to implement migration on MindSpore, you need to find the reference code that has been implemented in other frameworks. In principle, the reference code must meet at least one of the following requirements:</p>
<ol class="arabic simple">
<li><p>The author opens the paper to the public.</p></li>
<li><p>The implementation is starred and forked by many developers, which means it is widely recognized.</p></li>
<li><p>The code is new and maintained by developers.</p></li>
<li><p>The PyTorch reference code is preferred.</p></li>
</ol>
<p>If the results are not reproducible in the reference project or the version information is missing, check the project issue for information.</p>
<p>If a new paper has no reference implementation, you can refer to <a class="reference external" href="https://www.mindspore.cn/docs/en/master/migration_guide/model_development/model_development.html">Constructing MindSpore Network</a>.</p>
</section>
<section id="analyzing-algorithm-and-network-structure">
<h2>Analyzing Algorithm and Network Structure<a class="headerlink" href="#analyzing-algorithm-and-network-structure" title="Permalink to this headline"></a></h2>
<p>First, when reading the paper and reference code, you need to analyze the network structure to organize the code writing. The following shows the general network structure of YOLOX.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Module</p></th>
<th class="head"><p>Implementation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>backbone</p></td>
<td><p>CSPDarknet (s, m, l, x)</p></td>
</tr>
<tr class="row-odd"><td><p>neck</p></td>
<td><p>FPN</p></td>
</tr>
<tr class="row-even"><td><p>head</p></td>
<td><p>Decoupled Head</p></td>
</tr>
</tbody>
</table>
<p>Second, analyze the innovative points of the migration algorithm and record the tricks used during the training, for example, data augmentation added during data processing, shuffle, optimizer, learning rate attenuation policy, and parameter initialization. You can prepare a checklist and fill in the corresponding items during analysis.</p>
<p>For example, the following records some tricks used by the YOLOX network during training.</p>
<table>
    <tr>
        <th>Trick</th>
        <th>Record</th>
   </tr>
    <tr>
        <td rowspan="2">Data augmentation</td>
        <td >Mosaic, including random scaling, crop, and layout</td>
    </tr>
    <tr>
        <td >MixUp</td>
    </tr>
    <tr>
        <td >Learning rate attenuation policy</td>
        <td >Multiple attenuation modes are available. By default, the COS learning rate attenuation is used. </td>
    </tr>
    <tr>
        <td >Optimizer parameters</td>
        <td >SGD momentum=0.9, nesterov=True, and no weight decay</td>
    </tr>
    <tr>
        <td >Training parameters</td>
        <td >epoch: 300; batchsize: 8</td>
    </tr>
    <tr>
        <td >Network structure optimization points</td>
        <td >Decoupled Head; Anchor Free; SimOTA</td>
    </tr>
    <tr>
        <td >Training process optimization points </td>
        <td >EMA; Data augmentation is not performed for the last 15 epochs; mixed precision </td>
    </tr>
</table>
<p><strong>Note that the tricks used in the code are mainly reproduced. The tricks mentioned in some papers may not be useful.</strong></p>
<p>In addition, you need to determine whether the paper can be implemented by modifying the existing MindSpore model. If yes, you can greatly reduce the development workload. For example, WGAN-PG can be developed based on WGAN.
<a class="reference external" href="https://gitee.com/mindspore/models">MindSpore models</a> is a model repository. It covers mainstream models in multiple fields, such as machine vision, natural language processing, voice, and recommendation system. You can check whether there are required models from the repository.</p>
</section>
<section id="reproducing-paper-implementation">
<h2>Reproducing Paper Implementation<a class="headerlink" href="#reproducing-paper-implementation" title="Permalink to this headline"></a></h2>
<p>After obtaining the reference code, you need to reproduce the accuracy of the reference implementation and obtain the performance data of the reference implementation. This has the following advantages:</p>
<ol class="arabic simple">
<li><p>Identify some issues in advance.</p>
<ul class="simple">
<li><p>Check whether the third-party repository used by the reference code depends on a version to identify version adaptation problems in advance.</p></li>
<li><p>Check whether the dataset can be obtained. Some datasets are private or the author adds some datasets to the public dataset. This problem can be found at the reproduction reference implementation stage.</p></li>
<li><p>Check whether the reference implementation can reproduce the accuracy of the paper. Some official reference implementations may not reproduce the accuracy of the paper. In this case, detect the problem in time, replace the reference implementation, or adjust the accuracy baseline.</p></li>
</ul>
</li>
<li><p>Obtain some reference data for the MindSpore migration process.</p>
<ul class="simple">
<li><p>Obtain the loss decrease trend to check whether the training convergence trend on MindSpore is normal.</p></li>
<li><p>Obtain the parameter file for conversion and inference verification. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/migration_guide/model_development/training_and_evaluation_procession.html">Inference and Training Process</a>.</p></li>
<li><p>Obtain the performance baseline for performance tuning. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/migration_guide/debug_and_tune.html">Debugging and Tuning</a>.</p></li>
</ul>
</li>
</ol>
</section>
<section id="analyzing-api-compliance">
<h2>Analyzing API Compliance<a class="headerlink" href="#analyzing-api-compliance" title="Permalink to this headline"></a></h2>
<p>The API missing analysis here refers to APIs in the network execution diagram, including MindSpore <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore.ops.primitive.html">operators</a> and advanced encapsulated APIs, and excluding the APIs used in data processing. You are advised to use third-party APIs, such as NumPy, OpenCV, Pandas, and PIL, to replace APIs used in data processing.</p>
<section id="querying-the-api-mapping-table">
<h3>Querying the API Mapping Table<a class="headerlink" href="#querying-the-api-mapping-table" title="Permalink to this headline"></a></h3>
<p>Take the PyTorch code migration as an example. After obtaining the reference code implementation, you can filter keywords such as <code class="docutils literal notranslate"><span class="pre">torch</span></code>, <code class="docutils literal notranslate"><span class="pre">nn</span></code>, and <code class="docutils literal notranslate"><span class="pre">ops</span></code> to obtain the used APIs. If the method of another repository is invoked, you need to manually analyze the API. Then, check the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/note/api_mapping/pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a>.
Alternatively, the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore.ops.primitive.html">API</a> searches for the corresponding API implementation.</p>
<p>Generally the training process of a network contains forward calculation, backward gradient calculation and parameter update. In some special scenarios, another gradient calculation is needed for the gradient, such as <a class="reference external" href="https://arxiv.org/pdf/1704.00028.pdf">Gradient Penalty</a>, and this kind of scenario uses the second order gradient calculation. For scenarios where second-order gradient calculations are used in the network requires additional analysis of the second-order support of the APIs, the derivative links of the network need to be analyzed by code walk-through, and all APIs within the second-order derivative links need to support second order. The second-order support case can be viewed in <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/master/mindspore/python/mindspore/ops/_grad_experimental">MindSpore gradient section source code</a> to see if its first-order Grad has a corresponding of the bprop function definition.</p>
<p>For example, if the network second-order derivative links contain StridedSlice slicing operation, you can look up <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/ops/_grad_experimental/grad_array_ops.py">array_ops gradient definition file</a> in the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/python/mindspore/ops/_grad_experimental/grad_array_ops.py#L867">reverse registration code of StridedSliceGrad</a>. If it exists, the current version of MindSpore StridedSlice slicing operation supports second-order gradient calculation.</p>
<p>For details about the mapping of other framework APIs, see the <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/mindspore.html">API naming and function description</a>. For APIs with the same function, the names of MindSpore may be different from those of other frameworks. The parameters and functions of APIs with the same name may also be different from those of other frameworks. For details, see the official description.</p>
<p>If the corresponding API is not found, see specific missing API processing policy.</p>
</section>
<section id="missing-api-processing-policy">
<h3>Missing API Processing Policy<a class="headerlink" href="#missing-api-processing-policy" title="Permalink to this headline"></a></h3>
<p>You can use the following methods to process the missing API:</p>
<section id="1-use-equivalent-replacement">
<h4>1. Use equivalent replacement<a class="headerlink" href="#1-use-equivalent-replacement" title="Permalink to this headline"></a></h4>
<p>In some scenarios, API functions can be equivalently replaced. For example:</p>
<ul class="simple">
<li><p>As Squeeze, Flatten, and ExpandDims do not perform actual calculation, APIs with only Tensor shape changed can be replaced by Reshape.</p></li>
<li><p>When the output shape of AdaptiveAvgPool and AdaptiveMaxPool is 1, AdaptiveAvgPool and AdaptiveMaxPool are equivalent to ReduceMean and ReduceMax when <code class="docutils literal notranslate"><span class="pre">keep_dims</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p>MaxPool and MaxPoolWithArgmax are equivalent when indices are not used.</p></li>
<li><p>Sort is equivalent to TopK in the full sorting scenario.</p></li>
</ul>
</section>
<section id="2-use-existing-apis-to-package-equivalent-function-logic">
<h4>2. Use existing APIs to package equivalent function logic<a class="headerlink" href="#2-use-existing-apis-to-package-equivalent-function-logic" title="Permalink to this headline"></a></h4>
<p>For some missing APIs, equivalent functions can be implemented based on existing MindSpore APIs. The following is an example of <code class="docutils literal notranslate"><span class="pre">sigmoid</span> <span class="pre">focal</span> <span class="pre">loss</span></code>:</p>
<p>First, let’s analyze the algorithm basis of the API.</p>
<p>Focal Loss[1] is a method used to deal with the imbalance of positive and negative references and difficult references during the training of a single-phase target detector.</p>
<p>Generally, the sigmoid focal loss API is implemented by MMDetection. The following shows how PyTorch implements this API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">def</span> <span class="nf">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce loss as specified.</span>
<span class="sd">    Args:</span>
<span class="sd">        loss (Tensor): Elementwise loss tensor.</span>
<span class="sd">        reduction (str): Options are &quot;none&quot;, &quot;mean&quot; and &quot;sum&quot;.</span>
<span class="sd">    Return:</span>
<span class="sd">        Tensor: Reduced loss tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="c1"># none: 0, elementwise_mean:1, sum: 2</span>
    <span class="k">if</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="k">elif</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">weight_reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>

    <span class="c1"># if avg_factor is not specified, just reduce the loss</span>
    <span class="k">if</span> <span class="n">avg_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># if reduction is mean, then average the loss by avg_factor</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">avg_factor</span>
        <span class="c1"># if reduction is &#39;none&#39;, then do nothing, otherwise raise an error</span>
        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;avg_factor can not be used with reduction=&quot;sum&quot;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="k">def</span> <span class="nf">py_sigmoid_focal_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span>
                          <span class="n">target</span><span class="p">,</span>
                          <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                          <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                          <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                          <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PyTorch version of `Focal Loss &lt;https://arxiv.org/abs/1708.02002&gt;`_.</span>
<span class="sd">    Args:</span>
<span class="sd">        pred (torch.Tensor): The prediction with shape (N, C), C is the</span>
<span class="sd">            number of classes</span>
<span class="sd">        target (torch.Tensor): The learning label of the prediction.</span>
<span class="sd">        weight (torch.Tensor, optional): Sample-wise loss weight.</span>
<span class="sd">        gamma (float, optional): The gamma for calculating the modulating</span>
<span class="sd">            factor. Defaults to 2.0.</span>
<span class="sd">        alpha (float, optional): A balanced form for Focal Loss.</span>
<span class="sd">            Defaults to 0.25.</span>
<span class="sd">        reduction (str, optional): The method used to reduce the loss into</span>
<span class="sd">            a scalar. Defaults to &#39;mean&#39;.</span>
<span class="sd">        avg_factor (int, optional): Average factor that is used to average</span>
<span class="sd">            the loss. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pred_sigmoid</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_sigmoid</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">pred_sigmoid</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">focal_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span>
                    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">))</span> <span class="o">*</span> <span class="n">pt</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span> <span class="o">*</span> <span class="n">focal_weight</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
                <span class="c1"># For most cases, weight is of shape (num_priors, ),</span>
                <span class="c1">#  which means it does not have the second axis num_class</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Sometimes, weight per anchor per class is also needed. e.g.</span>
                <span class="c1">#  in FSAF. But it may be flattened of shape</span>
                <span class="c1">#  (num_priors x num_class, ), while loss is still of shape</span>
                <span class="c1">#  (num_priors, num_class).</span>
                <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">weight_reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">avg_factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>According to the API mapping table, the APIs used in the code have corresponding implementations on MindSpore.</p>
<p>Implement the MindSpore version by referring to the preceding PyTorch code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="k">class</span> <span class="nc">SigmoidFoaclLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SigmoidFoaclLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span> <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_factor</span> <span class="o">=</span> <span class="n">avg_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reduce_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reduce loss as specified.</span>
<span class="sd">        Args:</span>
<span class="sd">            loss (Tensor): Elementwise loss tensor.</span>
<span class="sd">        Return:</span>
<span class="sd">            Tensor: Reduced loss tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">weight_reduce_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="c1"># if avg_factor is not specified, just reduce the loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if reduction is mean, then average the loss by avg_factor</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_factor</span>
            <span class="c1"># if reduction is &#39;none&#39;, then do nothing, otherwise raise an error</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;avg_factor can not be used with reduction=&quot;sum&quot;&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">pred_sigmoid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">pred</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_sigmoid</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">pred_sigmoid</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">focal_weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">))</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">focal_weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_weight</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="c1"># For most cases, weight is of shape (num_priors, ),</span>
                    <span class="c1">#  which means it does not have the second axis num_class</span>
                    <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
                    <span class="c1"># Sometimes, weight per anchor per class is also needed. e.g.</span>
                    <span class="c1">#  in FSAF. But it may be flattened of shape</span>
                    <span class="c1">#  (num_priors x num_class, ), while loss is still of shape</span>
                    <span class="c1">#  (num_priors, num_class).</span>
                    <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">loss</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;weight shape </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not match to loss shape </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>Then, perform a test.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">ms_s_focal_loss</span> <span class="o">=</span> <span class="n">SigmoidFoaclLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                                       <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="n">avg_factor</span><span class="p">)</span>
    <span class="n">loss_ms</span> <span class="o">=</span> <span class="n">ms_s_focal_loss</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="n">loss_pt</span> <span class="o">=</span> <span class="n">py_sigmoid_focal_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span>
                                    <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="n">avg_factor</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">loss_ms</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">-</span> <span class="n">loss_pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">())))</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>The final error is less than 1e-5, which is a reasonable accuracy error.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>6.891787e-08
1.4305115e-06
2.8014183e-06
3.799796e-07
</pre></div>
</div>
</section>
<section id="3-customize-operators">
<h4>3. Customize operators<a class="headerlink" href="#3-customize-operators" title="Permalink to this headline"></a></h4>
<p>When existing APIs cannot be used for packaging, or the performance of cell encapsulation is poor, you need to customize operators. For details, see <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/operation/op_custom.html">Custom Operators</a>.</p>
<p>In addition to migrating APIs, you can also use the <code class="docutils literal notranslate"><span class="pre">aot</span></code> development mode of the <code class="docutils literal notranslate"><span class="pre">Custom</span></code> operator to call the PyTorch Aten operator for quick verification. For details, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/migration_guide/use_third_party_op.html">Using Third-party Operator Libraries Based on Customized Interfaces</a>.</p>
<p><strong>Note that it is convenient to migrate operators implemented by PyTorch to the GPU and CPU. Most of the operators displayed here are GPU and CPU operators. Ascend operators need to use the TBE for operator development, which has high requirements. Therefore, you are advised to use officially implemented operators for packaging.</strong></p>
</section>
<section id="4-seek-help-from-the-community">
<h4>4. Seek help from the community<a class="headerlink" href="#4-seek-help-from-the-community" title="Permalink to this headline"></a></h4>
<p>Commit an issue on <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">MindSpore Gitee</a> to suggest developing missing APIs.</p>
</section>
</section>
</section>
<section id="analyzing-function-compliance">
<h2>Analyzing Function Compliance<a class="headerlink" href="#analyzing-function-compliance" title="Permalink to this headline"></a></h2>
<p>During continuous delivery of MindSpore, some functions are restricted. If restricted functions are involved during network migration, some measures can be taken to avoid the impact of function restrictions.</p>
<section id="dynamic-shape">
<h3>Dynamic shape<a class="headerlink" href="#dynamic-shape" title="Permalink to this headline"></a></h3>
<p>To know dynamic shape, you need to know what is a static shape.
Static shape indicates that the shape of a tensor does not change during network execution.
For example, on the ResNet50 network, if the input shape of an image is always <code class="docutils literal notranslate"><span class="pre">224*224</span></code>, the shapes of the output Tesnor of the four residual modules are <code class="docutils literal notranslate"><span class="pre">B*64*56*56</span></code>, <code class="docutils literal notranslate"><span class="pre">B*128*28*28</span></code>, <code class="docutils literal notranslate"><span class="pre">B*256*14*14</span></code>, and <code class="docutils literal notranslate"><span class="pre">B*512*7*7</span></code> respectively in the network training phase. <code class="docutils literal notranslate"><span class="pre">B</span></code> indicates <code class="docutils literal notranslate"><span class="pre">BatchSize</span></code>, which is also fixed during the training. In this case, all shapes on the network are static and no dynamic shape is available.
If the input shape may no+t be <code class="docutils literal notranslate"><span class="pre">224*224</span></code>, the shape of the output tensor of the four residual modules varies with the input shape. In this case, the shape is dynamic instead of static. Generally, dynamic shape is introduced due to the following reasons:</p>
<section id="input-shape-not-fixed">
<h4>Input shape not fixed<a class="headerlink" href="#input-shape-not-fixed" title="Permalink to this headline"></a></h4>
<p>For example, the input image has different shapes, and the audio label has different lengths. In this case, dynamic shapes are introduced.</p>
<p>In this scenario, you can read the code to check whether the output shape of data processing is fixed, or directly print the output shape of data processing for comparison.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="apis-that-cause-shape-changes-during-network-execution">
<h4>APIs that cause shape changes during network execution<a class="headerlink" href="#apis-that-cause-shape-changes-during-network-execution" title="Permalink to this headline"></a></h4>
<p>During network execution, some operations may cause tensor shape changes.</p>
<p>The common APIs that cause this scenario are as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Dynamic Shape Scenario</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>StridedSlice/Slice</p></td>
<td><p>Specifies a slice. You can also use [start_idx:end_idx] during programming.</p></td>
<td><p>The slice subscript is a variable.</p></td>
</tr>
<tr class="row-odd"><td><p>TopK</p></td>
<td><p>Obtains the first K data.</p></td>
<td><p>The value of K is not fixed.</p></td>
</tr>
<tr class="row-even"><td><p>Gather</p></td>
<td><p>Obtains the slice consisting of the elements corresponding to the tensor index on the specified axis.</p></td>
<td><p>The index length is not fixed.</p></td>
</tr>
<tr class="row-odd"><td><p>UnsortedSegmentX</p></td>
<td><p>Specifies computation of an input tensor, including UnsortedSegmentSum and UnsortedSegmentMax.</p></td>
<td><p>The segment is not fixed.</p></td>
</tr>
<tr class="row-even"><td><p>Sampler</p></td>
<td><p>Specifies sampler-related operations, such as where and random.choice.</p></td>
<td><p>The sample quantity is not fixed.</p></td>
</tr>
<tr class="row-odd"><td><p>ReduceX</p></td>
<td><p>Specifies a reduction operation, such as ReduceSum and ReduceMean.</p></td>
<td><p>The axis is not fixed.</p></td>
</tr>
<tr class="row-even"><td><p>Transpose</p></td>
<td><p>Performs transformation based on the axis.</p></td>
<td><p>The axis is not fixed.</p></td>
</tr>
<tr class="row-odd"><td><p>Unique</p></td>
<td><p>Deduplicates data.</p></td>
<td><p>Dynamic shape is introduced when this API is used.</p></td>
</tr>
<tr class="row-even"><td><p>MaskedSelect</p></td>
<td><p>Obtains the value of mask based on the Boolean type.</p></td>
<td><p>Dynamic shape is introduced when this API is used.</p></td>
</tr>
<tr class="row-odd"><td><p>NonZero</p></td>
<td><p>Obtains the positions of all non-zero values.</p></td>
<td><p>Dynamic shape is introduced when this API is used.</p></td>
</tr>
</tbody>
</table>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 6</span>
<span class="c1"># (6,)</span>
</pre></div>
</div>
<p>During network training, there is a slicing operation <code class="docutils literal notranslate"><span class="pre">x[:k]</span></code>. Here, k is not a constant. As a result, the shape of <code class="docutils literal notranslate"><span class="pre">x[:k]</span></code> changes with the value of k, and the shape of all subsequent operations related to <code class="docutils literal notranslate"><span class="pre">x[:k]</span></code> is uncertain.</p>
</section>
<section id="shape-changes-introduced-by-different-branches-of-control-flows">
<h4>Shape changes introduced by different branches of control flows<a class="headerlink" href="#shape-changes-introduced-by-different-branches-of-control-flows" title="Permalink to this headline"></a></h4>
<p>The output of some control flows on the network may be different. When the condition control items of the control flows are not fixed, dynamic shape may be triggered. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">cond</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>

<span class="k">if</span> <span class="n">cond</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># [4.17021990e-01 7.20324516e-01 1.14374816e-04 3.02332580e-01</span>
<span class="c1">#  1.46755889e-01 9.23385918e-02 1.86260208e-01 3.45560730e-01</span>
<span class="c1">#  3.96767467e-01 5.38816750e-01]</span>
<span class="c1"># True</span>
<span class="c1"># [0.7203245  0.53881675]</span>
</pre></div>
</div>
<p>In this process, there are two dynamic shapes. One is that the shape of the <code class="docutils literal notranslate"><span class="pre">masked_select</span></code> result is dynamic if <code class="docutils literal notranslate"><span class="pre">cond=True</span></code>. The other is the control flow. Because <code class="docutils literal notranslate"><span class="pre">cond</span></code> is uncertain, the shape output of the two branches of the control flow is different, which also causes the dynamic shape.</p>
<p>Generally, the dynamic shape can be analyzed at the algorithm and code layers, or the tensor related to the reference code can be directly printed for judgment. If dynamic shape exists, we will introduce the workaround in <a class="reference external" href="https://www.mindspore.cn/docs/en/master/migration_guide/model_development/model_and_cell.html">Network Body and Loss Setup</a>.</p>
</section>
<section id="sparsity">
<h4>Sparsity<a class="headerlink" href="#sparsity" title="Permalink to this headline"></a></h4>
<p>A <a class="reference external" href="https://matteding.github.io/2019/04/25/sparse-matrices/">sparse tensor</a> is a special tensor in which the value of the most significant element is zero.</p>
<p>In some scenarios (such as recommendation systems, molecular dynamics, graph neural networks), the data is sparse. If you use common dense tensors to represent the data, you may introduce many unnecessary calculations, storage, and communication costs. In this case, it is better to use sparse tensor to represent the data.</p>
<p>MindSpore now supports the most commonly used <a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/tensor.html#sparse-tensor">CSR and COO data formats</a>. Currently, only a limited number of sparse operators are supported, and most sparse features are restricted. In this case, you are advised to check whether the corresponding operator supports sparse computing. If the operator does not support sparse computing, convert it into a common operator.
After the operator is converted into a dense operator, the video memory used increases. Therefore, the batch size implemented by referring to may not be used for training. In this case, you can use <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/optimize/gradient_accumulation.html">Gradient Accumulation</a> to simulate large batch training.</p>
</section>
</section>
</section>
<section id="mindspore-function/feature-recommendation">
<h2>MindSpore Function/Feature Recommendation<a class="headerlink" href="#mindspore-function/feature-recommendation" title="Permalink to this headline"></a></h2>
<section id="dynamic-and-static-graphs">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/advanced/compute_graph.html">Dynamic and Static Graphs</a><a class="headerlink" href="#dynamic-and-static-graphs" title="Permalink to this headline"></a></h3>
<p>Currently, there are two execution modes of a mainstream deep learning framework: a static graph mode (Graph) and a dynamic graph mode (PyNative).</p>
<ul class="simple">
<li><p>In static graph mode, when the program is built and executed, the graph structure of the neural network is generated first, and then the computation operations involved in the graph are performed. Therefore, in static graph mode, the compiler can achieve better execution performance by using technologies such as graph optimization, which facilitates large-scale deployment and cross-platform running.</p></li>
<li><p>In dynamic graph mode, the program is executed line by line according to the code writing sequence. In the forward execution process, the backward execution graph is dynamically generated according to the backward propagation principle. In this mode, the compiler delivers the operators in the neural network to the device one by one for computing, facilitating users to build and debug the neural network model.</p></li>
</ul>
</section>
<section id="calling-the-custom-class">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/network/jit_class.html">Calling the Custom Class</a><a class="headerlink" href="#calling-the-custom-class" title="Permalink to this headline"></a></h3>
<p>In static graph mode, you can use <code class="docutils literal notranslate"><span class="pre">ms_class</span></code> to modify a custom class. You can create and call an instance of the custom class, and obtain its attributes and methods.</p>
<p><code class="docutils literal notranslate"><span class="pre">ms_class</span></code> is applied to the static graph mode to expand the support scope of static graph compilation syntax. In dynamic graph mode, that is, PyNative mode, the use of <code class="docutils literal notranslate"><span class="pre">ms_class</span></code> does not affect the execution logic of PyNative mode.</p>
</section>
<section id="automatic-differential">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/en/master/beginner/autograd.html">Automatic Differential</a><a class="headerlink" href="#automatic-differential" title="Permalink to this headline"></a></h3>
<p>Automatic differentiation can calculate a derivative value of a derivative function at a certain point, which is a generalization of backward propagation algorithms. The main problem solved by automatic differential is to decompose a complex mathematical operation into a series of simple basic operations. This function shields a large number of derivative details and processes from users, greatly reducing the threshold for using the framework.</p>
</section>
<section id="mixed-precision">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/master/advanced/mixed_precision.html">Mixed Precision</a><a class="headerlink" href="#mixed-precision" title="Permalink to this headline"></a></h3>
<p>Generally, when a neural network model is trained, the default data type is FP32. In recent years, to accelerate training time, reduce memory occupied during network training, and store a trained model with same precision, more and more mixed-precision training methods are proposed in the industry. The mixed-precision training herein means that both single precision (FP32) and half precision (FP16) are used in a training process.</p>
</section>
<section id="auto-augmentation">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/dataset/augment.html">Auto Augmentation</a><a class="headerlink" href="#auto-augmentation" title="Permalink to this headline"></a></h3>
<p>MindSpore not only allows you to customize data augmentation, but also provides an automatic data augmentation mode to automatically perform data augmentation on images based on specific policies.</p>
</section>
<section id="gradient-accumulation-algorithm">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/master/optimize/gradient_accumulation.html">Gradient Accumulation Algorithm</a><a class="headerlink" href="#gradient-accumulation-algorithm" title="Permalink to this headline"></a></h3>
<p>Gradient accumulation is a method of splitting data samples for training neural networks into several small batches by batch and then calculating the batches in sequence. The purpose is to solve the out of memory (OOM) problem that the neural network cannot be trained or the network model cannot be loaded due to insufficient memory.</p>
</section>
<section id="summary">
<h3><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/summary_record.html">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline"></a></h3>
<p>Scalars, images, computational graphs, training optimization processes, and model hyperparameters during training are recorded in files and can be viewed on the web page.</p>
</section>
<section id="debugger">
<h3><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/en/master/debugger.html">Debugger</a><a class="headerlink" href="#debugger" title="Permalink to this headline"></a></h3>
<p>The MindSpore debugger is a debugging tool provided for graph mode training. It can be used to view and analyze the intermediate results of graph nodes.</p>
</section>
<section id="golden-stick">
<h3><a class="reference external" href="https://www.mindspore.cn/golden_stick/docs/en/master/index.html">Golden Stick</a><a class="headerlink" href="#golden-stick" title="Permalink to this headline"></a></h3>
<p>MindSpore Golden Stick is a model compression algorithm set jointly designed and developed by Huawei Noah’s team and Huawei MindSpore team. It contains basic quantization and pruning methods.</p>
</section>
</section>
<section id="differences-between-mindspore-and-pytorch-apis">
<h2>Differences Between MindSpore and PyTorch APIs<a class="headerlink" href="#differences-between-mindspore-and-pytorch-apis" title="Permalink to this headline"></a></h2>
<p>When migrating the network from PyTorch to MindSpore, pay attention to the differences between MindSpore and <a class="reference external" href="https://www.mindspore.cn/docs/en/master/migration_guide/typical_api_comparision.html">typical PyTorch APIs</a>.</p>
<p>[1] Lin, T. Y. , et al. “Focal Loss for Dense Object Detection.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence PP.99(2017):2999-3007.</p>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="enveriment_preparation.html" class="btn btn-neutral float-left" title="Environment Preparation and Information Acquisition" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="model_development/model_development.html" class="btn btn-neutral float-right" title="Constructing MindSpore Network" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>