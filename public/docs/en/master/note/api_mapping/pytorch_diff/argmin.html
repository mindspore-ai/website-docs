<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Differences with torch.argmin &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script><script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/js/theme.js"></script><script src="../../../_static/underscore.js"></script><script src="../../../_static/doctools.js"></script><script src="../../../_static/js/mermaid-9.3.0.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Design</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../design/overview.html">MindSpore Design Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/programming_paradigm.html">Functional and Object-Oriented Fusion Programming Paradigm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/dynamic_graph_and_static_graph.html">Combination of Dynamic and Static Graphs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/distributed_training_design.html">Distributed Parallel Native</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/data_engine.html">High Performance Data Processing Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/all_scenarios.html">Full-scenarios Unified Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/graph_fusion_engine.html">Graph-Kernel Fusion Acceleration Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/pluggable_device.html">Third-Party Hardware Interconnection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/glossary.html">Glossary</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../official_models.html">Official Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Mapping</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_api_mapping.html">PyTorch and MindSpore API Mapping Table</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Migration Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/overview.html">Overview of Migration Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/enveriment_preparation.html">Environment Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/analysis_and_preparation.html">Model Analysis and Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/model_development/model_development.html">Network Constructing Comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/debug_and_tune.html">Debugging and Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/sample_code.html">Network Migration Debugging Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/faq.html">FAQs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Syntax Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../static_graph_syntax_support.html">Static Graph Syntax Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../static_graph_syntax/operators.html">Static Graph Syntax - Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../static_graph_syntax/statements.html">Static Graph Syntax - Python Statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../static_graph_syntax/python_builtin_functions.html">Static Graph Syntax - Python Built-in Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../index_support.html">Tensor Index Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Environment Variables</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../env_var_list.html">Environment Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/data_processing.html">Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/implement_problem.html">Implement Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/network_compilation.html">Network Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/operators_compile.html">Operators Compile</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/precision_tuning.html">Precision Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/distributed_parallel.html">Distributed Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/inference.html">Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/feature_advice.html">Feature Advice</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Differences with torch.argmin</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/note/api_mapping/pytorch_diff/argmin.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="differences-with-torch-argmin">
<h1>Differences with torch.argmin<a class="headerlink" href="#differences-with-torch-argmin" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/mindspore/source_en/note/api_mapping/pytorch_diff/argmin.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<p>The following mapping relationships can be found in this file.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p>PyTorch APIs</p></th>
<th class="text-center head"><p>MindSpore APIs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>torch.argmin</p></td>
<td class="text-center"><p>mindspore.ops.argmin</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>torch.Tensor.argmin</p></td>
<td class="text-center"><p>mindspore.Tensor.argmin</p></td>
</tr>
</tbody>
</table>
<section id="torch-argmin">
<h2>torch.argmin<a class="headerlink" href="#torch-argmin" title="Permalink to this headline"></a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>torch.argmin(input, dim=None, keepdim=False) -&gt; Tensor
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="https://pytorch.org/docs/1.8.1/generated/torch.argmin.html">torch.argmin</a>.</p>
</section>
<section id="mindspore-ops-argmin">
<h2>mindspore.ops.argmin<a class="headerlink" href="#mindspore-ops-argmin" title="Permalink to this headline"></a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore.ops.argmin(input, axis=None, keepdims=False) -&gt; Tensor
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="https://www.mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.argmin.html">mindspore.ops.argmin</a>.</p>
</section>
<section id="differences">
<h2>Differences<a class="headerlink" href="#differences" title="Permalink to this headline"></a></h2>
<p>PyTorch: Return the index of the minimum value of the Tensor flattened or along the given dimension, and the return value is of type torch.int64. If there is more than one minimum value, the index of the first minimum value is returned.</p>
<p>MindSpore: The implementation function of API in MindSpore is basically the same as that of PyTorch, and the return value type is int32.</p>
<p>To ensure that the two output types are identical, use the <a class="reference external" href="https://mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.Cast.html">mindspore.ops.Cast</a> operator to convert the result of MindSpore to mindspore.int64, which is done in each of the following examples.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Categories</p></th>
<th class="head"><p>Subcategories</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MindSpore</p></th>
<th class="head"><p>Differences</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Input</p></td>
<td><p>Single input</p></td>
<td><p>input</p></td>
<td><p>input</p></td>
<td><p>Input Tensor</p></td>
</tr>
<tr class="row-odd"><td><p>Parameters</p></td>
<td><p>Parameter 1</p></td>
<td><p>dim</p></td>
<td><p>axis</p></td>
<td><p>Same function, different parameter names</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>Parameter 2</p></td>
<td><p>keepdim</p></td>
<td><p>keepdims</p></td>
<td><p>Same function, different parameter names</p></td>
</tr>
</tbody>
</table>
<section id="code-example-1">
<h3>Code Example 1<a class="headerlink" href="#code-example-1" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>For a zero-dimensional Tensor, PyTorch supports any combination of None/-1/0 for the dim parameter and True/False for the keepdim parameter, and the computation results are all consistent, all being a zero-dimensional Tensor. MindSpore version 1.8.1 does not support handling zero-dimensional Tensor at the moment, and you need to use <a class="reference external" href="https://mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ExpandDims.html">mindspore.ops.ExpandDims</a> to expand the Tensor to one dimension, and then follow the default parameter computation of the mindspore.ops.argmin operator.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">torch_argmin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmin</span>
<span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_argmin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">torch_out_np</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_out_np</span><span class="p">)</span>
<span class="c1"># 0</span>

<span class="c1"># MindSpore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(())</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ms_argmin</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">argmin</span>
<span class="n">ms_expanddims</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">ms_cast</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">ms_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">ms_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
    <span class="n">ms_tensor_tmp</span> <span class="o">=</span> <span class="n">ms_expanddims</span><span class="p">(</span><span class="n">ms_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ms_output</span> <span class="o">=</span> <span class="n">ms_argmin</span><span class="p">(</span><span class="n">ms_tensor_tmp</span><span class="p">)</span>

<span class="n">ms_output</span> <span class="o">=</span> <span class="n">ms_cast</span><span class="p">(</span><span class="n">ms_output</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">ms_out_np</span> <span class="o">=</span> <span class="n">ms_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ms_out_np</span><span class="p">)</span>
<span class="c1"># 0</span>
</pre></div>
</div>
</section>
<section id="code-example-2">
<h3>Code Example 2<a class="headerlink" href="#code-example-2" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>When the dim parameter is not explicitly given, PyTorch argmin operator computes the result of doing an argmin operation on the original array flattened as a one-dimensional tensor, while MindSpore only supports computation on a single dimension. Therefore, to get the same result, pass the mindspore.ops.argmin operator into flatten Tensor before the calculation.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">torch_argmin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmin</span>
<span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_argmin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">torch_out_np</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_out_np</span><span class="p">)</span>
<span class="c1"># 0</span>

<span class="c1"># MindSpore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">dim</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">3</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ms_argmin</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">argmin</span>
<span class="n">ms_expanddims</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">ms_cast</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">ms_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">ms_output</span> <span class="o">=</span> <span class="n">ms_argmin</span><span class="p">(</span><span class="n">ms_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">ms_argmin</span><span class="p">(</span>
    <span class="n">ms_tensor</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

<span class="n">ms_output</span> <span class="o">=</span> <span class="n">ms_cast</span><span class="p">(</span><span class="n">ms_output</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">ms_out_np</span> <span class="o">=</span> <span class="n">ms_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ms_out_np</span><span class="p">)</span>
<span class="c1"># 0</span>
</pre></div>
</div>
</section>
<section id="code-example-3">
<h3>Code Example 3<a class="headerlink" href="#code-example-3" title="Permalink to this headline"></a></h3>
<blockquote>
<div><p>The PyTorch operator has a keepdim parameter. When set to True, it serves to keep the dimension for which aggregation is performed and is set to 1. MindSpore keepdims parameter is consistent with its function. To achieve the same result, after the calculation is done, use the <a class="reference external" href="https://mindspore.cn/docs/en/master/api_python/ops/mindspore.ops.ExpandDims.html">mindspore.ops.ExpandDims</a> operator to expand the dimensionality.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># PyTorch</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">torch_argmin</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmin</span>
<span class="n">torch_output</span> <span class="o">=</span> <span class="n">torch_argmin</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
<span class="n">torch_out_np</span> <span class="o">=</span> <span class="n">torch_output</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch_out_np</span><span class="p">)</span>
<span class="c1"># [[0]</span>
<span class="c1">#  [0]]</span>

<span class="c1"># MindSpore</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">ms_argmin</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">argmin</span>
<span class="n">ms_expanddims</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">ms_cast</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">ms_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">ms_output</span> <span class="o">=</span> <span class="n">ms_argmin</span><span class="p">(</span><span class="n">ms_tensor</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
<span class="n">ms_output</span> <span class="o">=</span> <span class="n">ms_cast</span><span class="p">(</span><span class="n">ms_output</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">ms_out_np</span> <span class="o">=</span> <span class="n">ms_output</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ms_out_np</span><span class="p">)</span>
<span class="c1"># [[0]</span>
<span class="c1">#  [0]]</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>